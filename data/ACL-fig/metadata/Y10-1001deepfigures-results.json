{"raw_detected_boxes": [[], [], [{"x2": 553.0, "y1": 117.0, "x1": 275.0, "y2": 256.0}, {"x2": 618.0, "y1": 371.0, "x1": 215.0, "y2": 635.0}], [], [], [{"x2": 569.0, "y1": 120.0, "x1": 258.0, "y2": 293.0}, {"x2": 570.0, "y1": 393.0, "x1": 256.0, "y2": 472.0}], [], [], []], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Table", "name": "1", "captionBoundary": {"x2": 507.9469909667969, "y1": 239.03701782226562, "x1": 84.9749984741211, "y2": 256.77545166015625}, "imageText": ["\u2022", "With", "regard", "to", "(iii)", "a", "better", "user", "experience,", "users", "have", "had", "little", "opportunity", "to", "experience", "the", "better", "performance", "that", "results", "from", "ongoing", "improvements", "made", "by", "researchers.", "For", "example,", "even", "if", "speech", "recognizers", "are", "made", "available", "as", "open", "source", "software", "(e.g.,", "(Lee", "et", "al.,", "2001)),", "these", "are", "mainly", "aimed", "at", "developers;", "end", "users", "have", "little", "opportunity", "to", "use", "them", "directly.", "Also,", "users", "of", "most", "speech", "recognition", "products", "have", "only", "experienced", "performance", "improvements", "through", "infrequent", "software", "updates.", "a", "whole.", "Consequently,", "it", "has", "been", "dif\ufb01cult", "to", "motivate", "a", "number", "of", "users", "to", "contribute", "to", "collaborative", "efforts", "that", "will", "improve", "performance.", "The", "above", "table", "is", "in\ufb02uenced", "by", "the", "comparison", "of", "Web", "1.0", "and", "Web", "2.0", "by", "O\u2019Reilly", "(O\u2019Reilly).", "Research", "projects", "that", "feature", "more", "points", "on", "the", "right", "side", "of", "the", "table", "are", "more", "worthy", "of", "the", "name", "Speech", "Recognition", "Research", "2.0.", "However,", "as", "with", "Web", "2.0,", "this", "does", "not", "mean", "that", "all", "these", "points", "have", "to", "be", "featured", "in", "any", "one", "project.", "Personal", "wisdom", "Wisdom", "of", "crowds", "Completed", "version", "Perpetual", "beta", "Out-of-vocabulary", "words", "Not-yet-annotated", "words", "Specialist", "participation", "User", "participation", "Individual", "correction", "Social", "correction", "Limited", "topics", "Unlimited", "topics", "Transcription", "Annotation", "Stand-alone", "application", "Web", "service", "Dictation", "Searching/browsing", "Corpus", "Web-based", "data", "Speech", "Recognition", "Research", "1.0", "Speech", "Recognition", "Research", "2.0"], "regionBoundary": {"x2": 513.091552734375, "y1": 265.0, "x1": 98.05499267578125, "y2": 594.8300170898438}, "caption": "Table 1: A comparison of the conventional approach to speech recognition research (Speech Recognition Research 1.0) and the proposed approach (Speech Recognition Research 2.0).", "page": 2}, {"figType": "Figure", "name": "1", "captionBoundary": {"x2": 507.9269714355469, "y1": 196.56643676757812, "x1": 84.9749984741211, "y2": 214.30487060546875}, "imageText": ["(iii)", "Improved", "performance", "leads", "to", "a", "better", "user", "experience.", "(ii)", "Users", "contribute", "to", "improved", "speech", "recognition", "performance.", "(i)", "Allowing", "users", "to", "experience", "speech", "recognition", "lets", "them", "better", "understand", "its", "performance."], "regionBoundary": {"x2": 398.0, "y1": 84.0, "x1": 197.0, "y2": 185.0}, "caption": "Figure 1: A positive spiral leading towards greater use of speech recognition (through repetition of steps (i), (ii), and (iii)).", "page": 2}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 396.3499755859375, "y1": 356.6465759277344, "x1": 197.89500427246094, "y2": 362.3900146484375}, "imageText": ["User", "interface", "User", "interface", "User", "interface", "Speech", "recognition", "manager", "r", "w", "le", "c", "ra", "W", "eb", "S", ")", "R", "S", "3", "+", "(M", "P", "ca", "st", "P", "od", "Speech", "recognizerSpeech", "recognizerSpeech", "recognizer", "ne", "en", "gi", "rc", "h", "S", "ea", "Database", "manager", "Speech", "recognizerSpeech", "recognizer", "S", ")", "R", "S", "3", "+", "(M", "P", "ca", "st", "P", "od", "S", ")", "R", "S", "3", "+", "(M", "P", "ca", "st", "P", "od", "S", ")", "R", "S", "3", "+", "(M", "P", "ca", "st", "P", "od"], "regionBoundary": {"x2": 411.0, "y1": 283.0, "x1": 184.0, "y2": 340.0}, "caption": "Figure 3: Implementation overview of PodCastle.", "page": 5}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 508.32568359375, "y1": 227.52658081054688, "x1": 84.9749984741211, "y2": 269.015869140625}, "imageText": [], "regionBoundary": {"x2": 410.0, "y1": 85.0, "x1": 185.0, "y2": 212.0}, "caption": "Figure 2: PodCastle screen snapshot of an interface for correcting speech recognition errors (competitive candidates are presented underneath the normal recognition results). Five errors in this excerpt were corrected by selecting from the candidates. The corrected Japanese sentence means \u201c. . . well, actually the ratio of this price range and . . . \u201d.", "page": 5}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 705.4541269938151, "y1": 273.0089399549696, "x1": 118.02083121405707, "y2": 297.6456536187066}, "name": "1", "caption_text": "Figure 1: A positive spiral leading towards greater use of speech recognition (through repetition of steps (i), (ii), and (iii)).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 565.0, "y1": 117.0, "x1": 265.0, "y2": 273.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 705.481931898329, "y1": 331.99585808648004, "x1": 118.02083121405707, "y2": 356.632571750217}, "name": "1", "caption_text": "Table 1: A comparison of the conventional approach to speech recognition research (Speech Recognition Research 1.0) and the proposed approach (Speech Recognition Research 2.0).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 619.0, "y1": 354.0, "x1": 198.0, "y2": 646.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 706.0078938802083, "y1": 316.00914001464844, "x1": 118.02083121405707, "y2": 373.63315158420136}, "name": "2", "caption_text": "Figure 2: PodCastle screen snapshot of an interface for correcting speech recognition errors (competitive candidates are presented underneath the normal recognition results). Five errors in this excerpt were corrected by selecting from the candidates. The corrected Japanese sentence means \u201c. . . well, actually the ratio of this price range and . . . \u201d.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 569.0, "y1": 118.0, "x1": 258.0, "y2": 293.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 550.4860772026909, "y1": 495.34246656629773, "x1": 274.8541726006402, "y2": 503.3194647894965}, "name": "3", "caption_text": "Figure 3: Implementation overview of PodCastle.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 571.0, "y1": 393.0, "x1": 256.0, "y2": 472.0}, "page": 5, "dpi": 0}], "error": null, "pdf": "/work/host-output/0b0f544c6a93a451cfde5791fdfd24f7dc2050b7/Y10-1001.pdf", "dpi": 100}