{"raw_detected_boxes": [[], [{"x2": 724.0, "y1": 90.0, "x1": 103.0, "y2": 439.0}], [], [{"x2": 719.0, "y1": 91.0, "x1": 105.0, "y2": 270.0}, {"x2": 721.0, "y1": 359.0, "x1": 439.0, "y2": 670.0}], [], [{"x2": 724.0, "y1": 90.0, "x1": 107.0, "y2": 322.0}, {"x2": 716.0, "y1": 902.0, "x1": 441.0, "y2": 979.0}], [{"x2": 665.0, "y1": 90.0, "x1": 169.0, "y2": 316.0}, {"x2": 624.0, "y1": 399.0, "x1": 207.0, "y2": 511.0}], [{"x2": 636.0, "y1": 90.0, "x1": 194.0, "y2": 199.0}, {"x2": 640.0, "y1": 254.0, "x1": 190.0, "y2": 328.0}], [{"x2": 657.0, "y1": 89.0, "x1": 173.0, "y2": 164.0}, {"x2": 382.0, "y1": 240.0, "x1": 117.0, "y2": 315.0}, {"x2": 381.0, "y1": 397.0, "x1": 120.0, "y2": 493.0}, {"x2": 708.0, "y1": 242.0, "x1": 442.0, "y2": 278.0}], [], []], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Table", "name": "1", "captionBoundary": {"x2": 527.2003173828125, "y1": 717.5115966796875, "x1": 306.947021484375, "y2": 759.3790283203125}, "imageText": ["Task", "Baseline", "non-PLuGS", "PLuGS", "MT", "70.6", "66.6", "67.7", "MMT", "70.9", "64.7", "65.6", "IC-D4", "32.3", "30.6", "32.8"], "regionBoundary": {"x2": 515.0, "y1": 649.8900146484375, "x1": 318.0, "y2": 705.8900146484375}, "caption": "Table 1: Multi30K test set METEOR scores for Translation (MT), Multi Modal Translation (MMT), and Image Captioning (IC-D4). The baseline is from task 1 of (Caglayan et al., 2019).", "page": 5}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 525.5473022460938, "y1": 245.78256225585938, "x1": 72.0, "y2": 263.74005126953125}, "imageText": ["Now", "select", "individual", "ratings", "for", "each", "caption:", "Please", "compare", "Caption", "A", "to", "Caption", "B:", "Slightly", "Better", "Better", "Much", "Better", "About", "the", "same", "Slightly", "Better", "Much", "Better", "Better", "Excellent", "Good", "Acceptable", "Bad", "Not", "enough", "information", "How", "well", "does", "Caption", "B", "above", "describe", "the", "image?", "Excellent", "Good", "Acceptable", "Bad", "Not", "enough", "information", "How", "well", "does", "Caption", "A", "above", "describe", "the", "image?", "Caption", "A:", "tractor", "seed", "in", "the", "morning", "followed", "by", "seagulls", "Caption", "B:", "tractor", "plowing", "the", "field"], "regionBoundary": {"x2": 526.0, "y1": 61.8900146484375, "x1": 72.0, "y2": 233.8900146484375}, "caption": "Figure 4: Side-by-side human evaluation of two image captions. The same template is used for evaluating English as well as the 5 languages targeted.", "page": 5}, {"figType": "Figure", "name": "1", "captionBoundary": {"x2": 525.7164306640625, "y1": 330.2575378417969, "x1": 71.6409912109375, "y2": 384.0809631347656}, "imageText": ["the", "entrance", "to", "the", "gardens", "<de>", "der", "Eingang", "zu", "den", "G\u00e4rten", "Bronzestatue", "im", "Garten", "(bronze", "statue", "in", "the", "garden)", "eine", "Stadt", "im", "Garten", "(a", "city", "in", "the", "garden)", "Autoverkehr", "an", "einem", "regnerischen", "Tag", "(car", "traffic", "on", "a", "rainy", "day)", "Polizeiauto", "auf", "der", "Stra\u00dfe", "(police", "car", "on", "the", "street)", "a", "car", "in", "the", "city", "<de>", "ein", "auto", "in", "der", "stadt", "Das", "Logo", "ist", "auf", "dem", "Computer", "zu", "sehen.", "(the", "logo", "can", "be", "seen", "on", "the", "computer.)", "Bild", "mit", "dem", "Titel", "Live", "mit", "einem", "Schritt", "(Image", "titled", "Live", "with", "a", "step)", "the", "iphone", "is", "seen", "in", "this", "undated", "image", ".", "<de>", "Das", "iPhone", "ist", "in", "diesem", "undatierten", "Bild", "zu", "sehen", ".", "Stabilization", "PLuGS", "Pivot", "Language", "Generation", "TTG", "Translate", "Train", "Generate", "Image", "TGT", "Train", "Generate", "Translate"], "regionBoundary": {"x2": 525.0, "y1": 64.8900146484375, "x1": 73.0, "y2": 317.8900146484375}, "caption": "Figure 1: Examples of captions produced in German by Train-Generate-Translate (TGT), Translate-Train-Generate (TTG), and Pivot Language Generation Stabilization (PLuGS) approaches. Captions are shown in bold font. For TGT and TTG outputs, we show the English translation in parenthesis beside the caption. For the PLuGS outputs we mark the Stabilizer in the output using a light gray background. We do not explicitly show a translation for PLuGS outputs since the Stabilizer is already a translation.", "page": 1}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 526.79248046875, "y1": 239.71951293945312, "x1": 71.69100189208984, "y2": 269.63299560546875}, "imageText": ["Lang", "Wins", "Losses", "Gainsxs", "PLuGSAccept", "TGTAccept", "GainAccept", "Fr", "22.8", "19.4", "3.4", "68.7", "66.5", "2.2", "It", "22.5", "18.3", "4.2", "52.1", "49.9", "2.2", "De", "22.6", "19.1", "3.5", "69.2", "67.7", "1.5", "Es", "27.0", "22.1", "4.9", "58.8", "56.9", "1.9", "Hi", "26.8", "23.8", "3.0", "78.6", "75.9", "2.7", "Wins", "Losses", "Gainsxs", "PLuGSAccept", "TTGAccept", "GainAccept", "Fr", "18.2", "17.3", "0.9", "66.2", "64.2", "2.0", "It", "23.7", "20.8", "2.9", "55.1", "52.2", "2.9", "De", "21.9", "19.6", "2.3", "64.3", "63.0", "1.3", "Es", "24.9", "23.8", "1.1", "57.7", "56.8", "0.9", "Hi", "27.4", "25.5", "1.9", "71.3", "69.6", "1.7"], "regionBoundary": {"x2": 479.0, "y1": 62.8900146484375, "x1": 118.0, "y2": 227.8900146484375}, "caption": "Table 2: SxS performance of PLuGS vs. TGT models (upper half) and PLuGS vs. TTG models (lower half), across five target languages on OID1k. The PLuGS models perform better on both GainSxS and GainAccept metrics, for all five languages.", "page": 6}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 503.2794189453125, "y1": 379.9935302734375, "x1": 93.95600128173828, "y2": 385.9960021972656}, "imageText": ["Lang", "TGT", "TTG", "PLuGS", "PLuGS-TGT", "PLuGS-TTG", "Fr", "0.7890", "0.7932", "0.7820", "-0.0070", "-0.0112", "It", "0.7729", "0.7760", "0.7813", "0.0084", "0.0053", "De", "0.6220", "0.6079", "0.6170", "0.0050", "0.0091", "Es", "0.8042", "0.7907", "0.7854", "-0.0188", "-0.0053", "Hi", "0.7026", "0.7149", "0.7155", "0.0129", "0.0006"], "regionBoundary": {"x2": 450.0, "y1": 284.8900146484375, "x1": 147.0, "y2": 367.8900146484375}, "caption": "Table 3: CIDEr scores on CC-1.1 validation set for PLuGS, TGT, and TTG models for five languages.", "page": 6}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 471.21002197265625, "y1": 159.60653686523438, "x1": 126.0260009765625, "y2": 165.6090087890625}, "imageText": ["Lang", "TTG", "PLuGS-2L", "TTG-5L", "TTGlarge-5L", "PLuGS-5L", "Fr", "0.7932", "0.7820", "0.6834", "0.7064", "0.7264", "It", "0.7760", "0.7813", "0.6538", "0.6885", "0.6978", "De", "0.6079", "0.6170", "0.4992", "0.5367", "0.5503", "Es", "0.7907", "0.7854", "0.7093", "0.7203", "0.7284", "Hi", "0.7149", "0.7155", "0.5891", "0.6201", "0.6641"], "regionBoundary": {"x2": 458.0, "y1": 62.8900146484375, "x1": 139.0, "y2": 147.8900146484375}, "caption": "Table 4: CIDEr scores on CC-1.1 validation set for bilingual and multilingual models.", "page": 7}, {"figType": "Table", "name": "5", "captionBoundary": {"x2": 467.0503845214844, "y1": 248.47354125976562, "x1": 130.18499755859375, "y2": 254.47601318359375}, "imageText": ["Lang", "Wins", "Losses", "Gainsxs", "BAccept", "AAccept", "GainAccept", "Fr", "21.3", "18.3", "3.0", "69.8", "68.7", "1.1", "It", "22.2", "18.2", "4.0", "56.4", "55.5", "0.9", "Hi", "26.8", "27.0", "-0.2", "75.6", "79.5", "-3.9"], "regionBoundary": {"x2": 461.0, "y1": 183.53961181640625, "x1": 137.0, "y2": 236.8900146484375}, "caption": "Table 5: SxS performance of PLuGS-5L vs. PLuGS-2L models for three languages.", "page": 7}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 527.2898559570312, "y1": 504.6245422363281, "x1": 307.2760009765625, "y2": 594.31396484375}, "imageText": ["...", "parked", "in", "the", "city", "<", "de", ">", "Auto", "Decoder", "Layer", "k", "...", "Add", "&", "Normalize", "Add", "&", "Normalize", "FF", "FF", "FF", "FF", "FF", "FF", "Emb", "Voc", "Emb", "Voc", "Emb", "Voc", "Emb", "Voc", "Emb", "Voc", "Emb", "Voc", "Add", "&", "Normalize", "Previous", "tokens", "Fixed", "Trainable", "Masked", "Self-Attention", "Encoder-Decoder", "Attention", "Decoder", "Layer", "1", "Encoder", "Outputs", "...", "car", "parked", "in", "the", "city", "<", "de", ">"], "regionBoundary": {"x2": 526.0, "y1": 253.8900146484375, "x1": 307.0, "y2": 492.8900146484375}, "caption": "Figure 3: Caption\u2019s dependence on the Stabilizer. The target-language caption is conditioned on the Stabilizer through the Masked Self-Attention in the decoder, and on the input image through the Encoder-Decoder attention that attends to the outputs of the last encoder layer. Note that in this figure, FF stands for the feed forward network, Voc stands for the (fixed) text vocab, and Emb stands for the (trainable) text embeddings.", "page": 3}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 527.2010498046875, "y1": 208.16256713867188, "x1": 72.0, "y2": 226.12005615234375}, "imageText": ["Decoder", "Outputs", "Beam", "Search", "SoftMax", "Probs", "Linear", "Encoder-decoder", "Attention", "Embedding", "Transformer", "Decoder", "Encoder", "Outputs", "Vocabtext", "Caption", "Stabilizer", "Splitter", "Decoder", "Outputs", "(Shifted)", "Embedding", "Transformer", "Encoder", "Vocabtext", "DNNLangId", "VocabLangid", "LangId", "Transformer", "Inputs", "Text", "Pre-trained/fixed)", "Trainable", "Label", "Embeddings", "Global", "Features", "Extractor", "Image", "Object", "Classifier", "DNNobjectsDNNimage"], "regionBoundary": {"x2": 526.0, "y1": 61.8900146484375, "x1": 72.0, "y2": 195.8900146484375}, "caption": "Figure 2: The Transformer based PLuGS model. The text on the input side is used for the translation and multimodal translation experiments with the Multi30K dataset. For image captioning, no text input is provided.", "page": 3}, {"figType": "Table", "name": "6", "captionBoundary": {"x2": 527.2009887695312, "y1": 130.13052368164062, "x1": 71.69100189208984, "y2": 148.0889892578125}, "imageText": ["Model", "Wins", "Losses", "Gainsxs", "BAccept", "AAccept", "GainAccept", "PLuGS-Fr", "26.9", "21.8", "5.1", "70.4", "67.0", "3.4", "PLuGS-De", "26.6", "21.3", "5.3", "70.4", "69.7", "0.7", "PLuGS-Es", "28.0", "21.8", "6.2", "69.7", "67.8", "1.9"], "regionBoundary": {"x2": 473.0, "y1": 65.19666290283203, "x1": 124.0, "y2": 117.8900146484375}, "caption": "Table 6: Performance of Stabilizers used as captions from PLuGS models for three languages vs the captions produced by the baseline English model. The PLuGS Stabilizer outputs are better captions across all three languages.", "page": 8}, {"figType": "Table", "name": "7", "captionBoundary": {"x2": 291.9243469238281, "y1": 238.92251586914062, "x1": 71.69100189208984, "y2": 268.83599853515625}, "imageText": ["Model", "PLuGS", "Baseline", "Diff", "PLuGS-Fr", "0.8663", "0.8772", "-0.0139", "PLuGS-De", "0.8680", "0.8772", "-0.0092", "PLuGS-Es", "0.8590", "0.8772", "-0.0182"], "regionBoundary": {"x2": 281.0, "y1": 173.98858642578125, "x1": 81.0, "y2": 226.8900146484375}, "caption": "Table 7: CIDEr scores on CC-1.1 validation set for Baseline and PLuGS-Stabilizer outputs (English captions).", "page": 8}, {"figType": "Table", "name": "8", "captionBoundary": {"x2": 291.5158386230469, "y1": 367.37554931640625, "x1": 71.69100189208984, "y2": 385.3330078125}, "imageText": ["Model", "Spearman", "\u03c1", "TGT", "TTG", "PLuGS", "PLuGS-Fr", "0.3017", "0.3318", "0.5982", "PLuGS-De", "0.3246", "0.2900", "0.5862", "PLuGS-Es", "0.2928", "0.3201", "0.5566"], "regionBoundary": {"x2": 276.0, "y1": 285.8900146484375, "x1": 86.0, "y2": 354.8900146484375}, "caption": "Table 8: Spearman correlation of Stabilizer vs TGT, TTG and PLuGS Captions across three languages.", "page": 8}, {"figType": "Table", "name": "9", "captionBoundary": {"x2": 525.5464477539062, "y1": 212.22256469726562, "x1": 306.9670104980469, "y2": 230.1810302734375}, "imageText": ["Fr", "It", "De", "Es", "Hi", "BLEU", "93.3", "92.9", "88.2", "93.9", "88.2"], "regionBoundary": {"x2": 515.0, "y1": 170.8900146484375, "x1": 318.0, "y2": 199.8900146484375}, "caption": "Table 9: The BLEU-4 score of the translation of the stabilizer against the caption treated as the reference.", "page": 8}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 730.1617092556423, "y1": 458.69102478027344, "x1": 99.50137668185764, "y2": 533.4457821316189}, "name": "1", "caption_text": "Figure 1: Examples of captions produced in German by Train-Generate-Translate (TGT), Translate-Train-Generate (TTG), and Pivot Language Generation Stabilization (PLuGS) approaches. Captions are shown in bold font. For TGT and TTG outputs, we show the English translation in parenthesis beside the caption. For the PLuGS outputs we mark the Stabilizer in the output using a light gray background. We do not explicitly show a translation for PLuGS outputs since the Stabilizer is already a translation.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 729.0, "y1": 88.0, "x1": 101.0, "y2": 441.0}, "page": 1, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2236802842882, "y1": 289.1146765814887, "x1": 100.0, "y2": 314.0556335449219}, "name": "2", "caption_text": "Figure 2: The Transformer based PLuGS model. The text on the input side is used for the translation and multimodal translation experiments with the Multi30K dataset. For image captioning, no text input is provided.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 719.0, "y1": 88.0, "x1": 100.0, "y2": 272.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.3470221625433, "y1": 700.867419772678, "x1": 426.772223578559, "y2": 825.4360622829861}, "name": "3", "caption_text": "Figure 3: Caption\u2019s dependence on the Stabilizer. The target-language caption is conditioned on the Stabilizer through the Masked Self-Attention in the decoder, and on the input image through the Encoder-Decoder attention that attends to the outputs of the last encoder layer. Note that in this figure, FF stands for the feed forward network, Voc stands for the (fixed) text vocab, and Emb stands for the (trainable) text embeddings.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 721.0, "y1": 359.0, "x1": 439.0, "y2": 674.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9268086751301, "y1": 341.3646697998047, "x1": 100.0, "y2": 366.30562676323785}, "name": "4", "caption_text": "Figure 4: Side-by-side human evaluation of two image captions. The same template is used for evaluating English as well as the 5 languages targeted.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 729.0, "y1": 89.0, "x1": 102.0, "y2": 322.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.222663031684, "y1": 996.5438842773438, "x1": 426.3153076171875, "y2": 1054.693094889323}, "name": "1", "caption_text": "Table 1: Multi30K test set METEOR scores for Translation (MT), Multi Modal Translation (MMT), and Image Captioning (IC-D4). The baseline is from task 1 of (Caglayan et al., 2019).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 716.0, "y1": 902.0, "x1": 426.0, "y2": 996.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 731.6562228732639, "y1": 332.94376797146265, "x1": 99.57083596123589, "y2": 374.49027167426215}, "name": "2", "caption_text": "Table 2: SxS performance of PLuGS vs. TGT models (upper half) and PLuGS vs. TTG models (lower half), across five target languages on OID1k. The PLuGS models perform better on both GainSxS and GainAccept metrics, for all five languages.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 669.0, "y1": 86.0, "x1": 157.0, "y2": 333.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 698.9991929796007, "y1": 527.7687920464409, "x1": 130.4944462246365, "y2": 536.1055586073134}, "name": "3", "caption_text": "Table 3: CIDEr scores on CC-1.1 validation set for PLuGS, TGT, and TTG models for five languages.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 641.0, "y1": 395.0, "x1": 190.0, "y2": 528.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 654.4583638509114, "y1": 221.67574564615884, "x1": 175.0361124674479, "y2": 230.01251220703125}, "name": "4", "caption_text": "Table 4: CIDEr scores on CC-1.1 validation set for bilingual and multilingual models.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 636.0, "y1": 86.0, "x1": 194.0, "y2": 205.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 648.6810896131727, "y1": 345.10214063856336, "x1": 180.812496609158, "y2": 353.43890719943573}, "name": "5", "caption_text": "Table 5: SxS performance of PLuGS-5L vs. PLuGS-2L models for three languages.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 640.0, "y1": 253.0, "x1": 181.0, "y2": 345.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2235955132378, "y1": 180.73683844672308, "x1": 99.57083596123589, "y2": 205.6791517469618}, "name": "6", "caption_text": "Table 6: Performance of Stabilizers used as captions from PLuGS models for three languages vs the captions produced by the baseline English model. The PLuGS Stabilizer outputs are better captions across all three languages.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 657.0, "y1": 89.0, "x1": 156.0, "y2": 181.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.45048183865015, "y1": 331.8368275960286, "x1": 99.57083596123589, "y2": 373.3833312988281}, "name": "7", "caption_text": "Table 7: CIDEr scores on CC-1.1 validation set for Baseline and PLuGS-Stabilizer outputs (English captions).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 393.0, "y1": 240.0, "x1": 100.0, "y2": 332.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 404.8831091986762, "y1": 510.24381849500867, "x1": 99.57083596123589, "y2": 535.1847330729166}, "name": "8", "caption_text": "Table 8: Spearman correlation of Stabilizer vs TGT, TTG and PLuGS Captions across three languages.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 398.0, "y1": 397.0, "x1": 103.0, "y2": 510.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9256218804253, "y1": 294.7535620795356, "x1": 426.3430701361762, "y2": 319.6958753797743}, "name": "9", "caption_text": "Table 9: The BLEU-4 score of the translation of the stabilizer against the caption treated as the reference.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 720.0, "y1": 238.0, "x1": 426.0, "y2": 295.0}, "page": 8, "dpi": 0}], "error": null, "pdf": "/work/host-output/e3b619252ca4aa5003b6170fb8aa45091275834d/2020.acl-main.16.pdf", "dpi": 100}