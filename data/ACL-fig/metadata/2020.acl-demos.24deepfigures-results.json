{"raw_detected_boxes": [[{"x2": 725.0, "y1": 317.0, "x1": 430.0, "y2": 672.0}], [{"x2": 727.0, "y1": 713.0, "x1": 427.0, "y2": 942.0}], [], [{"x2": 693.0, "y1": 86.0, "x1": 137.0, "y2": 378.0}], [], [{"x2": 400.0, "y1": 86.0, "x1": 101.0, "y2": 247.0}, {"x2": 731.0, "y1": 87.0, "x1": 433.0, "y2": 182.0}, {"x2": 722.0, "y1": 706.0, "x1": 437.0, "y2": 763.0}], [{"x2": 402.0, "y1": 310.0, "x1": 108.0, "y2": 490.0}], [], [], [], [{"x2": 729.0, "y1": 242.0, "x1": 105.0, "y2": 493.0}, {"x2": 723.0, "y1": 593.0, "x1": 104.0, "y2": 839.0}]], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Figure", "name": "1", "captionBoundary": {"x2": 527.2003173828125, "y1": 496.2675476074219, "x1": 306.9469909667969, "y2": 562.0459594726562}, "imageText": ["Text-to-SQL", "Model", "Q:", "How", "many", "candidates", "are", "registered", "in", "statistics?", "\u2264", "N", "SQL:", "SELECT", "COUNT(*),", "Courses.course_name", "FROM", "Student_Course", "_Registrations", "WHERE", "\u2026", "executable", "non-executable", "uncorrectable", "correctable", "How", "about", "show", "me", "all", "the", "courses", "and", "the", "teacher", "names?", "It", "is", "a", "confusing", "question", "for", "me.", "please", "check", "the", "tables", "and", "ask", "again.", "yes", "I\u2019m", "not", "sure", "about", "candidates,", "do", "you", "mean", "students?", "It", "is", "an", "invalid", "query,", "please", "check", "the", "tables", "and", "ask", "again.", "Response", "Generation", "There", "are", "4", "students", "registered", "in", "statistics.", "Schema", "Grounding", "Confusing", "Span", "Detection", "User", "InteractionConfusion", "Detection", "Execute", "SQL", "SELECT", "COUNT(*),", "Courses.course_name", "FROM", "Courses", "JOIN", "Student_Course_Registratio", "ns", "WHERE", "Courses.course_name", "=", "\"statistics\"", "Execution", "on", "DB"], "regionBoundary": {"x2": 524.0, "y1": 223.8900146484375, "x1": 310.0, "y2": 482.2423095703125}, "caption": "Figure 1: PHOTON workflow. The question corrector (upper block) detects the untranslatable questions from user input, scans the confusion span(s) that need clarification or correction. The accepted question is mapped into a SQL query through a text-to-SQL model, and finally the SQL execution results are returned to the user.", "page": 0}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 527.1934204101562, "y1": 143.05252075195312, "x1": 306.9670104980469, "y2": 172.96600341796875}, "imageText": ["#", "Q", "8,659", "1,034", "13,392", "1,631", "#", "UTran", "Q", "0", "0", "4,733", "597", "#", "Schema", "146", "20", "918", "112", "Spider", "SpiderUTran", "Train", "Dev", "Train", "Dev"], "regionBoundary": {"x2": 527.0, "y1": 62.8900146484375, "x1": 311.0, "y2": 130.8900146484375}, "caption": "Table 1: Data split of Spider and SpiderUTran. Q represents the all the questions, UTran Q represents the untranslatable questions.", "page": 5}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 525.54638671875, "y1": 561.7545776367188, "x1": 306.9170227050781, "y2": 594.322998046875}, "imageText": ["Att-biLSTM", "66.6", "58.7", "59.2", "PHOTON", "79.7", "69.1", "72.9", "Tran", "Acc", "Span", "Acc", "Span", "F1"], "regionBoundary": {"x2": 520.0, "y1": 504.8900146484375, "x1": 315.0, "y2": 549.8900146484375}, "caption": "Table 2: Translatability prediction accuracy (\u201cTran Acc\u201d) and the confusing spans prediction accuracy and F1 on our SpiderUTran dataset (%).", "page": 5}, {"figType": "Figure", "name": "5", "captionBoundary": {"x2": 261.2076416015625, "y1": 193.46554565429688, "x1": 101.05999755859375, "y2": 199.468017578125}, "imageText": ["\u2026", "System:", "candidates", "is", "confusing", "here,", "do", "you", "mean", "students?", "Table&Column", "Names", "students", ".289", "teachers", ".017", "courses", ".013", "names", ".009", "student", "details", ".008", "BERT", "MLM"], "regionBoundary": {"x2": 290.0, "y1": 83.8900146484375, "x1": 72.6256103515625, "y2": 181.92340087890625}, "caption": "Figure 5: Token Correction in PHOTON.", "page": 5}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 526.79248046875, "y1": 368.0015563964844, "x1": 71.69100189208984, "y2": 397.91400146484375}, "imageText": ["Others", "Other", "cases", "that", "the", "question", "cannot", "be", "translated.", "Q:", "How", "many", "Russias", "have", "Summer's", "transfer", "window?", "Schema:", "||Name||Country||Type||Transfer", "Window||Transfer", "Fee||", "Not", "a", "query", "Input", "is", "not", "a", "linguistically", "valid", "question.", "Q:", "Cyrus", "teaches", "physics", "in", "department.", "Q:", "What", "is", "the", "trend", "of", "housing", "price", "this", "year?", "Schema:", "||House", "ID||Location||Price||Number", "of", "amenties||", "Beyond", "representation", "scope", "of", "SQL", "Input", "asks", "for", "information", "that", "cannot", "be", "obtained", "by", "SQL", "logic.", "Input", "contains", "ambiguous", "or", "vague", "expressions.", "Q:", "Show", "me", "homes", "with", "good", "schools", "Schema:", "||Address||Community||School", "Name||School", "Rating||", "Ambiguity", "&", "Vagueness", "Q:", "What", "is", "the", "name", "of", "the", "singer", "with", "the", "largest", "net", "worth?", "Schema:", "||Singer_ID||Name||Birth_Year||Citizenship||", "Overspecification", "Input", "asks", "for", "information", "that", "cannot", "be", "found", "in", "the", "DB.", "Q:", "What", "is", "the", "total?", "Schema:||Course_ID||Staring_Data||Course||\u2026", "Underspecification", "Input", "does", "not", "specify", "which", "data", "entries/attributes", "to", "query.", "Reason", "Description", "Example"], "regionBoundary": {"x2": 525.0, "y1": 167.8900146484375, "x1": 72.0, "y2": 354.8900146484375}, "caption": "Table 4: Types of untranslatable questions in text-to-SQL identified from manual analysis of CoSQL (Yu et al., 2019a) and Multi-WOZ (Budzianowski et al., 2018). A question span that is problematic for the translation is highlighted when applicable.", "page": 10}, {"figType": "Table", "name": "5", "captionBoundary": {"x2": 525.547119140625, "y1": 619.2845458984375, "x1": 71.69100189208984, "y2": 649.197021484375}, "imageText": ["WHOLE", "SENTENCE", "S1:", "||", "CoutryId", "||", "CountryName", "||", "Continent", "||", "Drop", "types", "S2:", "||", "Vote_ID", "||", "Phone_Number", "||", "Area_Code", "||", "State", "||", "Created", "||", "soloists", "S1:", "||", "Conductor_ID", "||", "Name", "||", "Age", "||", "Nationlity||", "Year_of_Work", "||", "Swap", "Question", "Transformation", "age", "Schema", "Drop", "surface", "area", "WHOLE", "SENTENCE", "S2:", "||", "CountryCode", "||", "HeadOfState", "||", "Captital", "||", "Language", "||", "IsOfficial", "||", "Percentage", "||", "Q1:", "How", "much", "surface", "area", "do", "the", "countires", "in", "the", "Carribean", "cover", "together?", "S1:", "||", "Name", "||Continent", "||", "Region", "||", "SurfaceArea", "||", "Population", "||", "LifeExpectancy", "||", "S1:", "||", "Name", "||Continent", "||", "Region", "||", "Population", "||", "LifeExpectancy", "||", "Q2:", "Find", "the", "name", "and", "age", "of", "the", "visitor", "who", "bought", "the", "most", "tickets", "at", "once.", "S2:", "||Customer_ID||Name||Level_of_membership||", "Age", "||", "S2:", "||Customer_ID||Name||Level_of_membership||", "Q2:", "What", "is", "the", "official", "language", "spoken", "in", "the", "country", "whose", "head", "of", "state", "is", "Beatrix?", "Q2:", "What", "are", "the", "people", "in", "the", "country", "where", "Beatrix", "is", "located?", "Q1:", "How", "many", "countries", "exist?", "Q1:", "How", "many", "are", "there?", "Q2:", "What", "are", "the", "maximum", "and", "minimum", "values", "of", "types?", "Q2:", "What", "are", "the", "maximum", "and", "minimum", "values", "of", "area", "codes", "?", "Original", "data", "Transformed", "data", "Confusing", "text", "span", "Q1:", "How", "many", "conductors", "are", "there?", "Q1:", "How", "many", "soloists", "are", "there", "?"], "regionBoundary": {"x2": 523.0, "y1": 425.8900146484375, "x1": 74.0, "y2": 606.8900146484375}, "caption": "Table 5: Examples of question-side and schema-side transformations for generating training data for untranslatable question detection. Let Q denote the question and S denote the schema. For each transformation, we provide two examples, i.e., (Q1, S1) and (Q2, S2). The italic and bold fonts highlight phrases before and after transformations.", "page": 10}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 527.1975708007812, "y1": 690.7325439453125, "x1": 307.2560119628906, "y2": 744.5550537109375}, "imageText": ["Response", "Template", "associated", "fields", "of", "interest.\u201d", "CONFIRM_RESULT", "\u201cSQL:", "{PRED_SQL}.", "{NL_RESPONSE}\u201d", "CONFIRM_CORRECTION", "\u201cSorry,", "{CONF_TOKENS}", "is", "confusing", "in", "our", "scenario,", "do", "you", "mean", "{CORR_TOKENS}?\u201d", "NEED_REPHRASE", "\u201cSorry,", "it", "is", "a", "confusing", "question", "for", "me,", "please", "rephrase", "your", "question", "and", "ask", "again.\u201d", "INVALID_QUERY", "\u201cSorry,", "it", "is", "an", "invalidate", "query,", "please", "check", "the", "table", "names", "and", "No", "Yes", "No", "Yes", "No", "Yes", "INVALID_QUERY", "CONFIRM_CORRECTION", "NEED_REPHRASE", "Has", "span", "or", "not", "CONFIRM_RESULT", "Executable", "or", "not", "CLARIFYTranslatable", "or", "not", "CONFIRM_SQL", "INIT"], "regionBoundary": {"x2": 526.0, "y1": 512.8900146484375, "x1": 307.0, "y2": 678.8900146484375}, "caption": "Figure 2: State transition map of interaction in PHOTON. States with darker background are the end states that can receive user reply, and switch to INIT state automatically. The bottom part is the system response templates in each end state.", "page": 1}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 290.2704772949219, "y1": 363.6525573730469, "x1": 71.6709976196289, "y2": 381.61102294921875}, "imageText": ["\u2020", "denotes", "unpublished", "work", "on", "arXiv.", "PHOTON", "63.2", "GNN", "(Bogin", "et", "al.,", "2019a)", "40.7", "Global-GNN", "(Bogin", "et", "al.,", "2019b)", "52.7", "EditSQL", "+", "BERT", "(Zhang", "et", "al.,", "2019)", "57.6", "GNN+Bertrand-DR\u2020", "(Kelkar", "et", "al.,", "2020)", "57.9", "EditSQL+Bertrand-DR\u2020", "(Kelkar", "et", "al.,", "2020)", "58.5", "IRNet", "+", "BERT", "(Guo", "et", "al.,", "2019)", "61.9", "RYANSQL", "+", "BERT", "\u2020", "(Choi", "et", "al.,", "2020)", "66.6", "Model", "EM", "Acc."], "regionBoundary": {"x2": 297.0, "y1": 222.8900146484375, "x1": 74.0, "y2": 352.5980224609375}, "caption": "Table 3: Experimental results on the Spider Dev set (%). EM Acc. denotes the exact set match accuracy.", "page": 6}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 444.08197021484375, "y1": 298.0225524902344, "x1": 153.46299743652344, "y2": 304.0250244140625}, "imageText": ["CLS", "Question", "\u2026", "\u201c13,000\u201d", "\u201c6,256\u201d", "V", "Carribean", "How", "much", "surface", "area", "do", "the", "countries", "in", "the", "Caribbean", "cover", "together?", "\u2026", "\u2026", "\u201cZH\u201d", "\u201cUS\u201d", "\u201cPorto", "Rico\u201d", "\u201cCarribean\u201d", "CountryT", "C", "\u2026", "C", "Region", "C", "\u2026", "C", "Surface", "AreaCodeC"], "regionBoundary": {"x2": 502.0, "y1": 220.8900146484375, "x1": 93.0, "y2": 293.8900146484375}, "caption": "Figure 4: Joint schema-question encoder augmented with picklist values.", "page": 3}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 380.83880615234375, "y1": 199.38156127929688, "x1": 216.7050018310547, "y2": 205.384033203125}, "imageText": ["\u2022", "Text2SQL", "given", "tabl", "schema", "and", "value", "set", "of", "each", "field", "Text-to-SQL", "with", "Value", "List", "Goal", "Field", "encoding", "Table", "encoding", "Field", "encoding", "Table", "encoding", "Bi-LSTM", "output", "states", "Foreign", "key", "Primary", "key", "Data", "type", "Connect", "to", "decoder", "LSTM", "SEP", "Bidirectional", "LSTM", "Text", "Encoder", "Bidirectional", "LSTM", "BERT", "Instructor", "Department", "\u2026T", "C", "T", "C\u2026CLS", "What", "is", "the", "average", "rating", "of", "physics", "instructors?"], "regionBoundary": {"x2": 500.61224365234375, "y1": 63.237518310546875, "x1": 91.83270263671875, "y2": 192.8900146484375}, "caption": "Figure 3: Joint schema-question encoder.", "page": 3}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 732.222663031684, "y1": 689.2604827880859, "x1": 426.3152652316623, "y2": 780.619388156467}, "name": "1", "caption_text": "Figure 1: PHOTON workflow. The question corrector (upper block) detects the untranslatable questions from user input, scans the confusion span(s) that need clarification or correction. The accepted question is mapped into a SQL query through a text-to-SQL model, and finally the SQL execution results are returned to the user.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 727.0, "y1": 311.0, "x1": 427.0, "y2": 689.0}, "page": 0, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2188483344183, "y1": 959.3507554796007, "x1": 426.7444610595703, "y2": 1034.104241265191}, "name": "2", "caption_text": "Figure 2: State transition map of interaction in PHOTON. States with darker background are the end states that can receive user reply, and switch to INIT state automatically. The bottom part is the system response templates in each end state.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 730.0, "y1": 712.0, "x1": 427.0, "y2": 959.0}, "page": 1, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 528.9427863226996, "y1": 276.9188351101345, "x1": 300.97916920979816, "y2": 285.25560167100696}, "name": "3", "caption_text": "Figure 3: Joint schema-question encoder.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 696.0, "y1": 86.0, "x1": 132.0, "y2": 391.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 362.78839111328125, "y1": 268.702146742079, "x1": 140.3611077202691, "y2": 277.03891330295136}, "name": "5", "caption_text": "Figure 5: Token Correction in PHOTON.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 403.0, "y1": 86.0, "x1": 101.0, "y2": 252.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2130839029948, "y1": 198.68405659993488, "x1": 426.3430701361762, "y2": 240.23056030273438}, "name": "1", "caption_text": "Table 1: Data split of Spider and SpiderUTran. Q represents the all the questions, UTran Q represents the untranslatable questions.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 731.0, "y1": 86.0, "x1": 426.0, "y2": 199.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.925537109375, "y1": 780.2146911621094, "x1": 426.27364264594183, "y2": 825.4486083984375}, "name": "2", "caption_text": "Table 2: Translatability prediction accuracy (\u201cTran Acc\u201d) and the confusing spans prediction accuracy and F1 on our SpiderUTran dataset (%).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 722.0, "y1": 700.0, "x1": 426.0, "y2": 780.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.1534406873915, "y1": 505.072996351454, "x1": 99.54305224948459, "y2": 530.0153096516927}, "name": "3", "caption_text": "Table 3: Experimental results on the Spider Dev set (%). EM Acc. denotes the exact set match accuracy.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 413.0, "y1": 293.0, "x1": 100.0, "y2": 507.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 731.6562228732639, "y1": 511.11327277289496, "x1": 99.57083596123589, "y2": 552.6583353678385}, "name": "4", "caption_text": "Table 4: Types of untranslatable questions in text-to-SQL identified from manual analysis of CoSQL (Yu et al., 2019a) and Multi-WOZ (Budzianowski et al., 2018). A question span that is problematic for the translation is highlighted when applicable.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 729.0, "y1": 233.0, "x1": 100.0, "y2": 510.0}, "page": 10, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9265543619791, "y1": 860.1174248589409, "x1": 99.57083596123589, "y2": 901.6625298394097}, "name": "5", "caption_text": "Table 5: Examples of question-side and schema-side transformations for generating training data for untranslatable question detection. Let Q denote the question and S denote the schema. For each transformation, we provide two examples, i.e., (Q1, S1) and (Q2, S2). The italic and bold fonts highlight phrases before and after transformations.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 726.0, "y1": 591.0, "x1": 103.0, "y2": 842.0}, "page": 10, "dpi": 0}], "error": null, "pdf": "/work/host-output/5231c297c9dc604d91db3c371af292e84e4bc7bc/2020.acl-demos.24.pdf", "dpi": 100}