{"raw_detected_boxes": [[{"x2": 722.0, "y1": 321.0, "x1": 432.0, "y2": 472.0}], [], [{"x2": 487.0, "y1": 93.0, "x1": 109.0, "y2": 426.0}], [], [{"x2": 730.0, "y1": 92.0, "x1": 429.0, "y2": 218.0}], [{"x2": 725.0, "y1": 95.0, "x1": 105.0, "y2": 278.0}], [{"x2": 405.0, "y1": 92.0, "x1": 102.0, "y2": 200.0}, {"x2": 724.0, "y1": 92.0, "x1": 428.0, "y2": 204.0}], [{"x2": 724.0, "y1": 99.0, "x1": 103.0, "y2": 318.0}], [], [], []], "raw_pdffigures_output": {"regionless-captions": [{"figType": "Figure", "boundary": {"x2": 411.5845947265625, "y1": 146.77352905273438, "x1": 307.8840026855469, "y2": 212.55206298828125}, "text": "Figure 3: The number of entities (shown in yaxis) that are incorrectly predicted by BERT-CRF, but get corrected by each multimodal method", "name": "3", "page": 6}, {"figType": "Figure", "boundary": {"x2": 524.2811889648438, "y1": 146.77352905273438, "x1": 418.3179931640625, "y2": 212.55206298828125}, "text": "Figure 4: The number of entities (shown in yaxis) that are correctly predicted by BERT-CRF, but wrongly predicted by each multimodal method", "name": "4", "page": 6}], "figures": [{"figType": "Figure", "name": "1", "captionBoundary": {"x2": 525.8604125976562, "y1": 356.67254638671875, "x1": 307.2760009765625, "y2": 382.6000061035156}, "imageText": ["(a).", "[Kevin", "Durant", "PER]", "enters", "[Oracle", "Arena", "LOC]", "wearing", "off", "\u2014", "White", "x", "[Jordan", "MISC]", "(b).", "Vote", "for", "[King", "of", "the", "Jungle", "MISC]", "\u2014", "[Kian", "PER]", "or", "[David", "PER]", "?"], "regionBoundary": {"x2": 520.7762451171875, "y1": 229.8900146484375, "x1": 308.9989929199219, "y2": 349.0810241699219}, "caption": "Figure 1: Two examples for Multimodal Named Entity Recognition (MNER). Named entities and their entity types are highlighted.", "page": 0}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 525.5402221679688, "y1": 207.76956176757812, "x1": 71.69100189208984, "y2": 223.7340087890625}, "imageText": ["Text+Image", "GVATT-BERT-CRF", "84.43", "80.87", "59.02", "38.14", "69.15", "74.46", "71.70", "90.94", "83.52", "81.91", "62.75", "83.64", "84.38", "84.01", "AdaCAN-BERT-CRF", "85.28", "80.64", "59.39", "38.88", "69.87", "74.59", "72.15", "90.20", "82.97", "82.67", "64.83", "85.13", "83.20", "84.10", "MT-BERT-CRF", "(Ours)", "85.30", "81.21", "61.10", "37.97", "70.48", "74.80", "72.58", "91.47", "82.05", "81.84", "65.80", "84.60", "84.16", "84.42", "UMT-BERT-CRF", "(Ours)", "85.24", "81.58\u2020", "63.03\u2020", "39.45\u2020", "71.67", "75.23", "73.41\u2020", "91.56\u2020", "84.73\u2020", "82.24", "70.10\u2020", "85.28", "85.34", "85.31\u2020", "GVATT-HBiLSTM-CRF", "82.66", "77.21", "55.06", "35.25", "73.96", "67.90", "70.80", "89.34", "78.53", "79.12", "62.21", "83.41", "80.38", "81.87", "AdaCAN-CNN-BiLSTM-CRF", "81.98", "78.95", "53.07", "34.02", "72.75", "68.74", "70.69", "89.63", "77.46", "79.24", "62.77", "84.16", "80.24", "82.15", "BERT", "84.72", "79.91", "58.26", "38.81", "68.30", "74.61", "71.32", "90.88", "84.00", "79.25", "61.63", "82.19", "83.72", "82.95", "BERT-CRF", "84.74", "80.51", "60.27", "37.29", "69.22", "74.59", "71.81", "90.25", "83.05", "81.13", "62.21", "83.32", "83.57", "83.44", "Text", "HBiLSTM-CRF", "82.34", "76.83", "51.59", "32.52", "70.32", "68.05", "69.17", "87.91", "78.57", "76.67", "59.32", "82.69", "78.16", "80.37", "BiLSTM-CRF", "76.77", "72.56", "41.33", "26.80", "68.14", "61.09", "64.42", "85.12", "72.68", "72.50", "52.56", "79.42", "73.43", "76.31", "CNN-BiLSTM-CRF", "80.86", "75.39", "47.77", "32.61", "66.24", "68.09", "67.15", "87.99", "77.44", "74.02", "60.82", "80.00", "78.76", "79.37", "Single", "Type", "(F1)", "Overall", "Single", "Type", "(F1)", "Overall", "Modality", "Methods", "PER.", "LOC.", "ORG.", "MISC.", "P", "R", "F1", "PER.", "LOC.", "ORG.", "MISC.", "P", "R", "F1", "TWITTER-2015", "TWITTER-2017"], "regionBoundary": {"x2": 524.0, "y1": 65.8900146484375, "x1": 74.0, "y2": 199.8900146484375}, "caption": "Table 2: Performance comparison on our two TWITTER datasets. \u2020 indicates that UMT-BERT-CRF is significantly better than GVATT-BERT-CRF and AdaCAN-BERT-CRF with p-value < 0.05 based on paired t-test.", "page": 5}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 290.2202453613281, "y1": 152.06356811523438, "x1": 71.69100189208984, "y2": 158.0660400390625}, "imageText": ["w/o", "ESD", "Module", "70.48", "74.80", "72.58", "84.60", "84.16", "84.42", "w/o", "Conversion", "Matrix", "70.43", "74.98", "72.63", "84.72", "84.97", "84.85", "w/o", "Image-Aware", "WR", "70.33", "75.44", "72.79", "83.83", "85.94", "84.87", "w/o", "Visual", "Gate", "71.34", "75.15", "73.19", "85.31", "84.68", "84.99", "UMT-BERT-CRF", "71.67", "75.23", "73.41", "85.28", "85.34", "85.31", "Methods", "P", "R", "F1", "P", "R", "F1", "TWITTER-2015", "TWITTER-2017"], "regionBoundary": {"x2": 293.0, "y1": 62.8900146484375, "x1": 72.0, "y2": 143.8900146484375}, "caption": "Table 3: Ablation Study of Unified Multimodal Transformer.", "page": 6}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 523.3720092773438, "y1": 321.4945373535156, "x1": 74.1760025024414, "y2": 327.49700927734375}, "imageText": ["Visual", "Gate", "......", "......", "Add", "&", "Norm", "qn+1......q1q0", "\u03c3", "+", "an+1......a1a0", "bn+1......b1b0", "p1", "p2", "......", "p49", "h0", "h1", "hn+1", "Feed", "Forward", "Add", "&", "Norm", "Add", "&", "Norm", "Q", "K", "V", "Cross-Modal", "Attention", "Q", "K", "V", "Text", "Modality", "rn+1......r1r0", "Feed", "Forward", "Add", "&", "Norm", "Cross-Modal", "Attention", "Visual", "Modality", "v49v2v1", "Feed", "Forward", "Add", "&", "Norm", "Add", "&", "Norm", "Q", "K", "V", "Cross-Modal", "Attention", "c0", "c1", "c2", "c3", "cn", "cn+1......", "......", "E[SEP]EJordanEenters", "EDurantEKevinE[CLS]", "......", "......", "EAEAEAEA", "EA", "EA", "E3E2E1E0", "En", "En+1", "rn+1", "B-PER", "Auxiliary", "Entity", "Span", "Detection", "Module", "+", "+", "BERT", "Encoder", "Module", "Multimodal", "Interaction", "Conversion", "Matrix", "Visual", "Modality", "Text", "Modality", "Multimodal", "Transformer", "for", "MNER", "v49......v2v1", "Visual", "Input", "Textual", "Input", "I-PER", "O", "......", "B-MISC", "E1", "E2", "E3", "......", "En", "h0", "h1", "h2", "h3", "......", "hn", "hn+1", "B", "I", "O", "......", "B", "Q", "K", "V", "Transformer", "Layer", "with", "Self-Attention", "Q", "K", "V", "Transformer", "Layer", "with", "Self-Attention", "F1", "F2", "F3", "......", "Fn", "t0", "t1", "t2", "t3", "......", "tn", "tn+1", "r0", "r1", "r2", "r3", "......", "rn", "ResNet", "[CLS]", "Kevin", "Durant", "enters", "......", "Jordan", "[SEP]"], "regionBoundary": {"x2": 518.0, "y1": 63.8900146484375, "x1": 74.0, "y2": 315.1666259765625}, "caption": "Figure 2: (a). Overall Architecture of Our Unified Multimodal Transformer. (b). Multimodal Interaction (MMI) Module.", "page": 2}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 525.6964111328125, "y1": 228.32748413085938, "x1": 71.69103240966797, "y2": 244.29193115234375}, "imageText": ["BERT-CRF:", "1-LOC7,", "2-LOC7", "1-PER3,", "2-MISC3,", "3-ORG3", "1-MISC7,", "2-ORG7", "1-MISC3", "AdaCAN-BERT-CRF:", "1-LOC7,", "2-LOC7", "1-PER3,", "2-NONE7,", "3-ORG3", "1-PER3,", "2-PER3", "1-PER7", "MT-BERT-CRF:", "1-MISC3,", "2-MISC3", "1-PER3,", "2-NONE7,", "3-ORG3", "1-PER3,", "2-PER3", "1-PER7", "UMT-BERT-CRF:", "1-MISC3,", "2-MISC3", "1-PER3,", "2-MISC3,", "3-ORG3", "1-PER3,", "2-PER3", "1-PER7", "A.", "Review", "of", "[Wolf", "Hall", "MISC]1,", "Episode", "1", ":", "Three", "Card", "Trick", "(bit.ly/1BHnWNb)", "#[WolfHall", "MISC]2", "B.", "[Kevin", "Love", "PER]1", "was", "more", "ex-", "cited", "about", "[GameofThrones", "MISC]2", "than", "beating", "the", "[Hawks", "ORG]3", "C.", "My", "mum", "took", "some", "awesome", "photos", "of", "@", "[iamrationale", "PER]1", "and", "@", "[bastilledan", "PER]2.", "D.", "Ask", "[Siri", "MISC]1", "what", "0", "divided", "by", "0", "is", "and", "watch", "her", "put", "you", "in", "your", "place.", "Importance", "of", "the", "MMI", "Module", "Importance", "of", "the", "ESD", "Module", "Importance", "of", "Associated", "Images", "Noise", "of", "Associated", "Images"], "regionBoundary": {"x2": 525.0, "y1": 76.08130645751953, "x1": 72.0, "y2": 222.8900146484375}, "caption": "Table 4: The second row shows several representative samples together with their manually labeled entities in the test set of our two TWITTER datasets, and the bottom four rows show predicted entities of different methods on these test samples.", "page": 7}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 517.8851318359375, "y1": 168.04855346679688, "x1": 314.62799072265625, "y2": 174.051025390625}, "imageText": ["Num", "of", "Tweets", "4000", "1000", "3257", "3373", "723", "723", "Total", "6176", "1546", "5078", "6049", "1324", "1351", "Person", "2217", "552", "1816", "2943", "626", "621", "Location", "2091", "522", "1697", "731", "173", "178", "Organization", "928", "247", "839", "1674", "375", "395", "Miscellaneous", "940", "225", "726", "701", "150", "157", "Entity", "Type", "Train", "Dev", "Test", "Train", "Dev", "Test", "TWITTER-2015", "TWITTER-2017"], "regionBoundary": {"x2": 526.0, "y1": 62.8900146484375, "x1": 307.0, "y2": 159.8900146484375}, "caption": "Table 1: The basic statistics of our two Twitter datasets.", "page": 4}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 730.3616841634114, "y1": 495.37853664822046, "x1": 426.772223578559, "y2": 531.3888973659939}, "name": "1", "caption_text": "Figure 1: Two examples for Multimodal Named Entity Recognition (MNER). Named entities and their entity types are highlighted.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 724.0, "y1": 320.0, "x1": 429.0, "y2": 487.0}, "page": 0, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 726.9055684407551, "y1": 446.52019076877167, "x1": 103.02222569783528, "y2": 454.8569573296441}, "name": "2", "caption_text": "Figure 2: (a). Overall Architecture of Our Unified Multimodal Transformer. (b). Multimodal Interaction (MMI) Module.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 487.0, "y1": 88.0, "x1": 104.0, "y2": 432.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 719.2849053276909, "y1": 233.40076870388455, "x1": 436.98332044813367, "y2": 241.73753526475693}, "name": "1", "caption_text": "Table 1: The basic statistics of our two Twitter datasets.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 730.0, "y1": 86.0, "x1": 427.0, "y2": 235.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9169752332899, "y1": 288.5688357883029, "x1": 99.57083596123589, "y2": 310.7416788736979}, "name": "2", "caption_text": "Table 2: Performance comparison on our two TWITTER datasets. \u2020 indicates that UMT-BERT-CRF is significantly better than GVATT-BERT-CRF and AdaCAN-BERT-CRF with p-value < 0.05 based on paired t-test.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 730.0, "y1": 86.0, "x1": 100.0, "y2": 295.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.0836741129557, "y1": 211.19940016004773, "x1": 99.57083596123589, "y2": 219.53616672092014}, "name": "3", "caption_text": "Table 3: Ablation Study of Unified Multimodal Transformer.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 406.0, "y1": 86.0, "x1": 100.0, "y2": 217.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 571.645270453559, "y1": 203.8521236843533, "x1": 427.6166703965929, "y2": 295.2111985948351}, "name": "3", "caption_text": "Figure 3: The number of entities (shown in yaxis) that are incorrectly predicted by BERT-CRF, but get corrected by each multimodal method", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 726.0, "y1": 92.0, "x1": 428.0, "y2": 221.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 730.1339043511284, "y1": 317.1215057373047, "x1": 99.57087834676106, "y2": 339.29434882269965}, "name": "4", "caption_text": "Table 4: The second row shows several representative samples together with their manually labeled entities in the test set of our two TWITTER datasets, and the bottom four rows show predicted entities of different methods on these test samples.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 731.0, "y1": 97.0, "x1": 100.0, "y2": 335.0}, "page": 7, "dpi": 0}], "error": null, "pdf": "/work/host-output/1cabd026cfe1350203a79cc966f6938f55518377/2020.acl-main.306.pdf", "dpi": 100}