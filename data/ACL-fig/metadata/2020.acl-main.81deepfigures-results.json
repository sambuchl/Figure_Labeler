{"raw_detected_boxes": [[{"x2": 716.0, "y1": 309.0, "x1": 440.0, "y2": 395.0}], [], [], [{"x2": 699.0, "y1": 90.0, "x1": 132.0, "y2": 285.0}], [{"x2": 713.0, "y1": 92.0, "x1": 444.0, "y2": 300.0}], [{"x2": 641.0, "y1": 87.0, "x1": 188.0, "y2": 436.0}], [{"x2": 601.0, "y1": 86.0, "x1": 229.0, "y2": 196.0}, {"x2": 680.0, "y1": 313.0, "x1": 462.0, "y2": 469.0}], [{"x2": 350.0, "y1": 99.0, "x1": 146.0, "y2": 258.0}, {"x2": 709.0, "y1": 93.0, "x1": 441.0, "y2": 290.0}, {"x2": 344.0, "y1": 357.0, "x1": 159.0, "y2": 494.0}], [{"x2": 394.0, "y1": 90.0, "x1": 108.0, "y2": 218.0}, {"x2": 394.0, "y1": 286.0, "x1": 108.0, "y2": 412.0}], [], []], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Table", "name": "1", "captionBoundary": {"x2": 527.2003784179688, "y1": 296.8345642089844, "x1": 305.95098876953125, "y2": 410.4339599609375}, "imageText": ["Gold", "Label", "\u9910\u5385\u7684\u73af\u5883\u975e\u5e38\u9002\u5408\u7ea6\u4f1a", "(phonics)", "ca\u0304n", "ti\u0304ng", "de\u0304", "hua\u0301n", "j\u0131\u0300ng", "fe\u0304i", "cha\u0301ng", "sh\u0131\u0300", "he\u0301", "yue\u0304", "hu\u0131\u0300", "BERT", "\u9910\u5385\u7684\u6708\u6d88\u8d39\u6700\u9002\u5408\u7ea6\u4f1a", "(phonics)", "ca\u0304n", "ti\u0304ng", "de\u0304", "yue\u0300", "xia\u0304o", "fe\u0300i", "zu\u0131\u0300", "sh\u0131\u0300", "he\u0301", "yue\u0304", "hu\u0131\u0300", "Input", "\u9910\u5385\u7684\u6362\u7ecf\u8d39\u4ea7\u9002\u5408\u7ea6\u4f1a", "(phonics)", "ca\u0304n", "ti\u0304ng", "de\u0304", "hua\u0300n", "ji\u0304ng", "fe\u0300i", "cha\u0301n", "sh\u0131\u0300", "he\u0301", "yue\u0304", "hu\u0131\u0300"], "regionBoundary": {"x2": 516.0, "y1": 222.8900146484375, "x1": 316.0, "y2": 284.8900146484375}, "caption": "Table 1: A CSC data sample from SIGHAN 2014 (Yu et al., 2014) with ID B1-3440-2, the incorrect/correct characters are in orange/blue. A BERT model modifies the text into a sentence that is semantically reasonable but dissimilar in pronunciation. By incorporating both phonological and visual similarities, our new method SpellGCN can generate a sentence that is both semantically sensible and phonically similar to the original sentence. The sentence output from SpellGCN means \u201cthis restaurant is very suitable for dating\u201d.", "page": 0}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 527.2008666992188, "y1": 326.216552734375, "x1": 71.69100189208984, "y2": 368.0849914550781}, "imageText": ["BERT", "87.5", "85.7", "86.6", "95.2", "81.5", "87.8", "73.7", "78.2", "75.9", "70.9", "75.2", "73.0", "SpellGCN", "88.9", "87.7", "88.3", "95.7", "83.9", "89.4", "74.8", "80.7", "77.7", "72.1", "77.7", "75.9", "LMC", "(Xie", "et", "al.,", "2015)", "83.8", "26.2", "40.0", "71.1", "50.2", "58.8", "(-)", "(-)", "(-)", "(-)", "(-)", "(-)", "SL", "(Wang", "et", "al.,", "2018)", "56.6", "69.4", "62.3", "(-)", "(-)", "57.1", "(-)", "(-)", "(-)", "(-)", "(-)", "(-)", "PN", "(Wang", "et", "al.,", "2019)", "66.8", "73.1", "69.8", "71.5", "59.5", "69.9", "(-)", "(-)", "(-)", "(-)", "(-)", "(-)", "FASpell", "(Hong", "et", "al.,", "2019)", "(-)", "(-)", "(-)", "(-)", "(-)", "(-)", "67.6", "60.0", "63.5", "66.6", "59.1", "62.6", "SIGHAN", "2015", "D-P", "D-R", "D-F", "C-P", "C-R", "C-F", "D-P", "D-R", "D-F", "C-P", "C-R", "C-F", "BERT", "82.9", "77.6", "80.2", "96.8", "75.2", "84.6", "65.6", "68.1", "66.8", "63.1", "65.5", "64.3", "SpellGCN", "83.6", "78.6", "81.0", "97.2", "76.4", "85.5", "65.1", "69.5", "67.2", "63.1", "67.2", "65.3", "LMC", "(Xie", "et", "al.,", "2015)", "56.4", "34.8", "43.0", "71.1", "50.2", "58.8", "(-)", "(-)", "(-)", "(-)", "(-)", "(-)", "SL", "(Wang", "et", "al.,", "2018)", "51.9", "66.2", "58.2", "(-)", "(-)", "56.1", "(-)", "(-)", "(-)", "(-)", "(-)", "(-)", "PN", "(Wang", "et", "al.,", "2019)", "63.2", "82.5", "71.6", "79.3", "68.9", "73.7", "(-)", "(-)", "(-)", "(-)", "(-)", "(-)", "FASpell", "(Hong", "et", "al.,", "2019)", "(-)", "(-)", "(-)", "(-)", "(-)", "(-)", "61.0", "53.5", "57.0", "59.4", "52.0", "55.4", "SIGHAN", "2014", "D-P", "D-R", "D-F", "C-P", "C-R", "C-F", "D-P", "D-R", "D-F", "C-P", "C-R", "C-F", "BERT", "80.6", "88.4", "84.3", "98.1", "87.2", "92.3", "79.0", "72.8", "75.8", "77.7", "71.6", "74.6", "SpellGCN", "82.6", "88.9", "85.7", "98.4", "88.4", "93.1", "80.1", "74.4", "77.2", "78.3", "72.7", "75.4", "LMC", "(Xie", "et", "al.,", "2015)", "79.8", "50.0", "61.5", "77.6", "22.7", "35.1", "(-)", "(-)", "(-)", "(-)", "(-)", "(-)", "SL", "(Wang", "et", "al.,", "2018)", "54.0", "69.3", "60.7", "(-)", "(-)", "52.1", "(-)", "(-)", "(-)", "(-)", "(-)", "(-)", "PN", "(Wang", "et", "al.,", "2019)", "56.8", "91.4", "70.1", "79.7", "59.4", "68.1", "(-)", "(-)", "(-)", "(-)", "(-)", "(-)", "FASpell", "(Hong", "et", "al.,", "2019)", "(-)", "(-)", "(-)", "(-)", "(-)", "(-)", "76.2", "63.2", "69.1", "73.1", "60.5", "66.2", "SIGHAN", "2013", "D-P", "D-R", "D-F", "C-P", "C-R", "C-F", "D-P", "D-R", "D-F", "C-P", "C-R", "C-F", "Detection-level", "Correction-level", "Detection-level", "Correction-level", "Character-level", "Sentence-level"], "regionBoundary": {"x2": 463.0, "y1": 62.8900146484375, "x1": 135.0, "y2": 313.8900146484375}, "caption": "Table 3: The performance of our method and baseline models (%). D, C denote the detection, correction, respectively. P, R, F denote the precision, recall and F1 score, respectively. The results of BERT are from our own implementation. Best results are in bold. We performed additional fine-tuning on SIGHAN13 for 6 epochs as the data distribution in SIGHAN13 differs from other datasets, e.g. \u201c\u7684\u201d, \u201c\u5f97\u201d and \u201c\u5730\u201d are rarely distinguished.", "page": 5}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 525.5474243164062, "y1": 156.66757202148438, "x1": 71.69100189208984, "y2": 186.5810546875}, "imageText": ["BERT", "13.6", "83.0", "81.5", "85.9", "78.9", "82.3", "85.5", "75.8", "80.5", "SpellGCN", "13.2", "83.7", "82.2", "85.9", "80.6", "83.1", "85.4", "77.6", "81.3", "SIGHAN", "2015", "FPR", "D-A", "C-A", "D-P", "D-R", "D-F", "C-P", "C-R", "C-F", "BERT", "15.3", "76.8", "75.7", "81.9", "68.9", "74.9", "81.4", "66.7", "73.3", "SpellGCN", "14.1", "77.7", "76.9", "83.1", "69.5", "75.7", "82.8", "67.8", "74.5", "SIGHAN", "2014", "FPR", "D-A", "C-A", "D-P", "D-R", "D-F", "C-P", "C-R", "C-F"], "regionBoundary": {"x2": 433.0, "y1": 62.8900146484375, "x1": 162.0, "y2": 144.8900146484375}, "caption": "Table 4: The performance of BERT and SpellGCN evaluated by official tools on SIGHAN 2014 and SIGHAN 2015. FPR denotes the false positive rate and A denotes the accuracy. D-A and C-A denote detection accuracy and correction accuracy.", "page": 6}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 525.7158203125, "y1": 353.38153076171875, "x1": 307.2760009765625, "y2": 383.2939758300781}, "imageText": ["BERT", "F1score", "SpellGCN", "F1score", "BERT", "Precision", "SpellGCN", "Precision", "BERT", "Recall", "SpellGCN", "Recall", "0.8", "0.7", "0.6", "0.5", "0.4", "0.3", "0", "1", "2", "3", "4", "5", "6", "epochs"], "regionBoundary": {"x2": 495.0, "y1": 224.8900146484375, "x1": 332.9799499511719, "y2": 337.35821533203125}, "caption": "Figure 2: The test curves for sentence-level correction metrics with and without SpellGCN w.r.t. the number of training epochs on SIGHAN 2015.", "page": 6}, {"figType": "Table", "name": "6", "captionBoundary": {"x2": 525.5465698242188, "y1": 221.54055786132812, "x1": 306.9670104980469, "y2": 275.36407470703125}, "imageText": ["...\u4e0d\u8fc7\u5728\u8bb8\u591a\u4f20\u7edf\u6587\u5316\u7684\u56fd\u5bb6\uff0c\u5973\u4eba\u5411\u672a\u5f97\u5230\u5e73\u7b49...", "...\u4e0d\u8fc7\u5728\u8bb8\u591a\u4f20\u7edf\u6587\u5316\u7684\u56fd\u5bb6\uff0c\u5973\u4eba\u4ece\u672a\u5f97\u5230\u5e73\u7b49...", "...\u4e0d\u8fc7\u5728\u8bb8\u591a\u4f20\u7edf\u6587\u5316\u7684\u56fd\u5bb6\uff0c\u5973\u4eba\u5c1a\u672a\u5f97\u5230\u5e73\u7b49...", "Shape:", "\u5411\u2192\u5c1a", "...\u56e0\u4e3a\u5988\u5988\u6216\u7238\u7238\u5728\u770b\u5f55\u97f3\u673a...\u5e2e\u5c0f\u5b69\u5b50\u89e3\u51b3\u95ee\u9898...", "...\u56e0\u4e3a\u5988\u5988\u6216\u7238\u7238\u5728\u770b\u5f55\u97f3\u673a...\u5e2e\u5c0f\u5b69\u5b50\u89e3\u51b3\u95ee\u9898...", "...\u56e0\u4e3a\u5988\u5988\u6216\u7238\u7238\u5728\u770b\u5f55\u5f71\u673a...\u5e2e\u5c0f\u5b69\u5b50\u89e3\u51b3\u95ee\u9898...", "Pronunciation:", "yi\u0304n\u2192yi\u0306ng", "...\u8d70\u8def\u771f\u7684\u9ebb\u574a\uff0c\u6211\u4e5f\u6ca1\u6709\u559d\u7684\u4e1c\u897f\uff0c\u5728\u5bb6\u6c6a\u4e86...", "...\u8d70\u8def\u771f\u7684\u9ebb\u6728\uff0c\u6211\u4e5f\u6ca1\u6709\u559d\u7684\u4e1c\u897f\uff0c\u5728\u5bb6\u5446\u4e86...", "...\u8d70\u8def\u771f\u7684\u9ebb\u70e6\uff0c\u6211\u4e5f\u6ca1\u6709\u559d\u7684\u4e1c\u897f\uff0c\u5728\u5bb6\u5fd8\u4e86...", "Pronunciation:", "fa\u0306ng\u2192fa\u0301n,", "wa\u0300ng\u2192wa\u0300ng"], "regionBoundary": {"x2": 513.0, "y1": 62.8900146484375, "x1": 318.0, "y2": 208.8900146484375}, "caption": "Table 6: Several prediction results on the test set. The first line in the block is the input sentence. The second line is corrected by BERT without SpellGCN. And the last line is the result from SpellGCN. We highlight the incorrect/correct characters by orange/blue color.", "page": 7}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 290.2706604003906, "y1": 206.09756469726562, "x1": 71.25299835205078, "y2": 236.01007080078125}, "imageText": ["1", "2", "3", "4", "68", "66", "64", "62", "SIGHAN13", "SIGHAN14", "SIGHAN15", "60"], "regionBoundary": {"x2": 254.0, "y1": 69.8900146484375, "x1": 105.34046936035156, "y2": 184.8934326171875}, "caption": "Figure 3: The character-level C-F results (%) w.r.t. the depth of SpellGCN. The results were obtained with 10K training samples.", "page": 7}, {"figType": "Table", "name": "5", "captionBoundary": {"x2": 291.9243469238281, "y1": 372.7135314941406, "x1": 71.69100189208984, "y2": 453.2869567871094}, "imageText": ["attentive", "combination", "(\u03b2=1)", "67.8", "attentive", "combination", "(\u03b2=3)", "68.2", "attentive", "combination", "(\u03b2=5)", "68.0", "attentive", "combination", "(\u03b2=10)", "67.7", "sum", "pooling", "66.3", "mean", "pooling", "67.5", "baseline", "(w/o", "SpellGCN)", "67.0", "combination", "method", "C-F"], "regionBoundary": {"x2": 248.0, "y1": 257.8900146484375, "x1": 114.0, "y2": 360.8900146484375}, "caption": "Table 5: The ablation results for graph combination method (%). The averaged character-level C-F scores of 4 runs on the SIGHAN 2013 are reported. The models were trained with 10K training samples. Mean pooling denotes that the output representation Cl of each layer is the average of fk\u2208{P,S}(Ak,Hl), while sum pooling summarizes fk\u2208{P,S}(Ak,Hl).", "page": 7}, {"figType": "Figure", "name": "1", "captionBoundary": {"x2": 525.8910522460938, "y1": 220.88552856445312, "x1": 71.99996948242188, "y2": 274.70904541015625}, "imageText": ["O", "utofthe", "C", "onfusion", "Set", "Dot", "Product", "\u7adf", "\u2026", "\u2026", "\u2026", "\u4eb0", "\u955c", "\u5883", "\u666f\u7ade", "\u2026", "\u2026", "\u2026", "\u2026", "\u7adf", "\u2026", "\u2026", "\u2026", "\u91d1", "\u9759", "\u5883", "\u5bc4\u4e95", "\u2026", "\u2026", "\u2026", "\u2026", "\u2026", "\u5230", "\u9006", "\u2026\u5883\u9047", "Layer-3", "\u2026", "\u2026", "\u5883", "\u91d1", "\u4e95\u955c\u4eb0", "\u9759", "\u5bc4", "\u7adf", "\u2026", "\ud835\udc16:", "Generated", "Classifier", "Attentive", "Com", "bination", "Pronunciation", "Similarity", "Graph", "Shape", "Similarity", "Graph", "Layer-2", "Layer-1", "\u2026", "\u5230", "\u9006", "\u2026\u7adf\u9047", "\ud835\udc04:", "Word", "Embedding", "\u2026", "\ud835\udc2f\ud835\udc8a$\ud835\udfd0", "\ud835\udc2f\ud835\udc8a$\ud835\udfcf", "\u2026", "\ud835\udc2f\ud835\udc8a", "\ud835\udc2f\ud835\udc8a$\ud835\udfd1", "BERT", "Extractor", "SpellGCN"], "regionBoundary": {"x2": 503.0, "y1": 62.8900146484375, "x1": 95.0, "y2": 204.8900146484375}, "caption": "Figure 1: The framework of the proposed SpellGCN. Left: The characters in the input sentence are processed by the extractor to obtain the semantic representation vectors. Right: The phonological or visual similarity knowledge of characters is learned by our SpellGCN. Two similarity graphs are used to model the pronunciation and shape similarities respectively, and they are combined via an attentive combination operation. Middle: The character embedding vectors derived from SpellGCN are used as the target character classifiers .", "page": 3}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 290.272216796875, "y1": 170.01052856445312, "x1": 70.67498779296875, "y2": 187.968017578125}, "imageText": ["(b)", "w/o", "SpellGCN", "75", "50", "25", "0", "-25", "-50", "-75", "-100", "-100", "-50", "0", "50", "100", "(a)", "w/", "SpellGCN", "40", "20", "0", "-20", "-40", "-60", "-60", "-40", "-20", "0", "20", "40", "60"], "regionBoundary": {"x2": 284.0, "y1": 64.8900146484375, "x1": 78.26586151123047, "y2": 154.6929931640625}, "caption": "Figure 4: The scatter of similar characters of \u201d\u957f\u201d and \u201c\u7940\u201d in terms of pronunciation by t-SNE.", "page": 8}, {"figType": "Figure", "name": "5", "captionBoundary": {"x2": 290.272216796875, "y1": 310.5085754394531, "x1": 70.67498779296875, "y2": 328.466064453125}, "imageText": ["(b)", "w/o", "SpellGCN", "100", "50", "0", "-50", "-100", "-100", "-50", "0", "50", "100", "(a)", "w/", "SpellGCN", "100", "50", "0", "-50", "-100", "-100", "-75", "-50", "-25", "0", "25", "50", "75", "100"], "regionBoundary": {"x2": 284.0, "y1": 205.8900146484375, "x1": 78.24231719970703, "y2": 295.1910400390625}, "caption": "Figure 5: The scatter of similar characters of \u201d\u957f\u201d and \u201c\u7940\u201d in terms of shape by t-SNE.", "page": 8}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 527.2001342773438, "y1": 228.12454223632812, "x1": 306.9670104980469, "y2": 258.03802490234375}, "imageText": ["Pronunciation", "Similarity", "Graph", "4753", "112,687", "Shape", "Similarity", "Graph", "4738", "115,561", "Graph", "#", "Character", "#", "Edges", "SIGHAN", "2013", "1000(1000)", "74.1", "1,227", "SIGHAN", "2014", "1062(526)", "50.1", "782", "SIGHAN", "2015", "1100(550)", "30.5", "715", "Test", "Data", "#", "Line", "Avg.", "Length", "#", "Errors", "Total", "281,379", "44.4", "397,378", "(Wang", "et", "al.,", "2018)", "271,329", "44.4", "382,704", "SIGHAN", "2013", "350", "49.2", "350", "SIGHAN", "2014", "6,526", "49.7", "10,087", "SIGHAN", "2015", "3,174", "30.0", "4,237", "Training", "Data", "#", "Line", "Avg.", "Length", "#", "Errors"], "regionBoundary": {"x2": 513.0, "y1": 62.8900146484375, "x1": 320.0, "y2": 215.8900146484375}, "caption": "Table 2: Statistics information of the used data resources. The number in the bracket in #Line column denotes the number of sentences with errors.", "page": 4}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 732.2227478027344, "y1": 412.2702280680338, "x1": 424.93192884657117, "y2": 570.0471666124132}, "name": "1", "caption_text": "Table 1: A CSC data sample from SIGHAN 2014 (Yu et al., 2014) with ID B1-3440-2, the incorrect/correct characters are in orange/blue. A BERT model modifies the text into a sentence that is semantically reasonable but dissimilar in pronunciation. By incorporating both phonological and visual similarities, our new method SpellGCN can generate a sentence that is both semantically sensible and phonically similar to the original sentence. The sentence output from SpellGCN means \u201cthis restaurant is very suitable for dating\u201d.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 724.0, "y1": 308.0, "x1": 426.0, "y2": 412.0}, "page": 0, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 730.4042392306858, "y1": 306.7854563395182, "x1": 99.99995761447482, "y2": 381.5403408474392}, "name": "1", "caption_text": "Figure 1: The framework of the proposed SpellGCN. Left: The characters in the input sentence are processed by the extractor to obtain the semantic representation vectors. Right: The phonological or visual similarity knowledge of characters is learned by our SpellGCN. Two similarity graphs are used to model the pronunciation and shape similarities respectively, and they are combined via an attentive combination operation. Middle: The character embedding vectors derived from SpellGCN are used as the target character classifiers .", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 699.0, "y1": 86.0, "x1": 132.0, "y2": 285.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.222408718533, "y1": 316.83964199490015, "x1": 426.3430701361762, "y2": 358.38614569769965}, "name": "2", "caption_text": "Table 2: Statistics information of the used data resources. The number in the bracket in #Line column denotes the number of sentences with errors.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 713.0, "y1": 86.0, "x1": 427.0, "y2": 317.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2234259711371, "y1": 453.07854546440973, "x1": 99.57083596123589, "y2": 511.2291547987196}, "name": "3", "caption_text": "Table 3: The performance of our method and baseline models (%). D, C denote the detection, correction, respectively. P, R, F denote the precision, recall and F1 score, respectively. The results of BERT are from our own implementation. Best results are in bold. We performed additional fine-tuning on SIGHAN13 for 6 epochs as the data distribution in SIGHAN13 differs from other datasets, e.g. \u201c\u7684\u201d, \u201c\u5f97\u201d and \u201c\u5730\u201d are rarely distinguished.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 642.0, "y1": 86.0, "x1": 188.0, "y2": 453.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9269782172308, "y1": 217.59385002983942, "x1": 99.57083596123589, "y2": 259.14035373263886}, "name": "4", "caption_text": "Table 4: The performance of BERT and SpellGCN evaluated by official tools on SIGHAN 2014 and SIGHAN 2015. FPR denotes the false positive rate and A denotes the accuracy. D-A and C-A denote detection accuracy and correction accuracy.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 601.0, "y1": 86.0, "x1": 225.0, "y2": 201.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 730.1608615451389, "y1": 490.80768161349823, "x1": 426.772223578559, "y2": 532.3527442084418}, "name": "2", "caption_text": "Figure 2: The test curves for sentence-level correction metrics with and without SpellGCN w.r.t. the number of training epochs on SIGHAN 2015.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 688.0, "y1": 313.0, "x1": 462.0, "y2": 470.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.15369500054254, "y1": 286.2466176350911, "x1": 98.96249771118164, "y2": 327.7917650010851}, "name": "3", "caption_text": "Figure 3: The character-level C-F results (%) w.r.t. the depth of SpellGCN. The results were obtained with 10K training samples.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 353.0, "y1": 96.0, "x1": 146.0, "y2": 258.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.925791422526, "y1": 307.6952192518446, "x1": 426.3430701361762, "y2": 382.4501037597656}, "name": "6", "caption_text": "Table 6: Several prediction results on the test set. The first line in the block is the input sentence. The second line is corrected by BERT without SpellGCN. And the last line is the result from SpellGCN. We highlight the incorrect/correct characters by orange/blue color.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 720.0, "y1": 86.0, "x1": 426.0, "y2": 307.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.45048183865015, "y1": 517.6576826307509, "x1": 99.57083596123589, "y2": 629.5652177598741}, "name": "5", "caption_text": "Table 5: The ablation results for graph combination method (%). The averaged character-level C-F scores of 4 runs on the SIGHAN 2013 are reported. The models were trained with 10K training samples. Mean pooling denotes that the output representation Cl of each layer is the average of fk\u2208{P,S}(Ak,Hl), while sum pooling summarizes fk\u2208{P,S}(Ak,Hl).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 345.0, "y1": 357.0, "x1": 159.0, "y2": 501.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.15585666232636, "y1": 236.125734117296, "x1": 98.15970526801215, "y2": 261.0666910807292}, "name": "4", "caption_text": "Figure 4: The scatter of similar characters of \u201d\u957f\u201d and \u201c\u7940\u201d in terms of pronunciation by t-SNE.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 402.0, "y1": 90.0, "x1": 100.0, "y2": 235.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.15585666232636, "y1": 431.2619103325738, "x1": 98.15970526801215, "y2": 456.2028672960069}, "name": "5", "caption_text": "Figure 5: The scatter of similar characters of \u201d\u957f\u201d and \u201c\u7940\u201d in terms of shape by t-SNE.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 394.0, "y1": 286.0, "x1": 108.0, "y2": 429.0}, "page": 8, "dpi": 0}], "error": null, "pdf": "/work/host-output/49b51b44f2b7bd10f7a006d7455e1a869d94a6f8/2020.acl-main.81.pdf", "dpi": 100}