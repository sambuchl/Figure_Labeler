{"raw_detected_boxes": [[], [{"x2": 384.0, "y1": 79.0, "x1": 128.0, "y2": 195.0}], [{"x2": 714.0, "y1": 724.0, "x1": 471.0, "y2": 895.0}], [{"x2": 718.0, "y1": 78.0, "x1": 448.0, "y2": 286.0}, {"x2": 712.0, "y1": 333.0, "x1": 481.0, "y2": 403.0}], [{"x2": 735.0, "y1": 119.0, "x1": 119.0, "y2": 171.0}, {"x2": 411.0, "y1": 249.0, "x1": 110.0, "y2": 578.0}], [{"x2": 407.0, "y1": 309.0, "x1": 110.0, "y2": 595.0}], [{"x2": 746.0, "y1": 76.0, "x1": 438.0, "y2": 351.0}], [{"x2": 406.0, "y1": 76.0, "x1": 104.0, "y2": 162.0}, {"x2": 751.0, "y1": 77.0, "x1": 436.0, "y2": 160.0}, {"x2": 753.0, "y1": 208.0, "x1": 435.0, "y2": 290.0}], [{"x2": 418.0, "y1": 76.0, "x1": 100.0, "y2": 185.0}, {"x2": 763.0, "y1": 91.0, "x1": 441.0, "y2": 483.0}, {"x2": 716.0, "y1": 790.0, "x1": 452.0, "y2": 876.0}], [{"x2": 751.0, "y1": 86.0, "x1": 108.0, "y2": 471.0}], [], []], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Figure", "name": "5", "captionBoundary": {"x2": 298.8052978515625, "y1": 432.08966064453125, "x1": 72.00080871582031, "y2": 462.0023498535156}, "imageText": [], "regionBoundary": {"x2": 298.0, "y1": 356.0, "x1": 79.0, "y2": 428.0}, "caption": "Figure 5: Example CRFs for targeted subjectivity with observed variables (dark nodes), predicted variables (white nodes) and hidden variables (light grey nodes).", "page": 5}, {"figType": "Figure", "name": "1", "captionBoundary": {"x2": 280.53387451171875, "y1": 144.71861267089844, "x1": 90.268798828125, "y2": 151.19927978515625}, "imageText": [], "regionBoundary": {"x2": 278.0, "y1": 57.0, "x1": 92.0, "y2": 140.0}, "caption": "Figure 1: Sentiment expressed across an entity.", "page": 1}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 540.0004272460938, "y1": 280.71063232421875, "x1": 313.2008056640625, "y2": 334.0553283691406}, "imageText": ["Sin", "a\u0301nimo", "de", "ofender", "a", "los", "Militares,", "que", "realmente", "se", "merecen", "ese", "aumento", "y", "ma\u0301s.", "Pero,", "do\u0301nde", "queda", "la", "misma", "recompensa", "para", "Me\u0301dicos.", "I", "do", "not", "intend", "to", "offend", "the", "military", "in", "the", "slightest,", "they", "truly", "deserve", "the", "raise", "and", "more.", "However,", "I\u2019m", "wondering", "whether", "doctors", "will", "ever", "receive", "a", "similar", "compensation.", "@[user]", "buenos", "d\u0131\u0301as", "Profe!!", "Nos", "quedamos", "acciden-", "tados", "otra", "vez", "en", "la", "carretera", "vieja", "guarenas", "echando", "gasoil,", "estamos", "a", "la", "interperie", "@[user]", "good", "morning,", "Prof!!", "We", "were", "wrecked", "again", "on", "the", "old", "guarenas", "highway", "while", "getting", "diesel,", "we\u2019re", "out", "in", "the", "open", "@[user]", "le", "dijo", "erralo", "muy", "por", "lo", "bajo", "jaja", "un", "grande", "juancito", "grandes", "amigos", "mios", "@[user]", "he", "told", "him", "it", "was", "very", "on", "the", "dl", "haha", "a", "great", "juancito", "great", "friends", "of", "mine"], "regionBoundary": {"x2": 538.361083984375, "y1": 57.18979263305664, "x1": 319.17779541015625, "y2": 274.47930908203125}, "caption": "Figure 2: Messages on Twitter use a wide range of formality, style, and errors, which makes extracting information particularly difficult. Examples from Spanish (screen names anonymized), with approximate translations in English.", "page": 1}, {"figType": "Table", "name": "5", "captionBoundary": {"x2": 492.57855224609375, "y1": 250.0975799560547, "x1": 360.623779296875, "y2": 256.5782470703125}, "imageText": ["SURFACE", "FEATURES", "binned", "word", "length,", "message", "length,", "and", "sen-", "tence", "position;", "Jerboa", "features;", "word", "identity;", "word", "lengthening;", "punctuation", "characters,", "has", "digit;", "has", "dash;", "is", "lower", "case;", "is", "3", "or", "4", "letters;", "\ufb01rst", "letter", "capi-", "talized;", "more", "than", "one", "letter", "capitalized,", "etc.", "LINGUISTIC", "FEATURES", "function", "words;", "can", "syllabify;", "curse", "words;", "laugh", "words;", "words", "for", "good,", "bad,", "no,", "my;", "slang", "words;", "ab-", "breviations;", "intensi\ufb01ers;", "subjective", "suf\ufb01xes", "and", "pre-", "\ufb01xes", "(such", "as", "diminutive", "forms);", "common", "verb", "end-", "ings;", "common", "noun", "endings", "BROWN", "CLUSTERING", "FEATURES", "cluster", "at", "length", "3;", "cluster", "at", "length", "5", "SENTIMENT", "FEATURES", "is", "sentiment-bearing", "word;", "prior", "sentiment", "polarity"], "regionBoundary": {"x2": 538.0, "y1": 55.0, "x1": 316.0, "y2": 247.0}, "caption": "Table 5: Features used in model.", "page": 6}, {"figType": "Table", "name": "13", "captionBoundary": {"x2": 403.2464599609375, "y1": 337.858642578125, "x1": 208.7567901611328, "y2": 344.3393249511719}, "imageText": ["Spanish:", "Salen", "del", "gobierno", "de", "Humala", "dos", "connotados", "izquierdistas,", "Giesecke", "y", "Eiguiguren", "Predicted:", "O", "O", "O", "O", "B-VOLITIONAL", "I-VOLITIONAL", "O", "O", "O", "B-VOLITIONAL", "O", "B-VOLITIONAL-", "-", "-", "-", "NOT-TARG", "NOT-TARG", "-", "-", "-", "NOT-TARG", "-", "NOT-TARG", "Gold:", "O", "O", "O", "O", "B-VOLITIONAL", "O", "O", "O", "O", "B-VOLITIONAL", "O", "B-VOLITIONAL-", "-", "-", "-", "NOT-TARG", "-", "-", "-", "-", "NEGATIVE", "-", "NOT-TARG", "English:", "Leaving", "the", "Humala", "government", "are", "two", "notorious", "leftists", ",", "Giesecke", "and", "Eiguiguren", "5.", "Sentiment", "and", "NE", "prediction", "errors", "3.", "Spanish:", "Mario", "que", "dio", "este", "contigo", "Predicted:", "NOT-TARG", "-", "-", "-", "-", "Gold:", "POSITIVE", "-", "-", "-", "-", "English:", "Mario", "may", "God", "be", "with", "you", "4.", "Spanish:", ".", ".", ".", "si", "de", "verdad", "estas", "en", "cielo", ",", "ayudame", "Superman", "!!!", "Predicted:", "-", "-", "-", "-", "-", "-", "-", "-", "POSITIVE", "-", "Gold:", "-", "-", "-", "-", "-", "-", "-", "-", "NOT-TARG", "-", "English:", ".", ".", ".", "if", "you", "really", "are", "in", "the", "skies", ",", "help", "me", "Superman", "!!!", "Sentiment", "prediction", "errors", "2.", "Spanish:", "Matias", "del", "r\u0131\u0301o", "fue", "una", "lata", ".", ".", ".", "Predicted:", "O", "O", "O", "O", "O", "O", ".", ".", ".", "Gold:", "B-VOLITIONAL", "I-VOLITIONAL", "I-VOLITIONAL", "O", "O", "O", ".", ".", ".", "English:", "Matias", "del", "r\u0131\u0301o", "was", "a", "drag", ".", ".", ".", "1.", "Spanish:", "Cuando", "estoy", "CANSADO", ",", "e\u0301l", "es", "mi", "DESCANSO", ".", "Mateo", ".", "11", ":", "29", ".", "Predicted:", "O", "O", "B-VOLITIONAL", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "Gold:", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-VOLITIONAL", "O", "O", "O", "O", "O", "English:", "When", "I\u2019m", "TIRED", ",", "he", "is", "my", "REST", ".", "Matthew", ".", "11", ":", "29", ".", "NE", "prediction", "errors"], "regionBoundary": {"x2": 544.0, "y1": 57.80558395385742, "x1": 77.97879791259766, "y2": 331.0}, "caption": "Table 13: Example errors made by joint models.", "page": 9}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 540.005859375, "y1": 644.378662109375, "x1": 313.20074462890625, "y2": 709.4393310546875}, "imageText": ["NE", "COUNT", "NEUTRAL", "POS", "NEG", "PERSON", "5462", "80%", "20%", "0%", "ORGANIZATION", "4408", "80%", "20%", "0%", "LOCATION", "1405", "100%", "0%", "0%", "URL", "1030", "100%", "0%", "0%", "TIME", "535", "70%", "10%", "20%", "DATE", "222", "100%", "0%", "0%", "MONEY", "95", "90%", "0%", "10%", "PERCENT", "81", "80%", "20%", "0%", "TELEPHONE", "23", "100%", "0%", "0%", "EMAIL", "8", "100%", "0%", "0%"], "regionBoundary": {"x2": 514.0, "y1": 521.0, "x1": 339.0, "y2": 642.0}, "caption": "Table 1: Distribution of named entities in our Spanish Twitter corpus. Targeted sentiment percentages are based on expert annotations from a random sample of 10 (or all) of of each entity. Most entities are not sentiment targets (NEUTRAL). PERSON and ORGANIZATION are most frequent, and among the top recipients of sentiment.", "page": 2}, {"figType": "Table", "name": "6", "captionBoundary": {"x2": 298.7968444824219, "y1": 112.67759704589844, "x1": 72.00080108642578, "y2": 130.874267578125}, "imageText": ["Spanish", "English", "Model", "Joint", "Pipe", "Coll", "Joint", "Pipe", "Coll", "NE", "prec", "65.2", "64.3", "65.1", "59.8", "62.3", "60.5", "NE", "rec", "65.8", "64.7", "61.2", "60.2", "57.2", "56.5", "NE", "spec", "95.4", "95.2", "95.6", "94.3", "95.1", "94.7"], "regionBoundary": {"x2": 295.0, "y1": 54.0, "x1": 76.0, "y2": 110.0}, "caption": "Table 6: Average precision, recall, and specificity for volitional entity NER (in %).", "page": 7}, {"figType": "Table", "name": "7", "captionBoundary": {"x2": 539.999267578125, "y1": 112.67759704589844, "x1": 313.2008056640625, "y2": 142.59027099609375}, "imageText": ["Spanish", "English", "Model", "Joint", "Pipe", "Coll", "Joint", "Pipe", "Coll", "Subj", "prec", "58.3", "58.8", "58.9", "46.6", "52.2", "45.9", "Subj", "rec", "40.1", "50.9", "19.1", "44.5", "48.5", "16.4", "Subj", "spec", "79.6", "77.5", "77.8", "77.6", "80.8", "74.0"], "regionBoundary": {"x2": 544.0, "y1": 54.0, "x1": 313.0, "y2": 110.0}, "caption": "Table 7: Average precision, recall, and specificity (in %) for subjectivity prediction (has/does not have sentiment) along the target entity.", "page": 7}, {"figType": "Table", "name": "8", "captionBoundary": {"x2": 539.999267578125, "y1": 207.80857849121094, "x1": 313.2008056640625, "y2": 237.72125244140625}, "imageText": ["Spanish", "English", "Model", "Joint", "Pipe", "Coll", "Joint", "Pipe", "Coll", "Sent", "prec", "36.6", "45.8", "42.5", "31.6", "42.9", "38.5", "Sent", "rec", "38.0", "40.6", "15.5", "36.6", "34.8", "9.7", "Sent", "spec", "67.1", "75.2", "73.3", "72.3", "82.0", "78.1"], "regionBoundary": {"x2": 543.0, "y1": 150.0, "x1": 313.0, "y2": 205.0}, "caption": "Table 8: Average precision, recall, and specificity (in %) for sentiment prediction (positive/negative/no sentiment) along the target entity.", "page": 7}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 539.9993896484375, "y1": 296.8796081542969, "x1": 313.2008056640625, "y2": 361.9403076171875}, "imageText": ["NEUTRAL", "707", "2151", "473", "NEG", "129", "726", "452", "or", "ity", "POS", "757", "1249", "130", "M", "in", "Majority", "POS", "NEUTRAL", "NEG"], "regionBoundary": {"x2": 513.0, "y1": 237.6656494140625, "x1": 350.8298034667969, "y2": 291.8282775878906}, "caption": "Table 2: Number of targeted sentiment instances where at least two of the three annotators (Majority) agreed. Common disagreements with a third annotator (Minority) were over whether no sentiment or positive sentiment was expressed, and whether no sentiment or negative sentment was expressed.", "page": 3}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 531.327880859375, "y1": 216.51463317871094, "x1": 321.8747863769531, "y2": 222.99530029296875}, "imageText": ["Positive", "Negative", "Neutral", "25", "00", "20", "00", "15", "00", "10", "00", "0", "50", "0", "ee", "ts", "T", "w", "y", "in", "ue", "nc", "Fr", "eq", "ORGANIZATION", "PERSON", "Named", "Entity"], "regionBoundary": {"x2": 517.0, "y1": 55.19829177856445, "x1": 325.164794921875, "y2": 206.02227783203125}, "caption": "Figure 4: Targeted sentiment annotated for Spanish.", "page": 3}, {"figType": "Table", "name": "10", "captionBoundary": {"x2": 298.7993469238281, "y1": 290.0415954589844, "x1": 72.00080108642578, "y2": 355.102294921875}, "imageText": ["*p<.05", "E", "ng", "Acc-all", "88.0", "88.1", "88.2", "88.4", "87.7", "88.1Acc-Bsent", "30.4", "30.6", "30.5", "30.8", "27.9", "29.8", "Sp", "a", "Acc-all", "89.4", "89.4", "89.0", "89.0", "89.2", "89.3", "Acc-Bsent", "29.7*", "29.0", "30.0", "29.2", "28.9", "29.0", "Model", "Joint", "Joint", "Base", "Pipe", "Pipe", "Base", "Coll", "Coll", "Base"], "regionBoundary": {"x2": 298.0, "y1": 208.0, "x1": 73.0, "y2": 284.1022644042969}, "caption": "Table 10: Average accuracy on Targeted Sentiment Prediction: Identifying volitional entities and the polarity of the sentiment expressed towards them. The Spanish JOINT models significantly improve over their baseline for the core task. In English, no models outperform their baseline.", "page": 8}, {"figType": "Table", "name": "11", "captionBoundary": {"x2": 540.0011596679688, "y1": 340.9085998535156, "x1": 313.2008056640625, "y2": 394.2532958984375}, "imageText": ["Positive", "not", "followed", "by", "sentiment", "lexicon", "word", "a", "negative", "pre\ufb01x;", "is", "preceded", "by", "a", "question", "mark;", "is", "immediately", "preceded", "by", "a", "curse", "word", "or", "laugh;", "is", "followed", "by", "an", "exclamation", "mark", "B-VOLITIONAL,", "NOT-TARG", "FEATURES", "Negative", "immediately", "followed", "by", "a", "\u2018no\u2019", "word", "or", "word", "with", "or", "more", "sentiment", "lexicon", "items"], "regionBoundary": {"x2": 550.0, "y1": 256.8050231933594, "x1": 313.0, "y2": 338.0}, "caption": "Table 11: Example strongly weighted features for a Spanish joint sentiment model. In addition to lexical identity, we find that curse words and positive and negative prefixes are used to detect volitional entities and the sentiment directed towards them.", "page": 8}, {"figType": "Table", "name": "12", "captionBoundary": {"x2": 540.0010986328125, "y1": 644.3786010742188, "x1": 313.2007751464844, "y2": 709.4392700195312}, "imageText": ["NEUT", "115", "61", "468", "POS", "68", "24", "42", "NEG", "58", "65", "102", "POS", "NEG", "NEUT", "b.", "Observed", "ic", "te", "d", "B", "423", "21", "186", "I", "36", "236", "135", "O", "197", "90", "7168", "Pr", "ed", "B", "I", "O", "a.", "Observed"], "regionBoundary": {"x2": 516.0, "y1": 569.4639282226562, "x1": 323.3558044433594, "y2": 635.8782958984375}, "caption": "Table 12: Predicted vs. observed values for a joint model. (a) For named entities, most common confusions were between B-VOLITIONAL and O labels. (b) For sentiment, most common mistakes were to predict that a positive sentiment was neutral (no sentiment), and that a neutral sentiment was negative.", "page": 8}, {"figType": "Table", "name": "9", "captionBoundary": {"x2": 298.8037109375, "y1": 136.94664001464844, "x1": 72.00077056884766, "y2": 202.00732421875}, "imageText": ["***p<.001", "**p<.01", "*p<.05", "E", "ng", "Acc-all", "88.0", "88.1", "88.6", "88.6", "87.9", "88.1Acc-Bsent", "30.4", "30.8", "30.7", "30.3", "28.1", "29.2", "Sp", "a", "Acc-all", "89.5*", "89.3", "89.3**", "89.1", "89.5*", "89.3", "Acc-Bsent", "32.1***", "29.5", "30.9***", "28.3", "30.1**", "28.1", "Model", "Joint", "Joint", "Base", "Pipe", "Pipe", "Base", "Coll", "Coll", "Base"], "regionBoundary": {"x2": 301.0, "y1": 54.0, "x1": 72.0, "y2": 131.00830078125}, "caption": "Table 9: Average accuracy on Targeted Subjectivity Prediction: Identifying volitional entities and whether they are a sentiment target. In the core task, Acc-Bsent, the best model in Spanish is JOINT, significantly outperforming the baseline. In English, the best model (PIPE) does not significantly improve over its baseline.", "page": 8}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 298.8056945800781, "y1": 402.0256042480469, "x1": 72.00080108642578, "y2": 431.93829345703125}, "imageText": ["Variable", "Possible", "values", "Sentiment", "(s)", "NOT-TARG,", "POS,", "NEG", "(PIPE", "&", "JOINT", "models)", "Named", "Entity", "(l)", "O,", "B-VOLITIONAL,", "I-VOLITIONAL", "(PIPE", "&", "JOINT", "models)", "Combined", "Sent/NE", "(y)", "O,", "B+NOT-TARG,", "I+NOT-TARG", "(COLL", "models)", "B+POS,", "I+POS", "B+NEG,", "I+NEG"], "regionBoundary": {"x2": 297.0, "y1": 303.0, "x1": 73.0, "y2": 399.0}, "caption": "Table 4: Possible values for random variables, targeted sentiment. The COLL models collapse both targeted sentiment and NE label into one node.", "page": 4}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 394.9225769042969, "y1": 159.7676239013672, "x1": 217.07980346679688, "y2": 166.248291015625}, "imageText": [], "regionBoundary": {"x2": 539.0, "y1": 54.0, "x1": 73.0, "y2": 158.0}, "caption": "Figure 3: Example Tweet shown to Turkers.", "page": 4}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 298.8007507324219, "y1": 267.3955993652344, "x1": 72.00080108642578, "y2": 297.30828857421875}, "imageText": ["Variable", "Possible", "values", "Sentiment", "(s)", "NOT-TARG,", "SENT-TARG", "(PIPE", "&", "JOINT", "models)", "Named", "Entity", "(l)", "O,", "B-VOLITIONAL,", "I-VOLITIONAL", "(PIPE", "&", "JOINT", "models)", "Combined", "Sent/NE", "(y)", "O,", "B+NOT-TARG,", "I+NOT-TARG", "(COLL", "models)", "B+SENT-TARG,", "I+SENT-TARG"], "regionBoundary": {"x2": 297.0, "y1": 180.0, "x1": 73.0, "y2": 265.0}, "caption": "Table 3: Possible values for random variables, targeted subjectivity (is/is not sentiment target). COLL models collapse targeted subjectivity and NE label into one node.", "page": 4}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 389.630381266276, "y1": 200.9980731540256, "x1": 125.37333170572916, "y2": 209.9989997016059}, "name": "1", "caption_text": "Figure 1: Sentiment expressed across an entity.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 389.0, "y1": 79.0, "x1": 125.0, "y2": 212.0}, "page": 1, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 750.0081380208333, "y1": 894.9703640407986, "x1": 435.0010342068142, "y2": 985.3324042426215}, "name": "1", "caption_text": "Table 1: Distribution of named entities in our Spanish Twitter corpus. Targeted sentiment percentages are based on expert annotations from a random sample of 10 (or all) of of each entity. Most entities are not sentiment targets (NEUTRAL). PERSON and ORGANIZATION are most frequent, and among the top recipients of sentiment.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 731.0, "y1": 724.0, "x1": 454.0, "y2": 912.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 737.9553900824652, "y1": 300.7147683037652, "x1": 447.0483144124349, "y2": 309.71569485134546}, "name": "4", "caption_text": "Figure 4: Targeted sentiment annotated for Spanish.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 731.0, "y1": 78.0, "x1": 447.0, "y2": 303.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 749.9991522894965, "y1": 412.33278910319007, "x1": 435.00111897786456, "y2": 502.69487169053815}, "name": "2", "caption_text": "Table 2: Number of targeted sentiment instances where at least two of the three annotators (Majority) agreed. Common disagreements with a third annotator (Minority) were over whether no sentiment or positive sentiment was expressed, and whether no sentiment or negative sentment was expressed.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 729.0, "y1": 329.0, "x1": 464.0, "y2": 420.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 548.5035790337456, "y1": 221.89947764078775, "x1": 301.4997270372179, "y2": 230.90040418836804}, "name": "3", "caption_text": "Figure 3: Example Tweet shown to Turkers.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 744.0, "y1": 102.0, "x1": 104.0, "y2": 188.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 415.0079091389974, "y1": 558.368894788954, "x1": 100.0011126200358, "y2": 599.9142964680989}, "name": "4", "caption_text": "Table 4: Possible values for random variables, targeted sentiment. The COLL models collapse both targeted sentiment and NE label into one node.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 415.0, "y1": 232.0, "x1": 100.0, "y2": 595.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 415.0073581271701, "y1": 600.1245286729601, "x1": 100.0011232164171, "y2": 641.669930352105}, "name": "5", "caption_text": "Figure 5: Example CRFs for targeted subjectivity with observed variables (dark nodes), predicted variables (white nodes) and hidden variables (light grey nodes).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 415.0, "y1": 294.0, "x1": 100.0, "y2": 612.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 684.1368781195746, "y1": 347.35774993896484, "x1": 500.8663601345486, "y2": 356.3586764865451}, "name": "5", "caption_text": "Table 5: Features used in model.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 747.0, "y1": 76.0, "x1": 438.0, "y2": 357.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 414.995617336697, "y1": 156.49666256374783, "x1": 100.0011126200358, "y2": 181.76981608072916}, "name": "6", "caption_text": "Table 6: Average precision, recall, and specificity for volitional entity NER (in %).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 414.0, "y1": 76.0, "x1": 100.0, "y2": 179.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 749.9989827473958, "y1": 156.49666256374783, "x1": 435.00111897786456, "y2": 198.0420430501302}, "name": "7", "caption_text": "Table 7: Average precision, recall, and specificity (in %) for subjectivity prediction (has/does not have sentiment) along the target entity.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 756.0, "y1": 76.0, "x1": 435.0, "y2": 177.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 749.9989827473958, "y1": 288.6230256822374, "x1": 435.00111897786456, "y2": 330.16840616861975}, "name": "8", "caption_text": "Table 8: Average precision, recall, and specificity (in %) for sentiment prediction (positive/negative/no sentiment) along the target entity.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 754.0, "y1": 191.0, "x1": 435.0, "y2": 307.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 415.0051540798611, "y1": 190.20366668701172, "x1": 100.00107023451064, "y2": 280.56572808159723}, "name": "9", "caption_text": "Table 9: Average accuracy on Targeted Subjectivity Prediction: Identifying volitional entities and whether they are a sentiment target. In the core task, Acc-Bsent, the best model in Spanish is JOINT, significantly outperforming the baseline. In English, the best model (PIPE) does not significantly improve over its baseline.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 418.0, "y1": 76.0, "x1": 100.0, "y2": 202.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 750.0016106499565, "y1": 473.4841664632161, "x1": 435.00111897786456, "y2": 547.5740220811632}, "name": "11", "caption_text": "Table 11: Example strongly weighted features for a Spanish joint sentiment model. In addition to lexical identity, we find that curse words and positive and negative prefixes are used to detect volitional entities and the sentiment directed towards them.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 764.0, "y1": 77.0, "x1": 435.0, "y2": 500.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 750.0015258789062, "y1": 894.9702792697482, "x1": 435.0010765923394, "y2": 985.3323194715712}, "name": "12", "caption_text": "Table 12: Predicted vs. observed values for a joint model. (a) For named entities, most common confusions were between B-VOLITIONAL and O labels. (b) For sentiment, most common mistakes were to predict that a positive sentiment was neutral (no sentiment), and that a neutral sentiment was negative.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 716.0, "y1": 790.0, "x1": 443.0, "y2": 883.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 560.0645277235243, "y1": 469.24811469184027, "x1": 289.9399863349067, "y2": 478.24906243218317}, "name": "13", "caption_text": "Table 13: Example errors made by joint models.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 755.0, "y1": 79.0, "x1": 108.0, "y2": 481.0}, "page": 9, "dpi": 0}], "error": null, "pdf": "/work/host-output/4a0c703fe51f60ae4e9b980b05e60046897a4b8c/D13-1171.pdf", "dpi": 100}