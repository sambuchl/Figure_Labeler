{"raw_detected_boxes": [[], [], [], [{"x2": 603.0, "y1": 181.0, "x1": 222.0, "y2": 332.0}], [{"x2": 600.0, "y1": 181.0, "x1": 224.0, "y2": 379.0}], [{"x2": 490.0, "y1": 482.0, "x1": 346.0, "y2": 573.0}], [], [{"x2": 608.0, "y1": 263.0, "x1": 201.0, "y2": 581.0}], [], [{"x2": 584.0, "y1": 197.0, "x1": 249.0, "y2": 260.0}], [], [{"x2": 655.0, "y1": 196.0, "x1": 184.0, "y2": 339.0}, {"x2": 624.0, "y1": 692.0, "x1": 205.0, "y2": 889.0}], [{"x2": 645.0, "y1": 364.0, "x1": 177.0, "y2": 623.0}], [{"x2": 626.0, "y1": 188.0, "x1": 210.0, "y2": 423.0}], [{"x2": 632.0, "y1": 195.0, "x1": 202.0, "y2": 365.0}, {"x2": 591.0, "y1": 537.0, "x1": 249.0, "y2": 680.0}], [{"x2": 628.0, "y1": 250.0, "x1": 205.0, "y2": 420.0}, {"x2": 557.0, "y1": 747.0, "x1": 286.0, "y2": 893.0}], [], []], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Table", "name": "1", "captionBoundary": {"x2": 474.77996826171875, "y1": 311.6905822753906, "x1": 117.59994506835938, "y2": 342.5998229980469}, "imageText": ["P1", "P2", "F1", "0.2", "0.1", "F2", "0.15", "0.3", "F3", "0.2", "0.1", "F4", "0.3", "0.4", "F5", "0.15", "0.1"], "regionBoundary": {"x2": 353.0, "y1": 346.0, "x1": 247.0, "y2": 415.0}, "caption": "Table 1. An example of feature subspace generation. When a threshold value of 0.2 is applied, the generated feature subspaces are {F1, F3, F4} and {F2, F4}.", "page": 5}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 471.76959228515625, "y1": 128.4508056640625, "x1": 133.32000732421875, "y2": 135.36004638671875}, "imageText": ["Surprise", "73.55%", "62.54%", "68.05%", "Disgust", "76.32%", "68.79%", "72.56%", "Neutral", "88.38%", "77.53%", "82.96%", "Average", "79.45%", "73.43%", "76.44%", "Happiness", "78.85%", "71.90%", "75.37%", "Sadness", "85.40%", "88.04%", "86.72%", "Anger", "81.52%", "75.00%", "78.26%", "Fear", "72.13%", "70.18%", "71.16%", "Male", "Female", "Average", "Recognition", "rate"], "regionBoundary": {"x2": 455.0, "y1": 140.0, "x1": 145.0, "y2": 263.0}, "caption": "Table 4. Emotion recognition results obtained, based on acoustic information.", "page": 14}, {"figType": "Table", "name": "5", "captionBoundary": {"x2": 473.97784423828125, "y1": 361.7308044433594, "x1": 138.72000122070312, "y2": 379.9200439453125}, "imageText": ["Total", "76.44%", "65.90%", "Sadness", "86.72%", "75.91%", "Anger", "78.26%", "66.57%", "Fear", "71.16%", "60.55%", "Surprise", "68.05%", "55.62%", "Disgust", "72.56%", "64.54%", "Neutral", "82.96%", "70.01%", "Multiple", "SVM", "Single", "SVM", "Happiness", "75.37%", "68.13%"], "regionBoundary": {"x2": 427.0, "y1": 385.0, "x1": 173.0, "y2": 490.0}, "caption": "Table 5. A comparison of the results obtained using the acoustic module with a single SVM and multiple SVMs.", "page": 14}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 464.2041015625, "y1": 128.45074462890625, "x1": 138.71998596191406, "y2": 135.3599853515625}, "imageText": ["423", "64", "9", "496", "Total", "emotional", "keyword", "1", "2", "3", "Number", "of", "tagged", "emotion", "labels", "of", "an"], "regionBoundary": {"x2": 421.0, "y1": 140.0, "x1": 179.0, "y2": 189.0}, "caption": "Table 2. The ambiguity of tagged emotion labels for an emotional keyword.", "page": 9}, {"figType": "Figure", "name": "6", "captionBoundary": {"x2": 473.7766418457031, "y1": 330.2907409667969, "x1": 143.27999877929688, "y2": 349.4399719238281}, "imageText": ["Ratio", "of", "selected", "emotion", "keywords", "(%)", ")", "te", "(%", "on", "ra", "gn", "iti", "re", "co", "io", "n", "m", "ot", "ua", "l", "e", "Te", "xt", "50", "55", "60", "65", "70", "75", "80", "85", "90", "95", "100", "100", "90", "80", "70", "60", "50", "40", "30", "20", "10", "0"], "regionBoundary": {"x2": 450.62994384765625, "y1": 135.09046936035156, "x1": 153.39999389648438, "y2": 321.4800109863281}, "caption": "Figure 6. The relationship between the keyword recognition rate and the emotion recognition rate.", "page": 13}, {"figType": "Figure", "name": "5", "captionBoundary": {"x2": 473.7332458496094, "y1": 455.6908264160156, "x1": 143.280029296875, "y2": 474.7200622558594}, "imageText": ["R2", "ac", "e", "ub", "sp", "re", "s", "ea", "tu", "o", "f", "f", "be", "rs", "N", "um", "85%", "87%", "89%", "91%", "93%", "95%", "97%", "99%", "100%", "10", "15", "20", "25", "30", "35", "40", "0", "5"], "regionBoundary": {"x2": 464.0, "y1": 262.0714416503906, "x1": 132.63999938964844, "y2": 448.32000732421875}, "caption": "Figure 5. The relationship between R2 and the number of feature sub-spaces.", "page": 12}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 427.87322998046875, "y1": 433.2507629394531, "x1": 174.72000122070312, "y2": 440.1600036621094}, "imageText": [], "regionBoundary": {"x2": 439.0, "y1": 190.0, "x1": 143.0, "y2": 421.0}, "caption": "Figure 3. Diagram of textual emotion recognition module.", "page": 7}, {"figType": "Figure", "name": "1", "captionBoundary": {"x2": 429.07171630859375, "y1": 250.49078369140625, "x1": 164.27999877929688, "y2": 257.4000244140625}, "imageText": [], "regionBoundary": {"x2": 437.0, "y1": 130.0, "x1": 155.0, "y2": 240.0}, "caption": "Figure 1. Diagram of the acoustic feature extraction module.", "page": 3}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 448.87103271484375, "y1": 648.6507568359375, "x1": 141.60000610351562, "y2": 692.1599731445312}, "imageText": [], "regionBoundary": {"x2": 450.0, "y1": 493.0, "x1": 142.0, "y2": 645.0}, "caption": "Figure 4. Emotion recognition rates for acoustic features under different PCA thresholds. The black line indicates the results obtained when R2 = 91%, and the two gray lines indicate the results obtained when R2 = 85% and 100%.", "page": 11}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 356.8055114746094, "y1": 128.45074462890625, "x1": 128.03997802734375, "y2": 135.3599853515625}, "imageText": ["Sadness", "121", "92", "Anger", "98", "80", "Fear", "60", "58", "Surprise", "196", "172", "Disgust", "106", "113", "Neutral", "1617", "1530", "Male", "Female", "Happiness", "126", "121", "Number", "of", "tagged", "sentences"], "regionBoundary": {"x2": 472.0, "y1": 140.0, "x1": 128.0, "y2": 244.0}, "caption": "Table 3. Tagged emotion labels in the testing corpus.", "page": 11}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 474.2122497558594, "y1": 284.33074951171875, "x1": 143.27999877929688, "y2": 327.8399658203125}, "imageText": [], "regionBoundary": {"x2": 432.0, "y1": 130.0, "x1": 161.0, "y2": 275.0}, "caption": "Figure 2. The ratio of up-slope sample number to the down-slope sample number. Two contours with the same wavelength are shown in parts A and B; the square symbols indicate the up-slope sample, and the circle symbols indicate the down-slope sample.", "page": 4}, {"figType": "Table", "name": "6", "captionBoundary": {"x2": 453.92120361328125, "y1": 167.93072509765625, "x1": 143.99998474121094, "y2": 174.8399658203125}, "imageText": ["Surprise", "66.85%", "58.46%", "62.66%", "Disgust", "57.12%", "55.34%", "56.23%", "Neutral", "77.98%", "64.76%", "71.37%", "Average", "67.79%", "63.17%", "65.48%", "Happiness", "66.35%", "63.64%", "64.99%", "Sadness", "59.12%", "61.96%", "60.54%", "Anger", "76.09%", "72.50%", "74.29%", "Fear", "71.03%", "65.51%", "68.27%", "Male", "Female", "Average", "Recognition", "rate"], "regionBoundary": {"x2": 455.0, "y1": 179.0, "x1": 145.0, "y2": 303.0}, "caption": "Table 6. Emotion recognition results obtained based on textual content.", "page": 15}, {"figType": "Table", "name": "7", "captionBoundary": {"x2": 460.5487976074219, "y1": 530.0908813476562, "x1": 138.72000122070312, "y2": 537.0001220703125}, "imageText": ["Surprise", "80.33%", "69.52%", "Disgust", "76.51%", "70.43%", "Neutral", "88.24%", "76.84%", "Average", "81.49%", "69.63%", "Happiness", "84.44%", "66.67%", "Sadness", "82.98%", "73.91%", "Anger", "79.66%", "67.65%", "Fear", "78.24%", "62.37%", "Inside", "Outside"], "regionBoundary": {"x2": 401.0, "y1": 542.0, "x1": 200.0, "y2": 646.0}, "caption": "Table 7. Emotion recognition results obtained using the integrated system.", "page": 15}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 595.9329393174913, "y1": 347.9038662380642, "x1": 228.16666497124564, "y2": 357.5000339084201}, "name": "1", "caption_text": "Figure 1. Diagram of the acoustic feature extraction module.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 606.0, "y1": 181.0, "x1": 217.0, "y2": 349.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 658.6281246609158, "y1": 394.903818766276, "x1": 198.99999830457898, "y2": 455.3332858615451}, "name": "2", "caption_text": "Figure 2. The ratio of up-slope sample number to the down-slope sample number. Two contours with the same wavelength are shown in parts A and B; the square symbols indicate the up-slope sample, and the circle symbols indicate the down-slope sample.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 600.0, "y1": 180.0, "x1": 207.0, "y2": 396.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 659.4166225857205, "y1": 432.9035864935981, "x1": 163.333257039388, "y2": 475.8330874972873}, "name": "1", "caption_text": "Table 1. An example of feature subspace generation. When a threshold value of 0.2 is applied, the generated feature subspaces are {F1, F3, F4} and {F2, F4}.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 491.0, "y1": 480.0, "x1": 343.0, "y2": 576.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 594.2683749728733, "y1": 601.7371707492405, "x1": 242.66666836208768, "y2": 611.3333384195963}, "name": "3", "caption_text": "Figure 3. Diagram of textual emotion recognition module.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 608.0, "y1": 263.0, "x1": 199.0, "y2": 584.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 644.7279188368055, "y1": 178.403811984592, "x1": 192.66664716932507, "y2": 187.9999796549479}, "name": "2", "caption_text": "Table 2. The ambiguity of tagged emotion labels for an emotional keyword.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 601.0, "y1": 180.0, "x1": 232.0, "y2": 262.0}, "page": 9, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 495.5632103814019, "y1": 178.403811984592, "x1": 177.8333028157552, "y2": 187.9999796549479}, "name": "3", "caption_text": "Table 3. Tagged emotion labels in the testing corpus.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 655.0, "y1": 179.0, "x1": 178.0, "y2": 339.0}, "page": 11, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 623.4319898817274, "y1": 900.903828938802, "x1": 196.6666751437717, "y2": 961.3332960340712}, "name": "4", "caption_text": "Figure 4. Emotion recognition rates for acoustic features under different PCA thresholds. The black line indicates the results obtained when R2 = 91%, and the two gray lines indicate the results obtained when R2 = 85% and 100%.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 624.0, "y1": 686.0, "x1": 198.0, "y2": 906.0}, "page": 11, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 657.9628414577908, "y1": 632.9039255777994, "x1": 199.00004069010416, "y2": 659.3334197998047}, "name": "5", "caption_text": "Figure 5. The relationship between R2 and the number of feature sub-spaces.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 657.0, "y1": 347.0, "x1": 163.0, "y2": 640.0}, "page": 12, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 658.0231136745876, "y1": 458.7371402316623, "x1": 198.99999830457898, "y2": 485.33329433865015}, "name": "6", "caption_text": "Figure 6. The relationship between the keyword recognition rate and the emotion recognition rate.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 626.0, "y1": 188.0, "x1": 207.0, "y2": 440.0}, "page": 13, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 655.2355448404948, "y1": 178.40389675564236, "x1": 185.1666768391927, "y2": 188.00006442599826}, "name": "4", "caption_text": "Table 4. Emotion recognition results obtained, based on acoustic information.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 649.0, "y1": 178.0, "x1": 185.0, "y2": 366.0}, "page": 14, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 658.3025614420573, "y1": 502.4038950602213, "x1": 192.66666836208768, "y2": 527.6667277018229}, "name": "5", "caption_text": "Table 5. A comparison of the results obtained using the acoustic module with a single SVM and multiple SVMs.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 593.0, "y1": 520.0, "x1": 240.0, "y2": 680.0}, "page": 14, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 630.4461161295573, "y1": 233.23711819118924, "x1": 199.9999788072374, "y2": 242.83328586154514}, "name": "6", "caption_text": "Table 6. Emotion recognition results obtained based on textual content.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 632.0, "y1": 233.0, "x1": 200.0, "y2": 420.0}, "page": 15, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 639.6511077880859, "y1": 736.2373352050781, "x1": 192.66666836208768, "y2": 745.833502875434}, "name": "7", "caption_text": "Table 7. Emotion recognition results obtained using the integrated system.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 574.0, "y1": 736.0, "x1": 269.0, "y2": 897.0}, "page": 15, "dpi": 0}], "error": null, "pdf": "/work/host-output/7c458fbdd1e4ee2f3226194db1d3758cf60ab836/O04-3004.pdf", "dpi": 100}