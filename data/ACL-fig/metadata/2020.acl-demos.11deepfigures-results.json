{"raw_detected_boxes": [[{"x2": 726.0, "y1": 312.0, "x1": 427.0, "y2": 510.0}], [{"x2": 725.0, "y1": 90.0, "x1": 102.0, "y2": 421.0}, {"x2": 390.0, "y1": 913.0, "x1": 107.0, "y2": 972.0}], [{"x2": 725.0, "y1": 96.0, "x1": 102.0, "y2": 485.0}], [], [{"x2": 402.0, "y1": 88.0, "x1": 101.0, "y2": 171.0}, {"x2": 730.0, "y1": 679.0, "x1": 427.0, "y2": 753.0}], [{"x2": 727.0, "y1": 95.0, "x1": 428.0, "y2": 424.0}], [{"x2": 396.0, "y1": 240.0, "x1": 107.0, "y2": 289.0}], [], [], []], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Figure", "name": "1", "captionBoundary": {"x2": 527.2003173828125, "y1": 382.7215576171875, "x1": 307.2760009765625, "y2": 400.67901611328125}, "imageText": [], "regionBoundary": {"x2": 526.0, "y1": 221.8900146484375, "x1": 307.0, "y2": 370.8900146484375}, "caption": "Figure 1: An example of cross-media knowledge fusion and a look inside the visual knowledge extraction.", "page": 0}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 527.2001342773438, "y1": 317.4155578613281, "x1": 306.697998046875, "y2": 419.0589294433594}, "imageText": ["Crossmedia", "Coreference", "Flickr30k", "Acc", "69.2%", "Visual", "Entity", "Coreference", "YoutubeBB", "Acc", "84.9%", "Flags", "AIDA", "F1", "72.0%", "Faces", "LFW", "Acc", "99.6%", "Landmarks", "Oxf105k", "mAP", "88.5%", "Visual", "Entity", "Linking", "Objects", "MSCOCO", "mAP", "43.0%", "Faces", "FDDB", "Acc", "95.4%", "Visual", "Entity", "Extraction", "En", "Trigger", "ERE", "F1", "65.4%", "Argument", "ERE", "F1", "85.0%", "Ru", "Trigger", "AIDA", "F1", "56.2%", "Argument", "AIDA", "F1", "58.2%", "Uk", "Trigger", "AIDA", "F1", "59.0%", "Argument", "AIDA", "F1", "61.1%", "Event", "Extraction", "English", "ACE&ERE", "F1", "65.6%", "Russian", "AIDA", "F1", "72.4%", "Ukrainian", "AIDA", "F1", "68.2%", "Relation", "Extraction", "Mention", "Extraction", "CoNLL-2003", "F1", "91.8%", "Component", "Benchmark", "Metric", "Score"], "regionBoundary": {"x2": 525.0, "y1": 62.8900146484375, "x1": 308.0, "y2": 304.8900146484375}, "caption": "Table 2: Performance of each component. The benchmarks references are: CoNLL-2003 (Sang and De Meulder, 2003), ACE (Walker et al., 2006), ERE (Song et al., 2015), AIDA (LDC2018E01:AIDA Seedling Corpus V2.0), MSCOCO (Lin et al., 2014), FDDB (Jain and Learned-Miller, 2010), LFW (Huang et al., 2008), Oxf105k (Philbin et al., 2007), YoutubeBB (Real et al., 2017), and Flickr30k (Plummer et al., 2015).", "page": 5}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 291.9243469238281, "y1": 717.5115966796875, "x1": 71.69100189208984, "y2": 759.3790283203125}, "imageText": ["Event", "47", "144", "Entity", "7", "187", "Relation", "23", "61", "Coarse-grained", "Types", "Fine-grained", "Types"], "regionBoundary": {"x2": 286.0, "y1": 652.8900146484375, "x1": 77.0, "y2": 704.8900146484375}, "caption": "Table 1: Compared to the coarse-grained knowledge extraction of previous work, GAIA can support finegrained entity, relation, and event extraction with types that are a superset of the previous coarse-grained types.", "page": 1}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 527.200927734375, "y1": 316.2435607910156, "x1": 72.0, "y2": 346.156005859375}, "imageText": [], "regionBoundary": {"x2": 526.0, "y1": 61.8900146484375, "x1": 72.0, "y2": 304.8900146484375}, "caption": "Figure 2: User-facing views of knowledge networks constructed with events automatically extracted from multimedia multilingual news reports. We display the event arguments, type, summary, similar events, as well as visual knowledge extracted from the corresponding image and video.", "page": 1}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 290.27044677734375, "y1": 225.29153442382812, "x1": 71.69100189208984, "y2": 243.2490234375}, "imageText": ["48.4%", "47.4%", "47.7%", "47.2%", "21.6%", "29.7%", "Class", "Queries", "Graph", "Queries", "AP-B", "AP-W", "AP-T", "Precision", "Recall", "F1"], "regionBoundary": {"x2": 295.0, "y1": 167.8900146484375, "x1": 72.0, "y2": 212.8900146484375}, "caption": "Table 3: GAIA achieves top performance on Task 1 at the recent NIST TAC SM-KBP2019 evaluation.", "page": 6}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 439.2899475097656, "y1": 364.64654541015625, "x1": 158.2550048828125, "y2": 370.6490173339844}, "imageText": ["Fusion", "and", "Pruning", "MTCNN", "Face", "Detector", "English", "Russian", "Ukrainian", "Images", "and\u00a0Video", "Key", "Frames", "Multi-lingual\u00a0Text", "Content", "Multimedia", "News", "News", "Recommendation", "Applications", "Cross-modal", "Entity", "Linking", "Multimedia\u00a0", "KB\u00a0", "Visual", "Grounding", "Cross-Media", "Fusion", "Visual", "KB\u00a0", "Heuristics", "Rules", "Face", "\u00a0Features", "Generic", "Features", "ClassActivation", "Map", "Model", "Flag", "Recognition", "Landmark", "Matching", "DBSCAN", "Clustering", "FaceNet", "Visual", "Entity", "Linking", "\u00a0ensemble", "Visual", "Entity", "CoreferenceVisual", "Entity", "Extraction", "Faster", "R-CNN", "Fine-Grained", "Event", "Typing", "Rule", "based", "Fine-Grained", "Event", "Typing", "Fine-Grained", "Event", "Typing", "FrameNet", "&", "Dependency", "based\u00a0", "Background", "KB", "Textual", "KB\u00a0", "Graph", "based", "Coreference", "Resolution", "Trigger", "Extractor", "CNN", "Argument", "Extractor", "Coarse-Grained", "Event", "Extraction", "Bi-LSTM", "CRFs", "Textual", "Event", "Coreference", "Assembled", "CNN", "Extractor", "Dependency", "based", "Fine-Grained", "Relation", "Typing", "Contextual", "Nominal", "Coreference", "and", "NIL", "Clustering", "Textual", "Entity", "Coreference", "Collective", "Entity", "Linking", "Attentive", "Fine-Grained", "Entity", "Typing", "ELMo-LSTM", "CRF", "Entity", "Extractor", "Textual", "Event", "Extraction", "Textual", "Relation", "ExtractionTextual", "Mention", "Extraction"], "regionBoundary": {"x2": 525.0, "y1": 68.8900146484375, "x1": 72.0, "y2": 351.8900146484375}, "caption": "Figure 3: The architecture of GAIA multimedia knowledge extraction.", "page": 2}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 291.92431640625, "y1": 136.53756713867188, "x1": 72.0, "y2": 166.4500732421875}, "imageText": ["Face", "Recognition", "Landmark", "Recognition", "Flag", "Recognition", "(b)(a)", "(c)"], "regionBoundary": {"x2": 291.0, "y1": 62.8900146484375, "x1": 72.0, "y2": 121.9193115234375}, "caption": "Figure 4: Examples of visual entity linking, based on face recognition, landmark recognition and flag recognition.", "page": 4}, {"figType": "Figure", "name": "5", "captionBoundary": {"x2": 527.2003784179688, "y1": 555.9835815429688, "x1": 307.2760009765625, "y2": 609.8060302734375}, "imageText": [], "regionBoundary": {"x2": 526.0, "y1": 485.8900146484375, "x1": 307.0, "y2": 543.8900146484375}, "caption": "Figure 5: The two green bounding boxes are coreferential since they are both linked to \u201cKirstjen Nielsen\u201d, and two red bounding boxes are coreferential based on face features. The yellow bounding boxes are unlinkable and also not coreferential to other bounding boxes.", "page": 4}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 732.222663031684, "y1": 531.5577189127604, "x1": 426.772223578559, "y2": 556.4986334906683}, "name": "1", "caption_text": "Figure 1: An example of cross-media knowledge fusion and a look inside the visual knowledge extraction.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 729.0, "y1": 309.0, "x1": 427.0, "y2": 514.0}, "page": 0, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2235107421875, "y1": 439.22716776529944, "x1": 100.0, "y2": 480.77223036024304}, "name": "2", "caption_text": "Figure 2: User-facing views of knowledge networks constructed with events automatically extracted from multimedia multilingual news reports. We display the event arguments, type, summary, similar events, as well as visual knowledge extracted from the corresponding image and video.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 730.0, "y1": 87.0, "x1": 100.0, "y2": 438.0}, "page": 1, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.45048183865015, "y1": 996.5438842773438, "x1": 99.57083596123589, "y2": 1054.693094889323}, "name": "1", "caption_text": "Table 1: Compared to the coarse-grained knowledge extraction of previous work, GAIA can support finegrained entity, relation, and event extraction with types that are a superset of the previous coarse-grained types.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 396.0, "y1": 906.0, "x1": 107.0, "y2": 980.0}, "page": 1, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 610.1249270968967, "y1": 506.45353529188367, "x1": 219.79861789279514, "y2": 514.7903018527561}, "name": "3", "caption_text": "Figure 3: The architecture of GAIA multimedia knowledge extraction.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 730.0, "y1": 95.0, "x1": 100.0, "y2": 489.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.450439453125, "y1": 189.63550991482205, "x1": 100.0, "y2": 231.18065728081595}, "name": "4", "caption_text": "Figure 4: Examples of visual entity linking, based on face recognition, landmark recognition and flag recognition.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 403.0, "y1": 88.0, "x1": 100.0, "y2": 171.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2227478027344, "y1": 772.1994188096788, "x1": 426.772223578559, "y2": 846.9528198242188}, "name": "5", "caption_text": "Figure 5: The two green bounding boxes are coreferential since they are both linked to \u201cKirstjen Nielsen\u201d, and two red bounding boxes are coreferential based on face features. The yellow bounding boxes are unlinkable and also not coreferential to other bounding boxes.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 730.0, "y1": 675.0, "x1": 427.0, "y2": 755.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.222408718533, "y1": 440.85494147406683, "x1": 425.9694417317708, "y2": 582.0262908935547}, "name": "2", "caption_text": "Table 2: Performance of each component. The benchmarks references are: CoNLL-2003 (Sang and De Meulder, 2003), ACE (Walker et al., 2006), ERE (Song et al., 2015), AIDA (LDC2018E01:AIDA Seedling Corpus V2.0), MSCOCO (Lin et al., 2014), FDDB (Jain and Learned-Miller, 2010), LFW (Huang et al., 2008), Oxf105k (Philbin et al., 2007), YoutubeBB (Real et al., 2017), and Flickr30k (Plummer et al., 2015).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 729.0, "y1": 86.0, "x1": 426.0, "y2": 441.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.15339830186633, "y1": 312.9049089219835, "x1": 99.57083596123589, "y2": 337.84586588541663}, "name": "3", "caption_text": "Table 3: GAIA achieves top performance on Task 1 at the recent NIST TAC SM-KBP2019 evaluation.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 409.0, "y1": 232.0, "x1": 100.0, "y2": 296.0}, "page": 6, "dpi": 0}], "error": null, "pdf": "/work/host-output/34ef926c1dd7407c6a728746bfdf081f5ee43c22/2020.acl-demos.11.pdf", "dpi": 100}