{"raw_detected_boxes": [[], [], [{"x2": 394.0, "y1": 87.0, "x1": 110.0, "y2": 264.0}], [], [{"x2": 376.0, "y1": 86.0, "x1": 125.0, "y2": 189.0}], [{"x2": 714.0, "y1": 91.0, "x1": 112.0, "y2": 374.0}], [{"x2": 404.0, "y1": 95.0, "x1": 110.0, "y2": 240.0}, {"x2": 705.0, "y1": 98.0, "x1": 447.0, "y2": 538.0}], [{"x2": 353.0, "y1": 91.0, "x1": 139.0, "y2": 333.0}], [], []], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Table", "name": "2", "captionBoundary": {"x2": 525.5470581054688, "y1": 284.0105285644531, "x1": 71.69100189208984, "y2": 301.968017578125}, "imageText": ["AdaComp", "(32)", "42.0", "x171", "92.1", "x173", "77.6", "x232", "110.8", "x105", "AdaComp", "(64)", "43.2", "x84", "93", "x89", "78.4", "x119", "106", "x52", "AdaComp", "(128)", "42.9", "x45", "93.1", "x44", "78.7", "x60", "108.9", "x26", "Bin", "(64)", "(Tissier", "et", "al.,", "2019)", "36.8", "x95", "91.5", "x100", "77.3", "x116", "116", "x74", "Bin(128)", "(Tissier", "et", "al.,", "2019)", "39.1", "x48", "92.7", "x49", "77.6", "x59", "110.1", "x37", "NC", "(16\u00d716)", "(Shu", "and", "Nakayama,", "2018)", "37.2", "x46", "91.8", "x50", "77.8", "x71", "119.2", "x30", "NC", "(32\u00d716)", "(Shu", "and", "Nakayama,", "2018)", "40.9", "x23", "92.4", "x25", "78.5", "x35", "112.4", "x15", "Pruning", "90%", "(Han", "et", "al.,", "2015)", "35.4", "x10", "90.4", "x10", "78", "x10", "113.2", "x10", "Pruning", "80%", "(Han", "et", "al.,", "2015)", "41.6", "x5", "91.7", "x5", "78.2", "x5", "124.7", "x5", "QWE", "(4-bit)", "(Ling", "et", "al.,", "2016)", "41.8", "x8", "93.1", "x8", "77.9", "x8", "113.8", "x8", "QWE", "(8-bit)", "(Ling", "et", "al.,", "2016)", "41.9", "x4", "93.3", "x4", "78.6", "x4", "109.1", "x4", "GloVe", "42.1", "x1", "93.1", "x1", "79", "x1", "100.3", "x1", "accuracy", "ratio", "F1", "ratio", "accuracy", "ratio", "ppl", "ratio", "Model", "SST-5", "CoNLL-2000", "SNLI", "PTB"], "regionBoundary": {"x2": 517.0, "y1": 65.8900146484375, "x1": 81.0, "y2": 271.8900146484375}, "caption": "Table 2: Comparison results on four tasks. The ratio on the table is calculated by dividing the size of the original embeddings by that of comparison models. Higher performance means better model except for PTB (perplexity).", "page": 5}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 527.2003173828125, "y1": 406.5195617675781, "x1": 307.2760009765625, "y2": 484.25396728515625}, "imageText": ["(b)", "(a)"], "regionBoundary": {"x2": 513.0, "y1": 61.8900146484375, "x1": 317.0, "y2": 391.2020263671875}, "caption": "Figure 3: Performance variation when we use different settings (the length and the number of code-books) on each task. Dashed line indicates the model which is not pretrained. Performance (y-axis) indicates evaluation metrics for each task. Note that the evaluation metric for language modeling is perplexity, thus, the performance is reversed on PTB. Best viewed in color.", "page": 6}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 291.9243469238281, "y1": 192.88253784179688, "x1": 72.0, "y2": 210.84002685546875}, "imageText": [], "regionBoundary": {"x2": 299.0, "y1": 61.8900146484375, "x1": 72.0, "y2": 180.8900146484375}, "caption": "Figure 2: Ratio of code-book assignment on each setting. Best viewed in color.", "page": 6}, {"figType": "Figure", "name": "1", "captionBoundary": {"x2": 291.9241943359375, "y1": 203.82351684570312, "x1": 71.6709976196289, "y2": 233.73602294921875}, "imageText": ["3", "3", "2", "2", "2", "4"], "regionBoundary": {"x2": 284.0, "y1": 65.8900146484375, "x1": 79.0, "y2": 191.8900146484375}, "caption": "Figure 1: Main strategy of our compression model (AdaComp). Solid line indicates the selected codebook.", "page": 2}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 291.9244384765625, "y1": 256.5455627441406, "x1": 72.0, "y2": 286.45806884765625}, "imageText": [], "regionBoundary": {"x2": 272.0, "y1": 61.8900146484375, "x1": 90.0, "y2": 244.8900146484375}, "caption": "Figure 4: Visualization of the reconstructed embeddings with their code-book assignment. Best viewed in color.", "page": 7}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 292.01373291015625, "y1": 148.79153442382812, "x1": 71.67100524902344, "y2": 190.6590576171875}, "imageText": ["PTB", "9,809", "11.8", "SNLI", "34,045", "40.9", "CoNLL-2000", "19,072", "22.9", "SST-5", "17,080", "20.5", "Task", "Vocabulary", "size", "Memory", "size", "(MB)"], "regionBoundary": {"x2": 272.0, "y1": 62.8900146484375, "x1": 90.0, "y2": 136.8900146484375}, "caption": "Table 1: Memory size of the original word embeddings (i.e., GloVe) and the number of words in each task. Each embedding is 300 dimensional vectors and is represented by 32 bit floating point.", "page": 4}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 405.4502699110243, "y1": 283.08821784125433, "x1": 99.54305224948459, "y2": 324.63336520724823}, "name": "1", "caption_text": "Figure 1: Main strategy of our compression model (AdaComp). Solid line indicates the selected codebook.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 394.0, "y1": 87.0, "x1": 110.0, "y2": 266.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.57462904188367, "y1": 206.6549089219835, "x1": 99.54306284586588, "y2": 264.8042466905382}, "name": "1", "caption_text": "Table 1: Memory size of the original word embeddings (i.e., GloVe) and the number of words in each task. Each embedding is 300 dimensional vectors and is represented by 32 bit floating point.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 383.0, "y1": 86.0, "x1": 114.0, "y2": 206.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9264695909288, "y1": 394.45906745062933, "x1": 99.57083596123589, "y2": 419.4000244140625}, "name": "2", "caption_text": "Table 2: Comparison results on four tasks. The ratio on the table is calculated by dividing the size of the original embeddings by that of comparison models. Higher performance means better model except for PTB (perplexity).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 718.0, "y1": 86.0, "x1": 112.0, "y2": 378.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.45048183865015, "y1": 267.8924136691623, "x1": 100.0, "y2": 292.83337063259546}, "name": "2", "caption_text": "Figure 2: Ratio of code-book assignment on each setting. Best viewed in color.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 405.0, "y1": 95.0, "x1": 110.0, "y2": 240.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.222663031684, "y1": 564.6105024549696, "x1": 426.772223578559, "y2": 672.574954562717}, "name": "3", "caption_text": "Figure 3: Performance variation when we use different settings (the length and the number of code-books) on each task. Dashed line indicates the model which is not pretrained. Performance (y-axis) indicates evaluation metrics for each task. Note that the evaluation metric for language modeling is perplexity, thus, the performance is reversed on PTB. Best viewed in color.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 706.0, "y1": 93.0, "x1": 447.0, "y2": 546.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.4506089952257, "y1": 356.31328158908417, "x1": 100.0, "y2": 397.8584289550781}, "name": "4", "caption_text": "Figure 4: Visualization of the reconstructed embeddings with their code-book assignment. Best viewed in color.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 353.0, "y1": 91.0, "x1": 139.0, "y2": 333.0}, "page": 7, "dpi": 0}], "error": null, "pdf": "/work/host-output/2c54e3ae6ceaeb6d75b135b682d696615d1a2875/2020.acl-main.364.pdf", "dpi": 100}