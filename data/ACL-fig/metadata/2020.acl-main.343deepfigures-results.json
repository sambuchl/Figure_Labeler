{"raw_detected_boxes": [[{"x2": 709.0, "y1": 309.0, "x1": 441.0, "y2": 658.0}], [], [{"x2": 373.0, "y1": 88.0, "x1": 104.0, "y2": 244.0}], [{"x2": 729.0, "y1": 90.0, "x1": 102.0, "y2": 321.0}], [{"x2": 387.0, "y1": 89.0, "x1": 113.0, "y2": 318.0}], [{"x2": 727.0, "y1": 92.0, "x1": 103.0, "y2": 346.0}, {"x2": 731.0, "y1": 460.0, "x1": 431.0, "y2": 540.0}], [{"x2": 401.0, "y1": 88.0, "x1": 102.0, "y2": 226.0}], [{"x2": 691.0, "y1": 103.0, "x1": 139.0, "y2": 396.0}, {"x2": 720.0, "y1": 518.0, "x1": 434.0, "y2": 690.0}], [], []], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Figure", "name": "1", "captionBoundary": {"x2": 527.2003784179688, "y1": 489.9845275878906, "x1": 307.2760009765625, "y2": 543.8079833984375}, "imageText": ["It", "is", "too", "unexpected", "\u2026", "(Others)", "(Ours)", "M:", "Negative", "M:", "Negative", "T", ":", "Positive", "A", ":", "Weakly", "Positive", "V", ":", "Negative"], "regionBoundary": {"x2": 510.765869140625, "y1": 221.8900146484375, "x1": 317.0, "y2": 471.7139892578125}, "caption": "Figure 1: An example of the annotation difference between CH-SIMS and other datasets. For each multimodal clip, in addition to multimodal annotations, our proposed dataset has independent unimodal annotations. M: Multimodal, T: Text, A: Audio, V: Vision.", "page": 0}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 526.787109375, "y1": 261.7875671386719, "x1": 71.69100189208984, "y2": 303.65509033203125}, "imageText": ["TFN", "80.66", "\u00b1", "1.4", "64.46", "\u00b1", "1.7", "38.38", "\u00b1", "3.6", "81.62", "\u00b1", "1.1", "42.52", "\u00b1", "1.1", "61.18", "\u00b1", "1.2", "MTFN\u2217", "82.45", "\u00b1", "1.3", "69.02", "\u00b1", "0.3", "37.20", "\u00b1", "1.8", "82.56", "\u00b1", "1.2", "40.66", "\u00b1", "1.1", "66.98", "\u00b1", "1.3", "5", "\u2191", "1.79", "\u2191", "4.56", "\u2193", "1.18", "\u2191", "0.94", "\u2193", "1.86", "\u2191", "5.80", "5", "\u2191", "2.41", "\u2191", "2.15", "\u2193", "3.59", "\u2191", "2.32", "\u2193", "1.37", "\u2191", "6.24", "LMF", "79.34", "\u00b1", "0.4", "64.38", "\u00b1", "2.1", "35.14", "\u00b1", "4.6", "79.96", "\u00b1", "0.6", "43.99", "\u00b1", "1.6", "60.00", "\u00b1", "1.3", "MLMF\u2217", "82.32", "\u00b1", "0.5", "67.70", "\u00b1", "2.2", "37.33", "\u00b1", "2.5", "82.66", "\u00b1", "0.7", "42.03", "\u00b1", "0.9", "63.13", "\u00b1", "1.9", "5", "\u2191", "2.98", "\u2191", "3.32", "\u2191", "2.19", "\u2191", "2.70", "\u2193", "1.96", "\u2191", "3.13", "LF-DNN", "79.87", "\u00b1", "0.6", "66.91", "\u00b1", "1.2", "41.62", "\u00b1", "1.4", "80.20", "\u00b1", "0.6", "42.01", "\u00b1", "0.9", "61.23", "\u00b1", "1.8", "MLF-DNN\u2217", "82.28", "\u00b1", "1.3", "69.06", "\u00b1", "3.1", "38.03", "\u00b1", "6.0", "82.52", "\u00b1", "1.3", "40.64", "\u00b1", "2.0", "67.47", "\u00b1", "1.8", "MFN", "77.86", "\u00b1", "0.4", "63.89", "\u00b1", "1.9", "39.39", "\u00b1", "1.8", "78.22", "\u00b1", "0.4", "45.19", "\u00b1", "1.2", "55.18", "\u00b1", "2.0", "MULT", "77.94", "\u00b1", "0.9", "65.03", "\u00b1", "2.1", "35.34", "\u00b1", "2.9", "79.10", "\u00b1", "0.9", "48.45", "\u00b1", "2.6", "55.94", "\u00b1", "0.6", "Model", "Acc-2", "Acc-3", "Acc-5", "F1", "MAE", "Corr", "EF-LSTM", "69.37", "\u00b1", "0.0", "51.73", "\u00b1", "2.0", "21.02", "\u00b1", "0.2", "81.91", "\u00b1", "0.0", "59.34", "\u00b1", "0.3", "-04.39", "\u00b1", "2.8"], "regionBoundary": {"x2": 527.0, "y1": 65.8900146484375, "x1": 72.0, "y2": 249.8900146484375}, "caption": "Table 2: (%) Results for sentiment analysis on the CH-SIMS dataset. The models with \u2217 are multi-task models, extended from single-task models by introducing independent unimodal annotations. For example, MLF-DNN\u2217 is the extension of LF-DNN. The rows with 5 means the improvements or reductions of new models compared to original ones in the current evaluation metric.", "page": 5}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 527.2003784179688, "y1": 401.0155334472656, "x1": 306.9670104980469, "y2": 430.927978515625}, "imageText": ["Item", "Total", "NG", "WN", "NU", "WP", "PS", "#Train", "1,368", "452", "290", "207", "208", "211", "#Valid", "456", "151", "97", "69", "69", "70", "#Test", "457", "151", "97", "69", "69", "71"], "regionBoundary": {"x2": 528.0, "y1": 326.8900146484375, "x1": 307.0, "y2": 388.8900146484375}, "caption": "Table 3: Dataset splits in SIMS. We split train, valid and test set in 6:2:2. NG: Negative, WN: Weakly Negative, NU: Neutral, WP: Weakly Positive, PS: Positive.", "page": 5}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 291.92437744140625, "y1": 178.10153198242188, "x1": 71.69100189208984, "y2": 219.96905517578125}, "imageText": ["A", "A", "67.70", "79.61", "53.80", "10.07", "M", "65.47", "71.44", "57.89", "14.54", "V", "V", "81.62", "82.73", "49.57", "57.61", "M", "74.44", "79.55", "54.46", "38.76", "T", "T", "80.26", "82.93", "41.79", "49.33", "M", "75.19", "78.43", "52.73", "38.55", "Task", "Label", "Acc-2", "F1", "MAE", "Corr"], "regionBoundary": {"x2": 290.0, "y1": 62.8900146484375, "x1": 72.0, "y2": 165.8900146484375}, "caption": "Table 4: (%) Results for unimodal sentiment analysis on the CH-SIMS dataset using MLF-DNN. The column of \u201cLabel\u201d indicates which annotation we use in this task.", "page": 6}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 252.27774047851562, "y1": 190.85354614257812, "x1": 109.68299865722656, "y2": 196.85601806640625}, "imageText": ["-", "Female", "781", "Total", "number", "of", "distinct", "speakers", "474", "Average", "length", "of", "segments", "(s)", "3.67", "Average", "word", "count", "per", "segments", "15", "Total", "number", "of", "segments", "2,281", "-", "Male", "1,500", "Item", "#", "Total", "number", "of", "videos", "60"], "regionBoundary": {"x2": 269.0, "y1": 62.8900146484375, "x1": 72.0, "y2": 178.8900146484375}, "caption": "Table 1: Statistics of SIMS Dataset.", "page": 2}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 525.7963256835938, "y1": 301.9075622558594, "x1": 72.0, "y2": 343.7760009765625}, "imageText": ["MLMF", "LMF", "MTFN", "TFN", "MLF-DNN", "LF-DNN"], "regionBoundary": {"x2": 498.0, "y1": 73.8900146484375, "x1": 96.0, "y2": 284.97467041015625}, "caption": "Figure 4: Visualization in Unimodal Representations. In each subfigure, red, green, and blue points represent the unimodal representations in text, audio, and video, respectively. The first row shows the learned representations from models with the multimodal task only. The second row shows the learned representations from multi-task models. The two subgraphs in the same column contrast each other", "page": 7}, {"figType": "Table", "name": "5", "captionBoundary": {"x2": 525.5465087890625, "y1": 508.8815612792969, "x1": 306.9170227050781, "y2": 550.75}, "imageText": ["Tasks", "Acc-2", "F1", "MAE", "Corr", "M", "80.04", "80.40", "43.95", "61.78", "M,", "T", "80.04", "80.25", "43.11", "63.34", "M,", "A", "76.85", "77.28", "46.98", "55.16", "M,", "V", "79.96", "80.38", "43.16", "61.87", "M,", "T,", "A", "80.88", "81.10", "42.54", "64.16", "M,", "T,", "V", "80.04", "80.87", "42.42", "60.66", "M,", "A,", "V", "79.87", "80.32", "43.06", "62.95", "M,", "T,", "A,", "V", "82.28", "82.52", "40.64", "64.74"], "regionBoundary": {"x2": 521.0, "y1": 366.8900146484375, "x1": 312.0, "y2": 496.8900146484375}, "caption": "Table 5: (%) Results for multimodal sentiment analysis with different tasks using MLF-DNN. \u201cM\u201d is the main task and \u201cT, A, V\u201d are auxiliary tasks. Only the results of task \u201cM\u201d are reported.", "page": 7}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 527.2003173828125, "y1": 245.21151733398438, "x1": 71.99998474121094, "y2": 275.1240234375}, "imageText": [], "regionBoundary": {"x2": 526.0, "y1": 61.8900146484375, "x1": 72.0, "y2": 232.8900146484375}, "caption": "Figure 2: Left: the distribution of sentiment over the entire dataset in one Multimodal annotation and three singlemodal (Text, Audio, and Vision) annotations. Right: the confusion matrix shows the annotations difference between different modalities in CH-SIMS. The larger the value, the greater the difference.", "page": 3}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 287.520263671875, "y1": 245.21853637695312, "x1": 74.7490005493164, "y2": 251.22100830078125}, "imageText": ["Text", "Input", "Audio", "Input", "Video", "Input", "Yv", "<latexit", "sha1_base64=\"bzTZxdatENu4bXnWop07ytYUGfQ=\">AAAB6nicbVC7SgNBFL3jM8ZXVLCxGQyCVdiNhZYhNpYJmockS5idzCZDZmeXmdlAWPIJNhaK2Nr6F36BnY3f4uRRaOKBC4dz7uXee/xYcG0c5wutrK6tb2xmtrLbO7t7+7mDw7qOEkVZjUYiUk2faCa4ZDXDjWDNWDES+oI1/MH1xG8MmdI8kndmFDMvJD3JA06JsdLtfWfYyeWdgjMFXibunORLx9Vv/l7+qHRyn+1uRJOQSUMF0brlOrHxUqIMp4KNs+1Es5jQAemxlqWShEx76fTUMT6zShcHkbIlDZ6qvydSEmo9Cn3bGRLT14veRPzPayUmuPJSLuPEMElni4JEYBPhyd+4yxWjRowsIVRxeyumfaIINTadrA3BXXx5mdSLBfeiUKzaNMowQwZO4BTOwYVLKMENVKAGFHrwAE/wjAR6RC/odda6guYzR/AH6O0HJQCRZw==</latexit>", "Ya", "<latexit", "sha1_base64=\"VHXi6TXkOUZjqZL7gHKLVk4dy1A=\">AAAB6nicbVC7SgNBFL0bXzG+ooKNzWAQrMJuLLQMsbFM0DwkWcLsZDYZMjO7zMwKYckn2FgoYmvrX/gFdjZ+i5NHoYkHLhzOuZd77wlizrRx3S8ns7K6tr6R3cxtbe/s7uX3Dxo6ShShdRLxSLUCrClnktYNM5y2YkWxCDhtBsOrid+8p0qzSN6aUUx9gfuShYxgY6Wbuy7u5gtu0Z0CLRNvTgrlo9o3e698VLv5z04vIomg0hCOtW57bmz8FCvDCKfjXCfRNMZkiPu0banEgmo/nZ46RqdW6aEwUrakQVP190SKhdYjEdhOgc1AL3oT8T+vnZjw0k+ZjBNDJZktChOOTIQmf6MeU5QYPrIEE8XsrYgMsMLE2HRyNgRv8eVl0igVvfNiqWbTqMAMWTiGEzgDDy6gDNdQhToQ6MMDPMGzw51H58V5nbVmnPnMIfyB8/YDBSyRUg==</latexit>", "Ym", "<latexit", "sha1_base64=\"aHTwel4yShEmOzj39M1ZJTFlT5Y=\">AAAB63icbVC7SgNBFL0bXzG+ooKNzWAQrMJuLLQMsbFMwDwkLmF2MpsMmZldZmaFsOQXbCwUsbX0L/wCOxu/xdkkhSYeuHA4517uvSeIOdPGdb+c3Mrq2vpGfrOwtb2zu1fcP2jpKFGENknEI9UJsKacSdo0zHDaiRXFIuC0HYyuMr99T5Vmkbwx45j6Ag8kCxnBJpNue6LQK5bcsjsFWibenJSqR41v9l77qPeKn3f9iCSCSkM41rrrubHxU6wMI5xOCneJpjEmIzygXUslFlT76fTWCTq1Sh+FkbIlDZqqvydSLLQei8B2CmyGetHLxP+8bmLCSz9lMk4MlWS2KEw4MhHKHkd9pigxfGwJJorZWxEZYoWJsfFkIXiLLy+TVqXsnZcrDZtGDWbIwzGcwBl4cAFVuIY6NIHAEB7gCZ4d4Tw6L87rrDXnzGcO4Q+ctx9MkZFy</latexit>", "Yt", "<latexit", "sha1_base64=\"uBDpO1uegTINUkXe5q6JRp/LRvE=\">AAAB6nicbVC7SgNBFL0bXzG+ooKNzWIQrMJuLLQMsbFM0DwkWcLsZDYZMju7zNwVwpJPsLFQxNbWv/AL7Gz8FiePQhMPXDiccy/33uPHgmt0nC8rs7K6tr6R3cxtbe/s7uX3Dxo6ShRldRqJSLV8opngktWRo2CtWDES+oI1/eHVxG/eM6V5JG9xFDMvJH3JA04JGunmrovdfMEpOlPYy8Sdk0L5qPbN3ysf1W7+s9OLaBIyiVQQrduuE6OXEoWcCjbOdRLNYkKHpM/ahkoSMu2l01PH9qlRenYQKVMS7an6eyIlodaj0DedIcGBXvQm4n9eO8Hg0ku5jBNkks4WBYmwMbInf9s9rhhFMTKEUMXNrTYdEEUomnRyJgR38eVl0igV3fNiqWbSqMAMWTiGEzgDFy6gDNdQhTpQ6MMDPMGzJaxH68V6nbVmrPnMIfyB9fYDIfiRZQ==</latexit>", "\u2026", "FCFC", "FCFC", "Feature", "Fusion", "Network", "Text", "SubNet", "Audio", "SubNet", "Video", "SubNet"], "regionBoundary": {"x2": 280.0, "y1": 63.8900146484375, "x1": 82.0, "y2": 228.42718505859375}, "caption": "Figure 3: Multimodal multi-task learning framework.", "page": 4}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 732.2227478027344, "y1": 680.5340660942925, "x1": 426.772223578559, "y2": 755.2888658311632}, "name": "1", "caption_text": "Figure 1: An example of the annotation difference between CH-SIMS and other datasets. For each multimodal clip, in addition to multimodal annotations, our proposed dataset has independent unimodal annotations. M: Multimodal, T: Text, A: Audio, V: Vision.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 709.0, "y1": 309.0, "x1": 441.0, "y2": 658.0}, "page": 0, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 350.38575066460504, "y1": 265.0743696424696, "x1": 152.3374981350369, "y2": 273.411136203342}, "name": "1", "caption_text": "Table 1: Statistics of SIMS Dataset.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 373.0, "y1": 86.0, "x1": 100.0, "y2": 248.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.222663031684, "y1": 340.57155185275604, "x1": 99.99997880723741, "y2": 382.11669921875}, "name": "2", "caption_text": "Figure 2: Left: the distribution of sentiment over the entire dataset in one Multimodal annotation and three singlemodal (Text, Audio, and Vision) annotations. Right: the confusion matrix shows the annotations difference between different modalities in CH-SIMS. The larger the value, the greater the difference.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 729.0, "y1": 90.0, "x1": 100.0, "y2": 323.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 399.3336995442708, "y1": 340.581300523546, "x1": 103.81805631849501, "y2": 348.9180670844184}, "name": "3", "caption_text": "Figure 3: Multimodal multi-task learning framework.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 390.0, "y1": 89.0, "x1": 113.0, "y2": 320.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 731.6487630208333, "y1": 363.5938432481554, "x1": 99.57083596123589, "y2": 421.7431810167101}, "name": "2", "caption_text": "Table 2: (%) Results for sentiment analysis on the CH-SIMS dataset. The models with \u2217 are multi-task models, extended from single-task models by introducing independent unimodal annotations. For example, MLF-DNN\u2217 is the extension of LF-DNN. The rows with 5 means the improvements or reductions of new models compared to original ones in the current evaluation metric.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 732.0, "y1": 86.0, "x1": 100.0, "y2": 363.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2227478027344, "y1": 556.9660186767578, "x1": 426.3430701361762, "y2": 598.5110812717014}, "name": "3", "caption_text": "Table 3: Dataset splits in SIMS. We split train, valid and test set in 6:2:2. NG: Negative, WN: Weakly Negative, NU: Neutral, WP: Weakly Positive, PS: Positive.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 734.0, "y1": 454.0, "x1": 426.0, "y2": 557.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.45052422417535, "y1": 247.36323886447482, "x1": 99.57083596123589, "y2": 305.5125766330295}, "name": "4", "caption_text": "Table 4: (%) Results for unimodal sentiment analysis on the CH-SIMS dataset using MLF-DNN. The column of \u201cLabel\u201d indicates which annotation we use in this task.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 403.0, "y1": 86.0, "x1": 100.0, "y2": 231.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 730.2726745605469, "y1": 419.31605868869354, "x1": 100.0, "y2": 477.4666680230035}, "name": "4", "caption_text": "Figure 4: Visualization in Unimodal Representations. In each subfigure, red, green, and blue points represent the unimodal representations in text, audio, and video, respectively. The first row shows the learned representations from models with the multimodal task only. The second row shows the learned representations from multi-task models. The two subgraphs in the same column contrast each other", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 691.0, "y1": 103.0, "x1": 133.0, "y2": 396.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9257066514757, "y1": 706.7799462212456, "x1": 426.27364264594183, "y2": 764.9305555555555}, "name": "5", "caption_text": "Table 5: (%) Results for multimodal sentiment analysis with different tasks using MLF-DNN. \u201cM\u201d is the main task and \u201cT, A, V\u201d are auxiliary tasks. Only the results of task \u201cM\u201d are reported.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 724.0, "y1": 509.0, "x1": 426.0, "y2": 707.0}, "page": 7, "dpi": 0}], "error": null, "pdf": "/work/host-output/97a91a33fab32e7d1677c2030bf52736ba44e4df/2020.acl-main.343.pdf", "dpi": 100}