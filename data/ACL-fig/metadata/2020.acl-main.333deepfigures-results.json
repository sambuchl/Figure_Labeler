{"raw_detected_boxes": [[{"x2": 724.0, "y1": 312.0, "x1": 432.0, "y2": 503.0}], [], [], [], [{"x2": 718.0, "y1": 87.0, "x1": 432.0, "y2": 214.0}], [{"x2": 645.0, "y1": 123.0, "x1": 175.0, "y2": 304.0}, {"x2": 387.0, "y1": 400.0, "x1": 113.0, "y2": 501.0}, {"x2": 720.0, "y1": 404.0, "x1": 438.0, "y2": 484.0}], [{"x2": 646.0, "y1": 123.0, "x1": 166.0, "y2": 306.0}, {"x2": 654.0, "y1": 372.0, "x1": 173.0, "y2": 443.0}, {"x2": 636.0, "y1": 512.0, "x1": 194.0, "y2": 585.0}], [{"x2": 705.0, "y1": 119.0, "x1": 111.0, "y2": 639.0}], [{"x2": 393.0, "y1": 86.0, "x1": 107.0, "y2": 253.0}, {"x2": 369.0, "y1": 377.0, "x1": 131.0, "y2": 439.0}, {"x2": 392.0, "y1": 552.0, "x1": 104.0, "y2": 625.0}], [], []], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Table", "name": "1", "captionBoundary": {"x2": 527.200439453125, "y1": 382.7655334472656, "x1": 306.9169921875, "y2": 472.45391845703125}, "imageText": ["the", "area.", "Context", "of", "Conversation", "Speaker", "A:", "Hey,", "what", "do", "you", "want", "to", "do", "tonight?", "Speaker", "B:", "Why", "don\u2019t", "we", "go", "see", "a", "movie?", "Model", "Response", "Speaker", "A:", "Nah,", "let\u2019s", "do", "something", "active.", "Reference", "Response", "Speaker", "A:", "Yeah,", "the", "\ufb01lm", "about", "Turing", "looks", "great!", "Context", "of", "Conversation", "Speaker", "A:", "Do", "you", "have", "maps", "of", "downtown", "area?", "Speaker", "B:", "Yes,", "here", "you", "are.", "Speaker", "A:", "How", "much", "is", "it?", "Model", "Response", "Speaker", "B:", "I\u2019ve", "no", "idea,", "I", "don\u2019t", "have", "any", "maps", "of"], "regionBoundary": {"x2": 522.0, "y1": 221.8900146484375, "x1": 311.0, "y2": 365.8900146484375}, "caption": "Table 1: Two responses from an dialogue system (Wolf et al., 2019) on Daily Dialogue Dataset. The first generated response appears reasonable within the opendomain dialogue, while its BLEU score and semantic similarity between model response and reference response is low. The second generated response conflicts with its prior utterances. The italic text highlights the logical contradiction.", "page": 0}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 525.546630859375, "y1": 360.6715393066406, "x1": 306.9670104980469, "y2": 402.53997802734375}, "imageText": ["Pearson", "Spearman", "GPT-2", "w/o", "Fine-tune", "0.43", "0.32", "GPT-2", "w/", "Fine-tune", "0.82", "0.81", "Inter-Rater", "Mean", "0.70", "0.70Max", "0.88", "0.85"], "regionBoundary": {"x2": 519.0, "y1": 287.8900146484375, "x1": 314.0, "y2": 348.8900146484375}, "caption": "Table 4: Correlation between response fluency metric f(r) and human ratings without and with fine-tuning of GPT-2. Pairwise mean and max correlations of human ratings.", "page": 5}, {"figType": "Figure", "name": "1", "captionBoundary": {"x2": 525.543701171875, "y1": 235.10757446289062, "x1": 71.99998474121094, "y2": 265.02008056640625}, "imageText": ["(a)", "GPT-2", "w/o", "Fine-tune", "(b)", "GPT-2", "w/", "Fine-tune"], "regionBoundary": {"x2": 483.0, "y1": 71.8900146484375, "x1": 114.0, "y2": 219.7900390625}, "caption": "Figure 1: Correlation between context coherence metric c(r|q) and human ratings without and with fine-tuning of GPT-2. Note that random jitters sampled fromN (0, 0.052) are added to human ratings in visualizing scatter plots showed in this paper to overlapping points.", "page": 5}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 291.9241943359375, "y1": 372.633544921875, "x1": 71.69100189208984, "y2": 402.5469970703125}, "imageText": ["Pearson", "Spearman", "RUBER+BERT", "0.47", "0.51", "GPT-2", "w/o", "Fine-tune", "0.59", "0.65", "GPT-2", "w/", "Fine-tune", "0.67", "0.76", "Inter-Rater", "Mean", "0.61", "0.57Max", "0.91", "0.87"], "regionBoundary": {"x2": 284.0, "y1": 287.8900146484375, "x1": 79.0, "y2": 360.8900146484375}, "caption": "Table 3: Correlation between RUBER+BERT and context coherence metric c(r|q) with human ratings (without and with fine-tuning of GPT-2).", "page": 5}, {"figType": "Table", "name": "6", "captionBoundary": {"x2": 525.5449829101562, "y1": 433.7025451660156, "x1": 71.69100189208984, "y2": 451.6600036621094}, "imageText": ["Inter-Rater", "Pearson", "Inter-Rater", "Spearman", "Human", "Variancemean", "max", "mean", "max", "Baseline", "Dataset", "0.21", "0.51", "0.23", "0.65", "0.93", "WS", "Dataset", "0.78", "0.89", "0.78", "0.92", "0.68", "CTG", "Dataset", "0.78", "0.86", "0.79", "0.81", "0.69"], "regionBoundary": {"x2": 458.0, "y1": 368.8900146484375, "x1": 140.0, "y2": 421.8900146484375}, "caption": "Table 6: Comparison of response diversity between the baseline dataset and and our paraphrasing-augmented datasets (WS and CTG datasets) using Inter-Rater Spearman and Pearson correlations.", "page": 6}, {"figType": "Table", "name": "5", "captionBoundary": {"x2": 525.5402221679688, "y1": 335.8375549316406, "x1": 71.69100189208984, "y2": 353.7950134277344}, "imageText": ["Baseline", "Dataset", "0.46", "0.32", "0.45", "0.33", "0.43", "0.33", "WS", "Dataset", "0.77", "0.69", "0.76", "0.67", "0.71", "0.61", "CTG", "Dataset", "0.72", "0.72", "0.72", "0.72", "0.66", "0.66", "1-Gram", "Entropy", "2-Gram", "Entropy", "3-Gram", "Entropy", "Pearson", "Spearman", "Pearson", "Spearman", "Pearson", "Spearman"], "regionBoundary": {"x2": 481.0, "y1": 267.8900146484375, "x1": 117.0, "y2": 323.8900146484375}, "caption": "Table 5: Comparison of response diversity metric between the baseline dataset and our paraphrasing-augmented datasets (WS and CTG datasets) using Spearman and Pearson correlations.", "page": 6}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 527.1990356445312, "y1": 235.10757446289062, "x1": 72.0, "y2": 253.0650634765625}, "imageText": ["(a)", "GPT-2", "w/o", "Fine-tune", "(b)", "GPT-2", "w/", "Fine-tune"], "regionBoundary": {"x2": 483.0, "y1": 71.8900146484375, "x1": 114.0, "y2": 219.7900390625}, "caption": "Figure 2: Correlation between response fluency metric f(r) and human ratings for GPT-2 without and with finetuning.", "page": 6}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 525.5470581054688, "y1": 475.42254638671875, "x1": 72.0, "y2": 493.3800048828125}, "imageText": ["(g)", "CTG", "w/", "1-gram", "Entropy", "(h)", "CTG", "w/", "2-gram", "Entropy", "(i)", "CTG", "w/", "3-gram", "Entropy", "(d)", "WS", "w/", "1-gram", "Entropy", "(e)", "WS", "w/", "2-gram", "Entropy", "(f)", "WS", "w/", "3-gram", "Entropy", "(a)", "Baseline", "w/", "1-gram", "Entropy", "(b)", "Baseline", "w/", "2-gram", "Entropy", "(c)", "Baseline", "w/", "3-gram", "Entropy"], "regionBoundary": {"x2": 522.0, "y1": 71.8900146484375, "x1": 75.0, "y2": 460.1050109863281}, "caption": "Figure 3: Correlation between n-gram entropy and human ratings on the baseline dataset, WS dataset and CTG dataset.", "page": 7}, {"figType": "Table", "name": "7", "captionBoundary": {"x2": 291.9243469238281, "y1": 202.60952758789062, "x1": 71.69100189208984, "y2": 256.43304443359375}, "imageText": ["Generated", "Utterance", "Speaker", "A:", "Are", "you", "more", "of", "a", "follower", "or", "a", "leader?", "Model", "Response", "Speaker", "B:", "I", "like", "to", "keep", "to", "myself.", "I\u2019m", "a", "person", "who", "does", "not", "want", "to", "be", "a", "follower.", "Our", "Score:", "0.09", "Human", "Score:", "0.20", "cooperate", "with", "everybody,", "and", "get", "the", "job", "done", "by", "working", "together.", "Context", "of", "Conversation", "Speaker", "A:", "Are", "you", "more", "of", "a", "leader", "or", "a", "follower?", "Speaker", "B:", "I", "don", "\u2019t", "try", "to", "lead", "people.", "I\u2019", "d", "rather"], "regionBoundary": {"x2": 287.0, "y1": 62.8900146484375, "x1": 75.0, "y2": 185.8900146484375}, "caption": "Table 7: Case study of logical self-consistency. Generated Utterance is generated by CTG. Blue italic words highlights the logic contradiction. Our automatic score is low indicating that the logic contradiction can be detected.", "page": 8}, {"figType": "Table", "name": "8", "captionBoundary": {"x2": 291.92144775390625, "y1": 328.4525451660156, "x1": 71.69100189208984, "y2": 382.2759704589844}, "imageText": ["Pearson", "Spearman", "Baseline", "Dataset", "0.26", "0.27", "WS", "Dataset", "0.59", "0.64", "CTG", "Dataset", "0.65", "0.66"], "regionBoundary": {"x2": 265.0, "y1": 271.8900146484375, "x1": 94.0, "y2": 316.8900146484375}, "caption": "Table 8: Comparison of logical self-consistency metric between the paraphrasing-augmented data (WS and CTG data) and the baseline data without augmentation using Spearman and Pearson correlations with human ratings.", "page": 8}, {"figType": "Table", "name": "9", "captionBoundary": {"x2": 291.92144775390625, "y1": 461.8795471191406, "x1": 71.69100189208984, "y2": 503.7469787597656}, "imageText": ["Baseline", "0.61", "0.75", "0.62", "0.74", "WS", "0.64", "0.80", "0.64", "0.79", "CTG", "0.65", "0.75", "0.66", "0.76", "Inter-Rater", "Pearson", "Inter-Rater", "Spearman", "mean", "max", "mean", "max"], "regionBoundary": {"x2": 288.0, "y1": 397.8900146484375, "x1": 74.0, "y2": 449.8900146484375}, "caption": "Table 9: Comparison of logical self-consistency metric between the paraphrasing-augmented data (WS and CTG data) and the baseline data without augmentation using Inter-Rater Spearman and Pearson correlations.", "page": 8}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 527.2001953125, "y1": 170.97756958007812, "x1": 306.9670104980469, "y2": 212.8460693359375}, "imageText": ["Human", "Score:", "0.20", "RUBER", "Score:", "0.97", "Our", "Score:", "0.19", "Speaker", "A:", "Of", "course.", "A", "two-week", "paid", "vacation", "ayear,", "a", "\ufb01ve-day", "workweek.", "Speaker", "B:", "So,", "if", "I", "get", "a", "margin", "card,", "I", "could", "take", "a", "margin", "card", "for", "you", "to", "travel", "to", "a", "com-", "pany", "as", "soon", "as", "possible.", "Context", "of", "Conversation"], "regionBoundary": {"x2": 522.0, "y1": 62.8900146484375, "x1": 311.0, "y2": 153.8900146484375}, "caption": "Table 2: Case study. Both our coherence metric and the human evaluation agreed that the generated response is not coherent with the given query, while RUBER indicated this reply is coherent.", "page": 4}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 732.2228325737847, "y1": 531.6187964545355, "x1": 426.27360026041663, "y2": 656.1859978569878}, "name": "1", "caption_text": "Table 1: Two responses from an dialogue system (Wolf et al., 2019) on Daily Dialogue Dataset. The first generated response appears reasonable within the opendomain dialogue, while its BLEU score and semantic similarity between model response and reference response is low. The second generated response conflicts with its prior utterances. The italic text highlights the logical contradiction.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 725.0, "y1": 308.0, "x1": 432.0, "y2": 508.0}, "page": 0, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2224934895833, "y1": 237.46884663899738, "x1": 426.3430701361762, "y2": 295.6195407443576}, "name": "2", "caption_text": "Table 2: Case study. Both our coherence metric and the human evaluation agreed that the generated response is not coherent with the given query, while RUBER indicated this reply is coherent.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 725.0, "y1": 86.0, "x1": 432.0, "y2": 214.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9218071831597, "y1": 326.53829786512586, "x1": 99.99997880723741, "y2": 368.08344523111975}, "name": "1", "caption_text": "Figure 1: Correlation between context coherence metric c(r|q) and human ratings without and with fine-tuning of GPT-2. Note that random jitters sampled fromN (0, 0.052) are added to human ratings in visualizing scatter plots showed in this paper to overlapping points.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 647.0, "y1": 123.0, "x1": 166.0, "y2": 308.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.4502699110243, "y1": 517.5465901692709, "x1": 99.57083596123589, "y2": 559.0930514865452}, "name": "3", "caption_text": "Table 3: Correlation between RUBER+BERT and context coherence metric c(r|q) with human ratings (without and with fine-tuning of GPT-2).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 394.0, "y1": 400.0, "x1": 100.0, "y2": 518.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9258761935764, "y1": 500.9326934814453, "x1": 426.3430701361762, "y2": 559.0833028157551}, "name": "4", "caption_text": "Table 4: Correlation between response fluency metric f(r) and human ratings without and with fine-tuning of GPT-2. Pairwise mean and max correlations of human ratings.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 723.0, "y1": 400.0, "x1": 426.0, "y2": 501.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2208828396267, "y1": 326.53829786512586, "x1": 100.0, "y2": 351.479254828559}, "name": "2", "caption_text": "Figure 2: Correlation between response fluency metric f(r) and human ratings for GPT-2 without and with finetuning.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 646.0, "y1": 123.0, "x1": 166.0, "y2": 308.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9169752332899, "y1": 466.44104851616754, "x1": 99.57083596123589, "y2": 491.3819630940755}, "name": "5", "caption_text": "Table 5: Comparison of response diversity metric between the baseline dataset and our paraphrasing-augmented datasets (WS and CTG datasets) using Spearman and Pearson correlations.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 667.0, "y1": 372.0, "x1": 162.0, "y2": 450.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.923587375217, "y1": 602.3646460639105, "x1": 99.57083596123589, "y2": 627.3055606418186}, "name": "6", "caption_text": "Table 6: Comparison of response diversity between the baseline dataset and and our paraphrasing-augmented datasets (WS and CTG datasets) using Inter-Rater Spearman and Pearson correlations.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 650.0, "y1": 512.0, "x1": 194.0, "y2": 602.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9264695909288, "y1": 660.309092203776, "x1": 100.0, "y2": 685.250006781684}, "name": "3", "caption_text": "Figure 3: Correlation between n-gram entropy and human ratings on the baseline dataset, WS dataset and CTG dataset.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 705.0, "y1": 119.0, "x1": 111.0, "y2": 642.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.45048183865015, "y1": 281.4021216498481, "x1": 99.57083596123589, "y2": 356.1570061577691}, "name": "7", "caption_text": "Table 7: Case study of logical self-consistency. Generated Utterance is generated by CTG. Blue italic words highlights the logic contradiction. Our automatic score is low indicating that the logic contradiction can be detected.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 398.0, "y1": 86.0, "x1": 105.0, "y2": 258.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.44645521375867, "y1": 456.18409050835504, "x1": 99.57083596123589, "y2": 530.9388478597006}, "name": "8", "caption_text": "Table 8: Comparison of logical self-consistency metric between the paraphrasing-augmented data (WS and CTG data) and the baseline data without augmentation using Spearman and Pearson correlations with human ratings.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 369.0, "y1": 377.0, "x1": 114.0, "y2": 456.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.44645521375867, "y1": 641.4993709988064, "x1": 99.57083596123589, "y2": 699.6485816107855}, "name": "9", "caption_text": "Table 9: Comparison of logical self-consistency metric between the paraphrasing-augmented data (WS and CTG data) and the baseline data without augmentation using Inter-Rater Spearman and Pearson correlations.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 400.0, "y1": 552.0, "x1": 100.0, "y2": 642.0}, "page": 8, "dpi": 0}], "error": null, "pdf": "/work/host-output/094c65946f3207b3e076000c2205e442fc956ebc/2020.acl-main.333.pdf", "dpi": 100}