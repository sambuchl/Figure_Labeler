{"raw_detected_boxes": [[{"x2": 392.0, "y1": 431.0, "x1": 78.0, "y2": 628.0}], [{"x2": 736.0, "y1": 110.0, "x1": 434.0, "y2": 324.0}, {"x2": 737.0, "y1": 409.0, "x1": 434.0, "y2": 634.0}], [{"x2": 399.0, "y1": 96.0, "x1": 74.0, "y2": 340.0}], [{"x2": 710.0, "y1": 95.0, "x1": 114.0, "y2": 230.0}, {"x2": 672.0, "y1": 308.0, "x1": 150.0, "y2": 377.0}, {"x2": 736.0, "y1": 482.0, "x1": 434.0, "y2": 700.0}, {"x2": 388.0, "y1": 482.0, "x1": 83.0, "y2": 712.0}], []], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Figure", "name": "4", "captionBoundary": {"x2": 287.0120849609375, "y1": 257.6355285644531, "x1": 52.15800094604492, "y2": 323.4140625}, "imageText": [], "regionBoundary": {"x2": 288.0, "y1": 68.8900146484375, "x1": 52.0, "y2": 245.8900146484375}, "caption": "Figure 4: cat BENEATH chair. Even if we have never seen beneath as a relation during the training phase but have seen cat UNDER chair (Figure 2), using the BERT predictions and re-scoring the choices using the GloVe embedding similarity, we want to be able to predict unseen relations at test time.", "page": 2}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 539.7142333984375, "y1": 476.8575439453125, "x1": 304.86602783203125, "y2": 494.81500244140625}, "imageText": ["(a)", "man", "RIDING", "surfboard", "(b)", "man", "CARRYING", "surf-", "board"], "regionBoundary": {"x2": 532.2494506835938, "y1": 293.8900146484375, "x1": 312.0, "y2": 461.24102783203125}, "caption": "Figure 3: Examples of implicit spatial relations from Visual Genome of (man, ?, surfboard)", "page": 1}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 539.714111328125, "y1": 250.39956665039062, "x1": 304.86602783203125, "y2": 268.3570556640625}, "imageText": ["(a)", "cat", "UNDER", "chair", "(b)", "cat", "ON", "chair"], "regionBoundary": {"x2": 530.0, "y1": 78.8900146484375, "x1": 312.0, "y2": 234.78302001953125}, "caption": "Figure 2: Examples of explicit spatial relations from Visual Genome of (cat, ?, chair)", "page": 1}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 539.717529296875, "y1": 188.69253540039062, "x1": 52.15800094604492, "y2": 206.6500244140625}, "imageText": ["FF", "0.8", "72.1", "74", "74.4", "74.7", "0", "75.0", "78.8", "79.0", "79.5", "FF+I", "0.71", "72.7", "74.1", "74.5", "74.72", "0.01", "75.79", "78.86", "79.3", "79.5", "BERT+FF+I", "36.4", "72.88", "74.7", "74.9", "75.2", "6.35", "75.7", "79", "79.3", "79.5", "f-BERT+FF+I", "73.06", "73.06", "74.2", "74.52", "74.74", "77.6", "77.5", "79.3", "79.7", "79.9", "BERT", "38.56", "38.56", "38.56", "38.56", "38.56", "6.9", "6.9", "6.9", "6.9", "6.9", "f-BERT", "73.03", "73.03", "73.03", "73.03", "73.03", "77.6", "77.6", "77.6", "77.6", "77.6", "Spatial", "Relation", "Explicit", "Implicit", "%", "of", "Training", "Data", "1%", "10%", "50%", "75%", "100%", "1%", "10%", "50%", "75%", "100%"], "regionBoundary": {"x2": 511.0, "y1": 69.8900146484375, "x1": 81.0, "y2": 166.8900146484375}, "caption": "Table 1: Comparison of performance (in percentage accuracy) of different models for explicit and implicit spatial relations, normal and f-BERT and combinations with the best spatial model for varying portions of training data.", "page": 3}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 539.717529296875, "y1": 283.5365295410156, "x1": 52.15800094604492, "y2": 313.44903564453125}, "imageText": ["BERT+FF+I", "67.4", "59.8", "62.5", "54.1", "24.0", "13.7", "Model", "Expl(S,", "R)", "Impl(S,", "R)", "Expl(O,", "R)", "Impl(O,", "R)", "Expl(R)", "Impl(R)", "BERT", "61.9", "14.4", "59.9", "27.0", "24.1", "13.7", "FF+I", "60.1", "68.6", "59.5", "50.1", "0", "0"], "regionBoundary": {"x2": 484.0, "y1": 221.8900146484375, "x1": 108.0, "y2": 271.8900146484375}, "caption": "Table 2: Experiments for unseen subjects, objects and relations (for both explicit and implicit relations). Spatial BERT gives better performance than BERT or FF+I for Impl(O, R) but not in Impl(S, R) potentially because the subject set is much sparser than the object set.", "page": 3}, {"figType": "Figure", "name": "5", "captionBoundary": {"x2": 287.0105895996094, "y1": 529.152587890625, "x1": 52.15800476074219, "y2": 571.0199584960938}, "imageText": ["(a)", "man", "RIDING", "elephant", "(b)", "woman", "RIDING", "ele-", "phant"], "regionBoundary": {"x2": 279.5482482910156, "y1": 346.8900146484375, "x1": 59.0, "y2": 513.5360107421875}, "caption": "Figure 5: In this example from Visual Genome,suppose we have seen (man, riding, elephant) in training. In the second image, we want to be able to predict riding, even if we have never seen the (woman, riding) combination before", "page": 3}, {"figType": "Figure", "name": "6", "captionBoundary": {"x2": 539.7144165039062, "y1": 518.1915283203125, "x1": 304.86602783203125, "y2": 560.0599365234375}, "imageText": ["(a)", "man", "RIDING", "elephant", "(b)", "man", "RIDING", "bike"], "regionBoundary": {"x2": 530.0, "y1": 346.8900146484375, "x1": 312.0, "y2": 502.57501220703125}, "caption": "Figure 6: In this example from Visual Genome, suppose we have seen (man, riding, elephant) in training. In the second image, we want to be able to predict riding, even if we have never seen the (riding, bike) combination before", "page": 3}, {"figType": "Figure", "name": "1", "captionBoundary": {"x2": 287.0063171386719, "y1": 469.3065490722656, "x1": 52.15800094604492, "y2": 499.218994140625}, "imageText": [], "regionBoundary": {"x2": 288.0, "y1": 305.8900146484375, "x1": 52.0, "y2": 457.8900146484375}, "caption": "Figure 1: woman [?] bed. Language models adopt the choice seen most commonly, i.e., sleeping on, but we propose an image-specific model.", "page": 0}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 398.619884914822, "y1": 651.8146514892578, "x1": 72.44166798061795, "y2": 693.3597140842014}, "name": "1", "caption_text": "Figure 1: woman [?] bed. Language models adopt the choice seen most commonly, i.e., sleeping on, but we propose an image-specific model.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 394.0, "y1": 429.0, "x1": 78.0, "y2": 632.0}, "page": 0, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 749.6029324001736, "y1": 347.7771759033203, "x1": 423.42503865559894, "y2": 372.7181328667535}, "name": "2", "caption_text": "Figure 2: Examples of explicit spatial relations from Visual Genome of (cat, ?, chair)", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 736.0, "y1": 110.0, "x1": 434.0, "y2": 329.0}, "page": 1, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 749.6031019422743, "y1": 662.3021443684895, "x1": 423.42503865559894, "y2": 687.2430589463976}, "name": "3", "caption_text": "Figure 3: Examples of implicit spatial relations from Visual Genome of (man, ?, surfboard)", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 739.0, "y1": 409.0, "x1": 434.0, "y2": 640.0}, "page": 1, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 398.62789577907984, "y1": 357.8271230061849, "x1": 72.44166798061795, "y2": 449.18619791666663}, "name": "4", "caption_text": "Figure 4: cat BENEATH chair. Even if we have never seen beneath as a relation during the training phase but have seen cat UNDER chair (Figure 2), using the BERT predictions and re-scoring the choices using the GloVe embedding similarity, we want to be able to predict unseen relations at test time.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 399.0, "y1": 96.0, "x1": 72.0, "y2": 357.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 749.607679578993, "y1": 262.07296583387586, "x1": 72.44166798061795, "y2": 287.013922797309}, "name": "1", "caption_text": "Table 1: Comparison of performance (in percentage accuracy) of different models for explicit and implicit spatial relations, normal and f-BERT and combinations with the best spatial model for varying portions of training data.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 710.0, "y1": 95.0, "x1": 112.0, "y2": 231.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 749.607679578993, "y1": 393.8007354736328, "x1": 72.44166798061795, "y2": 435.3458828396267}, "name": "2", "caption_text": "Table 2: Experiments for unseen subjects, objects and relations (for both explicit and implicit relations). Spatial BERT gives better performance than BERT or FF+I for Impl(O, R) but not in Impl(S, R) potentially because the subject set is much sparser than the object set.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 679.0, "y1": 308.0, "x1": 150.0, "y2": 394.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 749.6033562554253, "y1": 719.710456000434, "x1": 423.42503865559894, "y2": 777.8610229492188}, "name": "6", "caption_text": "Figure 6: In this example from Visual Genome, suppose we have seen (man, riding, elephant) in training. In the second image, we want to be able to predict riding, even if we have never seen the (riding, bike) combination before", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 736.0, "y1": 482.0, "x1": 434.0, "y2": 701.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 398.6258188883463, "y1": 734.9341498480902, "x1": 72.4416732788086, "y2": 793.083275689019}, "name": "5", "caption_text": "Figure 5: In this example from Visual Genome,suppose we have seen (man, riding, elephant) in training. In the second image, we want to be able to predict riding, even if we have never seen the (woman, riding) combination before", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 388.0, "y1": 482.0, "x1": 83.0, "y2": 716.0}, "page": 3, "dpi": 0}], "error": null, "pdf": "/work/host-output/9f096bd151bd57d87237bdd1b3038935ab0a4721/2020.lrec-1.288.pdf", "dpi": 100}