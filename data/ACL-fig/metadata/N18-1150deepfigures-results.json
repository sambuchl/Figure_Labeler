{"raw_detected_boxes": [[{"x2": 722.0, "y1": 312.0, "x1": 429.0, "y2": 451.0}], [{"x2": 695.0, "y1": 103.0, "x1": 135.0, "y2": 410.0}], [{"x2": 711.0, "y1": 90.0, "x1": 454.0, "y2": 322.0}], [], [], [{"x2": 701.0, "y1": 90.0, "x1": 126.0, "y2": 298.0}, {"x2": 685.0, "y1": 355.0, "x1": 142.0, "y2": 511.0}], [{"x2": 384.0, "y1": 232.0, "x1": 105.0, "y2": 299.0}], [{"x2": 734.0, "y1": 94.0, "x1": 100.0, "y2": 348.0}, {"x2": 395.0, "y1": 445.0, "x1": 105.0, "y2": 534.0}, {"x2": 708.0, "y1": 445.0, "x1": 429.0, "y2": 633.0}], [], [], [{"x2": 401.0, "y1": 97.0, "x1": 100.0, "y2": 283.0}], [], [], []], "raw_pdffigures_output": {"regionless-captions": [{"figType": "Table", "boundary": {"x2": 290.2731628417969, "y1": 389.83154296875, "x1": 72.00098419189453, "y2": 443.65496826171875}, "text": "Table 5: Head-to-Head and score-based comparison of human evaluations on random subset of CNN/DM dataset. SA=single, MA=multi-agent. \u2217 indicates statistical significance at p < 0.001 for focus and p < 0.03 for the overall.", "name": "5", "page": 7}, {"figType": "Figure", "boundary": {"x2": 525.547607421875, "y1": 464.3805236816406, "x1": 307.2770080566406, "y2": 518.2039794921875}, "text": "Figure 4: The average ROUGE-L scores for summaries that are binned by each agent\u2019s average attention when generating the summary (see Section 5.2). When the agents contribute equally to the summary, the ROUGEL score increases.", "name": "4", "page": 7}], "figures": [{"figType": "Figure", "name": "1", "captionBoundary": {"x2": 525.547607421875, "y1": 327.30255126953125, "x1": 307.2770080566406, "y2": 393.0809631347656}, "imageText": ["agent", "a", "agent", "b", "layer-1", "layer-2", "layer-3", "1", "2", "1", "2", "paragraphs", "\u2026", "\u2026", "\u2026", "\u2026", "messages", "\u2026", "Tired", "of", "counting", "sheep", "?", "Try", "one", "of", "these", "remedies", "and", "get", "a", "good", "nights", "sleep:Aroma", "..", "How", "to", "use", "it:", "Massage", "a", "dab", "of", "aroma", "therapeutic", "balmor", "oil\u2026", "\u2026", "\u2026", "\u2026"], "regionBoundary": {"x2": 524.0, "y1": 221.8900146484375, "x1": 309.0, "y2": 327.30255126953125}, "caption": "Figure 1: Illustration of deep communicating agents presented in this paper. Each agent a and b encodes one paragraph in multiple layers. By passing new messages through multiple layers the agents are able to coordinate and focus on the important aspects of the input text.", "page": 0}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 525.5469360351562, "y1": 222.96853637695312, "x1": 72.0009994506836, "y2": 240.926025390625}, "imageText": ["Model", "ROUGE-1", "ROUGE-2", "ROUGE-L", "SummaRuNNer", "(Nallapati", "et", "al.,", "2017)", "39.60", "16.20", "35.30", "graph-based", "attention", "(Tan", "et", "al.,", "2017)", "38.01", "13.90", "34.00", "pointer", "generator", "(See", "et", "al.,", "2017)", "36.44", "15.66", "33.42", "pointer", "generator", "+", "coverage", "(See", "et", "al.,", "2017)", "39.53", "17.28", "36.38", "controlled", "summarization", "with", "\ufb01xed", "values", "(Fan", "et", "al.,", "2017)", "39.75", "17.29", "36.54", "RL,", "with", "intra-attention", "(Paulus", "et", "al.,", "2018)", "41.16", "15.75", "39.08", "ML+RL,", "with", "intra-attention(Paulus", "et", "al.,", "2018)", "39.87", "15.82", "36.90", "(m1)", "MLE,", "pgen,", "no-comm", "(1-agent)", "(our", "baseline-1)", "36.12", "14.38", "33.83", "(m2)", "MLE+SEM,", "pgen,", "no-comm", "(1-agent)", "(our", "baseline-2)", "36.90", "15.02", "33.00", "(m3)", "MLE+RL,", "pgen,", "no-comm", "(1-agent)", "(our", "baseline-3)", "38.01", "16.43", "35.49", "(m4)", "DCA", "MLE+SEM,", "pgen,", "no-comm", "(3-agents)", "37.45", "15.90", "34.56", "(m5)", "DCA", "MLE+SEM,", "mpgen,", "with-comm", "(3-agents)", "39.52", "17.12", "36.90", "(m6)", "DCA", "MLE+SEM,", "mpgen,", "with-comm,", "with", "caa", "(3-agents)", "41.11", "18.21", "36.03", "(m7)", "DCA", "MLE+SEM+RL,", "mpgen,", "with-comm,", "with", "caa", "(3-agents)", "41.69", "19.47", "37.92"], "regionBoundary": {"x2": 505.0, "y1": 62.8900146484375, "x1": 90.0, "y2": 214.8900146484375}, "caption": "Table 1: Comparison results on the CNN/Daily Mail test set using the F1 variants of Rouge. Best model models are bolded.", "page": 5}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 525.5401611328125, "y1": 376.77752685546875, "x1": 72.0009994506836, "y2": 394.7349853515625}, "imageText": ["Model", "Rouge-1", "Rouge-2", "Rouge-L", "ML,", "no", "intra-attention", "(Paulus", "et", "al.,", "2018)", "44.26", "27.43", "40.41", "RL,", "no", "intra-attention", "(Paulus", "et", "al.,", "2018)", "47.22", "30.51", "43.27", "ML+RL,", "no", "intra-attention(Paulus", "et", "al.,", "2018)", "47.03", "30.72", "43.10", "(m1)", "MLE,", "pgen,", "no-comm", "(1-agent)", "(our", "baseline-1)", "44.28", "26.01", "37.87", "(m2)", "MLE+SEM,", "pgen,", "no-comm", "(1-agent)", "(our", "baseline-2)", "44.50", "28.04", "38.80", "(m3)", "MLE+RL,", "pgen,", "no-comm", "(1-agent)", "(our", "baseline-3)", "46.15", "29.50", "39.38", "(m4)", "DCA", "MLE+SEM,", "pgen,", "no-comm", "(3-agents)", "45.84", "28.23", "39.32", "(m5)", "DCA", "MLE+SEM,", "mpgen,", "with-comm", "(3-agents)", "46.20", "30.01", "40.65", "(m6)", "DCA", "MLE+SEM,", "mpgen,", "with-comm,", "with", "caa", "(3-agents)", "47.30", "30.50", "41.06", "(m7)", "DCA", "MLE+SEM+RL,", "mpgen", "with-comm,", "with", "caa", "(3-agents)", "48.08", "31.19", "42.33"], "regionBoundary": {"x2": 493.0, "y1": 255.8900146484375, "x1": 102.0, "y2": 367.8900146484375}, "caption": "Table 2: Comparison results on the New York Times test set using the F1 variants of Rouge. Best model models are bolded.", "page": 5}, {"figType": "Table", "name": "6", "captionBoundary": {"x2": 290.2716369628906, "y1": 216.09957885742188, "x1": 72.0009994506836, "y2": 234.05804443359375}, "imageText": ["Stats", "CNN/DM", "NYT", "Avg.", "#", "tokens", "document", "781", "549", "Avg.", "#", "tokens", "summary", "56", "40", "Total", "#", "train", "doc-summ.", "pair", "287,229", "589,284", "Total", "#", "validation", "doc-summ.", "pair", "13,368", "32,736", "Total", "#", "test", "doc-summ.", "pair", "11,490", "32,739", "Input", "token", "length", "400/800", "800", "Output", "token", "length", "100", "100", "(2-agent)", "Input", "token", "length", "/", "agent", "375", "400", "(3-agent)", "Input", "token", "length", "/", "agent", "250", "200", "(5-agent)", "Input", "token", "length", "/", "agent", "150", "160"], "regionBoundary": {"x2": 298.77366638183594, "y1": 91.8900146484375, "x1": 72.0, "y2": 203.8900146484375}, "caption": "Table 6: Summary statistics of CNN/DailyMail (DM) and New York Times (NYT) Datasets.", "page": 10}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 525.545166015625, "y1": 301.8415832519531, "x1": 72.00096893310547, "y2": 357.26806640625}, "imageText": ["B", "-L", "ST", "M", "\u2026", "\u2026", "\u2026", "B", "-L", "ST", "M", "B", "-L", "ST", "M", "rs", "La", "ye", "d", "er", "En", "co", "al", "xt", "u", "n", "te", "C", "o", "\u2026", "\u2026", "\u2026", "\u2026", "\u2026", "o", "n", "en", "ti", "A", "tt", "n", "t", "A", "ge", "al", "xt", "u", "n", "te", "C", "o", "agent", "attention", "(t-1)", "final", "distribution", "\u201cyen\u201d", "\u201ccalm\u201d", "a", "zoo", "vocabulary", "distribution", "a", "zoo", "LS", "TM", "d", "er", "ec", "o", "Fragrances<Start>", "that", "make", "you", "feel", "D", "vector", "(", ")", "agent", "context", "word", "context", "vector", "(", ")", "word", "context", "vector", "(", ")", "\u2026", "\u2026", "\u2026", "agent", "attention", "(t)", "vector", "(", ")", "word", "context", "B", "-L", "ST", "M", "Melatonin", "supplement", "-", "source", "paragraph-3", "B", "-L", "ST", "M", "n", "ti", "o", "W", "o", "rd", "A", "tt", "en", "B", "-L", "ST", "M", "You", "don\u2019t", "have", "source", "paragraph-2", "B", "-L", "ST", "M", "B", "-L", "ST", "M", "Tired", "of", "counting", "source", "paragraph-1", "B", "-L", "ST", "M", "er", "Lo", "ca", "l", "En", "co", "d"], "regionBoundary": {"x2": 501.0, "y1": 61.8900146484375, "x1": 97.0, "y2": 301.8415832519531}, "caption": "Figure 2: Multi-agent-encoder-decoder overview. Each agent a encodes a paragraph using a local encoder followed by multiple contextual layers with agent communication through concentrated messages z(k)a at each layer k. Communication is illustrated in Figure 3. The word context vectors cta are condensed into agent context c\u2217t . Agent specific generation probabilities, pta, enable voting for the suitable out-of-vocabulary words (e.g., \u2019yen\u2019) in the final distribution.", "page": 1}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 290.27154541015625, "y1": 216.18154907226562, "x1": 72.0009994506836, "y2": 246.09503173828125}, "imageText": ["Model", "ROUGE-1", "ROUGE-2", "ROUGE-L", "2-agent", "40.94", "19.16", "37.54", "3-agent", "41.69", "19.47", "37.92", "5-agent", "40.99", "19.02", "38.21"], "regionBoundary": {"x2": 276.0, "y1": 166.8900146484375, "x1": 86.0, "y2": 210.8900146484375}, "caption": "Table 3: Comparison of multi-agent models varying the number of agents using ROUGE results of model (m7) from Table 1 on CNN/Daily Maily Dataset.", "page": 6}, {"figType": "Table", "name": "9", "captionBoundary": {"x2": 525.5481567382812, "y1": 592.7525634765625, "x1": 72.00093841552734, "y2": 658.531005859375}, "imageText": ["everton", "beat", "burnley", "1", "-", "0", "at", "goodison", "park", "in", "the", "premier", "league", ".", "ross", "barkley", "steps", "up", "to", "take", "a", "10th", "minute", "penalty", "but", "missed", "it", ".", "barkley", "has", "scored", "15", "penalties", "from", "16", "attempts", "in", "the", "pitch", ".", "Multi", "Agent", "after", "ross", "barkley", "missed", "from", "the", "spot", "in", "their", "1", "-", "0", "win", "against", "burnley", "at", "goodison", "park", ".", "the", "untried", "barkley", "inexplicably", "took", "the", "10th", "minute", "kick", "awarded", "for", "a", "foul", "by", "david", "jones", "on", "aaron", "lennon", "rather", "than", "leighton", "baines", ",", "who", "has", "scored", "15", "penalties", "from", "16", "attempts", "in", "the", "premier", "league", ".", "everton", "manager", "roberto", "martinez", "was", "forced", "to", "defend", "another", "penalty", "\ufb01asco", "at", "the", "club", "Single", "Agent", "Baseline", "everton", "defeated", "burnley", "1", "-", "0", "at", "goodison", "park", "on", "saturday", ".", "kevin", "mirallas", "scored", "the", "only", "goal", "of", "the", "game", "in", "the", "29th", "minute", ".", "ross", "barkley", "had", "earlier", "missed", "a", "10th", "-", "minute", "penalty", ".", "leighton", "baines", "has", "scored", "15", "penalties", "from", "16", "attempts", "this", "season", ".", "Human", "(Gold)", "Document", "everton", "manager", "roberto", "martinez", "was", "forced", "to", "defend", "another", "penalty", "\ufb01asco", "at", "the", "club", "after", "ross", "barkley", "missed", "from", "the", "spot", "in", "their", "1", "-", "0", "win", "against", "burnley", "at", "goodison", "park", ".", "the", "untried", "barkley", "inexplicably", "took", "the", "10th", "minute", "kick", "awarded", "for", "a", "foul", "by", "david", "jones", "on", "aaron", "lennon", "rather", "than", "leighton", "baines", ",", "who", "has", "scored", "15", "penalties", "from", "16", "attempts", "in", "the", "premier", "league", ".", "although", "there", "was", "no", "dispute", "between", "the", "team", "-", "mates", "this", "time", ",", "it", "brought", "back", "memories", "of", "everton", "\u2019s", "match", "against", "west", "brom", "in", "january", "when", "kevin", "mirallas", "grabbed", "the", "ball", "from", "baines", "to", "take", "a", "penalty", "-", "and", "missed", ".", "ross", "barkley", "steps", "up", "to", "take", "a", "10th", "minute", "penalty", "despite", "the", "presence", "of", "leighton", "baines", "on", "the", "pitch", "barkley", "\u2019s", "effort", "is", "saved", "byburnley", "goalkeeper", "tom", "heaton", "at", "goodison", "park", "martinez", "insisted", "barkley", "was", "within", "his", "rights", "to", "request", "penalty", "-", "taking", "duties", "on", "saturday", ".", "\u2019", "if", "romelu", "lukaku", "had", "been", "on", "the", "pitch", ",", "he", "would", "have", "taken", "it", ".", "otherwise", ",", "i", "am", "happy", "to", "have", "three", "or", "four", "players", "who", "can", "take", "penalties", "and", "let", "it", "depend", "on", "how", "they", "feel", "at", "that", "moment", ",", "\u2019", "argued", "the", "everton", "manager", ".", "baines", "(", "left", ")has", "scored", "15", "penalties", "from", "16", "attempts", "in", "the", "premier", "league", "\u2019", "ross", "showed", "incredible", "responsibility", "to", "take", "it", ".", "i", "love", "seeing", "players", "take", "control", "of", "the", "big", "moments", "and", "leighton", "was", "happy", "to", "given", "him", "that", "responsibility", ".", "\u2019", "barkley", "\u2019s", "penalty", "was", "well", "-", "struck", "but", "wasn\u2019t", "put", "in", "the", "corner", "and", "burnley", "goalkeeper", "tom", "heaton", "dived", "to", "his", "right", "to", "save", ".", "fortunately", "for", "the", "young", "england", "player", ",", "it", "didn\u2019t", "prove", "costly", "as", "mirallas", "went", "on", "to", "score", "the", "only", "goal", "of", "the", "game", "after", "29", "minutes", ".", "everton", "boss", "roberto", "martinez", "issues", "instructions", "to", "his", "players", "during", "a", "break", "in", "play", "against", "burnley"], "regionBoundary": {"x2": 526.0, "y1": 162.8900146484375, "x1": 72.0, "y2": 589.8900146484375}, "caption": "Table 9: The single agent model generates summary with superfluous details and the facts are not clearly expressed. Although it was able to capture the statistics of the player correctly (e.g., 15 penalties, 16 attempts), it still missed the player who scored the only goal in the game (i.e., kevin mirallas). On the other hand multi-agent model was able to generate a concise summary with several key facts. However, similar to single agent model, it missed to capture the player who scored the only goal in the game. Interestingly, the document contains the word \u201ddefeated\u2019 but the multi-agent model chose to use beat instead, which does not exist in the original document.", "page": 13}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 525.54833984375, "y1": 243.39456176757812, "x1": 307.2769470214844, "y2": 310.776123046875}, "imageText": ["LSTM", "LSTM"], "regionBoundary": {"x2": 512.0, "y1": 64.8900146484375, "x1": 327.0, "y2": 233.8900146484375}, "caption": "Figure 3: Multi-agent encoder message passing. Agents b and c transmit the last hidden state output (I) of the current layer k as a message, which are passed through an average pool (Eq. (6)). The receiving agent a uses the new message z(k)a as additional input to its next layer.", "page": 2}, {"figType": "Table", "name": "8", "captionBoundary": {"x2": 525.54833984375, "y1": 646.152587890625, "x1": 72.00096893310547, "y2": 699.9760131835938}, "imageText": ["michelle", "pfeiffer", "is", "set", "to", "star", "in", "a", "new", "tv", "comedy", "about", "a", "morning", "news", "program", ".", "couric", "will", "serve", "as", "an", "executive", "producer", "and", "showrunner", "on", "the", "project", ".", "the", "series", "was", "created", "by", "diane", "english", ",", "the", "creator", "of", "murphy", "brown", ".", "pfeiffer", "is", "the", "one", "of", "the", "biggest", "stars", ".", "Multi", "Agent", "liaisons", "and", "the", "age", "of", "innocence", ",", "has", "teamed", "up", "with", "katie", "couric", "to", "pitch", "a", "new", "tele-", "vision", "comedy", "about", "a", "morning", "news", "program", ".", "also", "involved", "in", "the", "project", ",", "in", "which", "pfeiffer", "is", "attached", "to", "star", ",", "is", "diane", "english", ",", "the", "creator", "of", "murphy", "brown", ".", "the", "oscar", "nominated", "star", "known", "for", "her", "roles", "in", "iconic", "\ufb01lmssuch", "as", "scarface", ",", "dangerous", "Single", "Agent", "Baseline", "michelle", "pfeiffer", "is", "set", "to", "star", "in", "a", "new", "television", "comedy", "about", "a", "morning", "news", "program", ".", "katie", "couric", "will", "serve", "as", "an", "executive", "producer", ",", "drawing", "on", "her", "experience", "as", "an", "anchor", "on", "today", "for", "15", "years", ".", "the", "series", "was", "created", "by", "diane", "english", ",", "who", "was", "behind", "the", "show", "murphy", "brown", ",", "about", "a", "female", "news", "anchor", ".", "the", "ladies", "are", "currently", "in", "talks", "with", "hbo", ",", "showtime", ",", "amc", ",", "net\ufb02ix", "and", "amazon", "to", "pick", "up", "the", "program", ".", "Human", "(Gold)", "Document", "michelle", "pfeiffer", "is", "the", "latest", "hollywood", "star", "preparing", "to", "hit", "the", "small", "screen", ".", "the", "oscar", "nominated", "star", "known", "for", "her", "roles", "in", "iconic", "\ufb01lms", "such", "as", "scarface", ",", "dangerous", "liaisons", "andthe", "age", "of", "innocence", ",", "has", "teamed", "up", "with", "katie", "couric", "to", "pitch", "a", "new", "television", "comedy", "about", "a", "morning", "news", "program", ".", "also", "involved", "in", "the", "project", ",", "in", "which", "pfeiffer", "is", "attached", "to", "star", ",", "is", "diane", "english", ",", "the", "creator", "of", "murphy", "brown", ".", "scroll", "down", "for", "video", "michelle", "pfeiffer", "(", "left", ")", "is", "set", "to", "star", "in", "a", "new", "television", "comedy", "about", "a", "morning", "news", "program", "produced", "by", "katie", "couric", "(", "right", ")", "the", "series", "was", "created", "by", "diane", "english", "(", "above", "with", "candice", "bergen", ")", ",", "who", "was", "behind", "the", "show", "murphy", "brown", ",", "about", "a", "female", "news", "anchor", "according", "to", "variety", ",", "pfeiffer", "\u2019s", "role", "will", "be", "that", "of", "a", "morning", "news", "anchor", ",", "making", "it", "very", "similar", "to", "the", "real", "life", "role", "couric", "played", "as", "co", "-", "host", "of", "today", "for", "15", "years", ".", "couric", "will", "serve", "as", "an", "executive", "producer", "and", "help", "\u2019", "ensure", "the", "series", "strikes", "realistic", "notes", ".", "\u2019", "the", "creator", "behind", "the", "project", ",", "english", ",", "was", "previously", "the", "brains", "behind", "brown", ",", "the", "show", "starring", "candice", "bergen", "that", "centered", "around", "a", "female", "news", "anchor", "and", "ran", "for", "ten", "seasons", ",", "winning", "18", "emmys", ".", "english", "would", "also", "serve", "as", "a", "writer", ",", "producer", "and", "showrunner", "on", "the", "program.", "the", "ladies", "are", "currently", "in", "talks", "with", "hbo", ",", "showtime", ",", "amc", ",", "net\ufb02ix", "and", "amazon", "to", "pick", "up", "the", "program", ".", "couric", "will", "serve", "as", "an", "executive", "producer", ",", "drawing", "on", "her", "experience", "as", "an", "anchor", "on", "today", "for", "15", "years", "pfeiffer", "would", "be", "the", "one", "of", "the", "biggest", "stars", "yet", "to", "move", "to", "television", ",", "joining", "a", "group", "that", "now", "includes", "house", "of", "cards", "stars", "robin", "wright", "and", "kevin", "spacey", ",", "true", "detective", "leads", "matthew", "mcconaughey", "and", "woody", "harrelson", ",", "and", "even", "lady", "gaga", ",", "who", "recently", "announced", "she", "would", "be", "appearing", "on", "the", "next", "season", "of", "american", "horror", "story", ".", "the", "actress", "has", "kept", "a", "low", "pro\ufb01le", "for", "the", "past", "20", "years", "since", "becoming", "a", "mother", ",", "only", "doing", "a", "handful", "of", "\ufb01lms", "in", "that", "time", ".", "she", "most", "recently", "appeared", "alongside", "robert", "de", "niro", "in", "the", "mob", "comedy", "\u2019", "the", "family", ".", "\u2019"], "regionBoundary": {"x2": 526.0, "y1": 121.8900146484375, "x1": 72.0, "y2": 643.8900146484375}, "caption": "Table 8: The baseline model generates non-coherent summary that references the main character \u201cMichelle Pfeiffer\u201d in an ambiguous way towards the end of the generated summary. In contrast, the multi-agent model successfully captures the main character including the key facts. One interesting feature that the multi-agent model showcases is its simplification property, which accounts for its strength in abstraction. Specifically, it simplified the bold long sentence in the document starting with \u201dcouric will... and only generated the salient words.", "page": 12}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 525.54833984375, "y1": 255.56259155273438, "x1": 72.0009994506836, "y2": 273.52008056640625}, "imageText": ["\ufb02o", "dron", "&", "other", "hair", "collection", ".", "She", "was", "still", "commanding", "1,000", "a", "day", "for", "her", "work.", "Multi", "Daphne", "Selfe,", "86,", "has", "starred", "in", "the", "campaign", "for", "vans", "and", "&", "other", "stories.", "The", "model", "appears", "with", "22-year-old", "shoes", "are", "then", "worn", "with", "pieces", "from", "the", "brands", "ss2015", "collection.", "&", "other", "stories", "collection", "that", "is", "featured", "in", "this", "story", "is", "truly", "relaxed", "and", "timeless", "with", "a", "modern", "twist\u2019.", "The", "Human", "Mr", "Turnbull", "was", "interviewed", "about", "his", "childhood", "and", "his", "political", "stance.", "He", "also", "admitted", "he", "planned", "to", "run", "for", "prime", "minister", "if", "Tony", "Abbott", "had", "been", "successfully", "toppled", "in", "February\u2019s", "leadership", "spill.", "The", "words", "\u2019primed", "minister\u2019", "were", "controversially", "also", "printed", "on", "the", "cover.", "Single", "Malcolm", "Turnbull", "is", "set", "to", "feature", "on", "the", "front", "cover", "of", "the", "GQ", "Australia", "in", "a", "bold", "move", "that", "will", "no", "doubt", "set", "sena-", "tors\u2019", "tongues", "wagging.", "Posing", "in", "a", "suave", "blue", "suit", "with", "a", "pinstriped", "shirt", "and", "a", "contrasting", "red", "tie", ",", "Mr", "Turnbull\u2019s", "con\ufb01dent", "demeanour", "is", "complimented", "by", "the", "bold,", "confronting", "words", "printed", "across", "the", "page:", "\u2019primed", "minister\u2019.", "Multi", "Malcolm", "Turnbull", "was", "set", "to", "run", "for", "prime", "minister", "if", "Tony", "Abbott", "had", "been", "successfully", "toppled", "in", "Febru-", "ary\u2019s", "leadership", "spill.", "He", "is", "set", "to", "feature", "on", "the", "front", "cover", "of", "the", "liberal", "party\u2019s", "newsletter.", "Human", "Daphne", "Selfe", "has", "been", "modelling", "since", "the", "\ufb01fties.", "She", "has", "recently", "landed", "a", "new", "campaign", "with", "vans", "and", "&", "other", "stories.", "The", "86-year-old", "commands", "1,000", "a", "day", "for", "her", "work.", "Single", "Daphne", "Selfe,", "86,", "shows", "off", "the", "collaboration", "between", "the", "footwearsuper-brandand", "theetherealhigh", "street", "store", "with", "uncompromisinggrace.", "Daphne", "said", "of", "the", "collection", ",", "in", "which", "she", "appears", "with", "22-year-old", "\ufb02o", "dron:", "\u2019the"], "regionBoundary": {"x2": 530.0, "y1": 64.8900146484375, "x1": 72.0, "y2": 252.8900146484375}, "caption": "Table 4: Comparison of a human summary to best single- and multi-agent model summaries, (m3) and (m7) from CNN/DailyMail dataset. Although single-agent model generates a coherent summary, it is less focused and", "page": 7}, {"figType": "Table", "name": "7", "captionBoundary": {"x2": 525.5482788085938, "y1": 646.133544921875, "x1": 72.0009994506836, "y2": 723.8670043945312}, "imageText": ["abbey", "says", "she", "is", "proud", "to", "be", "part", "of", "a", "campaign", "that", "funds", "vital", "work", "towards", "ensuring", "the", "next", "generation", "of", "young", "women", "do", "not", "have", "been", "afraid", "of", "a", "diagnosis", "of", "breast", "cancer", ".", "the", "campaign", "has", "raised", "13", ".", "5m", "for", "breakthrough", "breast", "cancer", "\u2019s", "research", ".", "Multi", "Agent", "strictly", "come", "dancing", "joins", "singer", "foxes", ",", "25", ",", "victoria", "\u2019s", "secret", "angel", "lily", "donaldson", ",", "28", ",", "and", "model", "alice", "dellal", ",", "27", ",", "in", "the", "new", "series", "of", "pictures", "by", "photographer", "simon", "emmett", "for", "fashion", "targets", "breast", "cancer", ".", "clancy", ",", "29", ",", "looks", "chic", "as", "she", "shows", "off", "her", "famous", "legs", ",", "wearing", "just", "a", "plain", "white", "shirt", ".", "Single", "Agent", "Baseline", "models", "abbey", "and", "lily", "are", "joined", "by", "alice", "dellal", "and", "singer", "foxes", ".", "the", "women", "are", "pictured", "\u2019", "wearing", "\u2019", "their", "support", ".", "abbey", ",", "29", ",", "says", "she", "is", "proud", "to", "be", "part", "of", "a", "campaign", "that", "funds", "vital", "work", ".", "campaign", "has", "raised", "13", ".", "5m", "for", "breakthrough", "breast", "cancer", "\u2019s", "research", ".", "Human", "(Gold)", "Document", "model", "abbey", "clancy", "is", "helping", "to", "target", "breast", "cancer", ",", "by", "striking", "a", "sultry", "pose", "in", "a", "new", "charity", "campaign", ".", "the", "winner", "of", "2013", "\u2019s", "strictly", "come", "dancing", "joins", "singer", "foxes", ",", "25", ",", "victoria", "\u2019s", "secret", "angel", "lily", "donaldson", ",", "28", ",", "and", "model", "alice", "dellal", ",", "27", ",", "in", "the", "new", "series", "of", "pictures", "by", "photographer", "simon", "emmett", "for", "fashion", "targets", "breast", "cancer", ".", "clancy", ",", "29", ",", "looks", "chic", "as", "she", "shows", "off", "her", "famous", "legs", ",", "wearing", "just", "a", "plain", "white", "shirt", ".", "abbey", "clancy", "leads", "the", "glamour", "as", "she", "joins", "forces", "with", "her", "famous", "friends", "to", "target", "breast", "cancer", ",", "by", "striking", "a", "sultry", "pose", "in", "a", "new", "charity", "campaign", "the", "model", ",", "who", "is", "mother", "to", "four", "-", "year", "-", "old", "daughter", "sophia", "with", "footballer", "husband", "peter", "crouch", ",", "said:", "\u2019", "as", "a", "mum", ",", "it", "makes", "me", "proud", "to", "be", "part", "of", "a", "campaign", "that", "funds", "vital", "work", "towards", "ensuring", "the", "next", "generation", "of", "young", "women", "do", "not", "have", "be", "afraid", "of", "a", "diagnosis", "of", "breast", "cancer", ".", "\u2019", "i\u2019m", "wearing", "my", "support", ",", "and", "i", "want", "everyone", "across", "the", "uk", "to", "do", "the", "same", "and", "get", "behind", "this", "campaign", ".", "\u2019", "holding", "onto", "heaven", "singer", "foxes", "looks", "foxy", "in", "cropped", "stripy", "top", "and", "jeans", ".", "abbey", "says", "she", "is", "proud", "to", "be", "part", "of", "a", "campaign", "that", "funds", "vital", "work", "towards", "ensuring", "the", "next", "generation", "of", "young", "women", "do", "not", "have", "be", "afraid", "of", "a", "diagnosis", "of", "breast", "cancer", "victoria", "\u2019s", "secret", "angel", "lily", "donaldson", ",", "who", "has", "been", "in", "the", "industry", "for", "years", ",", "also", "adds", "some", "glamour", "to", "the", "charity", "campaign", "holding", "onto", "heaven", "singer", "foxes", "dons", "a", "stripy", "top", "and", "jeans", "for", "the", "campaign", "she", "says", "she", "\u2019s", "\u2019", "honoured", "\u2019", "to", "be", "a", "part", "of", "she", "said:", "\u2019", "i\u2019m", "so", "honoured", "to", "be", "taking", "part", "in", "this", "year", "\u2019s", "fashion", "targets", "breast", "cancer", ",", "and", "becoming", "part", "of", "the", "campaign", "\u2019s", "awesome", "heritage", ".", "\u2019", "fashion", "is", "a", "huge", "part", "of", "my", "life", ",", "and", "if", "by", "taking", "part", "i", "can", "inspire", "women", "to", "wear", "their", "support", ",", "join", "the", "\ufb01ght", "and", "take", "on", "breast", "cancer", "head", "on", ",", "then", "that", "will", "be", "something", "to", "be", "really", "proud", "of", ".", "\u2019", "now", "in", "its", "19th", "year", ",", "the", "campaign", "has", "so", "far", "raised", "13", ".", "5m", "for", "breakthrough", "breast", "cancer", "\u2019s", "research", "funding", ".", "this", "year", "the", "range", "of", "clothes", "and", "accessories", "have", "been", "produced", "in", "conjunction", "with", "high", "street", "partners", "m&s", ",", "river", "island", ",", "warehouse", ",", "topshop", ",", "laura", "ashley", ",", "debenhams", ",", "superga", ",", "baukjen", "and", "the", "cambridge", "satchel", "company", ".", "they", "can", "be", "viewed", "online", "at", "www", ".", "fashiontargetsbreastcancer", ".", "org", ".", "uk/lookbook", "the", "campaign", ",", "which", "also", "stars", "alice", "dellal", ",", "has", "so", "far", "raised", "13", ".", "5m", "for", "breakthrough", "breast", "cancer", "\u2019s", "research", "funding"], "regionBoundary": {"x2": 526.0, "y1": 97.8900146484375, "x1": 72.0, "y2": 643.8900146484375}, "caption": "Table 7: In this example both single- and multi-agent models demonstrate extractive behaviors. However, each select sentences from different sections of the document. While the single model extracts the second and the third sentences, the multi-agent model successfully selects salient sentences from sentences that are further down in the document, specifically sentence 8 and 10. This can be attributed to the fact that agents can successfully encode salient aspects distributed in distant sections of the document. An interesting result is that even though the multiagent model shows extractive behaviour in this example, it successfully selects the most salient sentences while the single agent model includes superfluous details.", "page": 11}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 729.927232530382, "y1": 454.58687676323785, "x1": 426.77362230088977, "y2": 545.9457821316189}, "name": "1", "caption_text": "Figure 1: Illustration of deep communicating agents presented in this paper. Each agent a and b encodes one paragraph in multiple layers. By passing new messages through multiple layers the agents are able to coordinate and focus on the important aspects of the input text.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 729.0, "y1": 312.0, "x1": 427.0, "y2": 466.0}, "page": 0, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.923841688368, "y1": 419.2244211832682, "x1": 100.00134574042426, "y2": 496.2056477864583}, "name": "2", "caption_text": "Figure 2: Multi-agent-encoder-decoder overview. Each agent a encodes a paragraph using a local encoder followed by multiple contextual layers with agent communication through concentrated messages z(k)a at each layer k. Communication is illustrated in Figure 3. The word context vectors cta are condensed into agent context c\u2217t . Agent specific generation probabilities, pta, enable voting for the suitable out-of-vocabulary words (e.g., \u2019yen\u2019) in the final distribution.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 712.0, "y1": 90.0, "x1": 118.0, "y2": 427.0}, "page": 1, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9282497829861, "y1": 338.0480024549696, "x1": 426.7735375298394, "y2": 431.6335042317708}, "name": "3", "caption_text": "Figure 3: Multi-agent encoder message passing. Agents b and c transmit the last hidden state output (I) of the current layer k as a message, which are passed through an average pool (Eq. (6)). The receiving agent a uses the new message z(k)a as additional input to its next layer.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 712.0, "y1": 90.0, "x1": 454.0, "y2": 339.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9263000488281, "y1": 309.6785227457682, "x1": 100.00138812594943, "y2": 334.61947970920136}, "name": "1", "caption_text": "Table 1: Comparison results on the CNN/Daily Mail test set using the F1 variants of Rouge. Best model models are bolded.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 718.0, "y1": 86.0, "x1": 109.0, "y2": 315.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9168904622395, "y1": 523.3021206325955, "x1": 100.00138812594943, "y2": 548.2430352105034}, "name": "2", "caption_text": "Table 2: Comparison results on the New York Times test set using the F1 variants of Rouge. Best model models are bolded.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 702.0, "y1": 355.0, "x1": 125.0, "y2": 528.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.1549241807726, "y1": 300.2521514892578, "x1": 100.00138812594943, "y2": 341.79865519205725}, "name": "3", "caption_text": "Table 3: Comparison of multi-agent models varying the number of agents using ROUGE results of model (m7) from Table 1 on CNN/Daily Maily Dataset.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 401.0, "y1": 232.0, "x1": 100.0, "y2": 316.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9282497829861, "y1": 354.9480438232422, "x1": 100.00138812594943, "y2": 379.88900078667535}, "name": "4", "caption_text": "Table 4: Comparison of a human summary to best single- and multi-agent model summaries, (m3) and (m7) from CNN/DailyMail dataset. Although single-agent model generates a coherent summary, it is less focused and", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 736.0, "y1": 88.0, "x1": 100.0, "y2": 365.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.15717061360675, "y1": 541.4326985677083, "x1": 100.00136693318684, "y2": 616.1874559190538}, "name": "5", "caption_text": "Table 5: Head-to-Head and score-based comparison of human evaluations on random subset of CNN/DM dataset. SA=single, MA=multi-agent. \u2217 indicates statistical significance at p < 0.001 for focus and p < 0.03 for the overall.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 403.0, "y1": 445.0, "x1": 100.0, "y2": 551.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.927232530382, "y1": 644.9729495578342, "x1": 426.77362230088977, "y2": 719.7277492947048}, "name": "4", "caption_text": "Figure 4: The average ROUGE-L scores for summaries that are binned by each agent\u2019s average attention when generating the summary (see Section 5.2). When the agents contribute equally to the summary, the ROUGEL score increases.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 725.0, "y1": 445.0, "x1": 427.0, "y2": 650.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.1550513373481, "y1": 300.1383039686415, "x1": 100.00138812594943, "y2": 325.0806172688802}, "name": "6", "caption_text": "Table 6: Summary statistics of CNN/DailyMail (DM) and New York Times (NYT) Datasets.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 418.0, "y1": 90.0, "x1": 100.0, "y2": 300.0}, "page": 10, "dpi": 0}], "error": null, "pdf": "/work/host-output/58e6c3e3396e7d15f50f101b157d2e93d84fdbb3/N18-1150.pdf", "dpi": 100}