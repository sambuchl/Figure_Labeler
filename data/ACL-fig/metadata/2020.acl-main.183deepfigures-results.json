{"raw_detected_boxes": [[], [], [{"x2": 723.0, "y1": 90.0, "x1": 432.0, "y2": 287.0}], [{"x2": 712.0, "y1": 97.0, "x1": 115.0, "y2": 466.0}, {"x2": 371.0, "y1": 585.0, "x1": 129.0, "y2": 673.0}], [{"x2": 358.0, "y1": 86.0, "x1": 139.0, "y2": 166.0}], [], [{"x2": 398.0, "y1": 86.0, "x1": 100.0, "y2": 181.0}], [{"x2": 702.0, "y1": 88.0, "x1": 446.0, "y2": 223.0}, {"x2": 400.0, "y1": 93.0, "x1": 100.0, "y2": 373.0}], [{"x2": 653.0, "y1": 93.0, "x1": 174.0, "y2": 236.0}], []], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Table", "name": "4", "captionBoundary": {"x2": 290.2706298828125, "y1": 142.36355590820312, "x1": 72.0, "y2": 196.18707275390625}, "imageText": ["ConvAI2", "64.4%", "38.9%", "61.1%", "48.1%", "WoW", "11.3%", "29.4%", "10.0%", "21.3%", "ED", "24.2%", "31.6%", "28.8%", "30.5%", "Utt.", "Selected", "orig.", "debiased", "orig.", "debiased", "MT", "Single-Skills", "MT", "S.-S.", "+", "BST"], "regionBoundary": {"x2": 288.0, "y1": 62.8900146484375, "x1": 72.0, "y2": 129.8900146484375}, "caption": "Table 4: Mitigating skill selection bias. Adding personas and topics during multi-task training (debias) results in the multi-task retrieval models selecting utterances more evenly when tested on BlendedSkillTalk compared to training on the original datasets (orig).", "page": 6}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 525.5465087890625, "y1": 221.48153686523438, "x1": 307.2760009765625, "y2": 287.26007080078125}, "imageText": ["1730ED", "494", "WoW", "602", "WoW", "ConvAI2", "634", "2221ED", "773", "WoW", "682", "ED", "ConvAI2", "766", "1599ED", "496", "WoW", "536", "ConvAI2", "ConvAI2", "567", "21468ED", "7257", "WoW", "6931", "Chosen", "suggestion", "Initial", "Context", "Count", "Total", "none", "ConvAI2", "7280"], "regionBoundary": {"x2": 522.0, "y1": 64.3799057006836, "x1": 309.0, "y2": 206.16400146484375}, "caption": "Table 1: Guided workers choice of suggestions in the train set of BlendedSkillTalk, broken down by provenance of the given initial context utterances. Guided workers often choose not to use the suggestions, but have a slight preference for ConvAI2 when the initial context is from that dataset, and similarly for ED.", "page": 2}, {"figType": "Table", "name": "6", "captionBoundary": {"x2": 525.5466918945312, "y1": 172.46951293945312, "x1": 307.2760009765625, "y2": 309.9791259765625}, "imageText": ["BST", "-", "79.2", "Random-Skill", "71.2", "-", "MT", "Two-Stage", "71.9", "-", "MT", "Single-Skills", "80.1", "83.8", "ConvAI2", "76.8", "81.7", "WoW", "67.5", "79.4", "ED", "69.0", "80.4", "Model", "BST,", "zero-shot", "+BST,", "FT"], "regionBoundary": {"x2": 509.0, "y1": 62.8900146484375, "x1": 321.0, "y2": 159.8900146484375}, "caption": "Table 6: Test results on BlendedSkillTalk. BST, zeroshot: the models are tested directly on the test set of BST without having been fine-tuned on the BST train set. +BST, FT: models are fine-tuned on the BST train set, then tested on the BST test set. Multi-Task SingleSkills + BlendedSkillTalk performs best. The MultiTask Two-Stage model outperforms two of the singleskill models, but the latter work well when combined with BlendedSkillTalk fine-tuning. We hypothesize that ConvAI2 alone performs well because it has been trained to use persona contexts, that are used throughout the BST dialogues.", "page": 7}, {"figType": "Table", "name": "5", "captionBoundary": {"x2": 290.2706604003906, "y1": 285.3755187988281, "x1": 72.0, "y2": 626.122802734375}, "imageText": ["Single-task", "82.1", "88.2", "60.2", "76.8", "MT", "Two-Stage", "77.2", "86.6", "59.0", "74.3", "MT", "Single-Skills", "85.2", "92.1", "61.1", "79.5", "Mixed-candidates", "evaluation", "MT", "Single-Skills", "88.9", "92.8", "63.2", "81.6", "Added-context", "benchmarks", "BST", "model", "78.5", "84.1", "52.0", "71.5", "Random-Skill", "71.0", "83.9", "52.0", "69.0", "MT", "Two-Stage", "84.7", "90.1", "63.4", "79.4", "MT", "Single-Skills", "88.8", "92.8", "63.2", "81.6", "ConvAI2", "89.4", "78.4", "42.6", "70.1", "WoW", "57.3", "91.8", "47.7", "65.6", "ED", "63.3", "81.0", "65.1", "69.8", "SOTA", "Reported", "87.3", "87.4", "66.0", "80.2", "Model", "ConvAI2", "WoW", "ED", "Avg.", "Single-skill", "benchmarks"], "regionBoundary": {"x2": 291.0, "y1": 62.8900146484375, "x1": 72.0, "y2": 271.8900146484375}, "caption": "Table 5: Results on single-skill benchmarks. Top: reported values published in the papers accompanying the benchmarks, and the Poly-encoder paper. ConvAI2, WoW, ED: models trained on the corresponding benchmark. These models perform very well on the benchmark they were trained on, but not as well on other benchmarks. BST: The model fine-tuned on BST shows more balanced performance (i.e., none of the single-skill benchmarks does better at all three skills), but it is noticeably lower than each specialized model. Random-Skill: the performance of choosing a random single-skill per response is comparable to the BST model, but slightly worse on ConvAI2. MT Two-Stage: guiding the generation by an actual task classifier as opposed to random selection increases performance on all skills. MT Single-Skills: this model performs best among the blended skills architectures, and nearly matches the single-skill model performance (and surpasses it in the WoW case). Added-context benchmarks: when the benchmark contexts are augmented with a persona and topic as described in section 3.2, the evaluation results barely change. Mixed-candidates evaluation: when the set of benchmark candidates is tripled by adding candidates from the other two benchmarks in equal proportion, the performance of the best respective single-task models suffers, while the MT Single-Skills model proves more resilient. Note that Single-task averages in italics do not correspond to a single model, but an average over 3 models.", "page": 7}, {"figType": "Figure", "name": "1", "captionBoundary": {"x2": 525.5472412109375, "y1": 351.79754638671875, "x1": 72.0, "y2": 393.66497802734375}, "imageText": ["Games", "are", "therapeutic", "to", "some.", "(S)", "U:", "I", "use", "games", "to", "relax", "after", "a", "stressful", "day,", "the", "small", "escape", "is", "relaxing.", "(PB)", "(G", "selected", "the", "ED", "suggestion:", "\u201dI", "enjoy", "doing", "that", "after", "a", "hard", "day", "at", "work", "as", "well.", "I", "hope", "it", "relaxes", "you!\u201d)", "G:", "I", "enjoy", "a", "good", "gaming", "session", "after", "a", "hard", "day", "at", "work", "as", "well.", "(PB)", "U:", "What", "other", "hobbies", "does", "your", "son", "have?", "(PB)", "G:", "Well", "he", "likes", "to", "\ufb02y", "kites", "and", "collect", "bugs,", "typical", "hobbies", "for", "an", "8", "year", "old,", "lol.", "(PB)", "U:", "My", "12", "year", "old", "is", "into", "sports.", "Football", "mostly.", "I", "however", "don;t", "enjoy", "watching", "him", "play.", "(PB)", "G:", "I", "wish", "I", "could", "play", "football,", "But", "I", "wear", "this", "cateye", "glasses", "and", "they", "would", "break", "if", "I", "tried.", "(PB)", "U:", "Sounds", "nice.", "Are", "they", "new", "or", "vintage?", "(E)", "G:", "They", "are", "new,", "I", "got", "them", "because", "of", "my", "love", "for", "cats", "lol.", "I", "have", "to", "show", "off", "my", "beautiful", "green", "eyes", "somehow.", "(S)", "been", "challenged", "as", "works", "of", "art", "by", "some", "critics.", "(K)", "U:", "Video", "games", "are", "undervalued", "by", "many", "and", "too", "easily", "blamed", "for", "problems", "like", "obesity", "or", "violence", "in", "kids", "(K)", "G:", "Indeed,", "Just", "last", "week", "my", "son", "was", "playing", "some", "Tine", "2", "and", "it", "was", "keeping", "him", "so", "calm.", "Actual", "utterances:", "U:", "Exactly!", "I", "think", "many", "people", "fail", "to", "notice", "how", "beautiful", "the", "art", "of", "video", "games", "can", "be.", "(PB)", "(G", "selected", "the", "WoW", "suggestion:", "\u201dIndeed,", "Some", "games", "games", "are", "purposely", "designed", "to", "be", "a", "work", "of", "a", "persons", "creative", "expression,", "many", "though", "have", "been", "challenged", "as", "works", "of", "art", "by", "some", "critics.\u201d)", "G:", "Indeed,", "Some", "games", "games", "are", "purposely", "designed", "to", "be", "a", "work", "of", "a", "persons", "creative", "expression,", "many", "though", "have", "competence", "AND", "writing", "skills.", "that", "is", "one", "part", "many", "people", "forget", "Wizard", "of", "Wikipedia", "topic:", "Video", "game", "design", "Previous", "utterances", "(shown", "to", "speakers):", "U:", "What", "video", "games", "do", "you", "like", "to", "play?", "G:", "all", "kinds,", "action,", "adventure,", "shooter,", "platformer,", "rpg,", "etc.", "but", "video", "game", "design", "requires", "both", "artistic", "and", "technical", "Persona", "for", "Unguided", "Speaker:", "Persona", "for", "Guided", "Speaker:", "My", "son", "plays", "on", "the", "local", "football", "team.", "My", "eyes", "are", "green.", "I", "design", "video", "games", "for", "a", "living.", "I", "wear", "glasses", "that", "are", "cateye."], "regionBoundary": {"x2": 518.0, "y1": 65.8900146484375, "x1": 80.0, "y2": 339.8900146484375}, "caption": "Figure 1: Sample conversation from the BlendedSkillTalk dataset, annotated with four conversation mode types (PB: personal background; K: knowledge; S: personal situation; E: empathy). The guided (G) and unguided (U) workers are given personas and a topic. The conversation has been seeded with two utterances from a conversation sampled from WoW. When the guided worker selected one of the suggestions, it is shown in shaded grey.", "page": 3}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 290.2706298828125, "y1": 496.7325439453125, "x1": 72.0, "y2": 574.4659423828125}, "imageText": ["ConvAI2", "29.6", "25.3", "25.5", "WoW", "49.6", "57.5", "30.3", "ED", "20.8", "17.1", "44.2", "%", "classi\ufb01ed", "as:", "ConvAI2", "WoW", "ED", "Source", "of", "Seed", "Context"], "regionBoundary": {"x2": 267.0, "y1": 416.8900146484375, "x1": 93.0, "y2": 484.8900146484375}, "caption": "Table 2: Percentages of utterances of unguided workers classified by the dataset classifier as coming from ConvAI2, WoW, or ED, broken down by provenance of the provided seed context. For each dataset, the fraction of utterances classified as coming from that dataset is highest when the seed context is from that same dataset.", "page": 3}, {"figType": "Table", "name": "7", "captionBoundary": {"x2": 525.5473022460938, "y1": 182.43252563476562, "x1": 72.0, "y2": 296.0321044921875}, "imageText": ["BST", "3.5", "3.6", "3.1", "3.3", "Random-Skill", "3.2", "2.9", "3.2", "2.7", "MT", "Two-Stage", "3.7", "3.6", "3.3", "3.5", "MT", "Single-Skills", "3.7", "3.6", "3.0", "3.4", "MT", "Single-Skills", "+BST", "\ufb01ne-tuning", "3.7", "3.8", "3.2", "3.6", "ConvAI2", "3.2", "3.1", "3.4", "3.0", "WoW", "3.3", "2.9", "2.7", "2.6", "ED", "3.4", "3.3", "3.0", "3.0", "Model", "Knowledge", "Empathy", "Personal", "Overall", "quality"], "regionBoundary": {"x2": 471.0, "y1": 62.8900146484375, "x1": 124.0, "y2": 169.8900146484375}, "caption": "Table 7: Human evaluation results on individual axes of knowledge, empathy, and being personal, as well as overall quality. All results here have a 95% confidence interval of \u00b1 0.2 or 0.3, omitted to avoid cluttering the table. Results that are within the confidence interval of the best model performance are bolded. ConvAI2, WoW, ED: models pre-trained on pushshift.io Reddit and fine-tuned on the respective datasets. For Empathy and Personal topics, the individual models tend to do better when trained on a dataset tailored for that, however they all perform similarly on the Knowledge dimension. BST: model pre-trained on pushshift.io Reddit and fine-tuned on BST. This model is showing better overall performance compared to single-skill datasets (i.e., none of the three single-skill dataset do better than BST in every dimension). MT Single-Skills with fine-tuning on BST and MT Two-Stage are performing very well on all dimensions. MT Single-Skills with fine-tuning on BST has fewer than a third of the parameters of the MT Two-Stage model, yet manages to perform as well, if not slightly better.", "page": 8}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 290.2705993652344, "y1": 137.10452270507812, "x1": 72.0, "y2": 214.83807373046875}, "imageText": ["1", "51", "6.9%", "2", "167", "22.6%", "3", "290", "39.2%", "4", "232", "31.4%", "Mode", "Count", "Conversations", "Pct", "(%)"], "regionBoundary": {"x2": 260.0, "y1": 62.8900146484375, "x1": 100.0, "y2": 124.8900146484375}, "caption": "Table 3: Breakdown of conversations by number of modes, showing that most BST dataset conversations exhibit multiple modes. Workers were asked to choose if each utterance of a conversation demonstrated knowledge, empathy, personal situations, or personal background. Over 70% of the conversations annotated demonstrated at least 3 of the 4 modes.", "page": 4}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 729.9257066514757, "y1": 307.6132456461588, "x1": 426.772223578559, "y2": 398.9723205566406}, "name": "1", "caption_text": "Table 1: Guided workers choice of suggestions in the train set of BlendedSkillTalk, broken down by provenance of the given initial context utterances. Guided workers often choose not to use the suggestions, but have a slight preference for ConvAI2 when the initial context is from that dataset, and similarly for ED.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 724.0, "y1": 88.0, "x1": 429.0, "y2": 287.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9267239040798, "y1": 488.60770331488715, "x1": 100.0, "y2": 546.7569139268663}, "name": "1", "caption_text": "Figure 1: Sample conversation from the BlendedSkillTalk dataset, annotated with four conversation mode types (PB: personal background; K: knowledge; S: personal situation; E: empathy). The guided (G) and unguided (U) workers are given personas and a topic. The conversation has been seeded with two utterances from a conversation sampled from WoW. When the guided worker selected one of the suggestions, it is shown in shaded grey.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 719.0, "y1": 86.0, "x1": 111.0, "y2": 472.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.15365261501734, "y1": 689.9063110351562, "x1": 100.0, "y2": 797.8693644205729}, "name": "2", "caption_text": "Table 2: Percentages of utterances of unguided workers classified by the dataset classifier as coming from ConvAI2, WoW, or ED, broken down by provenance of the provided seed context. For each dataset, the fraction of utterances classified as coming from that dataset is highest when the seed context is from that same dataset.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 371.0, "y1": 579.0, "x1": 113.0, "y2": 690.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.1536102294922, "y1": 190.42294820149738, "x1": 100.0, "y2": 298.3862135145399}, "name": "3", "caption_text": "Table 3: Breakdown of conversations by number of modes, showing that most BST dataset conversations exhibit multiple modes. Workers were asked to choose if each utterance of a conversation demonstrated knowledge, empathy, personal situations, or personal background. Over 70% of the conversations annotated demonstrated at least 3 of the 4 modes.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 361.0, "y1": 86.0, "x1": 139.0, "y2": 174.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.15365261501734, "y1": 197.72716098361545, "x1": 100.0, "y2": 272.48204549153644}, "name": "4", "caption_text": "Table 4: Mitigating skill selection bias. Adding personas and topics during multi-task training (debias) results in the multi-task retrieval models selecting utterances more evenly when tested on BlendedSkillTalk compared to training on the original datasets (orig).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 400.0, "y1": 86.0, "x1": 100.0, "y2": 198.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9259609646267, "y1": 239.54099019368488, "x1": 426.772223578559, "y2": 430.5265638563368}, "name": "6", "caption_text": "Table 6: Test results on BlendedSkillTalk. BST, zeroshot: the models are tested directly on the test set of BST without having been fine-tuned on the BST train set. +BST, FT: models are fine-tuned on the BST train set, then tested on the BST test set. Multi-Task SingleSkills + BlendedSkillTalk performs best. The MultiTask Two-Stage model outperforms two of the singleskill models, but the latter work well when combined with BlendedSkillTalk fine-tuning. We hypothesize that ConvAI2 alone performs well because it has been trained to use persona contexts, that are used throughout the BST dialogues.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 707.0, "y1": 86.0, "x1": 429.0, "y2": 240.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.15369500054254, "y1": 396.3548872205946, "x1": 100.0, "y2": 869.615003797743}, "name": "5", "caption_text": "Table 5: Results on single-skill benchmarks. Top: reported values published in the papers accompanying the benchmarks, and the Poly-encoder paper. ConvAI2, WoW, ED: models trained on the corresponding benchmark. These models perform very well on the benchmark they were trained on, but not as well on other benchmarks. BST: The model fine-tuned on BST shows more balanced performance (i.e., none of the single-skill benchmarks does better at all three skills), but it is noticeably lower than each specialized model. Random-Skill: the performance of choosing a random single-skill per response is comparable to the BST model, but slightly worse on ConvAI2. MT Two-Stage: guiding the generation by an actual task classifier as opposed to random selection increases performance on all skills. MT Single-Skills: this model performs best among the blended skills architectures, and nearly matches the single-skill model performance (and surpasses it in the WoW case). Added-context benchmarks: when the benchmark contexts are augmented with a persona and topic as described in section 3.2, the evaluation results barely change. Mixed-candidates evaluation: when the set of benchmark candidates is tripled by adding candidates from the other two benchmarks in equal proportion, the performance of the best respective single-task models suffers, while the MT Single-Skills model proves more resilient. Note that Single-task averages in italics do not correspond to a single model, but an average over 3 models.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 404.0, "y1": 86.0, "x1": 100.0, "y2": 378.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9268086751301, "y1": 253.37850782606336, "x1": 100.0, "y2": 411.15570068359375}, "name": "7", "caption_text": "Table 7: Human evaluation results on individual axes of knowledge, empathy, and being personal, as well as overall quality. All results here have a 95% confidence interval of \u00b1 0.2 or 0.3, omitted to avoid cluttering the table. Results that are within the confidence interval of the best model performance are bolded. ConvAI2, WoW, ED: models pre-trained on pushshift.io Reddit and fine-tuned on the respective datasets. For Empathy and Personal topics, the individual models tend to do better when trained on a dataset tailored for that, however they all perform similarly on the Knowledge dimension. BST: model pre-trained on pushshift.io Reddit and fine-tuned on BST. This model is showing better overall performance compared to single-skill datasets (i.e., none of the three single-skill dataset do better than BST in every dimension). MT Single-Skills with fine-tuning on BST and MT Two-Stage are performing very well on all dimensions. MT Single-Skills with fine-tuning on BST has fewer than a third of the parameters of the MT Two-Stage model, yet manages to perform as well, if not slightly better.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 663.0, "y1": 86.0, "x1": 158.0, "y2": 253.0}, "page": 8, "dpi": 0}], "error": null, "pdf": "/work/host-output/92b78ba373cda29da07005849e8328be962b7488/2020.acl-main.183.pdf", "dpi": 100}