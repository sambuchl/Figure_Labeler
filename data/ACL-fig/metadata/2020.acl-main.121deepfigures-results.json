{"raw_detected_boxes": [[{"x2": 723.0, "y1": 311.0, "x1": 435.0, "y2": 500.0}], [], [{"x2": 718.0, "y1": 93.0, "x1": 109.0, "y2": 404.0}], [{"x2": 724.0, "y1": 88.0, "x1": 107.0, "y2": 371.0}], [], [{"x2": 398.0, "y1": 86.0, "x1": 102.0, "y2": 239.0}, {"x2": 688.0, "y1": 86.0, "x1": 468.0, "y2": 271.0}, {"x2": 372.0, "y1": 415.0, "x1": 131.0, "y2": 572.0}], [{"x2": 605.0, "y1": 86.0, "x1": 227.0, "y2": 177.0}, {"x2": 381.0, "y1": 278.0, "x1": 133.0, "y2": 358.0}, {"x2": 709.0, "y1": 278.0, "x1": 447.0, "y2": 353.0}], [{"x2": 384.0, "y1": 93.0, "x1": 115.0, "y2": 195.0}, {"x2": 723.0, "y1": 90.0, "x1": 434.0, "y2": 393.0}], [], [], [], [{"x2": 372.0, "y1": 418.0, "x1": 131.0, "y2": 512.0}, {"x2": 341.0, "y1": 747.0, "x1": 156.0, "y2": 828.0}, {"x2": 375.0, "y1": 132.0, "x1": 128.0, "y2": 233.0}], []], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Figure", "name": "1", "captionBoundary": {"x2": 527.2003173828125, "y1": 372.8585510253906, "x1": 306.9169921875, "y2": 450.5919494628906}, "imageText": ["Translation", "Table", "Input", "(English)", "Output", "(English)", "Input", "(Chinese)", "10", "\u201410", "yuan", "\u2014detained", "\u2014migrant", "workers", "\u2014Foshan", "\u2014young", "couple", "\u2014train", "tickets", "A", "young", "couple", "in", "Foshan", "who", "help", "migrant", "workers", "book", "train", "tickets", "online", "have", "been", "detained", "after", "receiving", "a", "10-yuan", "handling", "fee", "for", "each", "ticket.", "Migrant", "workers", "called", "injustice,", "and", "they", "did", "not", "overcharge,", "much", "better", "than", "scalpers.", "Lawyers", "said", "that", "under", "the", "law,", "disguised", "mark-up", "scalping", "and", "other", "acts", "constitute", "scalping", "tickets", "serious,", "will", "be", "punished.", "Foshan", "young", "couple", "was", "detained", "for", "charging", "10", "yuan", "for", "buying", "train", "tickets", "online", "for", "migrant", "workers", "10"], "regionBoundary": {"x2": 520.0, "y1": 222.8900146484375, "x1": 313.0, "y2": 359.8900146484375}, "caption": "Figure 1: An example of the translation pattern in a sample extracted from Zh2EnSum (Zhu et al., 2019) which is a Chinese-to-English cross-lingual summarization dataset. It shows that some words in the summary are translated from the source words (in the same color). The translation table also gives the corresponding relation to these words. Best viewed in color.", "page": 0}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 292.01123046875, "y1": 187.90951538085938, "x1": 71.0040283203125, "y2": 277.59808349609375}, "imageText": ["ATS", "Naive", "40.40", "23.82\u2020", "36.63", "21.86*", "Equal", "40.10", "23.36*", "36.22", "21.41", "Adapt", "40.68", "24.12\u2020", "36.97", "22.15", "Baseline", "+Extra", "Data", "CLSMS", "40.34", "22.65", "36.39", "21.09", "CLSMT", "40.25", "22.58", "36.21", "21.06", "Baseline", "GETran", "24.34", "9.14", "20.13", "0.64", "GLTran", "35.45", "16.86", "31.28", "16.90", "TNCLS", "38.85", "21.93", "35.05", "19.43", "Model", "RG-1", "RG-2", "RG-L", "MVS"], "regionBoundary": {"x2": 289.0, "y1": 62.8900146484375, "x1": 74.0, "y2": 175.8900146484375}, "caption": "Table 1: ROUGE F1 scores (%) and MoverScore scores (%) on Zh2EnSum test set. RG and MVS refer to ROUGE and MoverScore, respectively. We adopt \u201csubword-subword\u201d segmentation granularity here. The improvement of all ATS models over the baseline TNCLS is statistically significant (p < 0.01). * (\u2020) indicates that the improvement over CLSMS is statistically significant where p < 0.05 (0.01).", "page": 5}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 291.9243469238281, "y1": 424.5135498046875, "x1": 71.69100189208984, "y2": 478.33697509765625}, "imageText": ["ATS", "Naive", "40.19", "21.84", "36.46", "Equal", "39.98", "21.63", "36.29", "Adapt", "40.47", "22.21", "36.89", "Baseline", "+Extra", "Data", "CLSMS", "38.25", "20.20", "34.76", "CLSMT", "40.23", "22.32", "36.59", "Baseline", "GETran", "28.19", "11.40", "25.77", "GLTran", "32.17", "13.85", "29.43", "TNCLS", "36.82", "18.72", "33.20", "Model", "RG-1", "RG-2", "RG-L"], "regionBoundary": {"x2": 268.0, "y1": 298.8900146484375, "x1": 94.0, "y2": 411.8900146484375}, "caption": "Table 2: ROUGE F1 scores (%) on En2ZhSum test set. RG refers to ROUGE for short. We adopt \u201cwordcharacter\u201d segmentation granularity here. The improvement of all ATS models over both TNCLS and CLSMS is statistically significant (p < 0.01).", "page": 5}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 527.2900390625, "y1": 212.87551879882812, "x1": 306.9169921875, "y2": 290.60906982421875}, "imageText": ["ATS-NE", "114.00", "24", "ATS-A", "115.05", "25", "TNCLS", "113.74", "24", "CLSMS", "190.23", "65", "CLSMT", "148.16", "72", "En-Zh", "ATS-NE", "136.55", "27", "ATS-A", "137.60", "30", "TNCLS", "134.92", "21", "CLSMS", "211.41", "48", "CLSMT", "208.84", "63", "Zh-En", "Src-Tgt", "Model", "Size", "(M)", "Train", "(S)"], "regionBoundary": {"x2": 496.0, "y1": 62.8900146484375, "x1": 337.0, "y2": 200.8900146484375}, "caption": "Table 3: Model size (number of trainable parameters and M denotes mega) and training time of various models. Train (S) denotes how many seconds required for each model to train the 100-batch cross-lingual summarization task of the same batch size (3072). ATS-NE refers to our method with the Naive or Equal strategy. ATS-A is the one with Adapt strategy.", "page": 5}, {"figType": "Table", "name": "6", "captionBoundary": {"x2": 527.2003173828125, "y1": 266.5115661621094, "x1": 306.947021484375, "y2": 332.28997802734375}, "imageText": ["En2Zh", "w-c", "14.91", "14.84", "14.27", "14.05", "Zh2En", "sw-sw", "21.41", "20.71", "21.86", "21.00", "Zh2En", "w-w", "21.17", "20.46", "21.90", "21.05", "Task", "Unit", "pmacrotrans", "pmicrotrans", "rmacro", "rmicro"], "regionBoundary": {"x2": 511.0, "y1": 195.8900146484375, "x1": 322.0, "y2": 253.8900146484375}, "caption": "Table 6: Statistics on ptrans in ATS-A models. pmacrotrans (%) and pmicrotrans (%) respectively represent the macroaverage and micro-average translating probability during decoding. rmacro (%) and rmicro (%) respectively represent the ratio of words where ptrans > 0.5 during decoding.", "page": 6}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 525.54736328125, "y1": 142.36355590820312, "x1": 70.67503356933594, "y2": 172.27703857421875}, "imageText": ["10", "40.68", "24.12", "36.97", "22.15", "40.47", "22.21", "36.89", "ATS-A", "1", "40.93", "24.17", "37.11", "22.31", "39.85", "21.45", "36.12", "5", "41.05", "24.31", "37.28", "22.77", "40.27", "21.96", "36.60", "RG-1", "RG-2", "RG-L", "MVS", "RG-1", "RG-2", "RG-L", "Model", "m", "Zh2En", "En2Zh"], "regionBoundary": {"x2": 436.0, "y1": 62.8900146484375, "x1": 162.0, "y2": 129.8900146484375}, "caption": "Table 4: Results of ATS on Zh2EnSum and En2ZhSum under different hyperparameters, where m is the limit on the number of translation candidates. RG and MVS refer to ROUGE and MoverScore, respectively. We adopt \u201csubword-subword\u201d and \u201cword-character\u201d segmentation granularities in Zh2En and En2Zh models, respectively.", "page": 6}, {"figType": "Table", "name": "5", "captionBoundary": {"x2": 291.92437744140625, "y1": 275.5625305175781, "x1": 71.69100189208984, "y2": 341.3400573730469}, "imageText": ["ATS-A", "w-w", "39.65", "23.79", "36.05", "22.06sw-sw", "40.68", "24.12", "36.97", "22.15", "TNCLS", "w-w", "37.70", "21.15", "34.05", "19.43sw-sw", "38.85", "21.93", "35.05", "19.07", "Model", "Unit", "RG-1", "RG-2", "RG-L", "MVS"], "regionBoundary": {"x2": 278.0, "y1": 195.8900146484375, "x1": 84.0, "y2": 262.8900146484375}, "caption": "Table 5: Results of models on Zh2EnSum with different segmentation granularities. Unit represents the granularity combination of text units. w and sw denote \u201cword\u201d and \u201csubword\u201d (Sennrich et al., 2016), respectively. The improvement of all ATS models over TNCLS is statistically significant (p < 0.01).", "page": 6}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 525.5471801757812, "y1": 305.7565612792969, "x1": 71.64099884033203, "y2": 361.0740051269531}, "imageText": ["'detained'", "Translation", "Distribution", "Final", "Distribution", "Neural", "Distribution", "Average", "Translation", "Probability", "\u5211\u62d8", "detained", "0.56", "\u5211\u62d8", "arrested", "0.25", "\u5211\u62d8", "interned", "0.19", "Zh", "EN", "P", "Attention", "Distribution", "Dynamic", "Gate", "Positional", "Encoding", "Multi-Head", "Self-Attention", "Foshan", "young", "couple", "was", "detained", "...", "Encoder", "Positional", "Encoding", "Multi-Head", "Self-Attention", "Feed", "Forward", "Network", "Decoder", "Encoder-Decoder", "Attention", "Feed", "Forward", "Network"], "regionBoundary": {"x2": 521.0, "y1": 63.8900146484375, "x1": 77.0, "y2": 291.8900146484375}, "caption": "Figure 2: Overview of our method. We first use encoder-decoder attention distribution to attend to some words and obtain the translation candidates from a probabilistic bilingual lexicon. Then a translating probability ptrans is calculated, which balances the probability of generating words from the neural distribution with that of selecting words from the translation candidates of the source text. The final distribution is obtained by the weighted sum (weighed by ptrans) of the neural distribution PN and the translation distribution PT. Best viewed in color.", "page": 2}, {"figType": "Figure", "name": "5", "captionBoundary": {"x2": 525.7167358398438, "y1": 761.142578125, "x1": 72.0, "y2": 780.5950317382812}, "imageText": ["ATS-A:", "(ed", "miliband's", "plan", "to", "cut", "tuition", "fees", "faces", "growing", "opposition.", "the", "cost", "reduction", "was", "paid", "by", "cutting", "middle-class", "pensions.", "but", "now", "it", "is", "expected", "that", "the", "party", "may", "fall", "into", "civil", "war.)", "CLSMS:", "(ed", "miliband's", "plan", "to", "cut", "college", "tuition", "faces", "internal", "opposition.", "it", "is", "predicted", "that", "the", "party", "may", "fall", "into", "civil", "war.", "labor", "leaders", "said", "that", "this", "would", "be", "a", "\"red", "line\"", "for", "any", "future", "joint", "talks.)", "CLSMT:", "(ed", "miliband's", "plan", "to", "cut", "college", "tuition", "is", "facing", "increasing", "opposition.", "it", "is", "expected", "that", "the", "party", "may", "fall", "into", "civil", "war", "because", "of", "these", "proposals.", "but", "plans", "to", "cut", "middle-class", "pensions", "have", "been", "accused", "of", "\"financial", "culture.\")", "tncls:", "\u00b7", "clsms:", "clsmt:", "trans:", "TNCLS:", "(ed", "miliband", "is", "under", "threat", "of", "internal", "tuition", "cuts.", "former", "aide", "hugh", "Evans'", "aide", "joined", "the", "opposition.", "the", "move", "could", "lead", "miliband", "to", "cut", "costs.)", "Reference:", "(ed", "miliband", "is", "facing", "internal", "opposition", "over", "plans", "to", "slash", "tuition", "fees", ".", "but", "the", "fee", "reductions", "are", "to", "paid", "for", "by", "cutting", "middle-class", "pension", "pots", ".", "it", "is", "now", "predicted", "the", "party", "could", "descend", "into", "civil", "war", "over", "his", "policy", ".)", "Input", "(English):", "ed", "miliband", "'s", "plan", "to", "cut", "university", "tuition", "fees", "is", "facing", "internal", "opposition", "with", "predictions", "it", "could", "cause", "a", "civil", "war", "within", "the", "party", ".", "ed", "miliband", "'s", "plan", "to", "cut", "university", "tuition", "fees", "was", "yesterday", "facing", "mounting", "opposition", "-", "with", "even", "a", "former", "labour", "no10", "aide", "joining", "the", "attack", ".", "there", "were", "predictions", "last", "night", "that", "the", "party", "could", "descend", "into", "civil", "war", "over", "the", "controversial", "proposals", "after", "ex-tony", "blair", "aide", "huw", "evans", "was", "joined", "by", "the", "leader", "of", "britain", "'s", "nurses", "in", "challenging", "the", "plans", ".", "mr", "miliband", "has", "said", "his", "pledge", "to", "slash", "the", "fees", "from", "\u00a3", "9,000", "a", "year", "to", "\u00a3", "6,000", "is", "'cast-iron", "'", ",", "adding", "the", "plan", "will", "be", "a", "'red", "line", "'", "in", "any", "possible", "future", "coalition", "talks", ".", "but", "the", "plan", "\u2013", "to", "be", "paid", "for", "by", "cutting", "middle-class", "pension", "pots", "\u2013", "has", "been", "condemned", "as", "'financial", "illiteracy", "'", "by", "some", "critics", ",", "while", "university", "chiefs", "warn", "it", "could", "jeopardise", "the", "scrutiny", "of", "their", "long-", "term", "funding", ".", "the", "policy", "has", "also", "led", "to", "more", "than", "four", "years", "of", "rows", "within", "the", "shadow", "cabinet", ",", "with", "claims", "that", "ed", "balls", "repeatedly", "warned", "mr", "miliband", "that", "the", "\u00a3", "2.9billion", "fees", "cut", "was", "difficult", "to", "fund", ".", "mr", "evans", ",", "speaking", "in", "his", "capacity", "as", "director", "general", "of", "the", "association", "of", "british", "insurers", "(", "abi", ")", ",", "joined", "a", "growing", "number", "of", "pensions", "experts", "to", "challenge", "labour", "'s", "plans", ".", "mr", "evans", ",", "who", "worked", "for", "mr", "blair", "from", "2005", "to", "2006", "and", "is", "also", "a", "former", "adviser", "to", "ex-home", "secretary", "david", "blunkett", ",", "said", ":", "'", "the", "pensions", "and", "long-term", "savings", "industry", "supports", "reform", "of", "tax", "relief", "but", "this", "is", "not", "the", "way", "to", "do", "it", ".", "'", "we", "need", "a", "focus", "on", "reforming", "the", "pension", "tax", "relief", "system", "as", "a", "whole", "to", "make", "it", "fairer", ",", "better", "value", "and", "encourage", "saving", "from", "middle", "earners", ",", "rather", "than", "piecemeal", "cutting", "back", "the", "existing", "system", "to", "pay", "for", "other", "policy", "objectives", ".", "'", "under", "the", "labour", "plan", ",", "tax", "relief", "for", "pensioners", "with", "incomes", "more", "than", "\u00a3", "150,000", "would", "be", "cut", "from", "45p", "to", "20p", "while", "the", "tax-free", "lifetime", "allowance", "on", "a", "pension", "would", "drop", "from", "\u00a3", "1.25million", "to", "\u00a3", "1million", ".", "but", "the", "proposals", "could", "also", "hit", "people", "due", "to", "retire", "with", "a", "pension", "pot", "worth", "just", "\u00a3", "26,000", "a", "year", "from", "an", "annuity", "while", "young", "people", "saving", "just", "\u00a3", "400", "a", "month", "may", "also", "be", "affected", ".", "the", "plans", "have", "led", "to", "fears", "nurses", ",", "teachers", "and", "firefighters", "could", "also", "be", "hit", ".", "dr", "peter", "carter", ",", "of", "the", "royal", "college", "of", "nursing", ",", "said", ":", "'", "helping", "students", "financially", "is", "important", ".", "however", ",", "this", "must", "not", "be", "at", "the", "expense", "of", "hard-working", "nurses", ".", "we", "will", "examine", "these", "proposals", "to", "ensure", "their", "pensions", "will", "not", "be", "affected", ".", "'", "last", "night", "the", "comments", "were", "seized", "on", "by", "health", "secretary", "jeremy", "hunt", ".", "he", "wrote", "to", "his", "labour", "opposite", "number", ",", "andy", "burnham", ",", "saying", ":", "'", "i", "wanted", "to", "ensure", "you", "are", "fully", "aware", "of", "the", "impact", "of", "this", "announcement", "on", "nhs", "staff", ".", "for", "example", ",", "if", "a", "nurse", "team", "leader", "earning", "around", "\u00a3", "35,500", ",", "who", "is", "in", "a", "final", "salary", ",", "defined", "benefit", "pension", "scheme", ",", "achieves", "the", "promotion", "to", "matron", "they", "have", "been", "working", "25", "years", "to", "achieve", ",", "they", "will", "face", "a", "tax", "charge", "of", "\u00a3", "5,000", "on", "their", "pension", "pot", ".", "this", "is", "what", "happens", "when", "policies", "are", "not", "properly", "thought", "through", ".", "'", "but", "mr", "miliband", "has", "claimed", "the", "pensions", "raid", "would", "hit", "only", "the", "very", "wealthy", ".", "he", "said", ":", "'", "the", "scourge", "of", "debt", "from", "tuition", "fees", "is", "not", "only", "holding", "back", "our", "young", "people", ",", "it", "is", "a", "burden", "on", "our", "country", ".", "'", "mr", "miliband", "pictured", "at", "leeds", "college", "of", "music", "yesterday", ",", "where", "he", "announced", "his", "plan", "to", "slash", "tuition", "fees", ".", "."], "regionBoundary": {"x2": 852.2847290039062, "y1": 60.56201171875, "x1": 78.0, "y2": 746.8900146484375}, "caption": "Figure 5: Examples of generated En2Zh summaries. The English translation of target-side text is also given for better reading. The blue shading intensity denotes the value of the translating probability ptrans.", "page": 12}, {"figType": "Table", "name": "7", "captionBoundary": {"x2": 291.92437744140625, "y1": 152.32656860351562, "x1": 71.69100189208984, "y2": 182.23907470703125}, "imageText": ["TNCLS", "3.34", "4.00", "3.78", "3.08", "3.28", "3.12", "CLSMS", "3.56", "4.12", "3.92", "3.28", "3.40", "3.36", "CLSMT", "3.44", "4.08", "4.04", "3.38", "3.56", "3.48", "ATS-A", "3.64", "4.16", "4.18", "3.36", "3.54", "3.52", "IF", "CC", "FL", "IF", "CC", "FL", "Model", "Zh2En", "En2Zh"], "regionBoundary": {"x2": 280.0, "y1": 62.8900146484375, "x1": 83.0, "y2": 139.8900146484375}, "caption": "Table 7: Human evaluation results. IF, CC, and FL represent informativeness, conciseness, and fluency, respectively.", "page": 7}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 527.2005004882812, "y1": 295.8755187988281, "x1": 307.2760009765625, "y2": 339.23797607421875}, "imageText": ["CLSMT:", "guo", "tiejun", ",", "former", "director", "of", "zengcheng", "health", "bureau", ",", "was", "sentenced", "to", "five", "and", "a", "half", "years", "'imprisonment", "for", "accepting", "bribes", "of", "340,000", "yuan", "According", "to", "the", "Guangzhou", "Intermediate", "People's", "Court,", "Guo", "Tiejun,", "former", "director", "of", "the", "Zengcheng", "Municipal", "Health", "Bureau", "in", "Guangdong", "Province,", "received", "bribes", "nearly", "340,000", "yuan", "in", "holiday", "gifts", "from", "20", "persons", "in", "charge", "of", "subordinate", "medical", "units.", "The", "court", "upheld", "the", "original", "judgment", "in", "the", "first", "instance,", "rejected", "Guo", "Tiejun's", "appeal", "and", "sentenced", "him", "to", "five", "and", "a", "half", "years", "in", "prison", "for", "bribery.", "(The", "English", "Translation", "of", "Source", "Text)", "CLSMS:", "the", "former", "director", "of", "zengcheng", "health", "bureau", "was", "sentenced", "to", "five", "and", "a", "half", "years", "'imprisonment", "for", "accepting", "bribes", "of", "nearly", "340,000", "yuan", "ATS-A:", "the", "former", "director", "of", "zengcheng", "health", "bureau", "was", "sentenced", "to", "five", "and", "a", "half", "years", "for", "bribery", "TNCLS:", "the", "former", "director", "of", "zengcheng", "health", "bureau", "was", "arrested", "on", "suspicion", "of", "accepting", "bribes", "Reference:", "zengcheng", "'s", "former", "director", "of", "health", "received", "bribes", "and", "was", "sentenced", "to", "five", "and", "a", "half", "years'", "imprisonment", "34", "5", "Input", "(Chinese):", "20"], "regionBoundary": {"x2": 521.0, "y1": 62.8900146484375, "x1": 313.0, "y2": 282.8900146484375}, "caption": "Figure 4: Examples of generated summaries. The English translation of source text is also given for better reading. The blue shading intensity denotes the value of the translating probability ptrans.", "page": 7}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 525.5471801757812, "y1": 280.9575500488281, "x1": 72.0, "y2": 298.9150390625}, "imageText": ["Translation", "Probability", "Probabilistic", "Bilingual", "Lexicon", "Translation", "Probability", "Probabilistic", "Bilingual", "Lexicon", "(c)", "Adapt", "Strategy", "(a)", "Naive", "Strategy", "(b)", "Equal", "Strategy", "Top-3", "interned", "arrested", "detained", "custody", "impround", "0.1", "0.2", "0.7", "0.0", "0.0", "Translation-attention", "Source", "Text", "Hidden", "States", "0.1", "0.2", "0.7", "\u2026", "Top-3", "interned", "arrested", "detained", "interned", "arrested", "detained", "custody", "impround", "0.15", "0.20", "0.45", "0.10", "0.10", "interned", "arrested", "detained", "custody", "impround", "0.33", "0.33", "0.33", "0.00", "0.00", "interned", "arrested", "detained", "custody", "impround", "0.15", "0.20", "0.45", "0.10", "0.10", "Translation", "Probability", "interned", "arrested", "detained", "custody", "impround", "0.19", "0.25", "0.56", "0.00", "0.00", "Top-3", "Probabilistic", "Bilingual", "Lexicon", "interned", "arrested", "detained", "custody", "impround", "0.15", "0.20", "0.45", "0.10", "0.10"], "regionBoundary": {"x2": 521.0, "y1": 63.8900146484375, "x1": 77.0, "y2": 267.8900146484375}, "caption": "Figure 3: Overview of our three strategies to obtain the translation probability from the probabilistic bilingual lexicon. We take m=3 for example.", "page": 3}, {"figType": "Table", "name": "10", "captionBoundary": {"x2": 290.2704162597656, "y1": 608.0715942382812, "x1": 71.69100189208984, "y2": 626.029052734375}, "imageText": ["En2Zh", "w-c", "100,000", "18,000", "Zh2En", "sw-sw", "100,000", "40,000", "Zh2En", "w-w", "100,000", "40,000", "Task", "Unit", "Source", "Target"], "regionBoundary": {"x2": 250.0, "y1": 537.8900146484375, "x1": 112.0, "y2": 595.8900146484375}, "caption": "Table 10: The vocabulary size of models with different segmentation granularities.", "page": 11}, {"figType": "Table", "name": "8", "captionBoundary": {"x2": 292.0140075683594, "y1": 179.58151245117188, "x1": 71.67098999023438, "y2": 269.27008056640625}, "imageText": ["#Documents", "1,693,713", "3,000", "3,000", "#AvgChars", "(S)", "103.59", "103.56", "140.06", "#AvgWords", "(R)", "13.70", "13.74", "13.84", "#AvgSentChars", "52.73", "52.41", "53.38", "#AvgSents", "2.32", "2.33", "2.30", "Zh2EnSum", "train", "valid", "test"], "regionBoundary": {"x2": 270.0, "y1": 94.8900146484375, "x1": 92.0, "y2": 166.8900146484375}, "caption": "Table 8: Corpus statistics of Zh2EnSum. #AvgChars (S) is the average number of Chinese characters in the source document. #AvgWords (R) means the average number of English words in the reference. #AvgSentChars refers to the average number of characters in a sentence in the source document. #AvgSents denotes the average number of sentences in the source document.", "page": 11}, {"figType": "Table", "name": "9", "captionBoundary": {"x2": 292.0140686035156, "y1": 380.76153564453125, "x1": 71.50201416015625, "y2": 470.450927734375}, "imageText": ["#Documents", "364,687", "3,000", "3,000", "#AvgWords", "(S)", "755.09", "759.55", "744.84", "#AvgChars", "(R)", "55.21", "55.28", "54.76", "#AvgSentWords", "19.62", "19.63", "19.61", "#AvgSents", "40.62", "41.08", "40.25", "En2ZhSum", "train", "valid", "test"], "regionBoundary": {"x2": 268.0, "y1": 296.8900146484375, "x1": 94.0, "y2": 368.8900146484375}, "caption": "Table 9: Corpus statistics of En2ZhSum. #AvgWords (S) is the average number of English words in the source document. #AvgChars (R) means the average number of Chinese characters in the reference. #AvgSentWords refers to the average number of Words in a sentence in the source document. #AvgSents denotes the average number of sentences in the source document.", "page": 11}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 732.222663031684, "y1": 517.8590986463759, "x1": 426.27360026041663, "y2": 625.8221520317925}, "name": "1", "caption_text": "Figure 1: An example of the translation pattern in a sample extracted from Zh2EnSum (Zhu et al., 2019) which is a Chinese-to-English cross-lingual summarization dataset. It shows that some words in the summary are translated from the source words (in the same color). The translation table also gives the corresponding relation to these words. Best viewed in color.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 723.0, "y1": 310.0, "x1": 427.0, "y2": 517.0}, "page": 0, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9266391330295, "y1": 424.66189066569007, "x1": 99.50138727823892, "y2": 501.4916737874349}, "name": "2", "caption_text": "Figure 2: Overview of our method. We first use encoder-decoder attention distribution to attend to some words and obtain the translation candidates from a probabilistic bilingual lexicon. Then a translating probability ptrans is calculated, which balances the probability of generating words from the neural distribution with that of selecting words from the translation candidates of the source text. The final distribution is obtained by the weighted sum (weighed by ptrans) of the neural distribution PN and the translation distribution PT. Best viewed in color.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 724.0, "y1": 89.0, "x1": 107.0, "y2": 405.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9266391330295, "y1": 390.2188195122613, "x1": 100.0, "y2": 415.15977647569446}, "name": "3", "caption_text": "Figure 3: Overview of our three strategies to obtain the translation probability from the probabilistic bilingual lexicon. We take m=3 for example.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 724.0, "y1": 88.0, "x1": 106.0, "y2": 372.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.57115342881946, "y1": 260.9854380289714, "x1": 98.61670600043402, "y2": 385.55289374457465}, "name": "1", "caption_text": "Table 1: ROUGE F1 scores (%) and MoverScore scores (%) on Zh2EnSum test set. RG and MVS refer to ROUGE and MoverScore, respectively. We adopt \u201csubword-subword\u201d segmentation granularity here. The improvement of all ATS models over the baseline TNCLS is statistically significant (p < 0.01). * (\u2020) indicates that the improvement over CLSMS is statistically significant where p < 0.05 (0.01).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 401.0, "y1": 86.0, "x1": 102.0, "y2": 244.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.3472764756945, "y1": 295.66044277615015, "x1": 426.27360026041663, "y2": 403.6237080891927}, "name": "3", "caption_text": "Table 3: Model size (number of trainable parameters and M denotes mega) and training time of various models. Train (S) denotes how many seconds required for each model to train the 100-batch cross-lingual summarization task of the same batch size (3072). ATS-NE refers to our method with the Naive or Equal strategy. ATS-A is the one with Adapt strategy.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 689.0, "y1": 86.0, "x1": 468.0, "y2": 279.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.45048183865015, "y1": 589.6021525065104, "x1": 99.57083596123589, "y2": 664.3569098578558}, "name": "2", "caption_text": "Table 2: ROUGE F1 scores (%) on En2ZhSum test set. RG refers to ROUGE for short. We adopt \u201cwordcharacter\u201d segmentation granularity here. The improvement of all ATS models over both TNCLS and CLSMS is statistically significant (p < 0.01).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 372.0, "y1": 415.0, "x1": 114.0, "y2": 589.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9268934461805, "y1": 197.72716098361545, "x1": 98.15976884629991, "y2": 239.27366468641492}, "name": "4", "caption_text": "Table 4: Results of ATS on Zh2EnSum and En2ZhSum under different hyperparameters, where m is the limit on the number of translation candidates. RG and MVS refer to ROUGE and MoverScore, respectively. We adopt \u201csubword-subword\u201d and \u201cword-character\u201d segmentation granularities in Zh2En and En2Zh models, respectively.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 605.0, "y1": 86.0, "x1": 225.0, "y2": 181.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.45052422417535, "y1": 382.7257368299696, "x1": 99.57083596123589, "y2": 474.08341301812067}, "name": "5", "caption_text": "Table 5: Results of models on Zh2EnSum with different segmentation granularities. Unit represents the granularity combination of text units. w and sw denote \u201cword\u201d and \u201csubword\u201d (Sennrich et al., 2016), respectively. The improvement of all ATS models over TNCLS is statistically significant (p < 0.01).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 386.0, "y1": 271.0, "x1": 117.0, "y2": 366.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.222663031684, "y1": 370.1549530029297, "x1": 426.3153076171875, "y2": 461.51385837131073}, "name": "6", "caption_text": "Table 6: Statistics on ptrans in ATS-A models. pmacrotrans (%) and pmicrotrans (%) respectively represent the macroaverage and micro-average translating probability during decoding. rmacro (%) and rmicro (%) respectively represent the ratio of words where ptrans > 0.5 during decoding.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 726.0, "y1": 271.0, "x1": 430.0, "y2": 370.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.45052422417535, "y1": 211.56467861599393, "x1": 99.57083596123589, "y2": 253.10982598198783}, "name": "7", "caption_text": "Table 7: Human evaluation results. IF, CC, and FL represent informativeness, conciseness, and fluency, respectively.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 399.0, "y1": 86.0, "x1": 100.0, "y2": 212.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2229173448351, "y1": 410.9382205539279, "x1": 426.772223578559, "y2": 471.16385565863715}, "name": "4", "caption_text": "Figure 4: Examples of generated summaries. The English translation of source text is also given for better reading. The blue shading intensity denotes the value of the translating probability ptrans.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 723.0, "y1": 88.0, "x1": 427.0, "y2": 410.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.5750952826606, "y1": 528.8354661729601, "x1": 99.30835300021701, "y2": 653.404066297743}, "name": "9", "caption_text": "Table 9: Corpus statistics of En2ZhSum. #AvgWords (S) is the average number of English words in the source document. #AvgChars (R) means the average number of Chinese characters in the reference. #AvgSentWords refers to the average number of Words in a sentence in the source document. #AvgSents denotes the average number of sentences in the source document.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 384.0, "y1": 411.0, "x1": 114.0, "y2": 529.0}, "page": 11, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.1533559163411, "y1": 844.5438808865017, "x1": 99.57083596123589, "y2": 869.4847954644097}, "name": "10", "caption_text": "Table 10: The vocabulary size of models with different segmentation granularities.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 348.0, "y1": 730.0, "x1": 141.0, "y2": 845.0}, "page": 11, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.57501051161023, "y1": 249.41876729329425, "x1": 99.5430416531033, "y2": 373.9862230088976}, "name": "8", "caption_text": "Table 8: Corpus statistics of Zh2EnSum. #AvgChars (S) is the average number of Chinese characters in the source document. #AvgWords (R) means the average number of English words in the reference. #AvgSentChars refers to the average number of characters in a sentence in the source document. #AvgSents denotes the average number of sentences in the source document.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 380.0, "y1": 132.0, "x1": 113.0, "y2": 250.0}, "page": 11, "dpi": 0}], "error": null, "pdf": "/work/host-output/8bebe998fb19b1969461fa0cd282ad9c0db8af82/2020.acl-main.121.pdf", "dpi": 100}