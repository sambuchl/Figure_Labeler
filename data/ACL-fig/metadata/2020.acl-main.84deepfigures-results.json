{"raw_detected_boxes": [[], [], [{"x2": 679.0, "y1": 86.0, "x1": 479.0, "y2": 441.0}, {"x2": 708.0, "y1": 793.0, "x1": 449.0, "y2": 898.0}], [], [], [{"x2": 706.0, "y1": 83.0, "x1": 124.0, "y2": 161.0}, {"x2": 706.0, "y1": 188.0, "x1": 128.0, "y2": 247.0}, {"x2": 706.0, "y1": 364.0, "x1": 118.0, "y2": 411.0}], [], [{"x2": 733.0, "y1": 90.0, "x1": 135.0, "y2": 402.0}], [{"x2": 401.0, "y1": 270.0, "x1": 105.0, "y2": 333.0}, {"x2": 376.0, "y1": 694.0, "x1": 121.0, "y2": 851.0}], [], [], [], [{"x2": 651.0, "y1": 83.0, "x1": 172.0, "y2": 122.0}, {"x2": 651.0, "y1": 226.0, "x1": 176.0, "y2": 264.0}, {"x2": 659.0, "y1": 353.0, "x1": 165.0, "y2": 394.0}], [{"x2": 675.0, "y1": 113.0, "x1": 156.0, "y2": 538.0}], [{"x2": 709.0, "y1": 87.0, "x1": 118.0, "y2": 1059.0}]], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Table", "name": "1", "captionBoundary": {"x2": 384.7543029785156, "y1": 136.86953735351562, "x1": 212.4810028076172, "y2": 142.87200927734375}, "imageText": ["MOVIEDATA", "CHINOOK", "COLLEGE", "DRIVING", "SCHOOL", "FORMULA1", "#TABLES", "18", "11", "11", "6", "13", "#ATTRIBUTES", "64", "63", "45", "39", "93", "#QUERIES", "1148", "1067", "462", "547", "568", "TIME", "PER", "ANNOTATION", "(SEC)", "104", "104", "77", "78", "104", "AVG.", "COMPLEXITY", "Hard", "Medium", "Medium", "Medium", "Medium"], "regionBoundary": {"x2": 513.0, "y1": 59.8900146484375, "x1": 85.0, "y2": 115.8900146484375}, "caption": "Table 1: Statistics of the new corpus OTTA", "page": 5}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 527.2009887695312, "y1": 207.87155151367188, "x1": 71.69100189208984, "y2": 249.74005126953125}, "imageText": ["#QUESTIONS", "#QUERIES", "#DB", "#TABLE/DB", "TABLE", "COV.", "ATTR", "COV.", "MSTTR", "AVG.", "#TOKENS", "ANN.", "TIME", "SPIDER", "10,181", "5,693", "200", "5.1", "0.917", "(0.87)", "0.621", "(0.496)", "0.519", "12.67", "360", "sec.", "LC-QUAD", "2.0", "30,000", "30,000", "1", "157,068", "0.019", "0.187", "0.761", "10.6", "-", "OTTA", "(OURS)", "3,792", "3,792", "5", "11.8", "0.949", "0.544", "0.67", "13.53", "98", "sec."], "regionBoundary": {"x2": 513.0, "y1": 154.8900146484375, "x1": 85.0, "y2": 186.8900146484375}, "caption": "Table 2: Comparison of our corpus OTTA to the Spider and LC-QuaD 2.0 corpora. Note that the number of databases in LC-QuaD 2.0 is only 1, since it is an open-domain knowledge base, and the number of tables corresponds to the number of different classes. Numbers in parentheses only consider databases with more than 5 tables.", "page": 5}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 525.5470581054688, "y1": 317.2325134277344, "x1": 71.69100189208984, "y2": 335.1899719238281}, "imageText": ["#AVG.", "JOIN", "#GROUP", "BY", "#ORDER", "BY", "#NESTED", "#HAVING", "#SET", "OP", "#AGGREGATIONS", "#BOOLEAN", "SPIDER", "0.537", "0.262", "0.234", "0.148", "0.068", "0.076", "0.519", "-", "LC-QUAD", "2.0", "2.05", "hops", "0", "0.041", "0", "0", "0", "0.048", "0.089", "OTTA", "(OURS)", "1.19", "0.133", "0", "0", "0.117", "0.02", "0.4", "0.161"], "regionBoundary": {"x2": 513.0, "y1": 261.8900146484375, "x1": 85.0, "y2": 296.8900146484375}, "caption": "Table 3: Comparison of the query complexity based on the ratio of components per query. For the aggregations in LC-QuaD 2.0, we report the number of queries that use a Count operation.", "page": 5}, {"figType": "Figure", "name": "5", "captionBoundary": {"x2": 525.5473022460938, "y1": 773.5675659179688, "x1": 71.99998474121094, "y2": 803.4800415039062}, "imageText": ["(b)", "(a)"], "regionBoundary": {"x2": 515.0, "y1": 61.8900146484375, "x1": 83.0, "y2": 760.696044921875}, "caption": "Figure 5: The annotation tool. (a) The OT and the constraints are shown to the annotators. For each node, the annotators can inspect the result of the execution. The annotators write a question and (b) assign the tokens of the question to the operations.", "page": 14}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 525.5477905273438, "y1": 418.2385559082031, "x1": 72.0, "y2": 460.10699462890625}, "imageText": ["Average(movie.vote_average)", "Tokens:", "What|is|the|average|movie|vote|average", "Distinct(movie.id)", "Tokens:", "different|movies", "Filter(oscar.year,", ">=,", "1991)", "Tokens:", "oscar|in|the|year|1991|or|later", "Filter(cast.character,", "=,", "Jesse)", "Tokens:", "cast|character|called|Jesse", "Join(cast.movie_id,", "movie.id)", "Tokens:", "movies|with|a|cast|character|nominated|for|an|oscar", "GetData(movie)", "Tokens:", "movies", "Join(oscar_nominee.person_id,", "person.id)", "Tokens:", "cast|character|were|nominated|for|an|oscar", "Join(person.id,", "cast.person_id)", "Tokens:", "cast|character", "GetData(cast)", "Tokens:", "cast", "GetData(person)", "Tokens:", "character", "Join(oscar.id,", "oscar_nominee.oscar_id)", "Tokens:", "nominated|for|an|oscar", "GetData(oscar_nominee)", "Tokens:", "nominated|for|an|oscar", "GetData(oscar)", "Tokens:", "oscar"], "regionBoundary": {"x2": 487.0, "y1": 78.8900146484375, "x1": 110.0, "y2": 389.8900146484375}, "caption": "Figure 4: Example of a randomly sampled tree. The nodes denote the node type with their arguments. The Tokens are assigned during the second phase of the annotation process. This tree is the answer to the question: What is the average movie vote of different movies having an Oscar nominee with a cast character called Jesse and were nominated for an Oscar in the year 1991 or later?", "page": 13}, {"figType": "Figure", "name": "1", "captionBoundary": {"x2": 525.7160034179688, "y1": 329.9855041503906, "x1": 307.2760009765625, "y2": 359.89794921875}, "imageText": ["(b)", "(a)", "Select(movie.title", "=", "The", "Notebook)", "Projection", "(person.name)", "Join(movie.id,", "cast.movie_id)", "Join(person.id,", "cast.person_id)", "TableScan", "(movie)", "TableScan", "(cast)", "TableScan", "(person)"], "regionBoundary": {"x2": 489.0, "y1": 61.8900146484375, "x1": 343.0, "y2": 317.114990234375}, "caption": "Figure 1: (a) Example of an Operation Tree (OT) for the query \u201dWho starred in \u2019The Notebook\u2019?\u201d (b) The corresponding database schema.", "page": 2}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 527.289306640625, "y1": 669.6905517578125, "x1": 306.967041015625, "y2": 759.3790283203125}, "imageText": ["TN", "::=", "table", "name", "A", "::=", "attributes", "OP", "::=", "<", "|", ">", "|", "<=", "|", ">=", "|", "==", "|", "!=", "V", "::=", "values", "intersection", "(T,", "T,", "A,", "A)", "|", "difference(T,", "T,", "A,", "A)", "|", "averageBy", "(T", ",A)", "|", "sumBy", "(T", ",A)", "|", "countBy", "(T", ",A)", "S", "::=", "done(R)", "|", "isEmpty(R)", "|", "sum", "(T,A)", "|", "average", "(T,A)", "|", "count(R)", "R", "::=", "projection(T,", "A)", "T", "::=", "tableScan(TN)", "|", "selection(T,", "A,", "OP", ",V)", "|", "min(T,", "A)", "|", "max(T,", "A)", "|", "distinct(T)", "|", "join(T,", "T,", "A,", "A)", "|", "union", "(T,T,A,", "A)", "|"], "regionBoundary": {"x2": 510.42236328125, "y1": 573.230224609375, "x1": 324.6158447265625, "y2": 646.5406494140625}, "caption": "Figure 2: The set of production rules for the contextfree grammar of the operation trees, where table name denotes the set of all entity types in the database, attributes denotes the set of all attributes of entity types, and values denotes the set of all entries in the database. The non-terminal symbols S, T,and R denote the startsymbol, intermediate tables, and result tables respectively.", "page": 2}, {"figType": "Table", "name": "6", "captionBoundary": {"x2": 527.2825317382812, "y1": 107.8625259399414, "x1": 71.69099426269531, "y2": 149.73004150390625}, "imageText": ["TOTAL", "CHINOOK", "COLLEGE", "DRIVING", "SCHOOL", "FORMULA", "1", "SPIDER", "0.", "917", "(0.87)", "0.727", "0.909", "1", "0.692", "OUR", "DATASET", "0.949", "1", "0.818", "1", "0.923"], "regionBoundary": {"x2": 476.0, "y1": 59.8900146484375, "x1": 116.0, "y2": 87.8900146484375}, "caption": "Table 6: Table Coverage, in % to total amount of existing tables. Our dataset shows better table coverage, except for one database (college), where the coverage differs by one table. The biggest improvement in coverage was achieved on the database formula 1, which is also the most complex database with the largest amount of tables. The number in braces indicates the average table coverage for the databases with more than 5 tables.", "page": 12}, {"figType": "Table", "name": "7", "captionBoundary": {"x2": 525.7164306640625, "y1": 211.93753051757812, "x1": 71.69100189208984, "y2": 241.85003662109375}, "imageText": ["TOTAL", "CHINOOK", "COLLEGE", "DRIVING", "SCHOOL", "FORMULA", "1", "SPIDER", "0.621", "(0.496)", "0.354", "0.383", "0.730", "0.221", "OUR", "DATASET", "0.544", "0.584", "0.384", "0.756", "0.442"], "regionBoundary": {"x2": 479.0, "y1": 161.8900146484375, "x1": 119.0, "y2": 189.8900146484375}, "caption": "Table 7: Attribute Coverage. Our method gives better attribute coverage in particular for larger datasets, for instance, FORMULA 1. The number in braces indicates the average attribute coverage for the databases with more than 5 tables.", "page": 12}, {"figType": "Table", "name": "8", "captionBoundary": {"x2": 386.5626525878906, "y1": 304.9945373535156, "x1": 210.67300415039062, "y2": 310.99700927734375}, "imageText": ["TOTAL", "CHINOOK", "COLLEGE", "DRIVING", "SCHOOL", "FORMULA", "1", "SPIDER", "0.504", "0.667", "0.412", "0.441", "0.925", "OUR", "DATASET", "1.15", "0.95", "1.18", "0.837", "1.2"], "regionBoundary": {"x2": 479.0, "y1": 253.8900146484375, "x1": 119.0, "y2": 283.8900146484375}, "caption": "Table 8: Average number of joins per query.", "page": 12}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 502.8785705566406, "y1": 300.226318359375, "x1": 94.38988494873047, "y2": 316.38873291015625}, "imageText": ["Count", "the", "number", "of", "artists", "who", "have", "not", "released", "an", "album.", "How", "many", "different", "genres", "do", "the", "tracks", "have,", "which", "were", "bought", "by", "customers", "who", "live", "in", "France?", "What", "are", "the", "album", "titles", "for", "albums", "containing", "both", "Reg-", "gae", "and", "Rock", "genre", "tracks?", "Which", "customers", "made", "at", "least", "35", "purchases,", "excluding", "titles", "from", "the", "Chico", "Science", "&", "Nacao", "Zumbi", "album?", "extra", "What", "is", "the", "name", "of", "the", "media", "type", "that", "is", "least", "common", "across", "all", "tracks?", "Whats", "the", "total", "unit", "price", "sold", "to", "customers", "with", "the", "email", "hholy@gmail.com", "and", "Argentina", "as", "billing", "country?", "What", "are", "the", "names", "of", "artists", "who", "have", "not", "released", "any", "albums?", "What", "is", "the", "album", "title", "having", "the", "track", "with", "the", "lowest", "length", "in", "milliseconds", "in", "the", "genre", "name", "Sci", "Fi", "&", "Fantasy?", "What", "are", "the", "last", "names", "of", "customers", "without", "invoice", "totals", "exceeding", "20?", "What", "are", "the", "genres", "from", "artists", "not", "named", "Scholars", "Baroque", "Ensemble?", "hard", "What", "is", "the", "average", "duration", "in", "milliseconds", "of", "tracks", "that", "belong", "to", "Latin", "or", "Pop", "genre?", "How", "many", "different", "playlists", "with", "a", "track", "that", "is", "bigger", "than", "7045314", "bytes", "do", "exist?", "Find", "the", "name", "of", "the", "artist", "who", "made", "the", "album", "\u201dBalls", "to", "the", "Wall\u201d.", "To", "which", "postal", "codes", "did", "we", "sell", "a", "track", "named", "Headspace?", "medium", "Count", "the", "number", "of", "tracks", "that", "are", "part", "of", "the", "rock", "genre.", "What", "is", "the", "average", "length", "of", "the", "tracks", "in", "the", "Grungeplaylist?", "Please", "show", "the", "employee", "\ufb01rst", "names", "and", "ids", "of", "employ-", "ees", "who", "serve", "at", "least", "10", "customers.", "When", "did", "we", "sell", "tracks", "larger", "than", "8675345", "bytes?", "Find", "all", "the", "customer", "information", "in", "state", "NY.", "To", "which", "country", "belongs", "the", "89503", "postal", "code?", "Find", "the", "number", "of", "albums.", "Where", "were", "the", "invoices", "with", "the", "total", "sum", "of", "1.99", "orsmaller", "issued?", "What", "is", "the", "average", "unit", "price", "of", "all", "the", "tracks?", "What", "are", "the", "unit", "prices", "of", "tracks", "composed", "by", "Alfred", "Ellis/James", "Brown?", "Hardness", "Spider", "OTTA", "easy"], "regionBoundary": {"x2": 529.0, "y1": 62.8900146484375, "x1": 94.0, "y2": 289.8900146484375}, "caption": "Table 4: Example questions from OTTA and Spider. We grouped the examples by the hardness scores. The examples are for the Chinook domain, which is an online music store database.", "page": 7}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 291.515869140625, "y1": 636.3595581054688, "x1": 72.0, "y2": 678.22705078125}, "imageText": ["Normal", "Token", "Assignment", "20%", "40%", "60%", "80%", "100%", "0.6", "0.5", "0.4", "0.3", "0.2", "0.1", "0.4936", "0.418", "0.342", "0.38", "0.2009", "0.4756", "0.3819", "0.3155", "0.3583", "0.1306"], "regionBoundary": {"x2": 277.0, "y1": 500.4635314941406, "x1": 86.79539489746094, "y2": 611.3406372070312}, "caption": "Figure 3: Learning curve for 20%, 40%, 60%, 80%, and 100% of the data on database MOVIEDATA as part of OTTA. We compare the scores for the training with and without token alignment.", "page": 8}, {"figType": "Table", "name": "5", "captionBoundary": {"x2": 291.9242248535156, "y1": 261.0095520019531, "x1": 71.69100189208984, "y2": 302.8780517578125}, "imageText": ["Easy", "Medium", "Hard", "Extra", "Hard", "Weighted", "Avg.", "MOVIEDATA", "0.645", "0.619", "0.437", "0.108", "0.475", "CHINOOK", "0.610", "0.442", "0.396", "0.482", "0.473", "COLLEGE", "0.525", "0.739", "0.294", "0.077", "0.468", "DRIVING", "School", "0.518", "0.272", "0.611", "0.187", "0.451", "FORMULA", "1", "0.355", "0.075", "0.0", "0.0", "0.263"], "regionBoundary": {"x2": 298.0, "y1": 193.8900146484375, "x1": 73.0, "y2": 239.8900146484375}, "caption": "Table 5: Precision of queries against our 5 datasets according to query complexity. \u201dWeighted Avg.\u201d refers to the mean average precision over all queries irrespective of the query complexity category.", "page": 8}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 730.1611158582899, "y1": 458.31320020887586, "x1": 426.772223578559, "y2": 499.8582628038194}, "name": "1", "caption_text": "Figure 1: (a) Example of an Operation Tree (OT) for the query \u201dWho starred in \u2019The Notebook\u2019?\u201d (b) The corresponding database schema.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 695.0, "y1": 86.0, "x1": 469.0, "y2": 458.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.3462592230902, "y1": 930.125766330295, "x1": 426.34311252170136, "y2": 1054.693094889323}, "name": "2", "caption_text": "Figure 2: The set of production rules for the contextfree grammar of the operation trees, where table name denotes the set of all entity types in the database, attributes denotes the set of all attributes of entity types, and values denotes the set of all entries in the database. The non-terminal symbols S, T,and R denote the startsymbol, intermediate tables, and result tables respectively.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 708.0, "y1": 793.0, "x1": 449.0, "y2": 898.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 534.3809763590494, "y1": 190.09657965766058, "x1": 295.1125038994683, "y2": 198.433346218533}, "name": "1", "caption_text": "Table 1: Statistics of the new corpus OTTA", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 712.0, "y1": 83.0, "x1": 118.0, "y2": 161.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2235955132378, "y1": 288.71048821343317, "x1": 99.57083596123589, "y2": 346.8611823187934}, "name": "2", "caption_text": "Table 2: Comparison of our corpus OTTA to the Spider and LC-QuaD 2.0 corpora. Note that the number of databases in LC-QuaD 2.0 is only 1, since it is an open-domain knowledge base, and the number of tables corresponds to the number of different classes. Numbers in parentheses only consider databases with more than 5 tables.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 712.0, "y1": 188.0, "x1": 118.0, "y2": 260.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9264695909288, "y1": 440.6007130940755, "x1": 99.57083596123589, "y2": 465.54162767198346}, "name": "3", "caption_text": "Table 3: Comparison of the query complexity based on the ratio of components per query. For the aggregations in LC-QuaD 2.0, we report the number of queries that use a Count operation.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 712.0, "y1": 347.0, "x1": 102.0, "y2": 411.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 698.4424591064453, "y1": 416.98099772135413, "x1": 131.09706242879233, "y2": 439.42879570855035}, "name": "4", "caption_text": "Table 4: Example questions from OTTA and Spider. We grouped the examples by the hardness scores. The examples are for the Chinook domain, which is an online music store database.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 735.0, "y1": 86.0, "x1": 131.0, "y2": 419.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.45031229654944, "y1": 362.51326666937933, "x1": 99.57083596123589, "y2": 420.66396077473956}, "name": "5", "caption_text": "Table 5: Precision of queries against our 5 datasets according to query complexity. \u201dWeighted Avg.\u201d refers to the mean average precision over all queries irrespective of the query complexity category.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 413.0, "y1": 253.0, "x1": 102.0, "y2": 333.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 404.88315158420136, "y1": 883.8327195909288, "x1": 100.0, "y2": 941.9820149739583}, "name": "3", "caption_text": "Figure 3: Learning curve for 20%, 40%, 60%, 80%, and 100% of the data on database MOVIEDATA as part of OTTA. We compare the scores for the training with and without token alignment.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 385.0, "y1": 694.0, "x1": 121.0, "y2": 851.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.3368496365017, "y1": 149.80906380547418, "x1": 99.5708253648546, "y2": 207.95839097764755}, "name": "6", "caption_text": "Table 6: Table Coverage, in % to total amount of existing tables. Our dataset shows better table coverage, except for one database (college), where the coverage differs by one table. The biggest improvement in coverage was achieved on the database formula 1, which is also the most complex database with the largest amount of tables. The number in braces indicates the average table coverage for the databases with more than 5 tables.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 661.0, "y1": 83.0, "x1": 162.0, "y2": 122.0}, "page": 12, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 730.1617092556423, "y1": 294.35768127441406, "x1": 99.57083596123589, "y2": 335.90282864040796}, "name": "7", "caption_text": "Table 7: Attribute Coverage. Our method gives better attribute coverage in particular for larger datasets, for instance, FORMULA 1. The number in braces indicates the average attribute coverage for the databases with more than 5 tables.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 665.0, "y1": 209.0, "x1": 165.0, "y2": 264.0}, "page": 12, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 536.8925730387369, "y1": 423.60352410210504, "x1": 292.6013946533203, "y2": 431.9402906629774}, "name": "8", "caption_text": "Table 8: Average number of joins per query.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 665.0, "y1": 336.0, "x1": 160.0, "y2": 394.0}, "page": 12, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.927486843533, "y1": 580.8868832058376, "x1": 100.0, "y2": 639.0374925401476}, "name": "4", "caption_text": "Figure 4: Example of a randomly sampled tree. The nodes denote the node type with their arguments. The Tokens are assigned during the second phase of the annotation process. This tree is the answer to the question: What is the average movie vote of different movies having an Oscar nominee with a cast character called Jesse and were nominated for an Oscar in the year 1991 or later?", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 675.0, "y1": 111.0, "x1": 156.0, "y2": 538.0}, "page": 13, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9268086751301, "y1": 1074.3993971082898, "x1": 99.99997880723741, "y2": 1115.9445020887586}, "name": "5", "caption_text": "Figure 5: The annotation tool. (a) The OT and the constraints are shown to the annotators. For each node, the annotators can inspect the result of the execution. The annotators write a question and (b) assign the tokens of the question to the operations.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 722.0, "y1": 87.0, "x1": 101.0, "y2": 1076.0}, "page": 14, "dpi": 0}], "error": null, "pdf": "/work/host-output/a1c0c8f87736150de2f080707cf716676c2aecfe/2020.acl-main.84.pdf", "dpi": 100}