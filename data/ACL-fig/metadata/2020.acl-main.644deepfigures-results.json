{"raw_detected_boxes": [[{"x2": 728.0, "y1": 311.0, "x1": 428.0, "y2": 536.0}], [{"x2": 656.0, "y1": 95.0, "x1": 171.0, "y2": 270.0}, {"x2": 644.0, "y1": 322.0, "x1": 358.0, "y2": 448.0}], [{"x2": 727.0, "y1": 92.0, "x1": 103.0, "y2": 163.0}], [], [{"x2": 705.0, "y1": 86.0, "x1": 451.0, "y2": 214.0}], [{"x2": 392.0, "y1": 137.0, "x1": 111.0, "y2": 196.0}, {"x2": 712.0, "y1": 91.0, "x1": 438.0, "y2": 268.0}, {"x2": 665.0, "y1": 361.0, "x1": 493.0, "y2": 455.0}, {"x2": 405.0, "y1": 515.0, "x1": 105.0, "y2": 613.0}], [{"x2": 721.0, "y1": 142.0, "x1": 436.0, "y2": 242.0}, {"x2": 713.0, "y1": 639.0, "x1": 444.0, "y2": 697.0}], [{"x2": 721.0, "y1": 95.0, "x1": 106.0, "y2": 319.0}], [{"x2": 396.0, "y1": 503.0, "x1": 107.0, "y2": 570.0}], [], [], [], [{"x2": 680.0, "y1": 130.0, "x1": 154.0, "y2": 192.0}, {"x2": 681.0, "y1": 332.0, "x1": 119.0, "y2": 609.0}, {"x2": 684.0, "y1": 679.0, "x1": 116.0, "y2": 958.0}], [{"x2": 701.0, "y1": 174.0, "x1": 122.0, "y2": 530.0}, {"x2": 697.0, "y1": 727.0, "x1": 137.0, "y2": 935.0}]], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Figure", "name": "1", "captionBoundary": {"x2": 527.2001953125, "y1": 405.45654296875, "x1": 307.2760009765625, "y2": 495.1449279785156}, "imageText": ["Random", "Starting", "Point", "3", "1", "2", "0", "look", "towards", "the", "door", "leading", "outside", "the", "cafe.", "notice", "the", "silver", "and", "black", "coffee", "pot", "closest", "to", "you", "on", "the", "bar.", "see", "the", "black", "trash", "bin", "on", "the", "floor", "in", "front", "of", "the", "coffee", "pot.", "waldo", "is", "on", "the", "face", "of", "the", "trash", "bin", "about", "1", "foot", "off", "the", "floor", "and", "also", "slightly", "on", "the", "brown", "wood."], "regionBoundary": {"x2": 525.0, "y1": 221.8900146484375, "x1": 308.0, "y2": 385.8900146484375}, "caption": "Figure 1: An example from Refer360\u00b0 . Orange frames represent the field-of-view (FoV) of the follower after interpreting each instruction. Numbers in the frames represent the sequential order. Green lines show how FoVs change continuously. After each instruction, the follower changes the FoV to align with what the instruction describes. Please see Figure 2a to see the correct location of Waldo.", "page": 0}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 290.2705078125, "y1": 453.7885437011719, "x1": 71.69100189208984, "y2": 471.7460021972656}, "imageText": ["2D", "Guess", "What?!", "24.99", "27713", "160745", "2D", "Google-Ref", "8.46", "12108", "142210", "2D", "Refer-UNC", "3.51", "21305", "414138", "360\u00b0", "Refer360\u00b0", "43.80", "11220", "17,137", "360\u00b0", "Touchdown-SDR", "26.97", "5705", "9325", "Scenes", "Dataset", "Avg.", "Text", "Length", "Vocab.", "Size", "Size"], "regionBoundary": {"x2": 297.0, "y1": 370.8900146484375, "x1": 73.0, "y2": 441.8900146484375}, "caption": "Table 4: Language statistics for Refer360\u00b0 dataset and other referring expression recognition datasets.", "page": 5}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 525.7157592773438, "y1": 207.93453979492188, "x1": 307.0270080566406, "y2": 225.89202880859375}, "imageText": ["Refer360\u00b0", "Touchdown-SDR", "Guess", "What?!", "Google-Ref", "Refer-UNC", "#", "of", "Tokens", "in", "an", "Instruction", "e", "re", "nc", "cc", "ur", "f", "O", "%", "o", "25", "20", "15", "10", "5", "0", "0", "20", "40", "60"], "regionBoundary": {"x2": 521.0, "y1": 66.98765563964844, "x1": 315.34844970703125, "y2": 192.45947265625}, "caption": "Figure 3: Distribution of the number of tokens for vision-and-language datasets similar to Refer360\u00b0", "page": 5}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 291.9241943359375, "y1": 153.55355834960938, "x1": 71.69100189208984, "y2": 195.42108154296875}, "imageText": ["Touchdown-SDR", "93.81", "15.93", "Refer360\u00b0", "62.44", "42.93", "Dataset", "Avg", "#", "of", "Objects", "Object", "Type", "PPL"], "regionBoundary": {"x2": 286.0, "y1": 98.8900146484375, "x1": 76.0, "y2": 140.8900146484375}, "caption": "Table 3: Statistics for detected objects per image in Touchdown-SDR and Refer360\u00b0 . On average, Refer360\u00b0 images contain fewere of objects. However, these objects are from a wider variety of object types.", "page": 5}, {"figType": "Table", "name": "5", "captionBoundary": {"x2": 526.7420043945312, "y1": 339.7075500488281, "x1": 306.9670104980469, "y2": 357.6650085449219}, "imageText": ["Train", "13287", "Validation", "Seen", "900", "Validation", "Unseen", "1009", "Test", "Seen", "900", "Test", "Unseen", "1041", "Split", "#", "of", "Instances"], "regionBoundary": {"x2": 484.0, "y1": 254.8900146484375, "x1": 349.0, "y2": 327.8900146484375}, "caption": "Table 5: Statistics for dataset splits in Refer360\u00b0 dataset.", "page": 5}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 526.792724609375, "y1": 366.5615539550781, "x1": 72.00001525878906, "y2": 420.3849792480469}, "imageText": ["(c)", "An", "example", "image", "from", "Google-Ref", "dataset", "with", "the", "referring", "expression", "\u201ca", "young", "elephant", "nudges", "its", "head", "into", "that", "of", "a", "slightly", "taller", "one.\u201d.", "(b)", "An", "example", "scene", "from", "Touchdown-SDR", "where", "the", "bullseye", "is", "pointing", "to", "the", "target", "location.", "In-", "structions", "for", "this", "instance", "are", "\u201ca", "black", "doorway", "with", "red", "brick", "to", "the", "right", "of", "it,", "and", "green", "brick", "to", "the", "left", "of", "it.", "it", "has", "a", "light", "just", "above", "the", "doorway,", "and", "on", "that", "light", "is", "where", "you", "will", "\ufb01nd", "touchdown.\u201d", "(a)", "An", "example", "scene", "from", "the", "Refer360\u00b0", "dataset.", "Note", "that", "both", "annotators", "and", "systems", "cannot", "observe", "the", "shaded", "area.", "They", "only", "observe", "a", "partial", "\ufb01eld", "of", "view", "which", "can", "be", "updated", "dynamically.", "Partial", "FoV"], "regionBoundary": {"x2": 523.594482421875, "y1": 61.8900146484375, "x1": 73.97100067138672, "y2": 351.2440185546875}, "caption": "Figure 2: Examples are from (a) Refer360\u00b0 (b) Touchdown-SDR, and (c) Google-Ref datasets. In Refer360\u00b0 , the target location could be any random location on the image. In (b), annotators chose an existing object as the target location. In (c), boxes for objects were used as targets. Refer360\u00b0 also seeks to increase the complexity of instruction following, making it more realistic by introducing a partial and dynamic FoV rather than providing a holistic oracle-like view of the image.", "page": 1}, {"figType": "Table", "name": "6", "captionBoundary": {"x2": 525.895263671875, "y1": 186.74453735351562, "x1": 306.9670104980469, "y2": 228.612060546875}, "imageText": ["The", "perplexity", "of", "the", "distribution", "of", "an", "object", "that", "the", "target", "is", "located", "on", "9.53", "17.86", "The", "perplexity", "of", "the", "distribution", "of", "the", "closest", "objects", "17.80", "46.84", "The", "average", "distance", "to", "the", "closest", "objects", "8.64", "23.88", "Comparison", "Touchdown-SDR", "Refer360\u00b0"], "regionBoundary": {"x2": 519.0, "y1": 101.8900146484375, "x1": 313.0, "y2": 174.8900146484375}, "caption": "Table 6: Statistics for target locations image in Touchdown-SDR and Refer360\u00b0 . Target is located on or near the wider variety of objects and further away from other objects.", "page": 6}, {"figType": "Table", "name": "7", "captionBoundary": {"x2": 527.2897338867188, "y1": 514.3635864257812, "x1": 306.9170227050781, "y2": 544.2760009765625}, "imageText": ["Instructions", "Average", "Distance", "Accuracy", "Last", "Sentence", "73.01", "0.37", "Last", "2", "Sentences", "42.32", "0.63", "All", "Sentences", "11.35", "0.88"], "regionBoundary": {"x2": 514.0, "y1": 460.8900146484375, "x1": 319.0, "y2": 501.8900146484375}, "caption": "Table 7: Results for instruction ablation human study. Annotators need all instructions to complete the task accurately.", "page": 6}, {"figType": "Figure", "name": "6", "captionBoundary": {"x2": 465.7304382324219, "y1": 398.0075378417969, "x1": 131.81399536132812, "y2": 404.010009765625}, "imageText": [], "regionBoundary": {"x2": 515.0, "y1": 114.8900146484375, "x1": 83.0, "y2": 385.8900146484375}, "caption": "Figure 6: The most frequently detected objects in Touchdown-SDR and Refer360\u00b0 .", "page": 13}, {"figType": "Figure", "name": "7", "captionBoundary": {"x2": 525.5472412109375, "y1": 688.6175537109375, "x1": 72.0, "y2": 706.5750122070312}, "imageText": ["Random", "Manual", "Total", "#", "of", "Tokens", "for", "Whole", "Instruction", "e", "re", "nc", "cc", "ur", "f", "O", "%", "o", "0.06", "0.05", "0.04", "0.03", "0.02", "0.01", "0", "20", "40", "60", "80", "100", "0.00", "Random", "Manual", "#", "of", "Tokens", "for", "Each", "Instruction", "Sentence", "e", "re", "nc", "cc", "ur", "f", "O", "%", "o", "0.35", "0.30", "0.25", "0.20", "0.15", "0.10", "0.05", "0", "10", "20", "30", "40", "0.00"], "regionBoundary": {"x2": 502.0, "y1": 524.8703002929688, "x1": 98.86567687988281, "y2": 672.7070922851562}, "caption": "Figure 7: Text length for different placement methods for single instruction and instruction sequences. Manual means annotators pick the target location, random means we randomly pick the target location in the scene.", "page": 13}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 525.5472412109375, "y1": 132.74752807617188, "x1": 71.69100189208984, "y2": 174.61505126953125}, "imageText": ["Refer360\u00b0", "Random", "Points", "Dynamic", "with", "partial", "FoV", "4", "Directions", "3", "Touchdown-SDR", "(Chen", "et", "al.,", "2018)", "Human", "Selected", "Points", "Oracle:", "Holistic", "&", "Static,", "360\u00b0scenes", "7", "7", "Google-Ref", "(Mao", "et", "al.,", "2016)", "Annotated", "Objects", "2D", "Images", "7", "7", "Ref-UNC", "(Kazemzadeh", "et", "al.,", "2014)", "Annotated", "Objects", "2D", "Images", "7", "7", "Dataset", "Target", "Location", "Selection", "Field", "of", "View", "(FoV)", "Action", "Space", "Intermediate", "Steps"], "regionBoundary": {"x2": 524.0, "y1": 67.73883056640625, "x1": 73.0, "y2": 120.8900146484375}, "caption": "Table 1: Comparison of referring expression datasets, including our proposed Refer360\u00b0 dataset. Refer360\u00b0 poses a more challenging scenario where the system observes only a partial and dynamic FoV. Refer360\u00b0 also has includes explicit alignments between intermediate instruction steps and human follower actions which can be used as an auxiliary evaluation metric or source of supervision.", "page": 2}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 525.5472412109375, "y1": 451.1845397949219, "x1": 72.0, "y2": 481.0979919433594}, "imageText": [], "regionBoundary": {"x2": 493.0, "y1": 235.8900146484375, "x1": 83.0, "y2": 438.8900146484375}, "caption": "Figure 4: Screenshot of Amazon Mechanical Turk interface for finding task. We ask annotators to complete each instruction before moving to the next one. To do so change the bullseye where they think the instruction is describing.", "page": 12}, {"figType": "Figure", "name": "5", "captionBoundary": {"x2": 525.9060668945312, "y1": 702.8966064453125, "x1": 71.53199768066406, "y2": 732.8099975585938}, "imageText": [], "regionBoundary": {"x2": 493.0, "y1": 484.8900146484375, "x1": 83.0, "y2": 690.8900146484375}, "caption": "Figure 5: Screenshot of Amazon Mechanical Turk interface for describing task. We ask annotators to first find Waldo themselves, then give detailed insturctions one by one so that anyone starting from a random field-of-view find it.", "page": 12}, {"figType": "Table", "name": "10", "captionBoundary": {"x2": 525.5469970703125, "y1": 153.81552124023438, "x1": 71.69100189208984, "y2": 171.77301025390625}, "imageText": ["Stage", "I:", "Hiring", "256", "854", "n\\a", "Stage", "II:", "Collection", "&", "Veri\ufb01cation", "86", "20630", "14062", "Stage", "III:", "Veri\ufb01cation", "86", "n\\a", "3073", "Annotation", "Stage", "#", "of", "Annotators", "#", "of", "Collected", "Instructions", "#", "of", "Veri\ufb01ed", "Instructions"], "regionBoundary": {"x2": 490.0, "y1": 88.8900146484375, "x1": 108.0, "y2": 141.8900146484375}, "caption": "Table 10: Statistics for data collection stages. Stage I is for hiring annotators. Stage II is for collecting and verifying the instructions. Last stage is further verifying hard instances that are not verified II.", "page": 12}, {"figType": "Table", "name": "8", "captionBoundary": {"x2": 525.7166137695312, "y1": 242.14651489257812, "x1": 71.69100189208984, "y2": 284.0150146484375}, "imageText": ["Coreference", "96", "1.6", "on", "the", "very", "upper", "left", "corner", "of", "the", "blue", "part", "of", "that", "window", "Comparison", "15", "0.1", "the", "smaller", "building", "to", "the", "right", "of", "the", "spire", "Sequencing", "13", "0.1", "go", "right", "just", "a", "smidge", "and", "then", "go", "up", "above", "Counting", "30", "0.3", "shaped", "like", "a", "football", "and", "has", "3", "silver", "legs", "Allocentric", "Spatial", "Mention", "46", "0.6", "\ufb01nd", "the", "shelves", "with", "books", "nearest", "to", "you", "Egocentric", "Spatial", "Mention", "35", "0.5", "waldo", "is", "sitting", "on", "the", "right", "side", "of", "the", "window", "Direction", "92", "1.6", "look", "at", "the", "knife", "on", "the", "wall", "to", "the", "left", "Temporal", "Condition", "13", "0.1", "turn", "right", "until", "you", "see", "a", "mirror", "on", "the", "wall", "3D", "understanding", "22", "0.2", "counter", "with", "the", "two", "bar", "stools", "sitting", "in", "front", "of", "it", "Inexact/Approximate", "Language", "28", "0.2", "in", "front", "of", "the", "white", "strip", "at", "the", "bottom", "slightly", "off", "center", "More", "than", "2", "Supporting", "Objects", "47", "0.5", "now", "look", "on", "the", "\ufb02oor", "in", "between", "the", "table", "and", "the", "chair", "Phenomenon", "c", "\u00b5", "Example", "from", "Refer360\u00b0"], "regionBoundary": {"x2": 521.0, "y1": 69.34329986572266, "x1": 76.0, "y2": 229.8900146484375}, "caption": "Table 8: Linguistic analysis of 100 randomly sampled examples from Refer360\u00b0 . We annotate each example for the presence and count of each phenomenon. c is the total number of instructions out of the 100 containing at least one example of the phenomenon. \u00b5 is the mean number of times each phenomenon appears per instruction sequence.", "page": 7}, {"figType": "Table", "name": "9", "captionBoundary": {"x2": 291.5160217285156, "y1": 422.4035339355469, "x1": 71.69100189208984, "y2": 464.2709655761719}, "imageText": ["Touchdown-SDR", "(reported)", "26.1", "708", "Touchdown-SDR", "(replication)", "23.5", "715", "Refer360\u00b0", "13.0", "1235", "Dataset", "Accuracy", "(%)", "Distance"], "regionBoundary": {"x2": 289.0, "y1": 357.8900146484375, "x1": 74.0, "y2": 409.8900146484375}, "caption": "Table 9: Results for the LingUNet on two benchmark datasets. Since LinGUNet designed for observing the full instruction set and the holistic view of the scene, and it performs significantly worse on Refer360\u00b0 .", "page": 8}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 527.2003173828125, "y1": 166.99252319335938, "x1": 306.9670104980469, "y2": 184.95001220703125}, "imageText": ["Restaurant", "Indoor", "500", "Shop", "Indoor", "250", "Expo", "Showroom", "Indoor", "250", "Living", "Room", "Indoor", "250", "Bedroom", "Indoor", "250", "Street", "Outdoor", "250", "Plaza", "Courtyard", "Outdoor", "250", "Scene", "Type", "Scene", "Location", "#", "of", "Images"], "regionBoundary": {"x2": 508.0, "y1": 62.8900146484375, "x1": 325.0, "y2": 154.8900146484375}, "caption": "Table 2: Statistics for Panoramic Images used in Refer360\u00b0 dataset.", "page": 4}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 732.2224934895833, "y1": 563.1340874565972, "x1": 426.772223578559, "y2": 687.7012888590494}, "name": "1", "caption_text": "Figure 1: An example from Refer360\u00b0 . Orange frames represent the field-of-view (FoV) of the follower after interpreting each instruction. Numbers in the frames represent the sequential order. Green lines show how FoVs change continuously. After each instruction, the follower changes the FoV to align with what the instruction describes. Please see Figure 2a to see the correct location of Waldo.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 729.0, "y1": 308.0, "x1": 428.0, "y2": 536.0}, "page": 0, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 731.6565619574652, "y1": 509.1132693820529, "x1": 100.00002119276259, "y2": 583.8680267333984}, "name": "2", "caption_text": "Figure 2: Examples are from (a) Refer360\u00b0 (b) Touchdown-SDR, and (c) Google-Ref datasets. In Refer360\u00b0 , the target location could be any random location on the image. In (b), annotators chose an existing object as the target location. In (c), boxes for objects were used as targets. Refer360\u00b0 also seeks to increase the complexity of instruction following, making it more realistic by introducing a partial and dynamic FoV rather than providing a holistic oracle-like view of the image.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 661.0, "y1": 305.0, "x1": 341.0, "y2": 463.0}, "page": 1, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9267239040798, "y1": 184.37156677246094, "x1": 99.57083596123589, "y2": 242.52090454101562}, "name": "1", "caption_text": "Table 1: Comparison of referring expression datasets, including our proposed Refer360\u00b0 dataset. Refer360\u00b0 poses a more challenging scenario where the system observes only a partial and dynamic FoV. Refer360\u00b0 also has includes explicit alignments between intermediate instruction steps and human follower actions which can be used as an auxiliary evaluation metric or source of supervision.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 728.0, "y1": 87.0, "x1": 102.0, "y2": 167.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.222663031684, "y1": 231.9340599907769, "x1": 426.3430701361762, "y2": 256.8750169542101}, "name": "2", "caption_text": "Table 2: Statistics for Panoramic Images used in Refer360\u00b0 dataset.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 717.0, "y1": 86.0, "x1": 441.0, "y2": 231.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.4502699110243, "y1": 213.26883104112412, "x1": 99.57083596123589, "y2": 271.41816880967883}, "name": "3", "caption_text": "Table 3: Statistics for detected objects per image in Touchdown-SDR and Refer360\u00b0 . On average, Refer360\u00b0 images contain fewere of objects. However, these objects are from a wider variety of object types.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 397.0, "y1": 120.0, "x1": 100.0, "y2": 213.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 730.1607767740885, "y1": 288.7979719373915, "x1": 426.42640007866754, "y2": 313.73892890082465}, "name": "3", "caption_text": "Figure 3: Distribution of the number of tokens for vision-and-language datasets similar to Refer360\u00b0", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 724.0, "y1": 90.0, "x1": 431.0, "y2": 268.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 731.5861172146267, "y1": 471.81604173448346, "x1": 426.3430701361762, "y2": 496.7569563123915}, "name": "5", "caption_text": "Table 5: Statistics for dataset splits in Refer360\u00b0 dataset.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 681.0, "y1": 354.0, "x1": 485.0, "y2": 472.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.15348307291663, "y1": 630.2618662516276, "x1": 99.57083596123589, "y2": 655.2027808295355}, "name": "4", "caption_text": "Table 4: Language statistics for Refer360\u00b0 dataset and other referring expression recognition datasets.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 413.0, "y1": 498.0, "x1": 100.0, "y2": 630.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 730.4100884331597, "y1": 259.3674129909939, "x1": 426.3430701361762, "y2": 317.5167507595486}, "name": "6", "caption_text": "Table 6: Statistics for target locations image in Touchdown-SDR and Refer360\u00b0 . Target is located on or near the wider variety of objects and further away from other objects.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 722.0, "y1": 142.0, "x1": 426.0, "y2": 259.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.3468526204426, "y1": 714.3938700358073, "x1": 426.27364264594183, "y2": 755.9388902452257}, "name": "7", "caption_text": "Table 7: Results for instruction ablation human study. Annotators need all instructions to complete the task accurately.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 722.0, "y1": 622.0, "x1": 427.0, "y2": 714.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 730.1619635687933, "y1": 336.3146040174696, "x1": 99.57083596123589, "y2": 394.46529812282984}, "name": "8", "caption_text": "Table 8: Linguistic analysis of 100 randomly sampled examples from Refer360\u00b0 . We annotate each example for the presence and count of each phenomenon. c is the total number of instructions out of the 100 containing at least one example of the phenomenon. \u00b5 is the mean number of times each phenomenon appears per instruction sequence.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 724.0, "y1": 86.0, "x1": 100.0, "y2": 336.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 404.88336351182727, "y1": 586.6715749104817, "x1": 99.57083596123589, "y2": 644.8207855224609}, "name": "9", "caption_text": "Table 9: Results for the LingUNet on two benchmark datasets. Since LinGUNet designed for observing the full instruction set and the holistic view of the scene, and it performs significantly worse on Refer360\u00b0 .", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 401.0, "y1": 497.0, "x1": 100.0, "y2": 587.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9263848198784, "y1": 213.63266838921442, "x1": 99.57083596123589, "y2": 238.57362535264755}, "name": "10", "caption_text": "Table 10: Statistics for data collection stages. Stage I is for hiring annotators. Stage II is for collecting and verifying the instructions. Last stage is further verifying hard instances that are not verified II.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 680.0, "y1": 123.0, "x1": 150.0, "y2": 197.0}, "page": 12, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9267239040798, "y1": 626.6451941596137, "x1": 100.0, "y2": 668.191655476888}, "name": "4", "caption_text": "Figure 4: Screenshot of Amazon Mechanical Turk interface for finding task. We ask annotators to complete each instruction before moving to the next one. To do so change the bullseye where they think the instruction is describing.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 684.0, "y1": 328.0, "x1": 102.0, "y2": 626.0}, "page": 12, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 730.4250929090712, "y1": 976.2452867296007, "x1": 99.34999677870009, "y2": 1017.7916632758246}, "name": "5", "caption_text": "Figure 5: Screenshot of Amazon Mechanical Turk interface for describing task. We ask annotators to first find Waldo themselves, then give detailed insturctions one by one so that anyone starting from a random field-of-view find it.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 688.0, "y1": 662.0, "x1": 100.0, "y2": 975.0}, "page": 12, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 646.8478308783637, "y1": 552.7882470024956, "x1": 183.07499355740018, "y2": 561.125013563368}, "name": "6", "caption_text": "Figure 6: The most frequently detected objects in Touchdown-SDR and Refer360\u00b0 .", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 707.0, "y1": 166.0, "x1": 120.0, "y2": 530.0}, "page": 13, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9267239040798, "y1": 956.4132690429688, "x1": 100.0, "y2": 981.3541836208767}, "name": "7", "caption_text": "Figure 7: Text length for different placement methods for single instruction and instruction sequences. Manual means annotators pick the target location, random means we randomly pick the target location in the scene.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 697.0, "y1": 727.0, "x1": 132.0, "y2": 935.0}, "page": 13, "dpi": 0}], "error": null, "pdf": "/work/host-output/07c16ba3c3bcb3edfc7e87cf2d97ef6b8bcb4c15/2020.acl-main.644.pdf", "dpi": 100}