{"raw_detected_boxes": [[{"x2": 743.0, "y1": 272.0, "x1": 440.0, "y2": 485.0}], [{"x2": 730.0, "y1": 87.0, "x1": 451.0, "y2": 323.0}], [], [], [{"x2": 375.0, "y1": 118.0, "x1": 136.0, "y2": 399.0}], [], [{"x2": 749.0, "y1": 87.0, "x1": 436.0, "y2": 212.0}, {"x2": 405.0, "y1": 80.0, "x1": 105.0, "y2": 346.0}], [{"x2": 347.0, "y1": 87.0, "x1": 170.0, "y2": 196.0}, {"x2": 746.0, "y1": 82.0, "x1": 441.0, "y2": 372.0}], [], []], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Figure", "name": "1", "captionBoundary": {"x2": 540.0062255859375, "y1": 361.6713562011719, "x1": 313.2010192871094, "y2": 389.1819763183594}, "imageText": ["(d)", "(c)", "(b)", "(a)", "and", "relate[in]", "lookup[Georgia]", "relate", "infind[city]", "relate", "Knowledge", "source", "Montgomery", "Atlanta", "Georgia", "find", "city", "lookup", "Georgia", "Network", "layout", "(Section", "4.1)", "and", "What", "cities", "are", "in", "Georgia?", "Atlanta", "and", "lookup", "find", "Module", "inventory", "(Section", "4.2)"], "regionBoundary": {"x2": 536.0, "y1": 196.0, "x1": 315.0, "y2": 351.0}, "caption": "Figure 1: A learned syntactic analysis (a) is used to assemble a collection of neural modules (b) into a deep neural network (c), and applied to a world representation (d) to produce an answer.", "page": 0}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 540.0062255859375, "y1": 242.40936279296875, "x1": 313.2010192871094, "y2": 313.27899169921875}, "imageText": ["(c)", "(d)", "find", "state(a)", "(b)", "describe", "color", "find", "bird", "true", "exists", "Montgomery", "Atlanta", "Georgia", "Montgomery", "Atlanta", "Georgia", "black", "and", "white"], "regionBoundary": {"x2": 525.6947631835938, "y1": 63.227806091308594, "x1": 324.4638366699219, "y2": 233.0}, "caption": "Figure 2: Simple neural module networks, corresponding to the questions What color is the bird? and Are there any states? (a) A neural find module for computing an attention over pixels. (b) The same operation applied to a knowledge base. (c) Using an attention produced by a lower module to identify the color of the region of the image attended to. (d) Performing quantification by evaluating an attention directly.", "page": 1}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 540.0061645507812, "y1": 162.3603515625, "x1": 313.2010192871094, "y2": 211.551025390625}, "imageText": ["Zhou", "(2015)", "76.6", "35.0", "42.6", "55.7", "55.9", "Noh", "(2015)", "80.7", "37.2", "41.7", "57.2", "57.4", "NMN", "77.7", "37.2", "39.3", "54.8", "55.1", "NMN*", "79.7", "37.1", "42.8", "57.3", "\u2013", "D-NMN", "80.5", "37.4", "43.1", "57.9", "58.0", "Yes/No", "Number", "Other", "All", "All", "test-dev", "test-std"], "regionBoundary": {"x2": 540.0, "y1": 58.0, "x1": 314.0, "y2": 152.0}, "caption": "Table 1: Results on the VQA test server. NMN is the parameter-tying model from Andreas et al. (2015), while NMN* is a reimplementation using the same image processing pipeline as D-NMN. The model with dynamic network structure prediction achieves the best published results on this task.", "page": 6}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 298.80609130859375, "y1": 263.247314453125, "x1": 72.0009994506836, "y2": 334.1159362792969}, "imageText": ["tag", "white", "boat", "(board)", "find[man])", "(describe[what]", "find[wear])", "(describe[color]", "find[ear]))", "(and", "find[sheep]", "(describe[what]", "What", "is", "the", "man", "dragging?", "What", "is", "in", "the", "sheep\u2019s", "ear?", "What", "color", "is", "she", "wearing?"], "regionBoundary": {"x2": 297.0, "y1": 58.0, "x1": 74.0, "y2": 253.0}, "caption": "Figure 4: Sample outputs for the visual question answering task. The second row shows the final attention provided as input to the top-level describe module. For the first two examples, the model produces reasonable parses, attends to the correct region of the images (the ear and the woman\u2019s clothing), and generates the correct answer. In the third image, the verb is discarded and a wrong answer is produced.", "page": 6}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 298.80609130859375, "y1": 149.80633544921875, "x1": 72.0009994506836, "y2": 198.9959716796875}, "imageText": ["LSP-F", "48", "\u2013", "LSP-W", "51", "\u2013", "NMN", "51.7", "35.7", "D-NMN", "54.3", "42.9", "Model", "GeoQA", "GeoQA+Q", "Accuracy"], "regionBoundary": {"x2": 252.0, "y1": 58.0, "x1": 117.0, "y2": 141.0}, "caption": "Table 2: Results on the GeoQA dataset, and the GeoQA dataset with quantification. Our approach outperforms both a purely logical model (LSP-F) and a model with learned perceptual predicates (LSP-W) on the original dataset, and a fixedstructure NMN under both evaluation conditions.", "page": 7}, {"figType": "Figure", "name": "5", "captionBoundary": {"x2": 540.0061645507812, "y1": 277.70635986328125, "x1": 313.2010192871094, "y2": 305.2179870605469}, "imageText": ["[none]", "(daytona-beach):", "wrong", "module", "behavior", "(relate[in]", "lookup[florida]))", "(and", "lookup[beach]", "lookup[city]", "What", "beach", "city", "is", "there", "in", "Florida?", "yes", "(daytona-beach):", "wrong", "parse", "(relate[in]", "lookup[florida])))", "(exists", "(and", "lookup[beach]", "What", "are", "some", "beaches", "in", "Florida?", "everglades:", "correct", "(and", "find[park]", "(relate[in]", "lookup[florida]))", "What", "national", "parks", "are", "in", "Florida?", "yes:", "correct", "(exists", "(and", "lookup[key-largo]", "find[island]))", "Is", "Key", "Largo", "an", "island?"], "regionBoundary": {"x2": 537.0, "y1": 58.0, "x1": 314.0, "y2": 272.0}, "caption": "Figure 5: Example layouts and answers selected by the model on the GeoQA dataset. For incorrect predictions, the correct answer is shown in parentheses.", "page": 7}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 298.8060302734375, "y1": 300.1873474121094, "x1": 72.0009994506836, "y2": 338.5379638671875}, "imageText": ["lookup[Georgia]", "relate[in]", "(d)", "(c)", "(b)", "(a)", "and", "lookup[Georgia]find[city]", "...", "relate[in]", "lookup[Georgia]", "relate[in]", "find[city]", "Georgia", "in", "be", "city", "what", "What", "cities", "are", "in", "Georgia?"], "regionBoundary": {"x2": 269.8670349121094, "y1": 66.86643981933594, "x1": 98.0, "y2": 288.0}, "caption": "Figure 3: Generation of layout candidates. The input sentence (a) is represented as a dependency parse (b). Fragments of this dependency parse are then associated with appropriate modules (c), and these fragments are assembled into full layouts (d).", "page": 4}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 750.0086466471354, "y1": 502.32132805718317, "x1": 435.0014156765408, "y2": 540.530522664388}, "name": "1", "caption_text": "Figure 1: A learned syntactic analysis (a) is used to assemble a collection of neural modules (b) into a deep neural network (c), and applied to a world representation (d) to produce an answer.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 744.0, "y1": 272.0, "x1": 435.0, "y2": 502.0}, "page": 0, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 750.0086466471354, "y1": 336.6796705457899, "x1": 435.0014156765408, "y2": 435.1097106933594}, "name": "2", "caption_text": "Figure 2: Simple neural module networks, corresponding to the questions What color is the bird? and Are there any states? (a) A neural find module for computing an attention over pixels. (b) The same operation applied to a knowledge base. (c) Using an attention produced by a lower module to identify the color of the region of the image attended to. (d) Performing quantification by evaluating an attention directly.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 747.0, "y1": 87.0, "x1": 435.0, "y2": 340.0}, "page": 1, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 415.0083753797743, "y1": 416.92687140570746, "x1": 100.00138812594943, "y2": 470.19161648220484}, "name": "3", "caption_text": "Figure 3: Generation of layout candidates. The input sentence (a) is represented as a dependency parse (b). Fragments of this dependency parse are then associated with appropriate modules (c), and these fragments are assembled into full layouts (d).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 375.0, "y1": 101.0, "x1": 136.0, "y2": 400.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 750.0085618760851, "y1": 225.50048828125, "x1": 435.0014156765408, "y2": 293.82086859809027}, "name": "1", "caption_text": "Table 1: Results on the VQA test server. NMN is the parameter-tying model from Andreas et al. (2015), while NMN* is a reimplementation using the same image processing pipeline as D-NMN. The model with dynamic network structure prediction achieves the best published results on this task.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 749.0, "y1": 80.0, "x1": 435.0, "y2": 229.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 415.00846015082465, "y1": 365.62127007378473, "x1": 100.00138812594943, "y2": 464.04991149902344}, "name": "4", "caption_text": "Figure 4: Sample outputs for the visual question answering task. The second row shows the final attention provided as input to the top-level describe module. For the first two examples, the model produces reasonable parses, attends to the correct region of the images (the ear and the woman\u2019s clothing), and generates the correct answer. In the third image, the verb is discarded and a wrong answer is produced.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 413.0, "y1": 80.0, "x1": 103.0, "y2": 352.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 415.00846015082465, "y1": 208.0643547905816, "x1": 100.00138812594943, "y2": 276.383293999566}, "name": "2", "caption_text": "Table 2: Results on the GeoQA dataset, and the GeoQA dataset with quantification. Our approach outperforms both a purely logical model (LSP-F) and a model with learned perceptual predicates (LSP-W) on the original dataset, and a fixedstructure NMN under both evaluation conditions.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 364.0, "y1": 80.0, "x1": 157.0, "y2": 213.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 750.0085618760851, "y1": 385.7032775878906, "x1": 435.0014156765408, "y2": 423.9138709174262}, "name": "5", "caption_text": "Figure 5: Example layouts and answers selected by the model on the GeoQA dataset. For incorrect predictions, the correct answer is shown in parentheses.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 749.0, "y1": 80.0, "x1": 435.0, "y2": 389.0}, "page": 7, "dpi": 0}], "error": null, "pdf": "/work/host-output/28a4e8c5c9062e3fb4547523e5fb5b16c7490a95/N16-1181.pdf", "dpi": 100}