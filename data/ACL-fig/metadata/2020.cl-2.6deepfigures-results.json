{"raw_detected_boxes": [[], [], [{"x2": 610.0, "y1": 95.0, "x1": 84.0, "y2": 314.0}], [{"x2": 603.0, "y1": 124.0, "x1": 70.0, "y2": 362.0}], [{"x2": 587.0, "y1": 98.0, "x1": 79.0, "y2": 314.0}], [{"x2": 345.0, "y1": 163.0, "x1": 75.0, "y2": 266.0}], [], [{"x2": 401.0, "y1": 98.0, "x1": 75.0, "y2": 250.0}], [{"x2": 565.0, "y1": 645.0, "x1": 79.0, "y2": 830.0}], [{"x2": 493.0, "y1": 695.0, "x1": 77.0, "y2": 837.0}], [], [{"x2": 593.0, "y1": 94.0, "x1": 69.0, "y2": 292.0}, {"x2": 480.0, "y1": 453.0, "x1": 74.0, "y2": 554.0}], [], [{"x2": 543.0, "y1": 163.0, "x1": 73.0, "y2": 420.0}], [], [], [], [], [], [], [{"x2": 510.0, "y1": 404.0, "x1": 80.0, "y2": 909.0}], [], [{"x2": 578.0, "y1": 91.0, "x1": 77.0, "y2": 434.0}], [{"x2": 589.0, "y1": 89.0, "x1": 78.0, "y2": 559.0}], [{"x2": 515.0, "y1": 177.0, "x1": 84.0, "y2": 394.0}], [], [], [], [{"x2": 597.0, "y1": 90.0, "x1": 78.0, "y2": 343.0}], [], [], [{"x2": 393.0, "y1": 178.0, "x1": 74.0, "y2": 354.0}], [{"x2": 593.0, "y1": 162.0, "x1": 85.0, "y2": 267.0}, {"x2": 567.0, "y1": 430.0, "x1": 88.0, "y2": 577.0}], [], [{"x2": 584.0, "y1": 95.0, "x1": 78.0, "y2": 354.0}], [], [{"x2": 589.0, "y1": 92.0, "x1": 75.0, "y2": 326.0}], [], [{"x2": 603.0, "y1": 177.0, "x1": 80.0, "y2": 560.0}], [{"x2": 587.0, "y1": 91.0, "x1": 72.0, "y2": 324.0}, {"x2": 591.0, "y1": 428.0, "x1": 82.0, "y2": 575.0}], [], [], [], [{"x2": 489.0, "y1": 91.0, "x1": 74.0, "y2": 159.0}], [{"x2": 499.0, "y1": 89.0, "x1": 75.0, "y2": 202.0}], [], [{"x2": 285.0, "y1": 89.0, "x1": 74.0, "y2": 208.0}, {"x2": 269.0, "y1": 366.0, "x1": 80.0, "y2": 541.0}], [{"x2": 485.0, "y1": 205.0, "x1": 77.0, "y2": 275.0}], [], [{"x2": 567.0, "y1": 91.0, "x1": 73.0, "y2": 235.0}], [], [], [{"x2": 593.0, "y1": 331.0, "x1": 82.0, "y2": 567.0}], [], [], [], [], [], [], [], [], []], "raw_pdffigures_output": {"regionless-captions": [{"figType": "Figure", "boundary": {"x2": 432.13592529296875, "y1": 192.53274536132812, "x1": 53.70899963378906, "y2": 217.99899291992188}, "text": "Figure 20 The difference between GF-based and JAMR linearization of a sample AMR. The reference is an informed human translation from the given AMR.", "name": "20", "page": 52}, {"figType": "Figure", "boundary": {"x2": 339.710205078125, "y1": 432.1647644042969, "x1": 53.70899963378906, "y2": 447.66900634765625}, "text": "Figure 21 The overall architecture for multilingual AMR-to-text generation via GF.", "name": "21", "page": 52}, {"figType": "Figure", "boundary": {"x2": 364.99395751953125, "y1": 188.36776733398438, "x1": 49.547000885009766, "y2": 203.87200927734375}, "text": "Figure 19 FrameNet functions as an abstraction layer above the syntax-oriented RGL API.", "name": "19", "page": 49}, {"figType": "Figure", "boundary": {"x2": 429.9017028808594, "y1": 186.15872192382812, "x1": 49.5469970703125, "y2": 231.55099487304688}, "text": "Figure 4 The precision\u2013coverage trade-off, as correlated with producer vs. consumer translation tasks. High precision can be achieved in tasks where a producer of content wants to translate a limited kind of content. High coverage is needed when consumers of content want to translate any kind of content.", "name": "4", "page": 7}, {"figType": "Figure", "boundary": {"x2": 427.2027587890625, "y1": 221.19473266601562, "x1": 49.547000885009766, "y2": 246.66098022460938}, "text": "Figure 7 Parse trees for English, Latin, Dutch, and Arabic (above); abstract syntax tree and word alignment (below). Words in the Arabic tree are shown left to right to illustrate their VSO order.", "name": "7", "page": 11}], "figures": [{"figType": "Table", "name": "1", "captionBoundary": {"x2": 421.822998046875, "y1": 77.72274017333984, "x1": 49.547000885009766, "y2": 103.18896484375}, "imageText": ["Section", "2", "GF", "background", "n/a", "Section", "3", "GF", "tutorial", "Section", "2", "Section", "4", "RGL", "Section", "2,", "3", "Section", "5", "Levels", "of", "interlingua", "Section", "2,", "3,", "4", "Section", "6", "UD", "and", "GF", "Section", "2,", "3,", "4", "Section", "7", "Other", "approaches", "Section", "2,", "3,", "4", "Section", "Topic", "Prerequisites"], "regionBoundary": {"x2": 435.0, "y1": 111.0, "x1": 51.0, "y2": 191.89495849609375}, "caption": "Table 1 Dependencies between Sections 2 to 7. Sections 2, 3, 4 should be read in sequence, while 5, 6, 7 only depend on these and not on each other.", "page": 5}, {"figType": "Table", "name": "5", "captionBoundary": {"x2": 429.947998046875, "y1": 77.72274017333984, "x1": 53.70899963378906, "y2": 113.1519775390625}, "imageText": ["AdAP", "AdA\u2192", "AP\u2192", "AP", "very", "warm", "advmod", "head", "AdjCN", "AP\u2192", "CN\u2192", "CN", "big", "house", "amod", "head", "AdvVP", "VP\u2192", "Adv\u2192", "VP", "sleep", "here", "head", "advmod", "ComplSlash", "VPSlash\u2192", "NP\u2192", "VP", "love", "it", "head", "obj", "ComplVV", "VV\u2192", "VP\u2192", "VP", "want", "to", "run", "head", "xcomp", "DetCN", "Det\u2192", "CN\u2192", "NP", "these", "men", "det", "head", "DetQuant", "Quant\u2192", "Num\u2192", "Det", "these", "\ufb01ve", "head", "nummod", "PositA", "A\u2192", "AP", "black", "head", "PredVP", "NP\u2192", "VP\u2192", "Cl", "John", "walks", "nsubj", "head", "SlashV2a", "V2\u2192", "VPSlash", "love", "(it)", "head", "UseCl", "Temp\u2192", "Pol\u2192", "Cl\u2192", "S", "she", "won\u2019t", "have", "slept", "aux", "advmod", "head", "UseN", "N\u2192", "CN", "cat", "head", "UsePron", "Pron\u2192", "NP", "we", "head", "UttS", "S\u2192", "Utt", "she", "sleeps", "head", "Function", "Type", "Example", "UD", "labels"], "regionBoundary": {"x2": 439.0, "y1": 121.0, "x1": 55.0, "y2": 281.55908203125}, "caption": "Table 5 A sample of the syntactic combination functions in the common abstract syntax, covering the functions used in the examples in this paper. The \u201cUD labels\u201d column refers to the dependency label associated with each argument place of the function, to be discussed in Section 6.", "page": 24}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 436.2781677246094, "y1": 181.15078735351562, "x1": 53.70899963378906, "y2": 276.3560791015625}, "imageText": ["Afr", "Afrikaans", "500", "\u2014", "\u2014", "\u2014", "1200", "2009", "Ara", "Arabic", "500", "\u2014", "++", "++", "3300", "2006", "Bul", "Bulgarian", "53k", "+", "++", "++", "4500*", "2008", "Cat", "Catalan", "18k", "+", "+", "\u2014", "900+1300*", "2006", "Chi", "Chinese", "37k", "+", "++", "++", "900", "2012", "Dan", "Danish", "500", "\u2014", "++", "\u2014", "800+1100", "2002", "Dut", "Dutch", "23k", "+", "++", "\u2014", "1600", "2009", "Eng", "English", "65k", "+", "++", "\u2014", "1600", "2001", "Est", "Estonian", "84k", "+", "+", "++", "2600", "2013", "Eus", "Basque", "10k", "\u2014", "\u2014", "\u2014", "1400", "2018", "Fin", "Finnish", "42k", "+", "++", "++", "2900", "2003", "Fre", "French", "93k", "+", "++", "\u2014", "1100+1300", "2002", "Ger", "German", "44k", "+", "++", "+", "1900", "2002", "Gre", "Greek", "500", "\u2014", "\u2014", "++", "2600", "2012", "Hin", "Hindi", "36k", "+", "+", "++", "800+1100", "2012", "Ice", "Icelandic", "500", "\u2014", "\u2014", "+", "2800", "2015", "Ita", "Italian", "30k", "+", "++", "\u2014", "800+1300*", "2003", "Jpn", "Japanese", "16k", "+", "+", "++", "3100", "2012", "Lav", "Latvian", "63k", "\u2014", "+", "++", "1300", "2011", "Mlt", "Maltese", "4k", "\u2014", "\u2014", "++", "4600", "2014", "Mon", "Mongolian", "24k", "\u2014", "\u2014", "++", "1700", "2015", "Nep", "Nepali", "500", "\u2014", "\u2014", "+", "2100", "2013", "Nno", "Norwegian(n)", "500", "\u2014", "\u2014", "\u2014", "600+1100", "2016", "Nor", "Norwegian(b)", "500", "\u2014", "++", "\u2014", "600+1100", "2002", "Pes", "Persian", "500", "\u2014", "++", "++", "1200", "2013", "Pnb", "Punjabi", "500", "\u2014", "\u2014", "++", "1600", "2012", "Pol", "Polish", "500", "\u2014", "++", "++", "5700*", "2010", "Por", "Portuguese", "129k", "\u2014", "++", "+", "1000+1300*", "2018", "Ron", "Romanian", "500", "\u2014", "++", "++", "3800*", "2009", "Rus", "Russian", "136k", "+", "\u2014", "++", "3000", "2001", "Snd", "Sindhi", "500", "\u2014", "\u2014", "+", "1500", "2013", "Spa", "Spanish", "43k", "+", "++", "+", "800+1300*", "2003", "Swe", "Swedish", "111k", "+", "++", "++", "700+1100", "2002", "Tha", "Thai", "44k", "+", "+", "\u2014", "600", "2011", "Urd", "Urdu", "15k", "\u2014", "+", "++", "800+1100", "2012", "ISO", "Language", "Dict", "Transl", "Appl", "Publ", "LoC", "Year"], "regionBoundary": {"x2": 439.0, "y1": 285.0, "x1": 55.0, "y2": 653.9781494140625}, "caption": "Table 4 The complete languages of the GF Resource Grammar as of February 2019. ISO = ISO-639-3 code (so-called B code when this makes a difference). Dict = number of lemmas in the dictionary (500 means that there is just a basic lexicon of around 500 lemmas). Transl = used in wide-coverage translation. Appl = used in applications, ++ means also in commercial applications. Publ = publications available, ++ means international publications in addition to academic theses. LoC = lines of GF code (non-empty, non-comment), excluding the dictionary. The sum of two figures means shared functor+language-specific code. The asterisk * means that there is an additional set of low-level morphological paradigms extracted from external sources. Year = when the main part of the grammar was released.", "page": 20}, {"figType": "Table", "name": "10", "captionBoundary": {"x2": 394.7639465332031, "y1": 228.86776733398438, "x1": 53.70899963378906, "y2": 244.37200927734375}, "imageText": ["Abstract", "107,955", "\u2014", "Bulgarian", "81,728", "35,906", "Catalan", "95,728", "4,470", "Chinese", "41,412", "93", "English", "107,826", "107,826", "Finnish", "97,059", "90,535", "Portuguese", "94,905", "53,324", "Slovenian", "86,489", "52,017", "Spanish", "98,255", "5,066", "Swedish", "76,327", "27,998", "Thai", "64,671", "109", "Turkish", "88,954", "3,611", "Language", "Senses", "Trusted"], "regionBoundary": {"x2": 439.0, "y1": 253.0, "x1": 55.0, "y2": 392.8531188964844}, "caption": "Table 10 State of GF WordNet in terms of number of senses and number of trusted translations.", "page": 46}, {"figType": "Figure", "name": "18", "captionBoundary": {"x2": 432.4498291015625, "y1": 169.29574584960938, "x1": 53.70899963378906, "y2": 194.761962890625}, "imageText": ["Bulgarian", "\u0440\u044f\u0434\u043a\u0430", "\u0434\u0443\u043c\u0430", "English", "a", "rare", "word", "Finnish", "harvinainen", "sana", "Portuguese", "uma", "palavra", "pouco", "frequente", "Slovenian", "redka", "beseda", "Spanish", "una", "palabra", "poco", "frecuente", "Swedish", "ett", "s\u00e4llsynt", "ord", "word_1_N:", "a", "unit", "of", "language", "that", "native", "speakers", "can", "identify"], "regionBoundary": {"x2": 206.0, "y1": 65.0, "x1": 54.0, "y2": 155.0}, "caption": "Figure 18 Snapshots of an example from GF WordNet. The highlighting shows the word alignment and at the bottom is shown the gloss for the selected word.", "page": 46}, {"figType": "Figure", "name": "10", "captionBoundary": {"x2": 429.374267578125, "y1": 256.03875732421875, "x1": 53.70899963378906, "y2": 281.5050048828125}, "imageText": [], "regionBoundary": {"x2": 435.0, "y1": 64.0, "x1": 55.0, "y2": 247.0}, "caption": "Figure 10 Semantic interlingua and word-alignment (top), followed by syntactic interlingua (bottom), for English and Italian.", "page": 28}, {"figType": "Figure", "name": "13", "captionBoundary": {"x2": 426.2001647949219, "y1": 457.2807922363281, "x1": 53.70899963378906, "y2": 512.635009765625}, "imageText": ["3.", "Data", "bootstrapping:", "Synthesize", "a", "treebank", "from", "a", "grammar", "with", "no", "previously", "given", "trees."], "regionBoundary": {"x2": 389.62689208984375, "y1": 543.3871459960938, "x1": 64.76699829101562, "y2": 561.5}, "caption": "Figure 13 A tree from the UD English training treebank interpreted in GF with a backup node, and the resulting linearizations to English, Finnish, and Swedish, where the backup part is enclosed in brackets. The grammar used here does not recognize the prepositionless noun phrase as an adverbial. The Swedish translation happens to be acceptable, whereas the Finnish translation would require a different case to make sense.", "page": 38}, {"figType": "Table", "name": "9", "captionBoundary": {"x2": 434.0636901855469, "y1": 77.72274017333984, "x1": 53.70899963378906, "y2": 113.1519775390625}, "imageText": ["next_A", "week_N", "PositA", "UseN", "AdjCN", "MassNP", "IndefArt", "NumPl", "plan_N", "DetQuant", "UseN", "change_N", "in_Prep", "DetCN", "IndefArt", "NumSg", "UseN", "PrepNP", "have_V2", "DetQuant", "AdvCN", "SlashV2a", "DetCN", "i_Pron", "ComplSlash", "NPBackup", "UsePron", "BackupVP", "PredVP", "?", "nsubj", "?", "det", "#", "?", "obj", "?", "case", "#", "?", "nmod", "?", "amod", "'", "$", "?", "nmod:tmod", "?", "ROOT", "I", "have", "a", "change", "in", "plans", "next", "week", "PRON", "VERB", "DET", "NOUN", "ADP", "NOUN", "ADJ", "NOUN", "English", "1,000", "59", "77", "96", "Finnish", "1,000", "49", "69", "95", "Finnish*", "1,000", "0", "62", "84", "Swedish", "1,000", "62", "71", "94", "Swedish*", "1,000", "0", "64", "83", "Language", "#", "Trees", "#", "Confs", "%", "Interpreted", "trees", "%", "Interpreted", "nodes"], "regionBoundary": {"x2": 439.0, "y1": 121.0, "x1": 53.70899963378906, "y2": 401.3380126953125}, "caption": "Table 9 Coverage of nodes in test sets from the PUD treebanks for English, Swedish and Finnish. L* (Swedish*, Finnish*) is with language-independent configurations only. #conf\u2019s is the number of language-specific configurations.", "page": 38}, {"figType": "Figure", "name": "6", "captionBoundary": {"x2": 417.9135437011719, "y1": 631.5617065429688, "x1": 49.547000885009766, "y2": 657.0279541015625}, "imageText": ["x", "*", "2", "+", "13", "iadd", "imul", "iload", "8", "iconst_2", "bipush", "13", "i_2", "x", "i_13", "Const", "Var", "Const", "Mul", "Add"], "regionBoundary": {"x2": 354.0, "y1": 499.45440673828125, "x1": 55.614200592041016, "y2": 614.8707275390625}, "caption": "Figure 6 An abstract syntax tree and word alignment for infix to postfix (Java to Java Virtual Machine) translation.", "page": 9}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 427.2923583984375, "y1": 77.72274017333984, "x1": 49.547000885009766, "y2": 103.18896484375}, "imageText": ["Abstract", "syntax", "category", "cat", "cat", "NP", "Abstract", "syntax", "function", "fun", "fun", "Pred:", "NP", "->", "VP", "->", "S", "Linearization", "type", "lincat", "lincat", "NP", "=", "{s:", "Case", "=>", "Str;", "a:", "Agr}", "Linearization", "rule", "lin", "lin", "Pred", "np", "vp", "=", "np.s!Nom", "++", "vp!np.a", "Parameter", "type", "param", "param", "Case", "=", "Nom", "|", "Acc", "Auxiliary", "operation", "oper", "oper", "d1", "s", "=", "table", "{Nom=>s;", "Acc=>s+\"m\"}", "String", "concatenation", "++", "\"loves\"", "++", "\"Mary\"", "Token", "concatenation", "+", "\"Maria\"", "+\"m\"", "\u21d3", "\"Mariam\"", "Function", "type", "->", "NP", "->", "VP", "->", "S", "Function", "application", "f", "a", "b", "Pred", "np", "vp", "Table", "type", "=>", "Case", "=>", "Str", "Table", "table", "table", "{Nom", "=>\"Maria\"", ";", "Acc", "=>\"Mariam\"}", "Selection", "from", "table", "!", "Mary", "!", "Acc", "\u21d3", "\"Mariam\"", "Record", "type", "{.", ".", ".:.", ".", ".}", "{s:", "Str;", "p:", "Str}", "Record", "{.", ".", ".", "=", ".", ".", ".}", "{s", "=", "\"heeft\"", ";", "p", "=", "\"lief\"}", "Projection", "from", "record", ".", "Love.p", "\u21d3", "\"lief\"", "Comment", "--", "--", "this", "is", "a", "comment", "Construct", "Notation", "Example"], "regionBoundary": {"x2": 435.0, "y1": 111.0, "x1": 51.0, "y2": 301.4840393066406}, "caption": "Table 3 Reading guide for GF notation. The first six rows list the kinds of rules. The rest are expressions for types and objects. The notation e \u21d3 v means that expression e is computed to value v.", "page": 13}, {"figType": "Figure", "name": "1", "captionBoundary": {"x2": 435.91070556640625, "y1": 230.895751953125, "x1": 53.708984375, "y2": 256.36199951171875}, "imageText": ["-", "??", "character", "transfer", "highly", "uncertain", "lexical", "transfer", "-v", "v", "correct", "by", "construction", "lexical", "form", "generation", "correct", "by", "construction", "syntactic", "structure", "generation", "well-understood", "lexical", "analysis", "uncertain", "syntactic", "parsing", "uncertain", "syntactic", "transfer", "-v", "v", "highly", "uncertain", "semantic", "interpretation", "incomplete", "semantic", "interlingua", "target", "v", "source", "@", "@", "@", "@", "@", "@", "@", "@", "@", "@", "@", "@R"], "regionBoundary": {"x2": 444.50634765625, "y1": 69.62291717529297, "x1": 56.542999267578125, "y2": 222.6080322265625}, "caption": "Figure 1 The Vauquois triangle, annotated with estimates of how difficult or how reliable each translation phase is.", "page": 2}, {"figType": "Table", "name": "7", "captionBoundary": {"x2": 423.0116271972656, "y1": 77.72274017333984, "x1": 53.708984375, "y2": 103.18896484375}, "imageText": ["be", "subject", "to", "X", "X", "unterliegen", "e\u0302tre", "soumise", "a\u0300", "X", "essere", "sottoposta", "a", "X", "ser", "sujeta", "a", "X", "be", "subject", "to", "X", "X", "unterworfen", "sein", "faire", "l\u2019objet", "de", "X", "essere", "soggiogata", "a", "X", "ser", "sujeta", "a", "X", "subject", "betroffen", "concerne\u0301", "soggetto", "sujeto", "subject", "Gegenstand", "sujet", "soggetto", "objeto", "subject-matter", "Gegenstand", "objet", "oggetto", "objeto", "subject", "to", "X", "vorbehaltlich", "X", "sous", "reserve", "de", "X", "soggetto", "a", "X", "siempre", "que", "se", "den", "X", "subject", "to", "X", "fallend", "unter", "X", "soumis", "a\u0300", "X", "soggetto", "a", "X", "interesados", "a", "X", "English", "German", "French", "Italian", "Spanish"], "regionBoundary": {"x2": 439.0, "y1": 110.0, "x1": 53.0, "y2": 216.0}, "caption": "Table 7 Example of the GDPR lexicon, showing a part of the search results for the word subject. Every line corresponds to a different abstract syntax construction involving this word.", "page": 32}, {"figType": "Table", "name": "8", "captionBoundary": {"x2": 433.1131896972656, "y1": 228.91976928710938, "x1": 53.70899963378906, "y2": 294.237060546875}, "imageText": ["functions,", "total", "3,525", "\u2013", "\u2013", "\u2013", "\u2013", "\u2013", "functions,", "atomic", "3,272", "\u2013", "\u2013", "\u2013", "\u2013", "\u2013", "functions,", "pure", "syntax", "139", "\u2013", "\u2013", "\u2013", "\u2013", "\u2013", "functions,", "constructions", "114", "\u2013", "\u2013", "\u2013", "\u2013", "\u2013", "word", "tokens", "\u2013", "55,186", "54,903", "62,198", "55,296", "57,383", "word", "types", "\u2013", "2,625", "4,153", "3,206", "3,520", "3,498", "lemma", "types", "\u2013", "2,555", "3,053", "2,467", "2,689", "2,478", "multiword", "functions", "\u2013", "590", "227", "574", "559", "594", "multiword", "frequency", "%", "\u2013", "8\u201313", "3\u201310", "10\u201321", "10\u201320", "11\u201321", "Abstract", "English", "German", "French", "Italian", "Spanish"], "regionBoundary": {"x2": 439.0, "y1": 302.0, "x1": 55.0, "y2": 412.830078125}, "caption": "Table 8 Statistics of the GDPR grammar and corpus. The numbers of different types of abstract syntax functions are independent of language. \u201cPure syntax\u201d means combination rules with no content words inserted. \u201cConstructions\u201d means mixtures of syntax and content words. \u201cMultiword functions\u201d means linearizations that introduce more than one token; one can see clearly that German is a compounding language. \u201cMultiword frequency\u201d gives two figures: how many lexical items in the corpus are multiwords, and how many tokens are parts of multiwords.", "page": 32}, {"figType": "Figure", "name": "11", "captionBoundary": {"x2": 424.91326904296875, "y1": 265.9327697753906, "x1": 53.70899963378906, "y2": 311.32501220703125}, "imageText": ["root", "amod", "obj", "advmod", "nsubj", "det", "il", "gatto", "nero", "ci", "vede", "oggi", "DET", "NOUN", "ADJ", "PRON", "VERB", "ADV", "root", "advmod", "amod", "nsubj", "obj", "det", "see_V2", "we_Pron", "the", "black", "cat", "sees", "us", "today", "DET", "ADJ", "NOUN", "VERB", "PRON", "ADV", "obj", "SlashV2a", "UsePron", "advmod", "ComplSlash", "today_Adv", "black_A", "cat_N", "UseN", "amod", "DefArt", "NumSg", "PositA", "AdjCN", "det", "DetQuant", "AdvVP", "nsubj", "DetCN", "PredVP"], "regionBoundary": {"x2": 421.87933349609375, "y1": 69.31864929199219, "x1": 56.213924407958984, "y2": 252.8541259765625}, "caption": "Figure 11 An RGL abstract syntax tree annotated with dependency labels, and the corresponding dependency trees for English and Italian. The thick red path from DefArt to cat N shows how the dependency tree algorithm finds the head of a word by walking up the tree until a label is reached, and then walking down along the head spine.", "page": 34}, {"figType": "Figure", "name": "8", "captionBoundary": {"x2": 436.27838134765625, "y1": 327.3937683105469, "x1": 53.70899963378906, "y2": 372.7850341796875}, "imageText": ["RP", "IQuant", "IDet", "VPSlash", "IP", "IAdv", "ClSlash", "AdA", "A,", "A2", "ListAP", "RCl", "CAdv", "Numeral,Digits", "AdN", "Card", "Art", "Quant", "Num", "Ord", "N,N2,N3", "RS", "Predet", "Pron", "PN", "Det", "CN", "ListNP", "AdV", "V,V2,V3,V*,V2*", "AP", "Subj", "ListAdj", "NP", "VP", "Adv", "Tense", "Ant", "Pol", "Cl", "ListS", "Conj", "QCl", "Imp", "S", "QS", "PConj", "Utt", "Voc", "Punct", "Phr", "Text"], "regionBoundary": {"x2": 416.0, "y1": 65.0, "x1": 54.0, "y2": 313.0}, "caption": "Figure 8 The categories of the common abstract syntax of the RGL, showing their main dependencies. The full dependency graph has several cycles. The rectangles show lexical categories subcategorized by their complement lists, such as V2 for two-place verbs taking one NP complement. V* and V2* mean sets of verb categories taking other complements as well, such as VP, AP, or S.", "page": 22}, {"figType": "Figure", "name": "17", "captionBoundary": {"x2": 308.1485595703125, "y1": 164.84878540039062, "x1": 53.70899963378906, "y2": 180.35302734375}, "imageText": ["yksikk\u00f6monikko", "nominatiivi", "talo", "talot", "genetiivi", "talon", "talojen", "partitiivi", "taloa", "taloja", "translatiivi", "taloksi", "taloksi", "...", "...", "...", ".", ".", ".", ".", "gen", "sghus", "huset", "plhus", "husen", "sghus", "husets", "plhus", "husens", "nom", "obest", "best", "nom", "gen", "sghouse", "house's", "plhouses", "houses'"], "regionBoundary": {"x2": 360.0, "y1": 64.0, "x1": 53.0, "y2": 146.0}, "caption": "Figure 17 Snapshots of morphological tables generated from GF WordNet.", "page": 44}, {"figType": "Figure", "name": "14", "captionBoundary": {"x2": 431.2914123535156, "y1": 238.86581420898438, "x1": 49.547000885009766, "y2": 274.2950744628906}, "imageText": ["ROOT", "aux", "obj", "neg", "nsubj", "amod", "det", "aux", "would", "the", "black", "cat", "not", "have", "seen", "us", "AUX", "DET", "ADJ", "NOUN", "PART", "AUX", "VERB", "PRON", "ROOT", "amod", "nsubj", "obj", "aux", "det", "did", "the", "black", "cat", "see", "us", "AUX", "DET", "ADJ", "NOUN", "VERB", "PRON", "ROOT", "neg", "obj", "aux", "nsubj", "amod", "det", "the", "black", "cat", "does", "not", "see", "us", "DET", "ADJ", "NOUN", "AUX", "PART", "VERB", "PRON", "gf2ud", "-", "Ant", "Neg", "UttQS", "Cond", "UttQS", "Past", "Pos", "UttS", "Pres", "Neg", "ud2gf", "-", "UttS", "Pres", "Pos", "augment", "-", "ROOT", "amod", "nsubj", "obj", "det", "the", "black", "cat", "sees", "us", "DET", "ADJ", "NOUN", "VERB", "PRON"], "regionBoundary": {"x2": 430.0, "y1": 63.0, "x1": 49.0, "y2": 234.0}, "caption": "Figure 14 Data augmentation: The tree for a sentence is transformed to trees in other tenses, polarities, and question/declarative forms. In the generated trees, morphological tags (in the UD notation) can be included to improve the training, even though not shown here.", "page": 39}, {"figType": "Figure", "name": "15", "captionBoundary": {"x2": 428.58355712890625, "y1": 415.82879638671875, "x1": 49.5469970703125, "y2": 471.1830749511719}, "imageText": ["Finnish", "#trees", "60", "LAS", "40", "20", "5,000", "10,000", "15,000", "20,000", "English", "#trees", "60", "LAS", "40", "20", "5,000", "10,000", "15,000", "20,000"], "regionBoundary": {"x2": 427.12408447265625, "y1": 306.9967956542969, "x1": 55.215999603271484, "y2": 414.5860290527344}, "caption": "Figure 15 Data bootstrapping: learning curves with synthetic (red) vs. natural (blue) treebanks for English and Finnish, showing the Labeled Attachment Score (LAS) as a function of training corpus size. Models trained on synthetic data need approximately twice the size of the real examples to achieve comparable scores in delexicalized parsing tasks. The results are from Kolachina and Ranta (2019).", "page": 39}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 373.1712951660156, "y1": 267.2117614746094, "x1": 49.547000885009766, "y2": 282.71600341796875}, "imageText": ["Bulgarian", "Thai", "English", "Finnish", "French", "German", "Hindi", "Italian", "Spanish", "Japanese", "Swedish", "Interlingua", "Chinese", "Thai", "Swedish", "Spanish", "Japanese", "Italian", "Hindi", "German", "French", "Finnish", "English", "Chinese", "Bulgarian"], "regionBoundary": {"x2": 439.0, "y1": 89.0, "x1": 50.0, "y2": 262.0}, "caption": "Figure 2 A translation system with 12 languages using transfer (left) vs. interlingua (right).", "page": 3}, {"figType": "Table", "name": "6", "captionBoundary": {"x2": 411.74493408203125, "y1": 77.72274017333984, "x1": 49.547000885009766, "y2": 113.1519775390625}, "imageText": ["Spanish,", "CNL", "34%", "76", "28", "Spanish,", "robust", "0%", "39", "25", "Spanish,", "all", "32%", "74", "28", "German,", "CNL", "44%", "75", "37", "German,", "robust", "0%", "33", "34", "German,", "all", "42%", "73", "37", "Finnish,", "CNL", "48%", "77", "31", "Finnish,", "robust", "0%", "31", "20", "Finnish,", "all", "46%", "73", "28", "Language", "Correct", "BLEU,", "GF", "BLEU,", "Google"], "regionBoundary": {"x2": 435.0, "y1": 121.0, "x1": 51.0, "y2": 251.6710205078125}, "caption": "Table 6 Quality evaluation for layered translation in Ranta, Unger, and Vidal Hussey (2015). \u201cCNL\u201d means translation with the semantic grammar, \u201crobust\u201d means CNL with other layers. The \u201ccorrect\u201d percentage indicates that no changes were made in post-editing.", "page": 31}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 403.5134582519531, "y1": 282.896728515625, "x1": 49.547000885009766, "y2": 318.32598876953125}, "imageText": ["\u2022", "discontinuous", "constituents,", "manifested", "by", "crossing", "branches", "in", "parse", "trees.", "\u2022", "agreement", "using", "morphological", "variation,", "and", "S", "::=", "NP", "VP", "Pred", ":", "NP", "->", "VP", "->", "S", "Pred", "np", "vp", "=", "np", "++", "vp", "VP", "::=", "V2", "NP", "Compl", ":", "V2", "->", "NP", "->", "VP", "Compl", "v", "np", "=", "v", "++", "np", "V2", "::=", "\"loves\"", "Love", ":", "V2", "Love", "=", "\"loves\"", "NP", "::=", "\"John\"", "John", ":", "NP", "John", "=", "\"John\"", "NP", "::=", "\"Mary\"", "Mary", ":", "NP", "Mary", "=", "\"Mary\"", "Phrase", "structure", "Abstract", "(fun)", "Concrete", "(lin)"], "regionBoundary": {"x2": 435.0, "y1": 327.0, "x1": 51.0, "y2": 458.6520080566406}, "caption": "Table 2 Division of a phrase structure grammar for English into abstract and concrete syntax. The division makes it possible to vary the concrete syntax while maintaining the abstract tree structure.", "page": 11}, {"figType": "Figure", "name": "16", "captionBoundary": {"x2": 307.4655456542969, "y1": 127.8377914428711, "x1": 49.547000885009766, "y2": 143.342041015625}, "imageText": ["Abstract", "Bulgarian", "English", "Finnish", "Portuguese", "Slovenian", "Spanish", "Swedish", "household_N", "household", "kotitalous", "casa", "gospodinjstvo", "casa", "Swedish", "house_10_N", "house", "suku", "casa", "hi\u0161a", "casa", "Swedish", "home_8_N", "home", "perhe", "lar", "dom", "hogar", "Swedish", "family_1_N", "family", "talous", "fam\u00edlia", "dru\u017eina", "familia", "Swedish"], "regionBoundary": {"x2": 355.0, "y1": 65.0, "x1": 50.0, "y2": 115.0}, "caption": "Figure 16 A snapshot of a synset from the search interface for GF WordNet.", "page": 43}, {"figType": "Figure", "name": "9", "captionBoundary": {"x2": 426.7723083496094, "y1": 419.9007263183594, "x1": 49.547000885009766, "y2": 455.3299865722656}, "imageText": ["gatto", "nero", "ci", "vede", "N", "A", "Pron", "V2", "oggiil", "Quant", "CN", "AP", "NP", "VPSlash", "Det", "CN", "VP", "Adv", "NP", "VP", "Cl", "S", "Utt", "black", "cat", "sees", "us", "A", "N", "V2", "Pron", "todaythe", "Quant", "AP", "CN", "VPSlash", "NP", "Det", "CN", "VP", "Adv", "NP", "VP", "Cl", "S", "Utt", "see_V2", "we_Pron", "SlashV2a", "UsePron", "ComplSlash", "today_Adv", "black_A", "cat_N", "DefArt", "NumSg", "PositA", "UseN", "DetQuant", "AdjCN", "TPres", "ASimul", "DetCN", "AdvVP", "TTAnt", "PPos", "PredVP", "UseCl", "UttS"], "regionBoundary": {"x2": 424.170654296875, "y1": 64.67833709716797, "x1": 56.1197624206543, "y2": 400.391845703125}, "caption": "Figure 9 An example of RGL abstract syntax trees (above), with corresponding phrase structure trees for English and Italian (below). The phrase structure trees show two differences in word order: AP\u2013CN and V2\u2013Pron.", "page": 23}, {"figType": "Figure", "name": "5", "captionBoundary": {"x2": 427.5091857910156, "y1": 611.63671875, "x1": 53.70900344848633, "y2": 657.0279541015625}, "imageText": ["back-linearization&", "%", "6", "explanation", "(interlingua)", "semantics", "morphology", "generation", "-source", "target", "+", "parsing", "and", "XMT"], "regionBoundary": {"x2": 419.139404296875, "y1": 465.7019348144531, "x1": 53.708984375, "y2": 598.0}, "caption": "Figure 5 XMT with interlingual representation as explanation. The interlingua can be translated back to the source language for comparison. The \u201cblack box\u201d lies between \u201cwhite\u201d parts that are well performed by grammars. The black part takes care of parser error recovery and semantic disambiguation, where grammars might not be sufficient.", "page": 8}, {"figType": "Figure", "name": "12", "captionBoundary": {"x2": 418.97314453125, "y1": 242.85076904296875, "x1": 53.70899963378906, "y2": 268.3180236816406}, "imageText": ["GF", "tree\u2019", "-", "GF", "linear.", "French", "?(gf+tt)2gf", "TT", "formula", "gf2tt", "English", "-", "UD", "parse", "UD", "tree", "-", "ud2gf", "GF", "tree", "-", "formula", "English", "-", "UD", "parse", "UD", "tree", "-", "ud2gf", "GF", "tree", "-", "gf2tt", "TT", "English", "-", "UD", "parse", "UD", "tree", "-", "ud2gf", "GF", "tree", "-", "GF", "linearize", "French"], "regionBoundary": {"x2": 431.0, "y1": 66.0, "x1": 53.0, "y2": 234.0}, "caption": "Figure 12 Three pipelines combining UD with GF: direct translation, type-theoretical semantics (gf2tt), translation instructed by semantics.", "page": 36}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 405.45220947265625, "y1": 230.89590454101562, "x1": 53.709228515625, "y2": 246.400146484375}, "imageText": ["\u2022", "Section", "8", "concludes", "and", "outlines", "suggestions", "for", "future", "work.", "\u2022", "Section", "7", "summarizes", "recent", "work", "where", "GF", "is", "related", "to", "other", "approaches:", "WordNet,", "FrameNet,", "Construction", "Grammar,", "and", "Abstract", "Meaning", "Representation", "(AMR).", "\u2022", "Section", "6", "relates", "GF", "to", "dependency", "parsing,", "in", "particular", "showing", "the", "close", "correspondence", "between", "RGL", "and", "Universal", "Dependencies", "(UD).", "\u2022", "Section", "5", "shows", "how", "NLP", "systems", "can", "be", "built", "by", "combining", "interlinguas", "of", "different", "levels,", "ranging", "from", "chunks", "of", "words", "to", "logical", "formulas.", "\u2022", "Section", "4", "gives", "a", "summary", "of", "GF\u2019s", "Resource", "Grammar", "Library", "(RGL),", "which", "aims", "to", "formalize", "the", "main", "syntactic", "structures", "of", "the", "world\u2019s", "languages", "and", "plays", "a", "major", "role", "in", "both", "CNL", "and", "wide-coverage", "applications.", "\u2022", "Section", "3", "gives", "a", "quick", "tutorial", "on", "GF,", "with", "a", "focus", "on", "how", "linguistic", "variation", "can", "be", "modeled", "in", "terms", "of", "abstract", "and", "concrete", "syntax.", "\u2022", "Section", "2", "outlines", "the", "background", "of", "GF", "and", "its", "development", "from", "CNL", "to", "open", "domain", "tasks."], "regionBoundary": {"x2": 417.204345703125, "y1": 278.3340759277344, "x1": 65.60423278808594, "y2": 499.6341552734375}, "caption": "Figure 3 The Vauquois triangle, with transfer shortcuts replaced by interlinguas on various levels.", "page": 4}, {"figType": "Table", "name": "11", "captionBoundary": {"x2": 432.1163024902344, "y1": 77.72274017333984, "x1": 49.547000885009766, "y2": 133.07696533203125}, "imageText": ["Bulgarian", "92.03", "45.27", "35.74", "37.58", "22.19", "84.61", "Swedish", "72.69", "40.74", "52.88", "65.49", "38.86", "65.04", "RGL", "RGL+parse", "Google", "RGL", "RGL+parse", "Google", "RGL", "reference", "Google", "reference"], "regionBoundary": {"x2": 435.0, "y1": 141.0, "x1": 49.0, "y2": 224.0}, "caption": "Table 11 BLEU scores for translation with RGL+WordNet versus Google Translate. RGL reference: reference obtained by post-editing RGL output. Google reference: reference obtained by post-editing Google output. RGL: output produced from manually disambiguated trees in the treebank by RGL linearization. RGL+parse: output produced by parsing the trees with automatic disambiguation and linearizing with RGL.", "page": 47}, {"figType": "Table", "name": "12", "captionBoundary": {"x2": 400.276611328125, "y1": 236.9727783203125, "x1": 49.547000885009766, "y2": 262.43902587890625}, "imageText": ["BLEU", "score", "for", "syntactic", "parses:", "45.27", "BLEU", "score", "for", "chunks:", "27.20", "Validated", "sentences:", "4,375", "(11%)", "Parsed", "sentences:", "29,376", "(76%)", "All", "sentences:", "38,592"], "regionBoundary": {"x2": 435.0, "y1": 268.0, "x1": 51.0, "y2": 330.2230224609375}, "caption": "Table 12 Coverage for English to Bulgarian and translation quality comparison between complete syntactic parses and chunks.", "page": 47}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 605.4315355088976, "y1": 320.68854437934027, "x1": 74.59581163194444, "y2": 356.0583326551649}, "name": "1", "caption_text": "Figure 1 The Vauquois triangle, annotated with estimates of how difficult or how reliable each translation phase is.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 617.0, "y1": 95.0, "x1": 75.0, "y2": 328.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 518.293465508355, "y1": 371.127446492513, "x1": 68.81527900695801, "y2": 392.6611158582899}, "name": "2", "caption_text": "Figure 2 A translation system with 12 languages using transfer (left) vs. interlingua (right).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 610.0, "y1": 124.0, "x1": 69.0, "y2": 376.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 563.1280687120226, "y1": 320.6887563069661, "x1": 74.59615071614583, "y2": 342.22242567274304}, "name": "3", "caption_text": "Figure 3 The Vauquois triangle, with transfer shortcuts replaced by interlinguas on various levels.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 599.0, "y1": 98.0, "x1": 75.0, "y2": 328.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 585.8652750651041, "y1": 107.94825024074979, "x1": 68.81527900695801, "y2": 143.31800672743054}, "name": "1", "caption_text": "Table 1 Dependencies between Sections 2 to 7. Sections 2, 3, 4 should be read in sequence, while 5, 6, 7 only depend on these and not on each other.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 358.0, "y1": 155.0, "x1": 71.0, "y2": 271.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 597.085698445638, "y1": 258.5537804497613, "x1": 68.81527370876736, "y2": 321.5986039903429}, "name": "4", "caption_text": "Figure 4 The precision\u2013coverage trade-off, as correlated with producer vs. consumer translation tasks. High precision can be achieved in tasks where a producer of content wants to translate a limited kind of content. High coverage is needed when consumers of content want to translate any kind of content.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 401.0, "y1": 88.0, "x1": 69.0, "y2": 264.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 593.7627580430773, "y1": 849.4954427083333, "x1": 74.59583812289767, "y2": 912.538825141059}, "name": "5", "caption_text": "Figure 5 XMT with interlingual representation as explanation. The interlingua can be translated back to the source language for comparison. The \u201cblack box\u201d lies between \u201cwhite\u201d parts that are well performed by grammars. The black part takes care of parser error recovery and semantic disambiguation, where grammars might not be sufficient.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 565.0, "y1": 645.0, "x1": 75.0, "y2": 830.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 580.4354773627387, "y1": 877.1690368652344, "x1": 68.81527900695801, "y2": 912.538825141059}, "name": "6", "caption_text": "Figure 6 An abstract syntax tree and word alignment for infix to postfix (Java to Java Virtual Machine) translation.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 493.0, "y1": 691.0, "x1": 77.0, "y2": 850.0}, "page": 9, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 593.337164984809, "y1": 307.21490648057727, "x1": 68.81527900695801, "y2": 342.5846947564019}, "name": "7", "caption_text": "Figure 7 Parse trees for English, Latin, Dutch, and Arabic (above); abstract syntax tree and word alignment (below). Words in the Arabic tree are shown left to right to illustrate their VSO order.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 595.0, "y1": 94.0, "x1": 69.0, "y2": 292.0}, "page": 11, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 560.4353586832682, "y1": 392.91212293836804, "x1": 68.81527900695801, "y2": 442.11942884657117}, "name": "2", "caption_text": "Table 2 Division of a phrase structure grammar for English into abstract and concrete syntax. The division makes it possible to vary the concrete syntax while maintaining the abstract tree structure.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 494.0, "y1": 440.0, "x1": 69.0, "y2": 554.0}, "page": 11, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 593.4616088867188, "y1": 107.94825024074979, "x1": 68.81527900695801, "y2": 143.31800672743054}, "name": "3", "caption_text": "Table 3 Reading guide for GF notation. The first six rows list the kinds of rules. The rest are expressions for types and objects. The notation e \u21d3 v means that expression e is computed to value v.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 556.0, "y1": 155.0, "x1": 71.0, "y2": 420.0}, "page": 13, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 605.941899617513, "y1": 251.5983157687717, "x1": 74.59583282470703, "y2": 383.827887641059}, "name": "4", "caption_text": "Table 4 The complete languages of the GF Resource Grammar as of February 2019. ISO = ISO-639-3 code (so-called B code when this makes a difference). Dict = number of lemmas in the dictionary (500 means that there is just a basic lexicon of around 500 lemmas). Transl = used in wide-coverage translation. Appl = used in applications, ++ means also in commercial applications. Publ = publications available, ++ means international publications in addition to academic theses. LoC = lines of GF code (non-empty, non-comment), excluding the dictionary. The sum of two figures means shared functor+language-specific code. The asterisk * means that there is an additional set of low-level morphological paradigms extracted from external sources. Year = when the main part of the grammar was released.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 524.0, "y1": 395.0, "x1": 77.0, "y2": 909.0}, "page": 20, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 605.9421963161892, "y1": 454.71356709798175, "x1": 74.59583282470703, "y2": 517.7569919162327}, "name": "8", "caption_text": "Figure 8 The categories of the common abstract syntax of the RGL, showing their main dependencies. The full dependency graph has several cycles. The rectangles show lexical categories subcategorized by their complement lists, such as V2 for two-place verbs taking one NP complement. V* and V2* mean sets of verb categories taking other complements as well, such as VP, AP, or S.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 578.0, "y1": 90.0, "x1": 76.0, "y2": 434.0}, "page": 22, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 592.7393171522352, "y1": 583.1954532199436, "x1": 68.81527900695801, "y2": 632.4027591281467}, "name": "9", "caption_text": "Figure 9 An example of RGL abstract syntax trees (above), with corresponding phrase structure trees for English and Italian (below). The phrase structure trees show two differences in word order: AP\u2013CN and V2\u2013Pron.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 589.0, "y1": 89.0, "x1": 78.0, "y2": 559.0}, "page": 23, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 597.1499972873264, "y1": 107.94825024074979, "x1": 74.59583282470703, "y2": 157.15552435980902}, "name": "5", "caption_text": "Table 5 A sample of the syntactic combination functions in the common abstract syntax, covering the functions used in the examples in this paper. The \u201cUD labels\u201d column refers to the dependency label associated with each argument place of the function, to be discussed in Section 6.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 528.0, "y1": 168.0, "x1": 77.0, "y2": 395.0}, "page": 24, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 596.3531494140625, "y1": 355.609385172526, "x1": 74.59583282470703, "y2": 390.9791734483507}, "name": "10", "caption_text": "Figure 10 Semantic interlingua and word-alignment (top), followed by syntactic interlingua (bottom), for English and Italian.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 604.0, "y1": 89.0, "x1": 75.0, "y2": 356.0}, "page": 28, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 571.8679640028212, "y1": 107.94825024074979, "x1": 68.81527900695801, "y2": 157.15552435980902}, "name": "6", "caption_text": "Table 6 Quality evaluation for layered translation in Ranta, Unger, and Vidal Hussey (2015). \u201cCNL\u201d means translation with the semantic grammar, \u201crobust\u201d means CNL with other layers. The \u201ccorrect\u201d percentage indicates that no changes were made in post-editing.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 406.0, "y1": 168.0, "x1": 71.0, "y2": 354.0}, "page": 31, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 587.5161488850911, "y1": 107.94825024074979, "x1": 74.59581163194444, "y2": 143.31800672743054}, "name": "7", "caption_text": "Table 7 Example of the GDPR lexicon, showing a part of the search results for the word subject. Every line corresponds to a different abstract syntax construction involving this word.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 606.0, "y1": 153.0, "x1": 74.0, "y2": 267.0}, "page": 32, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 601.5460968017578, "y1": 317.94412400987414, "x1": 74.59583282470703, "y2": 408.66258409288196}, "name": "8", "caption_text": "Table 8 Statistics of the GDPR grammar and corpus. The numbers of different types of abstract syntax functions are independent of language. \u201cPure syntax\u201d means combination rules with no content words inserted. \u201cConstructions\u201d means mixtures of syntax and content words. \u201cMultiword functions\u201d means linearizations that introduce more than one token; one can see clearly that German is a compounding language. \u201cMultiword frequency\u201d gives two figures: how many lexical items in the corpus are multiwords, and how many tokens are parts of multiwords.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 580.0, "y1": 420.0, "x1": 77.0, "y2": 577.0}, "page": 32, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 590.1573181152344, "y1": 369.35106913248694, "x1": 74.59583282470703, "y2": 432.3958502875434}, "name": "11", "caption_text": "Figure 11 An RGL abstract syntax tree annotated with dependency labels, and the corresponding dependency trees for English and Italian. The thick red path from DefArt to cat N shows how the dependency tree algorithm finds the head of a word by walking up the tree until a label is reached, and then walking down along the head spine.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 585.0, "y1": 95.0, "x1": 78.0, "y2": 354.0}, "page": 34, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 581.9071451822916, "y1": 337.292734781901, "x1": 74.59583282470703, "y2": 372.6639217800564}, "name": "12", "caption_text": "Figure 12 Three pipelines combining UD with GF: direct translation, type-theoretical semantics (gf2tt), translation instructed by semantics.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 599.0, "y1": 92.0, "x1": 74.0, "y2": 340.0}, "page": 36, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 602.8662363688151, "y1": 107.94825024074979, "x1": 74.59583282470703, "y2": 157.15552435980902}, "name": "9", "caption_text": "Table 9 Coverage of nodes in test sets from the PUD treebanks for English, Swedish and Finnish. L* (Swedish*, Finnish*) is with language-independent configurations only. #conf\u2019s is the number of language-specific configurations.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 610.0, "y1": 168.0, "x1": 75.0, "y2": 560.0}, "page": 38, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 599.0158504909939, "y1": 331.75807529025604, "x1": 68.81527900695801, "y2": 380.96538119845917}, "name": "14", "caption_text": "Figure 14 Data augmentation: The tree for a sentence is transformed to trees in other tenses, polarities, and question/declarative forms. In the generated trees, morphological tags (in the UD notation) can be included to improve the training, even though not shown here.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 597.0, "y1": 88.0, "x1": 68.0, "y2": 338.0}, "page": 39, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 595.2549404568142, "y1": 577.5399949815538, "x1": 68.81527370876736, "y2": 654.4209374321831}, "name": "15", "caption_text": "Figure 15 Data bootstrapping: learning curves with synthetic (red) vs. natural (blue) treebanks for English and Finnish, showing the Labeled Attachment Score (LAS) as a function of training corpus size. Models trained on synthetic data need approximately twice the size of the real examples to achieve comparable scores in delexicalized parsing tasks. The results are from Kolachina and Ranta (2019).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 593.0, "y1": 425.0, "x1": 69.0, "y2": 588.0}, "page": 39, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 427.0354800754123, "y1": 177.55248811509873, "x1": 68.81527900695801, "y2": 199.08616807725693}, "name": "16", "caption_text": "Figure 16 A snapshot of a synset from the search interface for GF WordNet.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 493.0, "y1": 89.0, "x1": 68.0, "y2": 160.0}, "page": 43, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 427.9841105143229, "y1": 228.95664638943143, "x1": 74.59583282470703, "y2": 250.49031575520831}, "name": "17", "caption_text": "Figure 17 Snapshots of morphological tables generated from GF WordNet.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 500.0, "y1": 89.0, "x1": 74.0, "y2": 202.0}, "page": 44, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 600.624762641059, "y1": 235.1329803466797, "x1": 74.59583282470703, "y2": 270.50272623697913}, "name": "18", "caption_text": "Figure 18 Snapshots of an example from GF WordNet. The highlighting shows the word alignment and at the bottom is shown the gloss for the selected word.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 288.0, "y1": 89.0, "x1": 74.0, "y2": 216.0}, "page": 46, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 548.2832590738932, "y1": 317.8718990749783, "x1": 74.59583282470703, "y2": 339.4055684407552}, "name": "10", "caption_text": "Table 10 State of GF WordNet in terms of number of senses and number of trusted translations.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 282.0, "y1": 360.0, "x1": 77.0, "y2": 548.0}, "page": 46, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 600.1615312364366, "y1": 107.94825024074979, "x1": 68.81527900695801, "y2": 184.82911851671005}, "name": "11", "caption_text": "Table 11 BLEU scores for translation with RGL+WordNet versus Google Translate. RGL reference: reference obtained by post-editing RGL output. Google reference: reference obtained by post-editing Google output. RGL: output produced from manually disambiguated trees in the treebank by RGL linearization. RGL+parse: output produced by parsing the trees with automatic disambiguation and linearizing with RGL.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 498.0, "y1": 196.0, "x1": 71.0, "y2": 275.0}, "page": 47, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 506.93605211046, "y1": 261.6218990749783, "x1": 68.81527900695801, "y2": 283.1555684407552}, "name": "19", "caption_text": "Figure 19 FrameNet functions as an abstraction layer above the syntax-oriented RGL API.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 574.0, "y1": 88.0, "x1": 69.0, "y2": 235.0}, "page": 49, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 600.1887851291233, "y1": 267.4065907796224, "x1": 74.59583282470703, "y2": 302.776379055447}, "name": "20", "caption_text": "Figure 20 The difference between GF-based and JAMR linearization of a sample AMR. The reference is an informed human translation from the given AMR.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 605.0, "y1": 331.0, "x1": 75.0, "y2": 567.0}, "page": 52, "dpi": 0}], "error": null, "pdf": "/work/host-output/49b4ebaa5a590152e51b616cd4e2f6cd62eefa95/2020.cl-2.6.pdf", "dpi": 100}