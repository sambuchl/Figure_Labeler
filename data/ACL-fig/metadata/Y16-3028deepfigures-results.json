{"raw_detected_boxes": [[], [{"x2": 396.0, "y1": 152.0, "x1": 106.0, "y2": 260.0}], [{"x2": 383.0, "y1": 147.0, "x1": 115.0, "y2": 408.0}], [{"x2": 399.0, "y1": 150.0, "x1": 99.0, "y2": 310.0}, {"x2": 715.0, "y1": 155.0, "x1": 439.0, "y2": 567.0}, {"x2": 394.0, "y1": 368.0, "x1": 111.0, "y2": 591.0}], [{"x2": 732.0, "y1": 153.0, "x1": 427.0, "y2": 368.0}], [{"x2": 728.0, "y1": 257.0, "x1": 428.0, "y2": 396.0}, {"x2": 733.0, "y1": 151.0, "x1": 427.0, "y2": 196.0}], [{"x2": 718.0, "y1": 179.0, "x1": 99.0, "y2": 259.0}], [{"x2": 728.0, "y1": 196.0, "x1": 426.0, "y2": 489.0}], []], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Figure", "name": "8", "captionBoundary": {"x2": 515.3131103515625, "y1": 298.1766662597656, "x1": 314.2168884277344, "y2": 317.74017333984375}, "imageText": [], "regionBoundary": {"x2": 524.0, "y1": 182.0, "x1": 305.0, "y2": 286.0}, "caption": "Figure 8: Kappa statistics in arousal dimension with and without knowledge", "page": 5}, {"figType": "Figure", "name": "7", "captionBoundary": {"x2": 485.85504150390625, "y1": 160.92605590820312, "x1": 343.6763000488281, "y2": 167.31640625}, "imageText": [], "regionBoundary": {"x2": 533.0, "y1": 105.0, "x1": 304.0, "y2": 149.0}, "caption": "Figure 7: Speech to text analytics", "page": 5}, {"figType": "Figure", "name": "1", "captionBoundary": {"x2": 284.70947265625, "y1": 202.94692993164062, "x1": 75.80590057373047, "y2": 222.51043701171875}, "imageText": [], "regionBoundary": {"x2": 288.0, "y1": 105.0, "x1": 73.0, "y2": 191.0}, "caption": "Figure 1: Call center calls (dark portions indicate problems)", "page": 1}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 448.14971923828125, "y1": 108.74996185302734, "x1": 146.87489318847656, "y2": 115.14031982421875}, "imageText": ["+Time", "Lapse", "49.3", "78.3", "53.9", "78.9", "44.2", "72.5", "59.7", "81.9", "+ASR", "and", "text", "analytics", "56.8", "80.9", "56.2", "82.1", "45.1", "76.1", "61.2", "85.2", "+Time", "Lapse", "+", "ASR", "and", "text", "analytics", "65.3", "87.6", "61.2", "88.9", "47", "85.6", "72.1", "89.6", "Full", "Utterance", "400ms", "Split", "Full", "Utterance", "400ms", "Split", "Full", "Utterance", "400ms", "Split", "Full", "Utterance", "400ms", "Split", "Affective", "Content", "Extractor", "32.3", "62.7", "36.1", "63.8", "31.3", "63.2", "39.8", "65.8", "Description", "Classi\ufb01ers", "SVM", "ANN", "k-NN", "SVM", "+", "ANN", "+", "k-NN"], "regionBoundary": {"x2": 526.0, "y1": 128.0, "x1": 70.0, "y2": 190.0}, "caption": "Table 1: Affective content detection accuracies for call center calls (%)", "page": 6}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 267.48443603515625, "y1": 308.0334777832031, "x1": 93.03040313720703, "y2": 327.59698486328125}, "imageText": [], "regionBoundary": {"x2": 283.0, "y1": 105.0, "x1": 77.0, "y2": 296.0}, "caption": "Figure 2: Error in emotion estimation for spontaneous call center calls", "page": 2}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 513.3212280273438, "y1": 108.7523422241211, "x1": 316.21197509765625, "y2": 128.31585693359375}, "imageText": ["-5", "20.3", "22.8", "32.9", "37.3", "0", "40.3", "42.7", "40.3", "52.9", "5", "49.1", "49.8", "42.1", "57.6", "10", "54.3", "54.8", "57.3", "71.3", "20", "66.7", "66.9", "72.7", "77.3", "Volvo", "-5", "22.8", "34.6", "41.1", "47.1", "0", "45.6", "61.4", "61.4", "73.3", "5", "70.8", "71.2", "63.2", "75.4", "10", "71.9", "73.2", "64.9", "77.2", "20", "72.3", "76", "68", "80.2", "Machine", "Gun", "-5", "20.8", "20.8", "28.3", "30.6", "0", "21.6", "22.3", "29.1", "32.4", "5", "22.7", "23.8", "30.5", "39.6", "10", "28.3", "30.1", "34.6", "43.7", "20", "30.7", "33.4", "38.6", "45.2", "F-16", "-5", "21.05", "22.3", "32.6", "33.2", "0", "22.1", "22.8", "32.9", "33.9", "5", "24.8", "25.6", "34.6", "35.5", "10", "28.9", "30.1", "35.2", "37.2", "20", "42.3", "45.6", "47.3", "53.3", "Babble", "Classi\ufb01ers", "Noise", "type", "SNR", "(dB)", "SVM", "ANN", "k-NN", "SVM+ANN+k-NN"], "regionBoundary": {"x2": 526.0, "y1": 141.0, "x1": 304.0, "y2": 354.0}, "caption": "Table 2: Detection accuracies for acted speech (EmoDB) contaminated by noise (Noisex-92)", "page": 7}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 281.8500671386719, "y1": 438.2642517089844, "x1": 78.66329956054688, "y2": 444.65460205078125}, "imageText": [], "regionBoundary": {"x2": 298.0, "y1": 260.0, "x1": 70.0, "y2": 426.0}, "caption": "Figure 4: Audio segmentation based knowledge", "page": 3}, {"figType": "Figure", "name": "5", "captionBoundary": {"x2": 494.7802734375, "y1": 428.1790466308594, "x1": 334.7510070800781, "y2": 434.56939697265625}, "imageText": [], "regionBoundary": {"x2": 518.0, "y1": 105.0, "x1": 312.0, "y2": 416.0}, "caption": "Figure 5: Affective content extraction", "page": 3}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 276.1237487792969, "y1": 238.05612182617188, "x1": 84.39189910888672, "y2": 244.44647216796875}, "imageText": [], "regionBoundary": {"x2": 290.0, "y1": 105.0, "x1": 71.0, "y2": 226.0}, "caption": "Figure 3: Call center call analysis framework", "page": 3}, {"figType": "Figure", "name": "6", "captionBoundary": {"x2": 518.114501953125, "y1": 277.4383239746094, "x1": 311.4382019042969, "y2": 297.00286865234375}, "imageText": [], "regionBoundary": {"x2": 533.0, "y1": 105.0, "x1": 304.0, "y2": 265.0}, "caption": "Figure 6: Knowledge regarding the time lapse of the segment in a call", "page": 4}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 395.42982313368054, "y1": 281.87073601616754, "x1": 105.2859730190701, "y2": 309.04227362738715}, "name": "1", "caption_text": "Figure 1: Call center calls (dark portions indicate problems)", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 399.0, "y1": 148.0, "x1": 106.0, "y2": 260.0}, "page": 1, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 371.5061611599392, "y1": 427.8242746988932, "x1": 129.20889324612088, "y2": 454.99581231011285}, "name": "2", "caption_text": "Figure 2: Error in emotion estimation for spontaneous call center calls", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 383.0, "y1": 147.0, "x1": 114.0, "y2": 409.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 383.5052066379123, "y1": 330.6335025363498, "x1": 117.21097098456488, "y2": 339.50898912217883}, "name": "3", "caption_text": "Figure 3: Call center call analysis framework", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 402.0, "y1": 147.0, "x1": 99.0, "y2": 310.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 687.19482421875, "y1": 594.693120320638, "x1": 464.9319542778863, "y2": 603.568606906467}, "name": "5", "caption_text": "Figure 5: Affective content extraction", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 718.0, "y1": 153.0, "x1": 439.0, "y2": 567.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 391.4584265814887, "y1": 608.7003495958116, "x1": 109.25458272298177, "y2": 617.5758361816406}, "name": "4", "caption_text": "Figure 4: Audio segmentation based knowledge", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 395.0, "y1": 362.0, "x1": 97.0, "y2": 608.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 719.6034749348958, "y1": 385.3310055202908, "x1": 432.5530582004123, "y2": 412.50398423936633}, "name": "6", "caption_text": "Figure 6: Knowledge regarding the time lapse of the segment in a call", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 734.0, "y1": 147.0, "x1": 427.0, "y2": 385.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 715.712653266059, "y1": 414.1342586941189, "x1": 436.41234503851996, "y2": 441.3057963053385}, "name": "8", "caption_text": "Figure 8: Kappa statistics in arousal dimension with and without knowledge", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 728.0, "y1": 252.0, "x1": 424.0, "y2": 396.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 674.7986687554253, "y1": 223.50841098361545, "x1": 477.3281945122613, "y2": 232.38389756944443}, "name": "7", "caption_text": "Figure 7: Speech to text analytics", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 734.0, "y1": 149.0, "x1": 427.0, "y2": 206.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 622.4301656087239, "y1": 151.0416136847602, "x1": 203.99290720621744, "y2": 159.9171108669705}, "name": "1", "caption_text": "Table 1: Affective content detection accuracies for call center calls (%)", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 730.0, "y1": 178.0, "x1": 97.0, "y2": 263.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 712.9461500379774, "y1": 151.04491975572373, "x1": 439.18329874674475, "y2": 178.21646796332465}, "name": "2", "caption_text": "Table 2: Detection accuracies for acted speech (EmoDB) contaminated by noise (Noisex-92)", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 730.0, "y1": 179.0, "x1": 422.0, "y2": 491.0}, "page": 7, "dpi": 0}], "error": null, "pdf": "/work/host-output/dbbfbdf2485c1e8597e1f3d2ef29d93af2cda2c4/Y16-3028.pdf", "dpi": 100}