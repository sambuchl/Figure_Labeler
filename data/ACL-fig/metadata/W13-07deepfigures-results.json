{"raw_detected_boxes": [[], [{"x2": 604.0, "y1": 624.0, "x1": 208.0, "y2": 851.0}], [], [], [], [], [], [], [{"x2": 603.0, "y1": 168.0, "x1": 252.0, "y2": 398.0}], [], [], [{"x2": 540.0, "y1": 256.0, "x1": 317.0, "y2": 376.0}], [], [], [{"x2": 646.0, "y1": 340.0, "x1": 177.0, "y2": 599.0}], [], [{"x2": 636.0, "y1": 432.0, "x1": 186.0, "y2": 571.0}], [], [], [], [{"x2": 643.0, "y1": 165.0, "x1": 197.0, "y2": 328.0}], [], [], [], [{"x2": 660.0, "y1": 163.0, "x1": 199.0, "y2": 362.0}], [{"x2": 660.0, "y1": 162.0, "x1": 192.0, "y2": 333.0}, {"x2": 624.0, "y1": 736.0, "x1": 226.0, "y2": 864.0}], [{"x2": 605.0, "y1": 212.0, "x1": 250.0, "y2": 362.0}], [], [], [], [], [], [], [], [], [], [], [], []], "raw_pdffigures_output": {"regionless-captions": [{"figType": "Figure", "boundary": {"x2": 432.50201416015625, "y1": 426.6114807128906, "x1": 355.8699951171875, "y2": 430.6299743652344}, "text": "Fig. 2. The IBL map.", "name": "2", "page": 14}, {"figType": "Figure", "boundary": {"x2": 299.5309753417969, "y1": 426.6114807128906, "x1": 135.86000061035156, "y2": 430.6299743652344}, "text": "Fig. 1. An example Conceptual Route Graph.", "name": "1", "page": 14}], "figures": [{"figType": "Figure", "name": "2", "captionBoundary": {"x2": 480.5835266113281, "y1": 280.3859558105469, "x1": 134.76499938964844, "y2": 295.82794189453125}, "imageText": [], "regionBoundary": {"x2": 480.0, "y1": 115.88995361328125, "x1": 132.0, "y2": 264.88995361328125}, "caption": "Fig. 2. Overview of our proposed system for reasoning on ternary spatial relations between detected visual objects to automatically understand scenes.", "page": 24}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 480.593017578125, "y1": 495.727783203125, "x1": 134.76504516601562, "y2": 511.1687927246094}, "imageText": ["B", "A", "D", "0.48", "0.00", "0.00", "0.00", "0.52", "0.18", "0.00", "0.00", "0.00", "0.82", "C", "A", "B", "0.00", "0.43", "0.00", "0.57", "0.00", "0.18", "0.00", "0.00", "1.00", "0.00", "C", "A", "D", "0.00", "0.20", "0.00", "0.00", "0.80", "0.00", "0.00", "0.00", "0.00", "1.00", "C", "D", "E", "0.00", "0.00", "0.50", "0.00", "0.50", "0.00", "0.68", "1.00", "0.00", "0.00", "D", "A", "C", "0.07", "0.00", "0.00", "0.93", "0.00", "1.00", "0.00", "0.00", "1.00", "0.00", "OREL", "OREF1", "OREF2", "ab", "bl", "bf", "af", "bt", "ab", "bl", "bf", "af", "bt", "Approaches", "from", "[9]", "ours", "(Sec.", "2)"], "regionBoundary": {"x2": 452.0, "y1": 523.8899536132812, "x1": 163.0, "y2": 626.8899536132812}, "caption": "Table 1. Quantitative evaluation of the ternary relations for the objects in the choir image in Fig. 3(a).", "page": 25}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 480.5965576171875, "y1": 254.08493041992188, "x1": 134.76499938964844, "y2": 291.44390869140625}, "imageText": ["(a)", "(b)"], "regionBoundary": {"x2": 478.0, "y1": 115.88995361328125, "x1": 138.0, "y2": 237.64593505859375}, "caption": "Fig. 3. Results of our system tested for an image of a choir. First column: Face recognition results. Second column: Schematic spatial representation of the above, below, before, after, and between relations of the visual objects detected in the image under study.", "page": 25}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 448.7361755371094, "y1": 235.551513671875, "x1": 146.4199981689453, "y2": 239.57000732421875}, "imageText": ["[POSTOFFICE", "the", "post-office", "]", "]", "[SEGMENT", "[DM", "and", "then]", "[TAKE", "take]", "]", "A", "Attacher", "[SEGMENT", "[TURN", "(direction:", "\u2192)", "turn]", "[LEFT", "left]", "[AT", "(landmark:", "\u2192)", "at]", "[DM", "eh]", "office]", "]", "[", "SEGMENT", "[SCONT", "and", "then]", "[ACTION", "take]", "]", "Segmenter", "[", "SEGMENT", "[ACTION", "turn]", "[DIRECTION", "left]", "[ROUTER", "at]", "[FP", "eh]", "[LANDMARK", "the", "post-", "[SCONT", "and", "then]", "[ACTION", "take]", "Chunker", "[ACTION", "turn]", "[DIRECTION", "left]", "[ROUTER", "at]", "[FP", "eh]", "[LANDMARK", "the", "post-office]"], "regionBoundary": {"x2": 472.0, "y1": 242.91998291015625, "x1": 124.0, "y2": 421.91998291015625}, "caption": "Table 1. The three stages of the Chunking parser for interpreting route descriptions.", "page": 14}, {"figType": "Figure", "name": "1", "captionBoundary": {"x2": 480.5915222167969, "y1": 252.09896850585938, "x1": 134.76498413085938, "y2": 278.49993896484375}, "imageText": ["(a)", "Clock", "model", "(b)", "Clock-modeled", "ternary", "spatial", "relations"], "regionBoundary": {"x2": 464.0, "y1": 115.88995361328125, "x1": 137.0, "y2": 235.65997314453125}, "caption": "Fig. 1. Illustration of the ternary spatial relations between visual objects using the clock model (a) and representing the semantic concepts which are above (ab), below (bl), before (bf), after (af), and between (bt) (b).", "page": 20}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 463.83404541015625, "y1": 419.1714782714844, "x1": 304.2699890136719, "y2": 434.22998046875}, "imageText": ["Annotation", "ASR", "ChunkingP", "Human", "Modeling", "'Error", "Source'", "0", "0", "0", "1", "0", "8", "0", "6", "0", "4", "0", "2", "%", "Action", "Controller", "Landmark", "Other", "SpatialR", "'Problem", "category'"], "regionBoundary": {"x2": 458.0, "y1": 312.3879699707031, "x1": 309.3125, "y2": 409.7303161621094}, "caption": "Fig. 4. Distribution of error sources across the problem categories.", "page": 16}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 292.7569885253906, "y1": 419.1714782714844, "x1": 129.74000549316406, "y2": 434.22998046875}, "imageText": ["ALM_THERE", "FOUND", "NOT_FOUND", "'Performance", "category'", "0", "0", "0", "1", "0", "8", "0", "6", "0", "4", "0", "2", "%", "crgASR", "crgCMT", "crgMAN", "ManTsc", "'Instruction", "type'"], "regionBoundary": {"x2": 290.665283203125, "y1": 312.3498229980469, "x1": 134.85647583007812, "y2": 409.7394714355469}, "caption": "Fig. 3. Human performances across the instructions types.", "page": 16}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 480.5937805175781, "y1": 118.57575988769531, "x1": 134.76499938964844, "y2": 166.8939208984375}, "imageText": ["total", "75", "29", "20", "(0.69)", "0.95", "1", "13", "4", "2", "(0.50)", "0.93", "2", "16", "5", "3", "(0.60)", "0.94", "3", "9", "3", "2", "(0.67)", "0.94", "4", "9", "3", "2", "(0.67)", "0.94", "5", "16", "10", "7", "(0.70)", "0.95", "6", "12", "4", "4", "(1.00)", "1.00", "subj", "segments", "tests", "succ", "rank"], "regionBoundary": {"x2": 389.0, "y1": 178.88995361328125, "x1": 226.0, "y2": 272.88995361328125}, "caption": "Table 1. For evaluation, we used the induced weights to compute costs on test sets and counted in how many cases the best option was a landmark used by the subject, including also reference to streets and squares. segments: total number of route segments, tests: number of test instances, succ: number (and percentage) of successful test instances, rank: percentage of landmarks with equal or higher cost", "page": 11}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 480.5877685546875, "y1": 118.57595825195312, "x1": 134.76499938964844, "y2": 134.0179443359375}, "imageText": ["A", "C", "E", "no", "yes", "yes", "no", "no", "no", "yes", "yes", "no", "no", "B", "A", "C", "yes", "no", "no", "no", "yes", "yes", "no", "no", "no", "yes", "B", "C", "D", "no", "no", "yes", "no", "no", "no", "no", "yes", "no", "no", "C", "B", "D", "no", "yes", "no", "no", "yes", "no", "yes", "no", "no", "yes", "D", "A", "B", "yes", "no", "no", "yes", "no", "yes", "no", "no", "yes", "no", "E", "A", "B", "no", "no", "no", "yes", "no", "no", "no", "no", "yes", "no", "OREL", "OREF1", "OREF2", "ab", "bl", "bf", "af", "bt", "ab", "bl", "bf", "af", "bt", "Approaches", "ground", "truth", "ours", "(Sec.", "2)"], "regionBoundary": {"x2": 437.0, "y1": 146.88995361328125, "x1": 178.0, "y2": 260.88995361328125}, "caption": "Table 2. Qualitative evaluation of the ternary relations for the objects in the choir image in Fig. 3(a).", "page": 26}, {"figType": "Figure", "name": "1", "captionBoundary": {"x2": 480.59564208984375, "y1": 303.51971435546875, "x1": 134.76499938964844, "y2": 329.9208984375}, "imageText": ["L", "A", "B"], "regionBoundary": {"x2": 437.0, "y1": 116.88995361328125, "x1": 179.0, "y2": 287.88995361328125}, "caption": "Fig. 1. An example route segment from A to B. The squares represent the landmarks in the contexts of A and B. L represents a landmark referred to by the user (a supermarket).", "page": 8}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 667.4939473470051, "y1": 421.5551588270399, "x1": 187.1736102634006, "y2": 458.2234700520833}, "name": "1", "caption_text": "Fig. 1. An example route segment from A to B. The squares represent the landmarks in the contexts of A and B. L represents a landmark referred to by the user (a supermarket).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 606.0, "y1": 162.0, "x1": 249.0, "y2": 399.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 667.4913618299696, "y1": 164.6885553995768, "x1": 187.1736102634006, "y2": 231.79711235894095}, "name": "1", "caption_text": "Table 1. For evaluation, we used the induced weights to compute costs on test sets and counted in how many cases the best option was a landmark used by the subject, including also reference to streets and squares. segments: total number of route segments, tests: number of test instances, succ: number (and percentage) of successful test instances, rank: percentage of landmarks with equal or higher cost", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 540.0, "y1": 249.0, "x1": 314.0, "y2": 379.0}, "page": 11, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 600.6972418891058, "y1": 592.5159454345703, "x1": 494.26388210720484, "y2": 598.0971866183811}, "name": "2", "caption_text": "Fig. 2. The IBL map.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 655.0, "y1": 323.0, "x1": 172.0, "y2": 616.0}, "page": 14, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 644.2139519585503, "y1": 582.1826087103949, "x1": 422.59720696343317, "y2": 603.0971950954861}, "name": "4", "caption_text": "Fig. 4. Distribution of error sources across the problem categories.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 644.0, "y1": 415.0, "x1": 173.0, "y2": 588.0}, "page": 16, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 667.4882253011067, "y1": 350.137456258138, "x1": 187.173589070638, "y2": 386.8054707845052}, "name": "1", "caption_text": "Fig. 1. Illustration of the ternary spatial relations between visual objects using the clock model (a) and representing the semantic concepts which are above (ab), below (bl), before (bf), after (af), and between (bt) (b).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 643.0, "y1": 162.0, "x1": 194.0, "y2": 330.0}, "page": 20, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 667.4771202935112, "y1": 389.4249386257595, "x1": 187.1736102634006, "y2": 410.87214152018225}, "name": "2", "caption_text": "Fig. 2. Overview of our proposed system for reasoning on ternary spatial relations between detected visual objects to automatically understand scenes.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 663.0, "y1": 163.0, "x1": 199.0, "y2": 362.0}, "page": 24, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 667.4952189127604, "y1": 352.89573669433594, "x1": 187.1736102634006, "y2": 404.783206515842}, "name": "3", "caption_text": "Fig. 3. Results of our system tested for an image of a choir. First column: Face recognition results. Second column: Schematic spatial representation of the above, below, before, after, and between relations of the visual objects detected in the image under study.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 663.0, "y1": 162.0, "x1": 192.0, "y2": 333.0}, "page": 25, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 667.4903021918402, "y1": 688.5108100043402, "x1": 187.17367384168836, "y2": 709.9566565619574}, "name": "1", "caption_text": "Table 1. Quantitative evaluation of the ternary relations for the objects in the choir image in Fig. 3(a).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 628.0, "y1": 728.0, "x1": 226.0, "y2": 871.0}, "page": 25, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 667.4830118815104, "y1": 164.68883090549045, "x1": 187.1736102634006, "y2": 186.13603379991318}, "name": "2", "caption_text": "Table 2. Qualitative evaluation of the ternary relations for the objects in the choir image in Fig. 3(a).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 607.0, "y1": 204.0, "x1": 248.0, "y2": 362.0}, "page": 26, "dpi": 0}], "error": null, "pdf": "/work/host-output/3d5fd13eafc91a7e7b1b21099bce10a2b4f0e11a/W13-07.pdf", "dpi": 100}