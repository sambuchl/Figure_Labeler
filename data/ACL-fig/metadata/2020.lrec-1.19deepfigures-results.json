{"raw_detected_boxes": [[], [{"x2": 400.0, "y1": 412.0, "x1": 72.0, "y2": 640.0}], [], [{"x2": 742.0, "y1": 204.0, "x1": 431.0, "y2": 432.0}], [{"x2": 386.0, "y1": 178.0, "x1": 130.0, "y2": 399.0}, {"x2": 316.0, "y1": 532.0, "x1": 148.0, "y2": 643.0}], [{"x2": 382.0, "y1": 315.0, "x1": 78.0, "y2": 643.0}, {"x2": 746.0, "y1": 399.0, "x1": 423.0, "y2": 690.0}], [], [], []], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Figure", "name": "4", "captionBoundary": {"x2": 275.2679138183594, "y1": 471.1783447265625, "x1": 63.901004791259766, "y2": 561.344970703125}, "imageText": [], "regionBoundary": {"x2": 285.0, "y1": 212.8900146484375, "x1": 54.0, "y2": 467.8900146484375}, "caption": "Figure 4: Schematics of the scene at one point in time from three angles, generated by the automated analysis of gaze and motion capture data. The glasses are visible as a triangle, the table as flat plain, and the three objects as upright planes. The black line represents the gaze vector calculated from the eye-tracking data. The individual points represent the markers on the right arm of the participant.", "page": 5}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 539.7186889648438, "y1": 506.3633117675781, "x1": 304.8659973144531, "y2": 620.44091796875}, "imageText": ["close", "jumping", "close-", "Object1-n", "trans", "alt-", "Object1-n", "Object1-", "Object2", "with", "Object1", "on", "smooth", "trans-", "Object1-n", "Object1*", "jumping", "*Object2", "smooth", "trans-", "Object1-n", "to", "Object1-", "Object2", "moving", "close-", "Object1-n", "jumping", "alt-", "Object1-n", "Object1-", "Object2", "Primary", "Label", "Reference", "Object", "Secondary", "Label", "Addition", "Supp-", "lement", "on", "Object1", "stable", "trans-", "Object1-n"], "regionBoundary": {"x2": 540.0, "y1": 284.8900146484375, "x1": 304.0, "y2": 502.8900146484375}, "caption": "Table 3: Coding scheme for gaze behaviours \u2013 Primary labels distinguish three different types of eye movements: gaze on object, from/to a certain location, and with an object/hand being moved. There can be up to two reference objects per annotation. Further labels describe features of the gaze behaviour including whether it is (a) smooth, (b) jumping, (c) stable, (d) crossing an object without change in speed (trans), or (e) alternating between two locations/objects. Aspects of the scene include whether another object is close to the current gaze location.", "page": 5}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 287.0082702636719, "y1": 614.3163452148438, "x1": 52.15800476074219, "y2": 656.6619873046875}, "imageText": ["Recordings", "22", "Utterances", "referring", "to", "actions", "327", "Instances", "of", "objects", "being", "moved", "209", "Gaze", "Behaviours", "3950"], "regionBoundary": {"x2": 222.92617797851562, "y1": 561.8900146484375, "x1": 58.13499450683594, "y2": 610.8900146484375}, "caption": "Table 2: Summary of the current state of the LKG-Corpus. Each instance of an object being moved contains at least two actions (first TAKE, second PUT/PUSH), though not necessarily two utterances referring to actions.", "page": 4}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 275.2674560546875, "y1": 474.7523193359375, "x1": 63.900997161865234, "y2": 505.1429748535156}, "imageText": [], "regionBoundary": {"x2": 240.0, "y1": 364.8900146484375, "x1": 99.0, "y2": 470.8900146484375}, "caption": "Figure 3: Eye-tracking glasses were fitted with three markers for tracking by the OptiTrak system, and featured a central, front-facing camera.", "page": 4}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 275.26751708984375, "y1": 308.5643615722656, "x1": 63.9010009765625, "y2": 338.9550476074219}, "imageText": [], "regionBoundary": {"x2": 295.9361572265625, "y1": 102.8900146484375, "x1": 52.0, "y2": 304.8900146484375}, "caption": "Figure 2: Participants sat at a table in the middle of an array of 10 ceiling-mounted cameras of the OptiTrak system.", "page": 4}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 287.0122375488281, "y1": 446.7763366699219, "x1": 52.157997131347656, "y2": 524.9879150390625}, "imageText": ["Reattention", "to", "the", "agent", "(motor)", "Motion", "Capture", "Reattention", "to", "the", "cup", "(haptic)", "Not", "applicable", "Deictic", "Sensory", "Measure", "Operation", "Consequence", "in", "LKG", "Attend", "to", "the", "agent", "(intero-", "ceptive)", "Attending", "to", "the", "agent", "(interoceptive)", "Not", "applicable", "Attend", "to", "the", "cup", "(visual)", "Attending", "to", "the", "cup", "(visual)", "Eye-tracking", "Activate", "motor", "action"], "regionBoundary": {"x2": 288.0, "y1": 296.8900146484375, "x1": 52.0, "y2": 442.8900146484375}, "caption": "Table 1: Knott (2012) proposes an alternating sequence of deictic operations and their sensory consequence, together a deictic routine. The sensory consequences necessarily follow from the preceding operation. Moreover, each sensory consequence prepares the agent for the subsequent deictic operation. Together, the processes summarised above constitute a single action episode.", "page": 1}, {"figType": "Figure", "name": "1", "captionBoundary": {"x2": 527.9755249023438, "y1": 315.7773132324219, "x1": 316.6090087890625, "y2": 346.16796875}, "imageText": [], "regionBoundary": {"x2": 535.0, "y1": 143.8900146484375, "x1": 310.0, "y2": 312.8900146484375}, "caption": "Figure 1: Participants performed the task seated at the table, with the three objects placed on a table in front of them.", "page": 3}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 398.6281077067057, "y1": 620.5226898193359, "x1": 72.44166268242729, "y2": 729.1498819986979}, "name": "1", "caption_text": "Table 1: Knott (2012) proposes an alternating sequence of deictic operations and their sensory consequence, together a deictic routine. The sensory consequences necessarily follow from the preceding operation. Moreover, each sensory consequence prepares the agent for the subsequent deictic operation. Together, the processes summarised above constitute a single action episode.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 400.0, "y1": 412.0, "x1": 72.0, "y2": 657.0}, "page": 1, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 733.299340142144, "y1": 438.579601711697, "x1": 439.7347344292535, "y2": 480.7888454861111}, "name": "1", "caption_text": "Figure 1: Participants performed the task seated at the table, with the three objects placed on a table in front of them.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 742.0, "y1": 200.0, "x1": 431.0, "y2": 449.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 382.31599595811633, "y1": 428.56161329481336, "x1": 88.75139024522569, "y2": 470.77089945475257}, "name": "2", "caption_text": "Figure 2: Participants sat at a table in the middle of an array of 10 ceiling-mounted cameras of the OptiTrak system.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 393.0, "y1": 178.0, "x1": 130.0, "y2": 399.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 382.315911187066, "y1": 659.3782212999132, "x1": 88.75138494703505, "y2": 701.5874650743273}, "name": "3", "caption_text": "Figure 3: Eye-tracking glasses were fitted with three markers for tracking by the OptiTrak system, and featured a central, front-facing camera.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 322.0, "y1": 530.0, "x1": 132.0, "y2": 660.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 382.31654696994354, "y1": 654.4143676757812, "x1": 88.75139554341634, "y2": 779.6457926432291}, "name": "4", "caption_text": "Figure 4: Schematics of the scene at one point in time from three angles, generated by the automated analysis of gaze and motion capture data. The glasses are visible as a triangle, the table as flat plain, and the three objects as upright planes. The black line represents the gaze vector calculated from the eye-tracking data. The individual points represent the markers on the right arm of the participant.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 386.0, "y1": 302.0, "x1": 78.0, "y2": 660.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 749.6092902289496, "y1": 703.2823774549696, "x1": 423.4249962700738, "y2": 861.7234971788195}, "name": "3", "caption_text": "Table 3: Coding scheme for gaze behaviours \u2013 Primary labels distinguish three different types of eye movements: gaze on object, from/to a certain location, and with an object/hand being moved. There can be up to two reference objects per annotation. Further labels describe features of the gaze behaviour including whether it is (a) smooth, (b) jumping, (c) stable, (d) crossing an object without change in speed (trans), or (e) alternating between two locations/objects. Aspects of the scene include whether another object is close to the current gaze location.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 750.0, "y1": 395.0, "x1": 423.0, "y2": 707.0}, "page": 5, "dpi": 0}], "error": null, "pdf": "/work/host-output/707d6a86b3e5462b8fbbab6b73d63020bf46ed2e/2020.lrec-1.19.pdf", "dpi": 100}