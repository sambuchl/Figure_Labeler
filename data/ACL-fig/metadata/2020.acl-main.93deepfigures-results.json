{"raw_detected_boxes": [[{"x2": 717.0, "y1": 309.0, "x1": 431.0, "y2": 580.0}], [], [{"x2": 590.0, "y1": 87.0, "x1": 240.0, "y2": 301.0}], [{"x2": 600.0, "y1": 90.0, "x1": 229.0, "y2": 343.0}], [], [{"x2": 638.0, "y1": 87.0, "x1": 198.0, "y2": 344.0}], [{"x2": 373.0, "y1": 95.0, "x1": 128.0, "y2": 350.0}, {"x2": 718.0, "y1": 95.0, "x1": 437.0, "y2": 366.0}], [{"x2": 714.0, "y1": 95.0, "x1": 433.0, "y2": 217.0}, {"x2": 397.0, "y1": 95.0, "x1": 100.0, "y2": 293.0}], [], []], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Figure", "name": "1", "captionBoundary": {"x2": 527.200439453125, "y1": 431.11053466796875, "x1": 306.9169921875, "y2": 580.5748291015625}, "imageText": [], "regionBoundary": {"x2": 526.0, "y1": 221.8900146484375, "x1": 307.0, "y2": 418.8900146484375}, "caption": "Figure 1: Intrinsic variance exists in a set of ground truth captions for an image. Differences between two references are commonly caused by two reasons: different concerns or different descriptions. Different concerns mean different expressions between references are caused by different regions of interest in an image, while different descriptions mean references focus on the same part but use different ways to explain it. Oneto-one metrics can hardly deal with the cases caused by different concerns. For example, they may regard Cand as a good caption compared with Ref1; while regard Cand as a bad caption compared with Ref2 or Ref3.", "page": 0}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 525.5473022460938, "y1": 260.3995666503906, "x1": 71.69100189208984, "y2": 302.26806640625}, "imageText": ["Task-speci\ufb01c", "SPICE", "0.715", "(0.009)", "0.688", "(0.013)", "LEIC", "0.939*", "(0.000)", "0.949*", "(0.000)", "Ours", "(BERT)", "0.875", "(0.000)", "0.797", "(0.002)", "Ours", "(RoBERTa)", "0.932", "(0.000)", "0.869", "(0.000)", "Task-agnostic", "ROUGE-L", "0.062", "(0.846)", "0.215", "(0.503)", "BLEU-1", "0.029", "(0.927)", "0.165", "(0.607)", "BLEU-4", "0.236", "(0.459)", "0.380", "(0.222)", "CIDEr", "0.440", "(0.151)", "0.539", "(0.071)", "METEOR", "0.703", "(0.011)", "0.701", "(0.011)", "BS", "(BERT-base)", "0.807", "(0.001)", "0.735", "(0.006)", "BS", "(RoBERTa-large)", "0.873", "(0.000)", "0.841", "(0.000)", "Type", "Metric", "M1", "M2"], "regionBoundary": {"x2": 460.0, "y1": 62.8900146484375, "x1": 138.0, "y2": 247.8900146484375}, "caption": "Table 1: Pearson correlation of system level metrics scores with human judgment in 2015 COCO Captioning Challenge. We use 12 teams results on validation set with \u201cKarpathy split\u201d. M1: the percentage of captions that are evaluated as better or equal to human captions; M2: the percentage of captions that are indistinguishable from human caption. BS means BERTScore and score with * are cited from (Cui et al., 2018).", "page": 5}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 291.9242248535156, "y1": 268.9085388183594, "x1": 71.64099884033203, "y2": 346.6420593261719}, "imageText": ["BS(BERT-base)", "0.807", "0.735", "Ours", "(Unigram)", "0.809", "0.714", "Ours", "(BERT+T)", "0.822", "0.741", "Ours", "(BERT+T+cat+R)", "0.857", "0.783", "Ours", "(BERT+TB)", "0.867", "0.755", "Ours", "(BERT+TBR)", "0.875", "0.797", "BLEU-1", "0.307", "0.338", "ROUGE-L", "0.062", "0.215", "ROUGE-L", "(cat)", "0.096", "0.180", "BLEU-1", "(cat)", "0.351", "0.175", "METEOR", "(cat)", "0.662", "0.571", "METEOR", "0.703", "0.701", "Metric", "M1", "M2"], "regionBoundary": {"x2": 270.0, "y1": 62.8900146484375, "x1": 92.0, "y2": 256.8900146484375}, "caption": "Table 2: We provide the ablation study and the replacement study in 2015 COCO Captioning dataset. As an additional experiment, we also compare concatenation with average in some standard metrics like: BLEU-1, ROUGE-L and METEOR. Abbreviation: BS means BERTScore, T means cut, B means combine, R means remove, cat means concatenation of references.", "page": 6}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 525.7160034179688, "y1": 282.4575500488281, "x1": 306.9169921875, "y2": 348.2360534667969}, "imageText": ["Inter-human", "0.736*", "-", "BLEU-1", "0.318", "0.282", "BLEU-4", "0.140", "0.199", "ROUGE-L", "0.323", "0.313", "BS", "(RoBERTa)", "0.367", "0.392", "BS", "(BERT)", "0.393", "0.399", "METEOR", "0.436", "0.381", "CIDEr", "0.447", "0.387", "SPICE", "0.458", "0.418", "LEIC", "0.466*", "-", "Ours", "(RoBERTa)", "0.451", "0.449", "Ours", "(Unigram)", "0.471", "0.420", "Ours", "(BERT)", "0.481", "0.423", "Flickr", "8K", "COMPOSITE"], "regionBoundary": {"x2": 518.0, "y1": 62.8900146484375, "x1": 314.0, "y2": 269.8900146484375}, "caption": "Table 3: In caption-level experiments, we compute the Kendall correlation between human judgments and scores of metrics. Two dataset results are given: Flickr 8K and COMPOSITE. Both our unigram metric and BERT based metric outperform other metrics. Scores with * are cited from (Cui et al., 2018)", "page": 6}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 525.5474243164062, "y1": 229.48953247070312, "x1": 72.0, "y2": 272.85205078125}, "imageText": [], "regionBoundary": {"x2": 425.0, "y1": 61.8900146484375, "x1": 172.0, "y2": 217.8900146484375}, "caption": "Figure 2: This figure explains the differences between an error and different concerns when mismatches occur. We show an image as nine parts grid chart (3 \u00d7 3). In error case, caption1 focuses on three parts while b3 cannot represent the part that matches a1. In different-concerns cases, two captions can represent well in different parts of the same image. However, a mismatch occurs since a3 and b3 attend to a different part of the image.", "page": 2}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 292.0137939453125, "y1": 222.78353881835938, "x1": 71.64099884033203, "y2": 288.56207275390625}, "imageText": ["BLEU-1", "53.1", "94.7", "90.9", "56.9", "73.9", "BLEU-4", "53.3", "92.8", "85.2", "60.5", "73.0", "ROUGE-L", "55.6", "95.1", "93.3", "57.7", "75.4", "METEOR", "61.4", "97.2", "94.9", "64.5", "79.5", "CIDEr", "55.0", "98.0", "91.0", "64.6", "77.2", "SPICE", "57.7", "96.1", "88.3", "65.3", "76.9", "Ours", "(RBT)", "62.5", "97.7", "95.0", "59.4", "78.7", "BS", "(BERT)", "64.4", "97.9", "96.6", "59.0", "79.5", "Ours", "(BERT)", "65.4", "98.1", "96.4", "60.3", "80.1", "HC", "HI", "HM", "MM", "All"], "regionBoundary": {"x2": 286.0, "y1": 62.8900146484375, "x1": 72.0, "y2": 210.8900146484375}, "caption": "Table 4: In PASCAL-50S, candidate sentences come from human written or model generated. There are 4 kinds of paired ways: human-correct (HC), humanincorrect (HI), human-model (HM), and model-model (MM). Ours (BERT) outperforms in HC, HI and HM. Abbreviation: RBT means RoBERTa.", "page": 7}, {"figType": "Table", "name": "5", "captionBoundary": {"x2": 527.2003173828125, "y1": 168.58651733398438, "x1": 306.9670104980469, "y2": 234.36505126953125}, "imageText": ["AoAnet", "0.3529", "1.296", "M2-Transformer", "0.3481", "1.321", "SAT", "0.3296", "0.893", "CNN+LSTM", "0.3055", "0.946", "NeuralTalk", "0.2845", "0.692", "Model", "Ours", "(BERT)", "CIDEr-D"], "regionBoundary": {"x2": 515.0, "y1": 62.8900146484375, "x1": 307.0, "y2": 155.8900146484375}, "caption": "Table 5: We present some results on current state-ofthe-art models (M2-Transformer and AoAnet) for image captioning models with respect to CIDEr-D. The experimental results show that: on both our metric and CIDEr-D, current models perform better. Abbreviation: SAT means Show, Attend and Tell.", "page": 7}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 525.5473022460938, "y1": 260.9895324707031, "x1": 71.99996948242188, "y2": 326.76806640625}, "imageText": [], "regionBoundary": {"x2": 433.0, "y1": 61.8900146484375, "x1": 164.0, "y2": 248.8900146484375}, "caption": "Figure 3: Combination of references comes from a phenomenon that: mismatches between two ground truth captions can\u2019t be errors but different concerns. After the greedy matching process, we collect all the mismatch tokens and create a combined contextual embedding as a combined caption. For example, the threshold value \u03b2 is 0.4, and all the tokens in Ref2 can\u2019t match a3 with a value bigger than 0.4. After the combination of all references, our metric provides a better recall score between the combined caption and the candidate caption with idf weighted.", "page": 3}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 732.2228325737847, "y1": 598.7646314832899, "x1": 426.27360026041663, "y2": 806.3539293077257}, "name": "1", "caption_text": "Figure 1: Intrinsic variance exists in a set of ground truth captions for an image. Differences between two references are commonly caused by two reasons: different concerns or different descriptions. Different concerns mean different expressions between references are caused by different regions of interest in an image, while different descriptions mean references focus on the same part but use different ways to explain it. Oneto-one metrics can hardly deal with the cases caused by different concerns. For example, they may regard Cand as a good caption compared with Ref1; while regard Cand as a bad caption compared with Ref2 or Ref3.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 717.0, "y1": 309.0, "x1": 430.0, "y2": 581.0}, "page": 0, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9269782172308, "y1": 318.7354617648654, "x1": 100.0, "y2": 378.961181640625}, "name": "2", "caption_text": "Figure 2: This figure explains the differences between an error and different concerns when mismatches occur. We show an image as nine parts grid chart (3 \u00d7 3). In error case, caption1 focuses on three parts while b3 cannot represent the part that matches a1. In different-concerns cases, two captions can represent well in different parts of the same image. However, a mismatch occurs since a3 and b3 attend to a different part of the image.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 590.0, "y1": 87.0, "x1": 238.0, "y2": 318.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9268086751301, "y1": 362.4854617648654, "x1": 99.99995761447482, "y2": 453.84453667534723}, "name": "3", "caption_text": "Figure 3: Combination of references comes from a phenomenon that: mismatches between two ground truth captions can\u2019t be errors but different concerns. After the greedy matching process, we collect all the mismatch tokens and create a combined contextual embedding as a combined caption. For example, the threshold value \u03b2 is 0.4, and all the tokens in Ref2 can\u2019t match a3 with a value bigger than 0.4. After the combination of all references, our metric provides a better recall score between the combined caption and the candidate caption with idf weighted.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 600.0, "y1": 90.0, "x1": 229.0, "y2": 345.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9268086751301, "y1": 361.66606479220917, "x1": 99.57083596123589, "y2": 419.81675889756946}, "name": "1", "caption_text": "Table 1: Pearson correlation of system level metrics scores with human judgment in 2015 COCO Captioning Challenge. We use 12 teams results on validation set with \u201cKarpathy split\u201d. M1: the percentage of captions that are evaluated as better or equal to human captions; M2: the percentage of captions that are indistinguishable from human caption. BS means BERTScore and score with * are cited from (Cui et al., 2018).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 653.0, "y1": 86.0, "x1": 192.0, "y2": 361.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.45031229654944, "y1": 373.4840816921658, "x1": 99.50138727823892, "y2": 481.44730461968317}, "name": "2", "caption_text": "Table 2: We provide the ablation study and the replacement study in 2015 COCO Captioning dataset. As an additional experiment, we also compare concatenation with average in some standard metrics like: BLEU-1, ROUGE-L and METEOR. Abbreviation: BS means BERTScore, T means cut, B means combine, R means remove, cat means concatenation of references.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 375.0, "y1": 86.0, "x1": 128.0, "y2": 357.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 730.1611158582899, "y1": 392.3021528455946, "x1": 426.27360026041663, "y2": 483.6611853705512}, "name": "3", "caption_text": "Table 3: In caption-level experiments, we compute the Kendall correlation between human judgments and scores of metrics. Two dataset results are given: Flickr 8K and COMPOSITE. Both our unigram metric and BERT based metric outperform other metrics. Scores with * are cited from (Cui et al., 2018)", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 720.0, "y1": 86.0, "x1": 437.0, "y2": 375.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.222663031684, "y1": 234.14794074164496, "x1": 426.3430701361762, "y2": 325.5070156521267}, "name": "5", "caption_text": "Table 5: We present some results on current state-ofthe-art models (M2-Transformer and AoAnet) for image captioning models with respect to CIDEr-D. The experimental results show that: on both our metric and CIDEr-D, current models perform better. Abbreviation: SAT means Show, Attend and Tell.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 729.0, "y1": 86.0, "x1": 426.0, "y2": 234.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.574713812934, "y1": 309.4215816921658, "x1": 99.50138727823892, "y2": 400.7806566026476}, "name": "4", "caption_text": "Table 4: In PASCAL-50S, candidate sentences come from human written or model generated. There are 4 kinds of paired ways: human-correct (HC), humanincorrect (HI), human-model (HM), and model-model (MM). Ours (BERT) outperforms in HC, HI and HM. Abbreviation: RBT means RoBERTa.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 398.0, "y1": 86.0, "x1": 100.0, "y2": 310.0}, "page": 7, "dpi": 0}], "error": null, "pdf": "/work/host-output/4dfcb49514b77abbd35551f8d425cd0f8d3bdf21/2020.acl-main.93.pdf", "dpi": 100}