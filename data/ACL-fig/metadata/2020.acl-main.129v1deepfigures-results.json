{"raw_detected_boxes": [[{"x2": 720.0, "y1": 312.0, "x1": 434.0, "y2": 433.0}], [], [], [{"x2": 306.0, "y1": 88.0, "x1": 190.0, "y2": 295.0}, {"x2": 688.0, "y1": 89.0, "x1": 469.0, "y2": 295.0}], [{"x2": 722.0, "y1": 121.0, "x1": 442.0, "y2": 547.0}], [], [{"x2": 394.0, "y1": 86.0, "x1": 103.0, "y2": 360.0}], [{"x2": 389.0, "y1": 92.0, "x1": 110.0, "y2": 366.0}, {"x2": 719.0, "y1": 93.0, "x1": 437.0, "y2": 363.0}], [{"x2": 387.0, "y1": 92.0, "x1": 110.0, "y2": 368.0}, {"x2": 389.0, "y1": 473.0, "x1": 113.0, "y2": 573.0}], [], [{"x2": 728.0, "y1": 249.0, "x1": 426.0, "y2": 592.0}, {"x2": 724.0, "y1": 821.0, "x1": 426.0, "y2": 1068.0}], [{"x2": 401.0, "y1": 103.0, "x1": 100.0, "y2": 386.0}]], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Figure", "name": "1", "captionBoundary": {"x2": 525.5465698242188, "y1": 325.55352783203125, "x1": 307.2760009765625, "y2": 391.3319396972656}, "imageText": [], "regionBoundary": {"x2": 521.0, "y1": 221.8900146484375, "x1": 312.0, "y2": 313.8900146484375}, "caption": "Figure 1: Illustration of reinforcement learning for dialog management. The agent (top right) interacts with the environment (left) by taking actions, and observing the resulting new state and reward. DQfD and RoFL RL agents are guided by an expert demonstrator (bottom right).", "page": 0}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 290.2706298828125, "y1": 274.3855285644531, "x1": 72.0, "y2": 387.9839782714844}, "imageText": ["FFN-ft", "9.62", "83.00", "90.79", "76.00", "FLE+R", "6.75", "90.00", "94.57", "92.47", "RLE+R", "6.38", "88.67", "90.62", "92.93", "NLE+R", "6.89", "89.00", "92.68", "91.00", "RE", "5.33", "92.33", "97.07", "98.33", "FLE", "6.81", "89.67", "94.12", "91.67", "RLE", "7.64", "81.33", "89.34", "85.03", "NLE", "7.20", "84.67", "85.31", "86.83", "Rule", "5.25", "94.00", "100", "100", "FFN", "11.67", "81.00", "52.63", "61.00", "DQN", "18.79", "28.50", "11.07", "11.85", "PPO", "5.79", "65.67", "72.51", "63.27", "Turns", "Inform", "Match", "Success"], "regionBoundary": {"x2": 288.0, "y1": 62.8900146484375, "x1": 74.0, "y2": 261.8900146484375}, "caption": "Table 1: Evaluation results of baseline systems (top) as well as DQfD with rule-based and our weak expert approaches trained in-domain. The middle section denotes DQfD agents trained without RoFL; the bottom section shows results for agents trained with RoFL. Evaluation is conducted using an agenda-based user-simulator for 1000 dialogs. Reported scores are average number of Turns, Inform F1, Match Rate, and Success Rate. Best performing weak expert agents are in bold.", "page": 6}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 290.2706298828125, "y1": 278.2285461425781, "x1": 71.99999237060547, "y2": 308.14202880859375}, "imageText": [], "regionBoundary": {"x2": 285.0, "y1": 61.8900146484375, "x1": 77.0, "y2": 266.8900146484375}, "caption": "Figure 4: Average Success Rates of our methods trained on in-domain data over the course of 2.5 million training steps.", "page": 7}, {"figType": "Figure", "name": "5", "captionBoundary": {"x2": 525.546630859375, "y1": 278.2285461425781, "x1": 307.2760009765625, "y2": 332.0520324707031}, "imageText": [], "regionBoundary": {"x2": 521.0, "y1": 61.8900146484375, "x1": 312.0, "y2": 266.8900146484375}, "caption": "Figure 5: Average Success Rate of RL agents over the course of 2.5 million training steps, with and without RoFL fine-tuning. Agents were evaluated every 2000 steps on 100 evaluation dialogs. Experts were trained on in-domain data.", "page": 7}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 509.53082275390625, "y1": 225.13754272460938, "x1": 323.2900085449219, "y2": 231.1400146484375}, "imageText": [], "regionBoundary": {"x2": 496.0, "y1": 61.8900146484375, "x1": 337.0, "y2": 212.8900146484375}, "caption": "Figure 3: No Label Expert (NLE) architecture.", "page": 3}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 285.32373046875, "y1": 225.13754272460938, "x1": 76.94599914550781, "y2": 231.1400146484375}, "imageText": [], "regionBoundary": {"x2": 226.0, "y1": 61.8900146484375, "x1": 137.0, "y2": 212.8900146484375}, "caption": "Figure 2: Reduced Label Expert (RLE) architecture.", "page": 3}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 290.27130126953125, "y1": 427.5885314941406, "x1": 71.99998474121094, "y2": 481.4119567871094}, "imageText": ["RLE", "7.60", "80.33", "87.30", "85.00", "NLE", "16.27", "40.00", "27.15", "26.55", "RLE+R", "6.64", "85.00", "89.03", "91.00", "NLE+R", "9.94", "70.00", "62.64", "68.90", "Turns", "Inform", "Match", "Success"], "regionBoundary": {"x2": 281.0, "y1": 334.8900146484375, "x1": 81.0, "y2": 414.8900146484375}, "caption": "Table 2: Results of out-of-domain weak experts with and without RoFL training, using an agenda-based user-simulator for 1000 evaluation dialogs. Reported scores are average number of Turns, Inform F1, Match Rate, and Success Rate.", "page": 8}, {"figType": "Figure", "name": "6", "captionBoundary": {"x2": 290.2705993652344, "y1": 278.2285461425781, "x1": 72.0, "y2": 320.0970458984375}, "imageText": [], "regionBoundary": {"x2": 285.0, "y1": 61.8900146484375, "x1": 77.0, "y2": 266.8900146484375}, "caption": "Figure 6: Average Success Rates of Reduced and No Label experts trained on out-of-domain data over the course of 2.5 million training steps, with and without RoFL fine-tuning.", "page": 8}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 729.925791422526, "y1": 452.15767754448785, "x1": 426.772223578559, "y2": 543.5165829128689}, "name": "1", "caption_text": "Figure 1: Illustration of reinforcement learning for dialog management. The agent (top right) interacts with the environment (left) by taking actions, and observing the resulting new state and reward. DQfD and RoFL RL agents are guided by an expert demonstrator (bottom right).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 722.0, "y1": 309.0, "x1": 434.0, "y2": 435.0}, "page": 0, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 396.282958984375, "y1": 312.69103156195746, "x1": 106.86944325764973, "y2": 321.02779812282984}, "name": "2", "caption_text": "Figure 2: Reduced Label Expert (RLE) architecture.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 320.0, "y1": 87.0, "x1": 181.0, "y2": 312.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 707.6816982693142, "y1": 312.69103156195746, "x1": 449.01390075683594, "y2": 321.02779812282984}, "name": "3", "caption_text": "Figure 3: No Label Expert (NLE) architecture.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 688.0, "y1": 88.0, "x1": 452.0, "y2": 312.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.15365261501734, "y1": 381.0910118950738, "x1": 100.0, "y2": 538.8666364881727}, "name": "1", "caption_text": "Table 1: Evaluation results of baseline systems (top) as well as DQfD with rule-based and our weak expert approaches trained in-domain. The middle section denotes DQfD agents trained without RoFL; the bottom section shows results for agents trained with RoFL. Evaluation is conducted using an agenda-based user-simulator for 1000 dialogs. Reported scores are average number of Turns, Inform F1, Match Rate, and Success Rate. Best performing weak expert agents are in bold.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 400.0, "y1": 86.0, "x1": 103.0, "y2": 364.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.15365261501734, "y1": 386.4285363091363, "x1": 99.9999894036187, "y2": 427.97504001193573}, "name": "4", "caption_text": "Figure 4: Average Success Rates of our methods trained on in-domain data over the course of 2.5 million training steps.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 393.0, "y1": 92.0, "x1": 110.0, "y2": 368.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9258761935764, "y1": 386.4285363091363, "x1": 426.772223578559, "y2": 461.1833784315321}, "name": "5", "caption_text": "Figure 5: Average Success Rate of RL agents over the course of 2.5 million training steps, with and without RoFL fine-tuning. Agents were evaluated every 2000 steps on 100 evaluation dialogs. Experts were trained on in-domain data.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 720.0, "y1": 92.0, "x1": 437.0, "y2": 368.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.1536102294922, "y1": 386.4285363091363, "x1": 100.0, "y2": 444.5792304144965}, "name": "6", "caption_text": "Figure 6: Average Success Rates of Reduced and No Label experts trained on out-of-domain data over the course of 2.5 million training steps, with and without RoFL fine-tuning.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 396.0, "y1": 92.0, "x1": 100.0, "y2": 385.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.15458509657117, "y1": 593.8729604085286, "x1": 99.99997880723741, "y2": 668.6277177598741}, "name": "2", "caption_text": "Table 2: Results of out-of-domain weak experts with and without RoFL training, using an agenda-based user-simulator for 1000 evaluation dialogs. Reported scores are average number of Turns, Inform F1, Match Rate, and Success Rate.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 390.0, "y1": 465.0, "x1": 113.0, "y2": 577.0}, "page": 8, "dpi": 0}], "error": null, "pdf": "/work/host-output/2637d7d0f66cfb7794d9bbce7d55f822939c7b99/2020.acl-main.129v1.pdf", "dpi": 100}