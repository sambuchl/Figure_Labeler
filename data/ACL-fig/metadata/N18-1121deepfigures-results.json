{"raw_detected_boxes": [[{"x2": 727.0, "y1": 311.0, "x1": 426.0, "y2": 452.0}], [], [{"x2": 349.0, "y1": 104.0, "x1": 134.0, "y2": 249.0}], [{"x2": 662.0, "y1": 91.0, "x1": 495.0, "y2": 326.0}], [{"x2": 694.0, "y1": 89.0, "x1": 133.0, "y2": 370.0}, {"x2": 690.0, "y1": 499.0, "x1": 468.0, "y2": 687.0}], [], [{"x2": 727.0, "y1": 87.0, "x1": 103.0, "y2": 209.0}], [{"x2": 724.0, "y1": 92.0, "x1": 100.0, "y2": 481.0}], [], []], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Figure", "name": "1", "captionBoundary": {"x2": 525.5474853515625, "y1": 343.59857177734375, "x1": 307.2770080566406, "y2": 409.3769836425781}, "imageText": ["Our", "Model", "Le", "canap\u00e9", "prend", "beaucoup", "de", "place", ".", "(space)Our", "Model", "(accuse)", "Source", "The", "couch", "takes", "up", "a", "lot", "of", "room", ".", "Reference", "Le", "canap\u00e9", "prend", "beaucoup", "de", "place", ".", "(space)", "Baseline", "Le", "canap\u00e9", "lit", "beaucoup", "de", "chambre", ".", "(bedroom)", "Source", "Charges", "against", "four", "other", "men", "were", "found", "not", "proven", ".", "(accuse)Reference", "(fee)Baseline"], "regionBoundary": {"x2": 526.0, "y1": 221.8900146484375, "x1": 307.0, "y2": 327.8900146484375}, "caption": "Figure 1: Homographs where the baseline system makes mistakes (red words) but our proposed system incorporating a more direct representation of context achieves the correct translation (blue words). Definitions of corresponding blue and red words are in parenthesis.", "page": 0}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 525.5482788085938, "y1": 162.34854125976562, "x1": 72.0009994506836, "y2": 204.216064453125}, "imageText": ["en\u2192", "de", "baseline", "0.401", "0.422", "0.382", "0.547", "0.569", "0.526", "best", "0.426", "(+0.025)", "0.449", "(+0.027)", "0.405", "(+0.023)", "0.553", "(+0.006)", "0.576", "(+0.007)", "0.532", "(+0.006)", "en\u2192", "fr", "baseline", "0.467", "0.484", "0.451", "0.605", "0.623", "0.587", "best", "0.480", "(+0.013)", "0.496", "(+0.012)", "0.465", "(+0.014)", "0.613", "(+0.008)", "0.630", "(+0.007)", "0.596", "(+0.009)", "en\u2192", "zh", "baseline", "0.578", "0.587", "0.570", "0.573", "0.605", "0.544", "best", "0.590", "(+0.012)", "0.599", "(+0.012)", "0.581", "(+0.011)", "0.581", "(+0.008)", "0.612", "(+0.007)", "0.552", "(+0.008)", "F1", "Precision", "Recall", "F1", "Precision", "Recall", "language", "System", "Homograph", "All", "Words"], "regionBoundary": {"x2": 526.0, "y1": 61.8900146484375, "x1": 74.0, "y2": 150.8900146484375}, "caption": "Table 3: Translation results for homographs and all words in our NMT vocabulary. We compare scores for baseline and our best proposed model on three different language pairs. Improvements are in italic. We performed bootstrap resampling for 1000 times: our best model improved more on homographs than all words in terms of either f1, precision, or recall with p < 0.05, indicating statistical significance across all measures.", "page": 6}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 290.27154541015625, "y1": 195.13357543945312, "x1": 72.0009994506836, "y2": 213.091064453125}, "imageText": ["en-de", "en-fr", "en-zh", "F1", "0.7", "0.6", "0.5", "0.4", "0.3", "0.2", "0.1", "0", "5", "10", "15", "20", "number", "of", "senses"], "regionBoundary": {"x2": 251.0, "y1": 74.78096008300781, "x1": 98.00367736816406, "y2": 178.88958740234375}, "caption": "Figure 2: Translation performance of words with different numbers of senses.", "page": 2}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 525.5482177734375, "y1": 359.3935546875, "x1": 72.00098419189453, "y2": 413.21697998046875}, "imageText": ["English-Chinese", "Translations", "src", "Ugandan", "president", "meets", "Chinese", "FM", ",", "anticipates", "closer", "cooperation", "ref", "\u4e4c\u5e72\u8fbe\u603b\u7edf\u4f1a\u89c1\u4e2d\u56fd\u5916\u957f\uff0c\u671f\u5f85\u589e\u8fdb\u5408\u4f5c(come", "together", "intentionally)", "best", "\u4e4c\u5e72\u8fbe\u603b\u7edf\u4f1a\u89c1\u4e2d\u56fd\u8c03\u9891\uff0c\u9884\u671f\u66f4\u5bc6\u5207\u5408\u4f5c(come", "together", "intentionally)", "base", "\u4e4c\u5e72\u8fbe\u603b\u7edf\u7b26\u5408", "\u4e2d\u56fd\u8c03\u9891\uff0c\u9884\u671f\u66f4\u52a0\u5408\u4f5c(satisfy)", "src", "Investigators", "are", "trying", "to", "establish", "whether", "Kermiche", "and", "Petitjean", "had", "accomplices", "in", "France", "and", "whether", "they", "had", "links", "with", "Islamic", "State", ",", "which", "has", "claimed", "responsibility", "for", "the", "attack", ".", "ref", "\u8c03\u67e5\u4eba\u5458\u6b63\u8bd5\u56fe\u786e\u5b9a\u514b\u5c14\u7c73\u5947\u548c\u5e15\u8fea\u8ba9\u5728\u6cd5\u56fd\u662f\u5426\u6709\u540c\u8c0b\uff0c\u4ee5\u53ca\u662f\u5426\u4e0e\u4f0a\u65af\u5170\u56fd\u6b66\u88c5\u5206\u5b50\u6709\u8054\u7cfb\uff0c", "\u4f0a\u65af\u5170\u56fd\u6b66\u88c5\u5206\u5b50\u58f0\u79f0\u5bf9\u6b64\u6b21\u88ad\u51fb\u8d1f\u8d23\u3002(get", "proof", "of", "something)", "best", "\u8c03\u67e5\u4eba\u5458\u6b63\u8bd5\u56fe\u786e\u5b9a", "Kermiche\u548cPetitjean\u5728\u6cd5\u56fd\u662f\u5426\u6709\u540c\u8c0b\uff0c\u4ed6\u4eec\u662f\u5426\u4e0e\u4f0a\u65af\u5170\u56fd\u6709\u8054\u7cfb\uff0c", "\u58f0\u79f0\u5bf9\u8fd9\u6b21\u88ad\u51fb\u8d1f\u8d23\u3002(get", "proof", "of", "something)", "base", "\u8c03\u67e5\u4eba\u5458\u6b63\u5728\u52aa\u529b\u5efa\u7acb\u6cd5\u56fd\u7684\u540c\u8c0b\u548c\u4ed6\u4eec\u662f\u5426\u4e0e\u4f0a\u65af\u5170\u56fd\u6709\u8054\u7cfb,\u8be5\u56fd\u58f0\u79f0\u5bf9\u8fd9\u6b21\u88ad\u51fb\u8d1f\u6709\u8d23\u4efb\u3002(to", "start)", "src", "The", "decrease", "of", "transaction", "settlement", "fund", "balance", "in", "the", "securities", "market", "in", "July", "was", "smaller", "than", "that", "in", "June", ",", "while", "the", "net", "bank", "@-@", "securities", "transfers", "stood", "at", "negative", "RMB", "66.6", "billion", ".", "ref", "7\u6708\u8bc1\u5238\u5e02\u573a\u4ea4\u6613\u7ed3\u7b97\u8d44\u91d1\u4f59\u989d\u51cf\u5c11\u989d\u8f836\u6708\u5927\u5e45\u964d\u4f4e\uff0c\u94f6\u8bc1\u8f6c\u8d26\u53d8\u52a8\u51c0\u989d\u4e3a-", "666\u4ebf\u5143\u3002(money", "left)", "best", "7\u6708\u4efd\u8bc1\u5238\u5e02\u573a\u4ea4\u6613\u7ed3\u7b97\u8d44\u91d1\u4f59\u989d\u7684\u51cf\u5c11\u5c0f\u4e8e6\u6708\u4efd\uff0c\u800c\u94f6\u884c\u8bc1\u5238\u8f6c\u8ba9\u51c0\u989d\u4e3anegative\u4ebf\u5143\u3002(money", "left)", "base", "\u4e03\u6708\u8bc1\u5238\u5e02\u573a\u4ea4\u6613\u7ed3\u7b97\u57fa\u91d1\u5e73\u8861\u7684\u51cf\u5c11\u6bd4\u516d\u6708\u4efd\u5c0f\uff0c\u800c\u51c0\u94f6\u884c\u8bc1\u5238\u8f6c\u8ba9\u5219\u4e3a\u8d1f\u5143\u3002(equal", "weight", "or", "force)", "src", "Initial", "reports", "suggest", "that", "the", "gunman", "may", "have", "shot", "a", "woman", ",", "believed", "to", "be", "his", "ex", "@-@", "partner", ".", "ref", "\u636e\u521d\u6b65\u62a5\u544a\u663e\u793a\uff0c\u5f00\u67aa\u8005\u53ef\u80fd\u51fb\u4e2d\u4e00\u540d\u5987\u5973\uff0c\u636e\u4fe1\u662f\u4ed6\u7684\u524d\u642d\u6863\u3002(been", "accepted", "as", "truth)", "best", "\u521d\u6b65\u7684\u62a5\u9053\u8868\u660e\uff0c\u67aa\u624b\u53ef\u80fd\u5df2\u7ecf\u5c04\u6740\u4e86\u4e00\u4e2a\u5973\u4eba\uff0c\u88ab\u8ba4\u4e3a\u662f\u4ed6\u7684\u524d\u4f19\u4f34\u3002(been", "known", "as)", "base", "\u6700\u521d\u7684\u62a5\u9053\u663e\u793a\uff0c\u67aa\u624b\u53ef\u80fd\u5df2\u7ecf\u5c04\u6740\u4e86\u4e00\u540d\u5987\u5973\uff0c\u76f8\u4fe1\u4ed6\u662f\u4ed6\u7684\u524d\u4f19\u4f34\u3002(accept", "as", "truth)", "src", "When", "the", "game", "came", "to", "the", "last", "3", "\u2019", "49", "\u2019", "\u2019", ",", "Nigeria", "closed", "to", "79", "@-@", "81", "after", "Aminu", "added", "a", "layup", ".", "ref", "\u6bd4\u8d5b\u8fd8\u67093\u520649\u79d2\u65f6\uff0c\u963f\u7c73\u52aa\u4e0a\u7bee\u5f97\u624b\u540e\uff0c\u5c3c\u65e5\u5229\u4e9a\u5c06\u6bd4\u5206\u8ffd\u6210\u4e8679-81\u3002(narrow)", "best", "\u5f53\u8fd9\u573a\u6bd4\u8d5b\u5230\u4e86\u6700\u540e\u4e09\u4e2a\u201c", "49", "\u201d\u65f6\uff0c\u5c3c\u65e5\u5229\u4e9a\u5728Aminu\u589e\u52a0\u4e86\u4e00\u4e2alayup\u4e4b\u540eMISSING", "TRANSLATION\u3002", "base", "\u5f53\u6e38\u620f\u5230\u8fbe\u6700\u540e3", "\u201c", "49", "\u201d\u65f6\uff0c\u5c3c\u65e5\u5229\u4e9a\u5df2\u7ecf\u5173\u95ed\u4e86Aminu\u3002(end)"], "regionBoundary": {"x2": 526.0, "y1": 65.53655242919922, "x1": 72.0, "y2": 347.8900146484375}, "caption": "Table 4: Sample translations - for each example, we show sentence in source language (src), the human translated reference (ref), the translation generated by our best context-aware model (best), and the translation generated by baseline model (base). We also highlight the word with multiple senses in source language in bold, the corresponding correctly translated words in blue and wrongly translated words in red. The definitions of words in blue or red are in parenthesis.", "page": 7}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 525.548583984375, "y1": 253.33755493164062, "x1": 307.2770080566406, "y2": 320.610107421875}, "imageText": ["x2x0", "\u2326", "\u2326", "x1", "\u2326", "c0", "c1", "c2", "x1", "x2x0", "Context", "Network", "y1y0"], "regionBoundary": {"x2": 477.0, "y1": 65.8900146484375, "x1": 356.0, "y2": 236.8900146484375}, "caption": "Figure 3: Illustration of our proposed model. The context network is a differentiable network that computes context vector ct for word xt taking the whole sequence as input. \u2297 represents the operation that combines original word embedding xt with corresponding context vector ct to form context-aware word embeddings.", "page": 3}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 525.5482788085938, "y1": 280.7955627441406, "x1": 72.00096893310547, "y2": 334.6190490722656}, "imageText": ["NBOW", "Concat", "\u2192", "2", "85M", "7.23", "20.44", "22.83", "NBOW", "Concat", "\u2194", "2", "83M", "7.28", "20.76", "23.61", "HoLSTM", "Concat", "\u2192", "2", "87M", "7.19", "20.67", "23.05", "HoLSTM", "Concat", "\u2194", "2", "86M", "7.04", "21.15", "23.53", "BiLSTM", "Concat", "\u2192", "2", "87M", "6.88", "21.80", "24.52", "BiLSTM", "Concat", "\u2194", "2", "85M", "6.87", "21.33", "24.37", "NBOW", "Gating", "\u2192", "2", "85M", "7.14", "20.20", "22.94", "NBOW", "Gating", "\u2194", "2", "83M", "6.92", "21.16", "23.52", "BiLSTM", "Gating", "\u2192", "2", "87M", "7.07", "20.94", "23.58", "BiLSTM", "Gating", "\u2194", "2", "85M", "7.11", "21.33", "24.05", "Context", "Integration", "uni/bi", "#layers", "#params", "Ppl", "WMT14", "WMT15", "None", "-", "\u2192", "2", "85M", "7.12", "20.49", "22.95", "None", "-", "\u2194", "2", "83M", "7.20", "21.05", "23.83", "None", "-", "\u2194", "3", "86M", "7.50", "20.86", "23.14"], "regionBoundary": {"x2": 503.0, "y1": 65.30884552001953, "x1": 94.0, "y2": 269.8900146484375}, "caption": "Table 1: WMT\u201914, WMT\u201915 English-German results - We show perplexities (Ppl) on development set and tokenized BLEU on WMT\u201914 and WMT\u201915 test set of various NMT systems. We also show different settings for different systems. \u2192 represents uni-directional, and \u2194 represents bi-directional. We also highlight the best baseline model and the best proposed model in bold. The best baseline model will be referred as base or baseline and the best proposed model will referred to as best for further experiments.", "page": 4}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 525.5484008789062, "y1": 506.4045715332031, "x1": 307.2770080566406, "y2": 560.22802734375}, "imageText": ["best", "24.81", "best", "28.77", "32.39", "en\u2192", "zh", "WMT\u201917", "baseline", "24.07", "best", "21.80", "24.52", "en\u2192", "fr", "WMT\u201913", "WMT\u201914", "baseline", "28.21", "31.55", "System", "BLEU", "en\u2192", "de", "WMT\u201914", "WMT\u201915", "baseline", "21.05", "23.83"], "regionBoundary": {"x2": 497.0, "y1": 355.8900146484375, "x1": 336.0, "y2": 494.8900146484375}, "caption": "Table 2: Results on three different language pairs - The best proposed models (BiLSTM+Concat+uni) are significantly better (p-value < 0.001) than baseline models using paired bootstrap resampling (Koehn, 2004).", "page": 4}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 729.9270629882812, "y1": 477.2202385796441, "x1": 426.77362230088977, "y2": 568.5791439480251}, "name": "1", "caption_text": "Figure 1: Homographs where the baseline system makes mistakes (red words) but our proposed system incorporating a more direct representation of context achieves the correct translation (blue words). Definitions of corresponding blue and red words are in parenthesis.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 730.0, "y1": 309.0, "x1": 426.0, "y2": 455.0}, "page": 0, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.1549241807726, "y1": 271.0188547770182, "x1": 100.00138812594943, "y2": 295.95981174045136}, "name": "2", "caption_text": "Figure 2: Translation performance of words with different numbers of senses.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 349.0, "y1": 104.0, "x1": 133.0, "y2": 249.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9285888671875, "y1": 351.85771518283417, "x1": 426.77362230088977, "y2": 445.29181586371527}, "name": "3", "caption_text": "Figure 3: Illustration of our proposed model. The context network is a differentiable network that computes context vector ct for word xt taking the whole sequence as input. \u2297 represents the operation that combines original word embedding xt with corresponding context vector ct to form context-aware word embeddings.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 662.0, "y1": 91.0, "x1": 495.0, "y2": 328.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9281650119358, "y1": 389.99383714463977, "x1": 100.00134574042426, "y2": 464.7486792670356}, "name": "1", "caption_text": "Table 1: WMT\u201914, WMT\u201915 English-German results - We show perplexities (Ppl) on development set and tokenized BLEU on WMT\u201914 and WMT\u201915 test set of various NMT systems. We also show different settings for different systems. \u2192 represents uni-directional, and \u2194 represents bi-directional. We also highlight the best baseline model and the best proposed model in bold. The best baseline model will be referred as base or baseline and the best proposed model will referred to as best for further experiments.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 699.0, "y1": 89.0, "x1": 131.0, "y2": 374.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9283345540364, "y1": 703.3396826850043, "x1": 426.77362230088977, "y2": 778.094482421875}, "name": "2", "caption_text": "Table 2: Results on three different language pairs - The best proposed models (BiLSTM+Concat+uni) are significantly better (p-value < 0.001) than baseline models using paired bootstrap resampling (Koehn, 2004).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 690.0, "y1": 495.0, "x1": 462.0, "y2": 704.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9281650119358, "y1": 225.4840850830078, "x1": 100.00138812594943, "y2": 283.6334228515625}, "name": "3", "caption_text": "Table 3: Translation results for homographs and all words in our NMT vocabulary. We compare scores for baseline and our best proposed model on three different language pairs. Improvements are in italic. We performed bootstrap resampling for 1000 times: our best model improved more on homographs than all words in terms of either f1, precision, or recall with p < 0.05, indicating statistical significance across all measures.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 730.0, "y1": 86.0, "x1": 100.0, "y2": 226.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9280802408854, "y1": 499.15771484375, "x1": 100.00136693318684, "y2": 573.9124721950955}, "name": "4", "caption_text": "Table 4: Sample translations - for each example, we show sentence in source language (src), the human translated reference (ref), the translation generated by our best context-aware model (best), and the translation generated by baseline model (base). We also highlight the word with multiple senses in source language in bold, the corresponding correctly translated words in blue and wrongly translated words in red. The definitions of words in blue or red are in parenthesis.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 730.0, "y1": 86.0, "x1": 100.0, "y2": 498.0}, "page": 7, "dpi": 0}], "error": null, "pdf": "/work/host-output/4de6022683413af832cd91bcd5d9a68ba16ea58f/N18-1121.pdf", "dpi": 100}