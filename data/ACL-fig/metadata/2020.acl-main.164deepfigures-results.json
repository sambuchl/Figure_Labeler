{"raw_detected_boxes": [[], [], [], [], [{"x2": 713.0, "y1": 86.0, "x1": 115.0, "y2": 158.0}], [{"x2": 717.0, "y1": 95.0, "x1": 103.0, "y2": 357.0}], [{"x2": 652.0, "y1": 90.0, "x1": 113.0, "y2": 315.0}, {"x2": 721.0, "y1": 439.0, "x1": 106.0, "y2": 626.0}, {"x2": 364.0, "y1": 737.0, "x1": 133.0, "y2": 822.0}, {"x2": 694.0, "y1": 736.0, "x1": 460.0, "y2": 803.0}], [{"x2": 720.0, "y1": 86.0, "x1": 107.0, "y2": 484.0}], [], [], [], [{"x2": 725.0, "y1": 88.0, "x1": 431.0, "y2": 291.0}], [{"x2": 575.0, "y1": 86.0, "x1": 255.0, "y2": 203.0}, {"x2": 561.0, "y1": 255.0, "x1": 269.0, "y2": 330.0}, {"x2": 362.0, "y1": 432.0, "x1": 103.0, "y2": 971.0}, {"x2": 671.0, "y1": 442.0, "x1": 487.0, "y2": 499.0}, {"x2": 635.0, "y1": 630.0, "x1": 509.0, "y2": 974.0}], [{"x2": 722.0, "y1": 251.0, "x1": 108.0, "y2": 849.0}], [{"x2": 727.0, "y1": 204.0, "x1": 109.0, "y2": 403.0}, {"x2": 578.0, "y1": 686.0, "x1": 256.0, "y2": 900.0}]], "raw_pdffigures_output": {"regionless-captions": [{"figType": "Figure", "boundary": {"x2": 290.2705993652344, "y1": 719.9185791015625, "x1": 72.0, "y2": 749.8310546875}, "text": "Figure 5: On average, it takes much less text for raters to decide an excerpt is human-written than to decide an excerpt is machine-generated.", "name": "5", "page": 12}], "figures": [{"figType": "Figure", "name": "1", "captionBoundary": {"x2": 525.5433349609375, "y1": 271.4245300292969, "x1": 72.0, "y2": 313.29205322265625}, "imageText": ["(b)", "k40-1wordcond", "p0.96-1wordcond", "p1.0-1wordcond", "Fraction", "of", "BERT", "Discriminator", "Errors", "that", "are", "Machine-generated", "Labeled", "as", "Human-written", "2", "4", "8", "16", "32", "64", "96", "128", "160", "192", "Sequence", "length", "in", "tokens", "1", "0", "0.1", "0.2", "0.3", "0.4", "0.5", "0.6", "0.7", "0.8", "0.9", "(a)", "k40-1wordcond", "k40-nowordcond", "p0.96-1wordcond", "p0.96-nowordcond", "p1.0-1wordcond", "p1.0-nowordcond", "Accuracy", "of", "BERT", "Fine-tuned", "Discriminator", "Sequence", "length", "in", "tokens", "y", "ur", "ac", "A", "cc", "0", "32", "64", "96", "128", "160", "192", "100%", "95%", "90%", "85%", "80%", "75%", "70%", "65%", "60%", "55%", "50%"], "regionBoundary": {"x2": 516.0, "y1": 69.23784637451172, "x1": 75.95838928222656, "y2": 256.10699462890625}, "caption": "Figure 1: In (a), accuracy increases as the length of the sequences used to train the discriminator is increased. In (b), we see that the BERT fine-tuned discriminator predicts about the same number of false-positives as falsenegatives when trained with samples generated using top-p sampling. However, for top-k, it more often mistakes machine-generated text to be human-written, while for untruncated random sampling the opposite is the case.", "page": 5}, {"figType": "Figure", "name": "6", "captionBoundary": {"x2": 525.5474853515625, "y1": 304.3755187988281, "x1": 72.0, "y2": 322.3330078125}, "imageText": [], "regionBoundary": {"x2": 531.0, "y1": 143.8900146484375, "x1": 72.0, "y2": 291.8900146484375}, "caption": "Figure 6: The interface of the task used for human evaluation. Each time the user presses next, the passage\u2019s length is doubled. On the left, we show the first step of evaluation, on the right, the second to last.", "page": 14}, {"figType": "Figure", "name": "7", "captionBoundary": {"x2": 525.5472412109375, "y1": 661.1825561523438, "x1": 72.0, "y2": 679.1409912109375}, "imageText": [], "regionBoundary": {"x2": 416.0, "y1": 493.8900146484375, "x1": 181.0, "y2": 648.8900146484375}, "caption": "Figure 7: For some of the questions, the text \u201dDear AMT Worker: to show you\u2019re reading, please select definitely [X] for this one.\u201d was inserted into the last text segment, and \u201dDid you read carefully?\u201d was appended to the end.", "page": 14}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 290.2705993652344, "y1": 604.026611328125, "x1": 72.0, "y2": 657.8500366210938}, "imageText": ["Tr", "ai", "n", "top-k", "90.1", "57.1", "43.8", "nucleus", "79.1", "81.3", "78.4", "random", "47.8", "63.7", "81.7", "mixed", "88.7", "74.2", "72.2", "Eval", "top-k", "nucleus", "random"], "regionBoundary": {"x2": 267.0, "y1": 529.8900146484375, "x1": 96.0, "y2": 591.8900146484375}, "caption": "Table 2: Accuracy of BERT fine-tuned discriminator when trained on samples from one strategy (rows) and evaluated on another (columns). Trained on samples with 192 tokens. The \u2018mixed\u2019 dataset is one containing an equal portion of samples from each strategy.", "page": 6}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 525.5479125976562, "y1": 464.94854736328125, "x1": 72.0, "y2": 506.81597900390625}, "imageText": ["(a)", "(b)", "(c)", "9&!B#H8<C180C", "I!\"+(B#H8<C180C", "I#\"!B#H8<C180C", ";<=1578028>2?=5-<2@<<8<:256=52=<-2A=1670-B", "4-0-<=5-C2D=E-3-C2=:2F/G=0BH<755-0", "#(", "%$", "(&", "#$*", "#+$", ",-./-01-23-04562702589-0:", "!", "!\"#", "!\"$", "!\"%", "!\"&", "!\"'", "!\"(", "!\")", "!\"*", "!\"+", "#"], "regionBoundary": {"x2": 523.9608154296875, "y1": 313.8900146484375, "x1": 74.0, "y2": 448.52880859375}, "caption": "Figure 3: (a) and (b) show human rater accuracy of correctly identifying an excerpt as human-written or machinewritten, shown with 80% confidence internals, in (a), broken up by decoding strategy and in (b), overall. Accuracy increases as raters observe more tokens. (c) shows that for short excerpts, most rater mistakes are them incorrectly thinking machine-generated text is human written. The two errors types become more balanced at longer lengths.", "page": 6}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 525.5465087890625, "y1": 593.6655883789062, "x1": 307.2760009765625, "y2": 635.5339965820312}, "imageText": ["Tr", "ai", "n", "top-k", "60.9", "27.9", "14.5", "nucleus", "49.2", "51.7", "48.9", "random", "7.3", "22.6", "38.3", "Eval", "top-k", "nucleus", "random"], "regionBoundary": {"x2": 502.0, "y1": 529.8900146484375, "x1": 331.0, "y2": 581.8900146484375}, "caption": "Table 3: Average probability of \u2018machine-generated\u2019 according to each length-192 discriminator. The expected in-domain probability is 0.5. One token of conditioning.", "page": 6}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 525.5454711914062, "y1": 244.14553833007812, "x1": 71.99998474121094, "y2": 297.96807861328125}, "imageText": ["(b)", "p1.0-1wordcond", "k40-1wordcond", "p0.96-1wordcond", "webtext", "ke", "n", "s", "ll", "to", "%", "a", "100%", "80%", "60%", "40%", "20%", "0%", "k", "most", "common", "unique", "tokens", "0", "500", "1000", "1500", "2000", "2500", "(a)", "p0.96-nowordcond", "p0.96-1wordcond", "Mean", "k", "Chosen", "at", "each", "Position", "during", "Generation", "with", "Nucleus", "Sampling", "4000", "4500", "k", "3500", "3000", "2500", "2000", "1500", "1000", "500", "Position", "in", "sequence", "0", "50", "100", "150", "200"], "regionBoundary": {"x2": 469.11126708984375, "y1": 65.34849548339844, "x1": 80.90641021728516, "y2": 228.8280029296875}, "caption": "Figure 2: In (a), the average (over sequences in the test set) k chosen at each step during generating with nucleus sampling is plotted. Adding a single word of priming strongly impacts the ks chosen for the first few positions, but this difference quickly dissipates. In (b), we consider the first token generated in each sequence by top-k, and plot what fraction of these are captured by the k most common unique tokens from the vocabulary. Overall, at its first step, top-k concentrates 80% of its probability mass in the 500 most common tokens from the vocabulary.", "page": 6}, {"figType": "Table", "name": "9", "captionBoundary": {"x2": 525.5473022460938, "y1": 623.173583984375, "x1": 72.0, "y2": 641.1309814453125}, "imageText": ["Human", "I", "recently", "got", "the", "chance", "to", "try", "the", "new", "Oil", "Essentials", "line.", "With", "six", "potent", "blends", "to", "choose", "from\u2013at", "$13", "each\u2013these", "cute", "little", "bottles", "offer", "a", "great,", "affordable", "way", "to", "partake", "in", "the", "skin", "and", "hair", "care", "oil", "craze.", "I", "tested", "each", "product", "in", "the", "line,", "massaging", "them", "onto", "my", "face", "every", "night", "before", "bed", "and", "running", "any", "leftover", "oil", "through", "my", "hair", "to", "tame", "frizziness.", "You", "could", "also", "add", "a", "few", "drops", "to", "your", "bath,", "favorite", "moisturizer,", "or", "even", "your", "shampoo", "and", "conditioner.", "Here\u2019s", "a", "quick", "rundown", "of", "each", "oil.", "Revitalize:", "Omega", "3,", "6,", "9", "&", "Evening", "Primrose", "This", "was", "the", "\ufb01rst", "one", "I", "tried", "(I", "went", "in", "ROYGBIV", "order", "to", "keep", "things", "straight)", "and", "my", "\ufb01rst", "impression", "was", "that", "it", "smells", "lovely", "but", "a", "little", "strong.", "The", "fragrance", "smells", "genuinely", "like", "\ufb02owers.", "Machine", "Red", "Lanterns,", "the", "lead", "exposure", "to", "a", "movie", "starring", "the", "Batman", "solo", "movie", "alum", "Margot", "Robbie", "taken", "under", "Wonder", "Woman\u2019s", "wing", "have", "reignited", "that", "rivalry", "with", "their", "whispery", "premiere.", "They", "played", "it", "as", "much", "as", "they", "possibly", "could,", "even", "though", "people", "who", "didn\u2019t", "ever", "watch", "Justice", "League", "or", "might", "have", "missed", "it", "waiting", "in", "line", "for", "the", "theater", "were", "still", "talking", "about", "as", "I", "spilled", "coffee.", "The", "gist?", "An", "overextended", "(OK,", "a", "sore)", "Adam", "West", "\ufb01lms", "set", "up", "a", "Legion", "of", "Super-Heroes", "situation.", "How", "aggro?", "Super", "laws", "and", "paramilitary", "groups", "watch", "over", "the", "world\u2019s", "superheroes,", "which", "is", "a", "mix", "of", "that", "schtick", "ending,", "Planet", "Of", "The", "Apes", "II", "bit,", "and", "the", "Batman/Venom", "bit", "of", "last", "appeared", "in", "The", "Seventh", "Seal", "when", "Chris", "O\u2019Donnell", "in\ufb01ltrated", "one", "of", "the", "teams", "at", "some", "point,", "also", "wearing", "Staff.", "Machine", "He", "is", "considered", "to", "be", "the", "most", "terrifying", "man", "on", "the", "planet", "and", "people", "stay", "away", "from", "him.", "A", "guy", "asks", "him", "to", "do", "something", "and", "he", "says,", "\u201dMy", "girlfriend\u2019s", "so", "important", "to", "me...", "I", "don\u2019t", "need", "to", "\ufb01ght", "her", "any", "more.\u201d", "And", "then,", "boom,", "there\u2019s", "some", "in", "a", "corner", "crying", "inappropriately.", "Men:", "It\u2019s", "gone", "in", "\ufb01ve", "minutes.", "Why", "do", "I", "have", "to", "be", "so", "sad?", "It\u2019s", "cute,\u201d", "says", "female", "member,", "who", "asks", "to", "remain", "anonymous.", "\u201dIt\u2019s", "what", "grew", "up", "to", "drive", "me", "crazy", "when", "I", "was", "a", "kid,", "seeing", "these", "women", "become", "the", "nurturing,", "wealthy", "things", "they", "are", "in", "this", "professional", "world", "I", "truly", "love.\u201d", "And", "it\u2019s", "nothing", "to", "do", "with", "her", "success.", "These", "men", "still", "actively", "fear", "being", "around", "the", "idea", "of", "a", "woman", "who", "might", "win", "Oscars,", "make", "movies", "or", "be", "audacious", "drivers.", "Human", "Dropbox", "and", "Google", "Drive", "are", "very", "different", "services", "that", "appeal", "to", "different", "users.", "While", "Drive", "is", "connected", "to", "the", "entire", "Google", "Apps", "(now", "known", "as", "G", "Suite)", "ecosystem,", "Dropbox", "is", "a", "lightweight,", "simple", "alternative", "for", "\ufb01le", "storage.", "While", "both", "are", "useful,", "users", "need", "to", "look", "beyond", "features,", "and", "make", "sure", "the", "service", "they", "choose", "can", "adequately", "protect", "their", "data.", "Here\u2019s", "how", "Dropbox", "encryption", "and", "Google", "Drive", "encryption", "stack", "up.", "Dropbox", "and", "Google", "Drive", "Encryption", "To", "their", "credit,", "both", "Dropbox", "and", "Google", "Drive", "protect", "user", "\ufb01les", "with", "encryption.", "Both", "also", "allow", "users", "to", "enable", "two-step", "veri\ufb01cation,", "which", "requires", "an", "extra", "code", "texted", "to", "the", "user\u2019s", "phone", "to", "access", "the", "account,", "making", "it", "harder", "for", "hackers", "to", "access", "a", "user\u2019s", "data.", "Human", "EVE", "Isk", "Per", "Hour(Eveiph)", "is", "hands", "down", "the", "best", "tool", "I\u2019ve", "ever", "used", "to", "make", "isk", "in", "New", "Eden.", "It", "is", "a", "market", "helper", "program", "that", "is", "able", "to", "do", "a", "great", "deal", "of", "the", "work", "that", "is", "typically", "done", "by", "a", "traders", "spreadsheet.", "I\u2019ve", "used", "it", "to", "go", "from", "a", "200m/month", "trading", "income", "to", "3b/month", "on", "my", "main", "trading", "character.", "Above", "you", "can", "see", "the", "blueprint", "manufacturing", "page", "which", "is", "located", "on", "the", "\ufb01rst", "tab", "of", "Eveiph.", "Here", "you", "can", "see", "the", "components", "required", "to", "make", "an", "item,", "the", "settings", "for", "the", "blueprint,", "and", "a", "brief", "market", "analysis", "of", "what", "you", "can", "expect", "to", "make", "manufacturing", "the", "item", "and", "selling", "it", "at", "the", "market", "you\u2019ve", "selected.", "You", "can", "enter", "the", "amount", "of", "runs", "you", "want", "to", "make,", "the", "ME", "and", "PE", "of", "your", "blueprint", "and", "click", "add", "to", "shopping", "list,", "and", "it", "will", "be", "added", "to", "a", "list", "of", "items", "to", "purchase", "when", "you", "are", "next", "at", "a", "trade", "hub.", "Machine", "So,", "not", "only", "was", "the", "speech", "a", "thoroughly", "mediocre", "diatribe", "about", "what", "he", "now", "thinks", "we", "should", "do", "for", "the", "next", "45", "minutes,", "but", "also", "how", "much", "credit", "we", "should", "give", "to", "Mumford", "and", "Sons", "for", "bringing", "Obama", "to", "the", "campaign", "trail.", "Behold:", "At", "the", "DNC,", "we", "drew", "strength", "from", "something", "even", "more", "powerful", "than", "the", "power", "of", "words.", "We", "drew", "strength", "from", "the", "power", "of", "families", "in", "this", "country.", "We", "drew", "strength", "from", "the", "power", "of", "family", "values.", "We", "drew", "strength", "from", "the", "power", "of", "a", "common", "purpose\u2013We", "drew", "strength", "from", "our", "shared", "commitment", "to", "\ufb01ghting", "against", "everything", "that", "undermines", "our", "potential", "in", "this", "country", "and", "our", "freedom.", "It", "is", "with", "that", "same", "conviction", "that", "we", "launch", "this", "campaign", "today", "and", "we", "urge", "every", "American", "in", "America", "to", "join", "us", "tonight.", "To", "allow", "the", "same", "attempt", "to", "succeed", "in", "this", "election.", "Machine", "The", "year", "is", "twenty-eight,", "and", "the", "boy", "is", "Harry,", "the", "sixth", "year", "at", "Hogwarts", "School", "of", "Witchcraft", "and", "Wizardry.", "He", "can\u2019t", "walk", "without", "spells", "covering", "his", "feet", "(or", "in", "his", "case,", "his", "feet", "are", "so", "badly", "burned", "that", "he,", "for", "practical", "purposes,", "can\u2019t", "even", "walk", "for", "that", "long", "without", "them)", "and", "he\u2019s", "just", "starting", "to", "feel", "more", "secure", "about", "things.", "This", "is", "a", "pretty", "dull", "aspect", "of", "the", "book,", "I\u2019d", "say.", "They", "probably", "spent", "way", "too", "much", "time", "on", "the", "fact", "that", "he", "can\u2019t", "use", "the", "stick", "of", "silver", "from", "his", "wand,", "despite", "his", "friends", "bewitching", "all", "the", "knives", "they", "had.", "Harry", "had", "been", "having", "some", "dif\ufb01culty", "getting", "to", "sleep", "until", "Hermione", "pulled", "him", "out", "of", "his", "state", "of", "near-death-conversation.", "Thanks", "to", "Hermione\u2019s", "meddling,", "he\u2019s", "gotten", "some", "sleep", "for", "the", "past", "two", "days.", "They", "also", "learnt", "a", "fair", "amount", "about", "getting", "used", "to", "his", "new", "surroundings.", "Machine", "Coincidentally,", "just", "a", "few", "days", "after", "the", "\ufb01rst", "tweet", "came", "out,", "a", "fellow", "named", "Kevin", "McReynolds", "sent", "out", "an", "interview", "with", "GQ", "to", "promote", "their", "upcoming", "issue.", "McReynolds", "describes", "himself", "as", "\u201da", "conservative", "Catholic\u201d", "who", "\u201dcannot", "fathom", "this", "guy", "being", "a", "real", "person", "and", "should", "be", "ashamed", "that", "he", "was", "able", "to", "be", "elected", "president.\u201d", "It\u2019s", "true.", "If", "you", "believe", "Hillary", "Clinton", "gave", "away", "20", "percent", "of", "the", "American", "Uranium", "to", "Russia,", "then", "you", "should", "be", "ashamed", "that", "you", "voted", "for", "Trump.", "No", "one", "should", "be", "able", "to", "give", "or", "receive", "anything", "that\u2019s", "not", "supposed", "to,", "so", "long", "as", "they", "have", "a", "warrant.", "If", "you\u2019ve", "been", "in", "a", "relationship", "for", "more", "than", "six", "months", "with", "a", "person", "who\u2019s", "also", "convicted", "of", "being", "a", "felon", "(or", "convicted", "of", "stealing),", "that\u2019s", "just", "stupid,", "especially", "as", "a", "married", "man.", "If", "you\u2019re", "married", "to", "someone", "convicted", "of", "a", "crime,", "and", "they", "go", "on", "their", "honeymoon", "with", "you,", "that\u2019s", "a", "felony,", "not", "a", "honeymoon.", "Human", "CHIP", "DESIGNER", "Texas", "Instruments", "unveiled", "a", "family", "of", "system", "on", "chip", "(SoC)", "processors", "aimed", "at", "automakers", "today,", "which", "are", "designed", "for", "use", "in", "self-driving", "cars.", "Named", "the", "TDA2x,", "the", "SoC", "family", "integrates", "safety", "features,", "such", "as", "aiding", "auto", "designers", "to", "create", "advanced", "driver", "assistance", "systems", "(ADAS),", "which", "in", "turn", "help", "\u201dreduce", "the", "number", "of", "collisions", "on", "the", "road", "and", "enable", "autonomous", "driving", "experiences\u201d.", "\u201dTDA2x", "device", "family", "combines", "an", "optimal", "mix", "of", "high", "performance,", "vision", "analytics,", "video,", "graphics", "and", "general", "purpose", "processing", "cores", "in", "a", "low", "power", "envelope,", "enabling", "a", "broad", "range", "of", "ADAS", "applications", "including", "front", "camera,", "surround", "view", "and", "sensor", "fusion,\u201d", "Texas", "Instruments", "said", "in", "its", "release.", "Machine", "Description", "This", "classic", "blend", "of", "coffee,", "cream,", "and", "sugar", "is", "the", "perfect", "drink!", "It", "is", "a", "smooth", "and", "creamy", "coffee", "with", "hints", "of", "cream", "and", "sweet", "sugar", "that", "can", "be", "enjoyed", "even", "after", "a", "full", "day", "of", "work", "or", "playing!", "The", "sugar", "provides", "a", "wonderful", "texture", "to", "the", "coffee", "beans,", "so", "that", "it", "can", "be", "scooped", "out", "into", "a", "cup.", "Available", "in", "four", "\ufb02avours:", "vanilla", "cream,", "caramel", "cream,", "coffee", "creme,", "and", "chocolate", "cream.", "Note:", "Coffee", "can", "be", "prepared", "in", "less", "than", "120", "minutes.", "Note:", "Serves", "one."], "regionBoundary": {"x2": 520.0, "y1": 180.8900146484375, "x1": 78.0, "y2": 610.8900146484375}, "caption": "Table 9: The 10 examples that \u201cexpert\u201d raters were guided through before they were asked to perform the detection task. These are hand-selected to showcase the spectrum of generated text and human-written text.", "page": 13}, {"figType": "Table", "name": "6", "captionBoundary": {"x2": 525.5473022460938, "y1": 256.4575500488281, "x1": 72.0, "y2": 274.4150390625}, "imageText": ["#", "Annotations", "Expert", "Raters", "AMT", "Workers", "webtext", "239", "450", "k0-1wordcond", "87", "150", "k40-1wordcond", "75", "150", "p0.96-1wordcond", "74", "150", "total", "machine", "236", "450"], "regionBoundary": {"x2": 404.0, "y1": 179.8900146484375, "x1": 193.0, "y2": 244.8900146484375}, "caption": "Table 6: The number of human annotations collected. In total, there were 50 examples from each sampling strategy and 150 examples of web text. Each example was shown to at most three raters.", "page": 12}, {"figType": "Table", "name": "7", "captionBoundary": {"x2": 525.5465087890625, "y1": 371.8205871582031, "x1": 307.2760009765625, "y2": 401.7330322265625}, "imageText": ["top-k", "=", "40", "88.06", "0.59", "top-p", "=", "0.96", "74.4", "0.76", "Dataset", "\u00b5", "\u03c3", "random", "sampling", "72.47", "1.02"], "regionBoundary": {"x2": 485.0, "y1": 317.8900146484375, "x1": 348.0, "y2": 359.8900146484375}, "caption": "Table 7: Average (\u00b5) and standard deviation (\u03c3) of accuracy on out-of-domain datasets across five runs of automatic discriminator finetuning.", "page": 12}, {"figType": "Table", "name": "8", "captionBoundary": {"x2": 525.5465698242188, "y1": 696.9925537109375, "x1": 307.2760009765625, "y2": 738.8600463867188}, "imageText": ["0.0%", "2", "0.0%", "1", "100.0%", "1", "0.0%", "1", "100.0%", "9", "50.0%", "8", "60.0%", "5", "100.0%", "5", "100.0%", "2", "Accuracy", "Count", "61.3%", "83", "57.8%", "51", "66.7%", "51", "69.8%", "51", "79.5%", "48", "84.6%", "40", "82.4%", "39", "65.6%", "36", "78.1%", "34", "84.0%", "26", "58.8%", "18", "92.3%", "14", "90.0%", "11"], "regionBoundary": {"x2": 460.0, "y1": 453.8900146484375, "x1": 373.0, "y2": 684.8900146484375}, "caption": "Table 8: Our expert rater pool consisted of 22 raters. The average accuracy of each rater on the longest excerpt length (192 tokens) is shown here along with the total number of excerpts they annotated.", "page": 12}, {"figType": "Table", "name": "5", "captionBoundary": {"x2": 445.6759338378906, "y1": 158.82351684570312, "x1": 151.86900329589844, "y2": 164.82598876953125}, "imageText": ["Method", "#", "train", "#", "valid", "#", "test", "large-744M-k40-1wordcond", "211148", "4226", "4191", "large-744M-k40-nocond", "218825", "4362", "4360", "large-744M-p0.96-1wordcond", "210587", "4248", "4208", "large-744M-p0.96-nocond", "209390", "4174", "4185", "large-744M-p1.0-1wordcond", "209334", "4169", "4173", "large-744M-p1.0-nocond", "208219", "4187", "4168", "human-written", "201344", "4031", "4030"], "regionBoundary": {"x2": 414.0, "y1": 62.8900146484375, "x1": 183.0, "y2": 146.8900146484375}, "caption": "Table 5: The number of excerpts used for training, validation, and testing.", "page": 12}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 525.5472412109375, "y1": 360.6665344238281, "x1": 72.0, "y2": 402.5339660644531}, "imageText": ["Truth", "Raters", "p1.0", "k40", "p0.96", "Truth", "Raters", "p1.0", "k40", "p0.96", "M", "M", "H", "-", "-", "M", "M", "-", "-", "H", "First", "off,", "this", "thread", "has", "done", "a", "pretty", "good", "job", "of", "describing", "in", "detail", "yet", "another", "broken", "touchscreen.", "That\u2019s", "the", "difference", "between", "a", "smartphone", "and", "a", "PC", "with", "no", "prying", "eyes", "having", "to", "snap", "shots", "for", "the", "police", "to", "\ufb01nd.", "\u00b6What", "I", "would", "like", "to", "address", "is", "the", "mindset", "that", "generally", "surrounds", "Chrome", "OS", "users.", "To", "me", "this", "is", "analogous", "to", "saying", "that", "Apple", "does\u201chate", "their", "Windows\u201d,", "or", "that", "HP", "does\u201chate", "their", "Macs\u201d", "as", "if", "http://twitter.com/)", "(and", "that", "quote", "is", "from", "two", "years", "ago),", "that", "anyone", "who", "covers", "smartphones", "and", "tablets", "from", "a", "\u201cPC\u201d", "perspective", "is", "just", "jealous.", "\u00b6Chrome", "OS", "is", "for", "browsing", "the", "web,", "PC", "processors", "can", "do", "stronger", "things", "in", "that", "regard,", "Windows", "is", "a", "juggernaut", "on", "those", "fronts.", "This", "is", "how", "I", "see", "it.", "Yes,", "it", "can", "be", "slow.", "And", "yes,", "you", "need", "a", "fast", "CPU", "FOR", "ALABAMA,", "GOOD", "WEEKS", "\u00b6AND", "A", "TOUR", "OF", "CAIRO", "\u00b6THE", "ALABAMA", "COMMITTEE", "ON", "THE", "STUDY", "OF", "THE", "AMERICAN", "SECURITY", "AGENDA,", "\u00b6Amer-", "ica\u2019s", "future", "has", "been", "mapped", "out", "in", "carved", "stone.", "Metro", "Atlanta\u2019s", "last", "US", "congressman,", "Bill", "Posey,", "was", "a", "inextricable", "integral", "element", "of", "the", "Citadel", "project", "as", "it", "became", "another", "metaphor", "for", "Atlanta\u2019s", "transformation", "from", "an", "industry", "backwater", "into", "the", "\ufb01nance", "and", "infor-", "mation", "hub", "of", "the", "nation\u2019s", "capital.", "Meanwhile,", "Cobb", "County", "\u2013", "Atlanta\u2019s", "geode", "of", "change", "\u2013", "is", "home", "to", "some", "of", "the", "largest", "industrial", "parks", "in", "the", "South,", "a", "regional", "cultural", "center,", "a", "100-", "year-old", "manufacturing", "town", "and", "a", "potent", "symbol", "of", "the", "former", "city\u2019s", "cherished", "Georgian", "past.", "The", "gentry", "still", "live", "there,", "the", "defunct", "industrial", "landscapes", "carry", "the", "names", "of", "Truth", "Raters", "p1.0", "k40", "p0.96", "Truth", "Raters", "p1.0", "k40", "p0.96", "M", "H", "-", "-", "M", "M", "H", "-", "M", "-", "Exidentia", "at", "Eurnari,", "is", "an", "upcoming", "Cryptopia", "event", "which", "is", "currently", "still", "in", "devel-", "opment.", "Be", "a", "part", "of", "the", "\ufb01rst", "live", "stream", "of", "this", "year\u2019s", "event", "on", "15-16", "January", "2016!", "\u00b6Since", "the", "release", "of", "v1.22,", "Exidentia", "has", "received", "a", "fair", "amount", "of", "user", "feedback.", "This", "event", "takes", "place", "in", "the", "underwater", "Cryptopia", "they", "have", "built.", "During", "this", "event,", "you", "will", "learn", "about", "the", "ocean", "and", "areas", "around", "it,", "and", "be", "reached", "by", "a", "treasure", "hunter", "that", "helps", "you", "explore", "the", "different", "areas.", "\u00b6There", "will", "be", "six", "different", "levels", "in", "this", "event", "that", "you", "will", "become", "acquainted", "with:", "thought", "Polar", "Lava,", "Ocean", "Seared", "Cones", "and", "Celestine", "Floors,", "Sea", "Damaged", "Aerie", "Bricks,", "coast", "Puddle", "(congipit", "stopping", "at", "red", "water),", "Shaikh", "Swamp", "and", "Bugmite.", "At", "rotating", "points,", "you", "will", "learn", "how", "to", "access", "various", "types", "of", "creatures", "Ever", "since", "the", "opening", "of", "the", "North", "American", "College", "of", "Art", "Education", "in", "1990,", "the", "demand", "for", "art", "education", "in", "America", "has", "grown", "steadily,", "and", "in", "recent", "years", "we", "have", "seen", "the", "rise", "of", "students", "that", "pursue", "art", "education", "not", "in", "the", "classroom", "but", "at", "art", "academies.", "This", "year", "saw", "another", "50", "percent", "increase", "in", "the", "number", "of", "art", "academies", "in", "the", "United", "States", "offering", "courses", "\u2013", "with", "an", "additional", "10", "percent", "of", "students", "in", "2017", "taking", "art.", "\u00b6Some", "major", "changes", "have", "occurred", "in", "recent", "years", "with", "regard", "to", "the", "art", "curriculum", "and", "the", "way", "students", "learn,", "and", "we", "will", "explore", "each", "of", "these", "in", "coming", "months", "as", "we", "look", "at", "the", "various", "forms", "of", "art", "education.", "There", "is", "no", "one-size-\ufb01ts-all", "approach", "for", "this", "or", "any", "other", "\ufb01eld", "of", "study,", "and", "students", "who", "begin", "a", "course", "in", "art", "education", "may", "change", "their", "plans", "based", "on", "what", "they", "see", "that", "course,", "including", "what", "lessons", "they", "have", "completed", "and", "the", "resources", "available,", "to", "create", "meaningful", "experiences", "of", "artistic", "creation.", "\u00b6One", "important", "area", "Image", "copyright", "Getty", "Images", "Image", "caption", "Women", "mourn", "over", "the", "cof\ufb01n", "of", "one", "of", "the", "victim\u2019s", "of", "Sunday\u2019s", "bombing", "in", "Ankara", "\u00b6Who\u2019d", "be", "in", "Turkey\u2019s", "shoes", "right", "now?", "\u00b6Since", "July", "last", "year,", "hundreds", "of", "soldiers", "and", "civilians", "have", "been", "killed", "in", "terrorist", "attacks.", "Suicide", "bombs", "have", "torn", "into", "crowds", "of", "demonstrators", "and", "tourists.", "Military", "convoys", "have", "been", "targeted", "in", "the", "heart", "of", "the", "capital.", "\u00b6A", "long-running", "Kurdish", "insurgency,", "once", "thought", "to", "be", "close", "to", "resolution", "after", "years", "of", "painstaking", "efforts", "to", "build", "bridges,", "has", "erupted", "once", "more.", "\u00b6The", "country", "is", "awash", "with", "Syrian", "and", "other", "refugees.", "The", "government", "has", "been", "under", "pressure", "to", "stop", "them", "moving", "on", "into", "Europe", "and", "prevent", "would-be", "jihadis", "travelling", "the", "other", "way.", "\u00b6How", "dangerous", "is", "Turkey\u2019s", "unrest?", "\u00b6Tears", "and", "destruction", "amid", "PKK", "crackdown", "\u00b6Turkey", "v", "Islamic", "State", "v", "the", "Kurds", "Truth", "Raters", "p1.0", "k40", "p0.96", "Truth", "Raters", "p1.0", "k40", "p0.96", "H", "M", "H", "H", "M", "H", "H", "M", "M", "M", "EDIT:OKAY!,", "I", "guess", "that\u2019ll", "work", "for", "now.", ">", "http://www.teamfortress.com/", "and", "then", "go", "buy", "the", "game", "and", "experience", "some", "of", "the", "best", "online", "gaming", "I", "have", "ever", "played.", "\u02c6", "\u02c6Both", "girls", "had", "a", "really", "fun", "time", "and", "I", "had", "a", "GREAT", "time", "making", "both", "of", "these", "costumes.", "Everything", "was", "altered", "even", "a", "little", "bit(dying", "the", "pants", "a", "darker", "grey", "and", "painting", "the", "boots", "and", "shirts)", "But", "my", "piece", "de", "resistance", "would", "have", "to", "be", "my", "eldest\u2019s", "Medi-Gun.If", "you", "have", "any", "questions", "about", "the", "costumes,", "I", "would", "be", "happy", "to", "assist", "you!Oh", "and", "here\u2019s", "a", "video", "of", "my", "daughter", "before", "the", "costume", "was", "completed.Thanks!"], "regionBoundary": {"x2": 522.0, "y1": 63.8900146484375, "x1": 76.0, "y2": 348.8900146484375}, "caption": "Table 4: Some 192-token examples where at least two expert raters agreed with each other, but were not in agreement with the automatic discriminators. The first row shows examples where the ground-truth was human-written, the second shows machine-generated examples where the corresponding discriminator guessed incorrectly, and the third shows machine-generated examples where the discriminator was correct, but raters got it wrong.", "page": 7}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 525.5465087890625, "y1": 221.96054077148438, "x1": 307.2760009765625, "y2": 263.82806396484375}, "imageText": [], "regionBoundary": {"x2": 522.0, "y1": 62.8900146484375, "x1": 310.0, "y2": 209.8900146484375}, "caption": "Figure 4: Number of votes expert raters made for each label as a function of number of tokens observed. As raters observe more tokens, their predictions become more confident.", "page": 11}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 525.54736328125, "y1": 126.14554595947266, "x1": 72.0, "y2": 179.96905517578125}, "imageText": ["BERT", "BagOfWords", "HistGLTRBuckets", "Hist50Buckets", "TotalProb", "Human", "Method", "acc", "AUC", "acc", "AUC", "acc", "AUC", "acc", "AUC", "acc", "acc", "k40-1wordcond", "0.88", "0.99", "0.79", "0.87", "0.52", "0.52", "0.69", "0.76", "0.61", "0.64", "p0.96-1wordcond", "0.81", "0.89", "0.60", "0.65", "0.53", "0.56", "0.54", "0.56", "0.63", "0.77", "p1.0-1wordcond", "0.79", "0.92", "0.59", "0.62", "0.53", "0.55", "0.54", "0.55", "0.65", "0.71"], "regionBoundary": {"x2": 515.0, "y1": 63.8900146484375, "x1": 83.0, "y2": 113.8900146484375}, "caption": "Table 1: Performance (accuracy and AUC) of the fine-tuned BERT classifier and several simple baselines on detecting length-192 sequences generated with one word of priming (1worccond). Note that p1.0 refers to untruncated random sampling, where we sample from 100% of the probability mass. The last column shows human performance on the same task where accuracy with a 50% baseline is computed by randomly pairing samples from each decoding strategy with a human-written sample.", "page": 4}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 729.9268934461805, "y1": 175.20214716593424, "x1": 100.0, "y2": 249.95702107747394}, "name": "1", "caption_text": "Table 1: Performance (accuracy and AUC) of the fine-tuned BERT classifier and several simple baselines on detecting length-192 sequences generated with one word of priming (1worccond). Note that p1.0 refers to untruncated random sampling, where we sample from 100% of the probability mass. The last column shows human performance on the same task where accuracy with a 50% baseline is computed by randomly pairing samples from each decoding strategy with a human-written sample.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 715.0, "y1": 86.0, "x1": 100.0, "y2": 175.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9212985568577, "y1": 376.978513929579, "x1": 100.0, "y2": 435.12785169813367}, "name": "1", "caption_text": "Figure 1: In (a), accuracy increases as the length of the sequences used to train the discriminator is increased. In (b), we see that the BERT fine-tuned discriminator predicts about the same number of false-positives as falsenegatives when trained with samples generated using top-p sampling. However, for top-k, it more often mistakes machine-generated text to be human-written, while for untruncated random sampling the opposite is the case.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 717.0, "y1": 95.0, "x1": 102.0, "y2": 358.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9242655436198, "y1": 339.09102545844183, "x1": 99.99997880723741, "y2": 413.84455362955725}, "name": "2", "caption_text": "Figure 2: In (a), the average (over sequences in the test set) k chosen at each step during generating with nucleus sampling is plotted. Adding a single word of priming strongly impacts the ks chosen for the first few positions, but this difference quickly dissipates. In (b), we consider the first token generated in each sequence by top-k, and plot what fraction of these are captured by the k most common unique tokens from the vocabulary. Overall, at its first step, top-k concentrates 80% of its probability mass in the 500 most common tokens from the vocabulary.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 652.0, "y1": 89.0, "x1": 109.0, "y2": 321.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9276563856337, "y1": 645.7618713378906, "x1": 100.0, "y2": 703.9110819498698}, "name": "3", "caption_text": "Figure 3: (a) and (b) show human rater accuracy of correctly identifying an excerpt as human-written or machinewritten, shown with 80% confidence internals, in (a), broken up by decoding strategy and in (b), overall. Accuracy increases as raters observe more tokens. (c) shows that for short excerpts, most rater mistakes are them incorrectly thinking machine-generated text is human written. The two errors types become more balanced at longer lengths.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 728.0, "y1": 436.0, "x1": 103.0, "y2": 626.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.1536102294922, "y1": 838.9258490668402, "x1": 100.0, "y2": 913.6806064181857}, "name": "2", "caption_text": "Table 2: Accuracy of BERT fine-tuned discriminator when trained on samples from one strategy (rows) and evaluated on another (columns). Trained on samples with 192 tokens. The \u2018mixed\u2019 dataset is one containing an equal portion of samples from each strategy.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 374.0, "y1": 736.0, "x1": 120.0, "y2": 839.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9257066514757, "y1": 824.5355394151476, "x1": 426.772223578559, "y2": 882.6861063639323}, "name": "3", "caption_text": "Table 3: Average probability of \u2018machine-generated\u2019 according to each length-192 discriminator. The expected in-domain probability is 0.5. One token of conditioning.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 697.0, "y1": 736.0, "x1": 460.0, "y2": 808.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9267239040798, "y1": 500.92574225531683, "x1": 100.0, "y2": 559.074952867296}, "name": "4", "caption_text": "Table 4: Some 192-token examples where at least two expert raters agreed with each other, but were not in agreement with the automatic discriminators. The first row shows examples where the ground-truth was human-written, the second shows machine-generated examples where the corresponding discriminator guessed incorrectly, and the third shows machine-generated examples where the discriminator was correct, but raters got it wrong.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 724.0, "y1": 86.0, "x1": 100.0, "y2": 501.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9257066514757, "y1": 308.2785288492838, "x1": 426.772223578559, "y2": 366.4278666178385}, "name": "4", "caption_text": "Figure 4: Number of votes expert raters made for each label as a function of number of tokens observed. As raters observe more tokens, their predictions become more confident.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 726.0, "y1": 86.0, "x1": 427.0, "y2": 308.0}, "page": 11, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 618.9943525526259, "y1": 220.58821784125433, "x1": 210.92917124430338, "y2": 228.92498440212674}, "name": "5", "caption_text": "Table 5: The number of excerpts used for training, validation, and testing.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 575.0, "y1": 86.0, "x1": 247.0, "y2": 220.0}, "page": 12, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9268086751301, "y1": 356.1910417344835, "x1": 100.0, "y2": 381.13199869791663}, "name": "6", "caption_text": "Table 6: The number of human annotations collected. In total, there were 50 examples from each sampling strategy and 150 examples of web text. Each example was shown to at most three raters.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 561.0, "y1": 250.0, "x1": 269.0, "y2": 339.0}, "page": 12, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.1536102294922, "y1": 999.8869154188368, "x1": 100.0, "y2": 1041.4320203993054}, "name": "5", "caption_text": "Figure 5: On average, it takes much less text for raters to decide an excerpt is human-written than to decide an excerpt is machine-generated.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 362.0, "y1": 427.0, "x1": 101.0, "y2": 972.0}, "page": 12, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9257066514757, "y1": 516.417482164171, "x1": 426.772223578559, "y2": 557.9625447591145}, "name": "7", "caption_text": "Table 7: Average (\u00b5) and standard deviation (\u03c3) of accuracy on out-of-domain datasets across five runs of automatic discriminator finetuning.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 681.0, "y1": 442.0, "x1": 481.0, "y2": 516.0}, "page": 12, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.925791422526, "y1": 968.0452134874132, "x1": 426.772223578559, "y2": 1026.1945088704426}, "name": "8", "caption_text": "Table 8: Our expert rater pool consisted of 22 raters. The average accuracy of each rater on the longest excerpt length (192 tokens) is shown here along with the total number of excerpts they annotated.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 652.0, "y1": 630.0, "x1": 492.0, "y2": 991.0}, "page": 12, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9268086751301, "y1": 865.5188666449652, "x1": 100.0, "y2": 890.4596964518229}, "name": "9", "caption_text": "Table 9: The 10 examples that \u201cexpert\u201d raters were guided through before they were asked to perform the detection task. These are hand-selected to showcase the spectrum of generated text and human-written text.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 722.0, "y1": 251.0, "x1": 100.0, "y2": 866.0}, "page": 13, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9270629882812, "y1": 422.7437761094835, "x1": 100.0, "y2": 447.68473307291663}, "name": "6", "caption_text": "Figure 6: The interface of the task used for human evaluation. Each time the user presses next, the passage\u2019s length is doubled. On the left, we show the first step of evaluation, on the right, the second to last.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 737.0, "y1": 199.0, "x1": 100.0, "y2": 406.0}, "page": 14, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9267239040798, "y1": 918.309105767144, "x1": 100.0, "y2": 943.2513766818576}, "name": "7", "caption_text": "Figure 7: For some of the questions, the text \u201dDear AMT Worker: to show you\u2019re reading, please select definitely [X] for this one.\u201d was inserted into the last text segment, and \u201dDid you read carefully?\u201d was appended to the end.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 583.0, "y1": 686.0, "x1": 252.0, "y2": 917.0}, "page": 14, "dpi": 0}], "error": null, "pdf": "/work/host-output/ffeca033c1048160e4dea2a72ea65c5c9dd65877/2020.acl-main.164.pdf", "dpi": 100}