{"raw_detected_boxes": [[{"x2": 730.0, "y1": 311.0, "x1": 427.0, "y2": 818.0}], [{"x2": 730.0, "y1": 100.0, "x1": 431.0, "y2": 708.0}], [], [{"x2": 718.0, "y1": 136.0, "x1": 429.0, "y2": 591.0}], [], [], [], [{"x2": 725.0, "y1": 273.0, "x1": 429.0, "y2": 534.0}, {"x2": 720.0, "y1": 718.0, "x1": 434.0, "y2": 850.0}], [{"x2": 724.0, "y1": 91.0, "x1": 431.0, "y2": 446.0}, {"x2": 394.0, "y1": 200.0, "x1": 106.0, "y2": 393.0}, {"x2": 401.0, "y1": 536.0, "x1": 100.0, "y2": 753.0}, {"x2": 723.0, "y1": 521.0, "x1": 432.0, "y2": 711.0}], [], [], [{"x2": 728.0, "y1": 94.0, "x1": 430.0, "y2": 465.0}, {"x2": 396.0, "y1": 581.0, "x1": 100.0, "y2": 796.0}, {"x2": 729.0, "y1": 533.0, "x1": 427.0, "y2": 1008.0}]], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Figure", "name": "1", "captionBoundary": {"x2": 528.2329711914062, "y1": 603.3785400390625, "x1": 306.73797607421875, "y2": 657.2020263671875}, "imageText": [], "regionBoundary": {"x2": 526.0, "y1": 221.8900146484375, "x1": 307.0, "y2": 591.8900146484375}, "caption": "Figure 1: An example from Visual Genome (https: //visualgenome.org/). The region-grounded captions provide useful clues to answer questions. For example, to answer Where are the cats?, orange and white cat laying on a wooden bench is informative.", "page": 0}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 525.5465698242188, "y1": 522.985595703125, "x1": 307.2760009765625, "y2": 564.85400390625}, "imageText": [], "regionBoundary": {"x2": 526.0, "y1": 61.8900146484375, "x1": 307.0, "y2": 510.8900146484375}, "caption": "Figure 2: Multimodal Neural Graph Memory Networks for VQA. The visual features/captions are extracted from white/black bounding-boxes and are used as node features to construct the visual/textual Graph Network.", "page": 1}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 527.29150390625, "y1": 397.9265441894531, "x1": 306.947021484375, "y2": 487.61492919921875}, "imageText": ["Test-dev", "Test-std", "Model", "Y/N", "Num", "Other", "All", "Y/N", "Num", "Other", "All", "MAN1", "-", "-", "-", "-", "79.2", "39.5", "52.6", "62.1", "Count2", "83.1", "51.6", "59.0", "68.1", "83.6", "51.4", "59.1", "68.4", "MFH3", "84.3", "50.7", "60.5", "68.8", "-", "-", "-", "-", "Buttom-Up4", "81.8", "44.2", "56.1", "65.3", "-", "-", "-", "65.7", "G-learner5", "-", "-", "-", "-", "82.9", "47.1", "56.2", "66.2", "v-AGCN6", "82.4", "45.9", "56.5", "65.9", "82.6", "45.1", "56.7", "66.2", "RAMEN7", "-", "-", "-", "66.0", "-", "-", "-", "-", "MuRel8", "84.8", "49.8", "57.9", "68.0", "-", "-", "-", "68.4", "VCTREE9", "84.3", "47.8", "59.1", "68.2", "84.6", "47.4", "59.3", "68.5", "BAN10", "85.4", "54.0", "60.5", "70.0", "-", "-", "-", "70.4", "ReGAT11", "86.1", "54.4", "60.3", "70.3", "-", "-", "-", "70.6", "LXRT12", "89.3", "56.9", "65.1", "74.2", "89.5", "56.7", "65.2", "74.3", "MSM@MSRA", "13", "89.8", "58.9", "65.4", "74.7", "89.8", "58.4", "65.7", "74.9", "MIL@HDU14", "90.1", "59.2", "65.7", "75.0", "90.4", "59.2", "65.8", "75.2", "MN+ResNet", "84.2", "43.4", "58.1", "67.3", "84.5", "44.0", "58.1", "67.5", "N-GMN", "86.1", "53.5", "61.2", "70.6", "86.7", "53.6", "61.8", "71.2", "MN-GMN\u2212", "88.0", "53.5", "63.8", "72.6", "88.5", "53.7", "64.2", "73.1", "MN-GMN", "88.2", "56.0", "64.2", "73.2", "88.3", "56.1", "64.5", "73.5"], "regionBoundary": {"x2": 524.0, "y1": 197.02992248535156, "x1": 309.0, "y2": 385.8900146484375}, "caption": "Table 1: Accuracy percentage on the VQA-v2.0 dataset. The references are Ma et al. (2018)1, Zhang et al. (2018)2, Yu et al. (2018)3, Teney et al. (2018); Anderson et al. (2018)4, Norcliffe-Brown et al. (2018)5, Yang et al. (2018)6, Shrestha et al. (2019)7, Cadene et al. (2019)8, Tang et al. (2019)9, Kim et al. (2018)10, Li et al. (2019)11, Tan and Bansal (2019)12, Liu et al. (2019)13, and Yu et al. (2019)14.", "page": 7}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 527.289794921875, "y1": 626.5166015625, "x1": 306.947021484375, "y2": 656.4290161132812}, "imageText": ["Model", "What", "Where", "When", "Who", "Why", "How", "Avg", "Human1", "96.5", "95.7", "94.4", "96.5", "92.7", "94.2", "95.7", "LSTM-ATT1", "51.5", "57.0", "75.0", "59.5", "55.5", "49.8", "54.3", "Concat+ATT", "2", "47.8", "56.9", "74.1", "62.3", "52.7", "51.2", "52.8", "MCB+ATT2", "60.3", "70.4", "79.5", "69.2", "58.2", "51.1", "62.2", "MAN3", "62.2", "68.9", "76.8", "66.4", "57.8", "52.9", "62.8", "MLP4", "64.5", "75.9", "82.1", "72.9", "68.0", "56.4", "67.1", "N-GMN", "66.2", "77.2", "83.3", "74.0", "69.2", "58.5", "68.6", "MN-GMN\u2212", "67.1", "77.4", "84.0", "75.1", "70.1", "59.2", "69.3", "MN-GMN", "67.3", "77.4", "84.0", "75.0", "70.3", "59.4", "69.5"], "regionBoundary": {"x2": 521.0, "y1": 514.317138671875, "x1": 312.0, "y2": 611.1439208984375}, "caption": "Table 2: Accuracy percentage on Visual7W dataset. The references are Zhu et al. (2015)1, Fukui et al. (2016)2, Ma et al. (2018)3, and Jabri et al. (2016)4.", "page": 7}, {"figType": "Figure", "name": "8", "captionBoundary": {"x2": 525.5465087890625, "y1": 741.3545532226562, "x1": 306.77801513671875, "y2": 759.31201171875}, "imageText": [], "regionBoundary": {"x2": 526.0, "y1": 378.8900146484375, "x1": 307.0, "y2": 729.8900146484375}, "caption": "Figure 8: Visualization of the attention weights for a 14\u00d7 14 grid of cells. Red regions get higher attention.", "page": 11}, {"figType": "Figure", "name": "6", "captionBoundary": {"x2": 287.12530517578125, "y1": 591.1965942382812, "x1": 75.14299774169922, "y2": 597.1990356445312}, "imageText": [], "regionBoundary": {"x2": 291.0, "y1": 416.8900146484375, "x1": 72.0, "y2": 578.8900146484375}, "caption": "Figure 6: Example VQA with N-GMN+ on CLEVR.", "page": 11}, {"figType": "Figure", "name": "7", "captionBoundary": {"x2": 525.7159423828125, "y1": 350.1665344238281, "x1": 307.2760009765625, "y2": 368.1239929199219}, "imageText": [], "regionBoundary": {"x2": 526.0, "y1": 61.8900146484375, "x1": 307.0, "y2": 337.8900146484375}, "caption": "Figure 7: The MN-GMN provides the correct answer using white and black tennis shoes.", "page": 11}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 525.5465087890625, "y1": 338.7395324707031, "x1": 306.77801513671875, "y2": 356.697998046875}, "imageText": [], "regionBoundary": {"x2": 523.0, "y1": 61.8900146484375, "x1": 309.0, "y2": 326.8900146484375}, "caption": "Figure 4: Visualization of the attention weights for a 14\u00d7 14 grid of cells. Red regions get higher attention.", "page": 8}, {"figType": "Figure", "name": "5", "captionBoundary": {"x2": 522.400390625, "y1": 526.0535888671875, "x1": 310.4179992675781, "y2": 532.0560302734375}, "imageText": [], "regionBoundary": {"x2": 526.0, "y1": 371.8900146484375, "x1": 307.0, "y2": 513.8900146484375}, "caption": "Figure 5: Example VQA with N-GMN+ on CLEVR.", "page": 8}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 290.2705383300781, "y1": 557.3955688476562, "x1": 72.0, "y2": 575.35302734375}, "imageText": [], "regionBoundary": {"x2": 291.0, "y1": 362.8900146484375, "x1": 72.0, "y2": 545.8900146484375}, "caption": "Figure 3: N-GMN versus MN-GMN. The MN-GMN provides the correct answer using a cloudy blue sky.", "page": 8}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 292.013427734375, "y1": 297.7095642089844, "x1": 71.67100524902344, "y2": 339.5780334472656}, "imageText": ["Model", "All", "Exist", "Count", "Cmp-Int", "Q-At", "Cmp-At", "HUMAN1", "92.6", "96.6", "86.7", "86.5", "95.0", "96.0", "Q-TYPE", "MODE1", "41.8", "50.2", "34.6", "51.0", "36.0", "51.3", "LSTM1", "46.8", "61.1", "41.7", "69.8", "36.8", "51.3", "CNN+BOW1", "48.4", "59.5", "38.9", "51.1", "48.3", "51.8", "CNN+LSTM1", "52.3", "65.2", "43.7", "67.1", "49.3", "53.0", "CNN+LSTM+MCB1", "51.4", "63.4", "42.1", "66.4", "49.0", "51.0", "CNN+LSTM+SA1", "68.5", "71.1", "52.2", "73.5", "85.3", "52.3", "N2NMN2", "83.3", "85.7", "68.5", "85.0", "90.0", "88.8", "CNN+LSTM+RN3", "95.5", "97.8", "90.1", "93.6", "97.9", "97.1", "PROGRAM-GEN4", "96.9", "97.1", "92.7", "98.7", "98.2", "98.9", "RAMEN5", "96.9", "98.9", "94.1", "88.5", "98.9", "99.3", "NS-VQA6", "99.8", "99.9", "99.7", "99.9", "99.8", "99.8", "N-GMN\u2212", "95.6", "97.7", "90.3", "93.5", "98.0", "97.3", "N-GMN+", "96.3", "98.0", "91.8", "94.8", "98.1", "98.1"], "regionBoundary": {"x2": 289.0, "y1": 142.6402587890625, "x1": 73.0, "y2": 282.496826171875}, "caption": "Table 3: Accuracy on CLEVR dataset. The references are Johnson et al. (2017a)1, Hu et al. (2017)2, Santoro et al. (2017)3, Johnson et al. (2017b)4, Shrestha et al. (2019)5, and Yi et al. (2018)6.", "page": 8}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 733.6569044325087, "y1": 838.0257500542534, "x1": 426.02496676974823, "y2": 912.7805921766493}, "name": "1", "caption_text": "Figure 1: An example from Visual Genome (https: //visualgenome.org/). The region-grounded captions provide useful clues to answer questions. For example, to answer Where are the cats?, orange and white cat laying on a wooden bench is informative.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 730.0, "y1": 309.0, "x1": 427.0, "y2": 821.0}, "page": 0, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.925791422526, "y1": 726.368882921007, "x1": 426.772223578559, "y2": 784.5194498697916}, "name": "2", "caption_text": "Figure 2: Multimodal Neural Graph Memory Networks for VQA. The visual features/captions are extracted from white/black bounding-boxes and are used as node features to construct the visual/textual Graph Network.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 730.0, "y1": 89.0, "x1": 427.0, "y2": 725.0}, "page": 1, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.3493109809027, "y1": 552.6757558186848, "x1": 426.3153076171875, "y2": 677.2429572211371}, "name": "1", "caption_text": "Table 1: Accuracy percentage on the VQA-v2.0 dataset. The references are Ma et al. (2018)1, Zhang et al. (2018)2, Yu et al. (2018)3, Teney et al. (2018); Anderson et al. (2018)4, Norcliffe-Brown et al. (2018)5, Yang et al. (2018)6, Shrestha et al. (2019)7, Cadene et al. (2019)8, Tang et al. (2019)9, Kim et al. (2018)10, Li et al. (2019)11, Tan and Bansal (2019)12, Liu et al. (2019)13, and Yu et al. (2019)14.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 728.0, "y1": 256.0, "x1": 429.0, "y2": 536.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.346937391493, "y1": 870.1619466145833, "x1": 426.3153076171875, "y2": 911.7069668240017}, "name": "2", "caption_text": "Table 2: Accuracy percentage on Visual7W dataset. The references are Zhu et al. (2015)1, Fukui et al. (2016)2, Ma et al. (2018)3, and Jabri et al. (2016)4.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 723.0, "y1": 713.0, "x1": 433.0, "y2": 850.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9257066514757, "y1": 470.47157287597656, "x1": 426.080576578776, "y2": 495.41388617621527}, "name": "4", "caption_text": "Figure 4: Visualization of the attention weights for a 14\u00d7 14 grid of cells. Red regions get higher attention.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 726.0, "y1": 87.0, "x1": 431.0, "y2": 453.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.57420518663196, "y1": 413.48550584581164, "x1": 99.54306284586588, "y2": 471.63615756564667}, "name": "3", "caption_text": "Table 3: Accuracy on CLEVR dataset. The references are Johnson et al. (2017a)1, Hu et al. (2017)2, Santoro et al. (2017)3, Johnson et al. (2017b)4, Shrestha et al. (2019)5, and Yi et al. (2018)6.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 401.0, "y1": 196.0, "x1": 102.0, "y2": 393.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.15352545844183, "y1": 774.1605122884114, "x1": 100.0, "y2": 799.1014268663195}, "name": "3", "caption_text": "Figure 3: N-GMN versus MN-GMN. The MN-GMN provides the correct answer using a cloudy blue sky.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 403.0, "y1": 519.0, "x1": 100.0, "y2": 755.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 725.5560980902777, "y1": 730.6299845377604, "x1": 431.1361100938585, "y2": 738.9667087131077}, "name": "5", "caption_text": "Figure 5: Example VQA with N-GMN+ on CLEVR.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 728.0, "y1": 516.0, "x1": 428.0, "y2": 713.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 730.1610310872395, "y1": 486.34240892198346, "x1": 426.772223578559, "y2": 511.2833234998915}, "name": "7", "caption_text": "Figure 7: The MN-GMN provides the correct answer using white and black tennis shoes.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 730.0, "y1": 89.0, "x1": 427.0, "y2": 469.0}, "page": 11, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 398.78514607747394, "y1": 821.1063808865017, "x1": 104.36527464124892, "y2": 829.4431050618489}, "name": "6", "caption_text": "Figure 6: Example VQA with N-GMN+ on CLEVR.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 404.0, "y1": 579.0, "x1": 100.0, "y2": 804.0}, "page": 11, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9257066514757, "y1": 1029.6591016981336, "x1": 426.080576578776, "y2": 1054.6000162760417}, "name": "8", "caption_text": "Figure 8: Visualization of the attention weights for a 14\u00d7 14 grid of cells. Red regions get higher attention.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 730.0, "y1": 527.0, "x1": 427.0, "y2": 1012.0}, "page": 11, "dpi": 0}], "error": null, "pdf": "/work/host-output/2ab59f8b2d13d908fd15375537f281fa0903e759/2020.acl-main.643.pdf", "dpi": 100}