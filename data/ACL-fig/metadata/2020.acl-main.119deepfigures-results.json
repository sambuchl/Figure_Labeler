{"raw_detected_boxes": [[], [{"x2": 688.0, "y1": 87.0, "x1": 466.0, "y2": 337.0}], [{"x2": 687.0, "y1": 88.0, "x1": 144.0, "y2": 325.0}], [], [], [{"x2": 394.0, "y1": 87.0, "x1": 109.0, "y2": 218.0}, {"x2": 724.0, "y1": 112.0, "x1": 434.0, "y2": 600.0}], [], [{"x2": 707.0, "y1": 89.0, "x1": 120.0, "y2": 287.0}, {"x2": 380.0, "y1": 361.0, "x1": 123.0, "y2": 516.0}, {"x2": 692.0, "y1": 361.0, "x1": 465.0, "y2": 552.0}], [{"x2": 707.0, "y1": 88.0, "x1": 123.0, "y2": 245.0}, {"x2": 692.0, "y1": 306.0, "x1": 465.0, "y2": 496.0}], [], [], [{"x2": 686.0, "y1": 323.0, "x1": 470.0, "y2": 785.0}]], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Figure", "name": "3", "captionBoundary": {"x2": 291.9244079589844, "y1": 169.83456420898438, "x1": 72.0, "y2": 211.70306396484375}, "imageText": ["c2", "\u03b2", "predict", "new", "concept", "c1", "c3", "h1", "h2", "h3", "h4", "s3", "s2", "s1", "max", "s0", "0.7", "0.2", "0.8", "0.8", "0.1", "0.1", "0.7", "0.05", "0.05", "0.1", "0.2", "0.1", "0.8", "0.1", "0.0", "0.05", "0.05", "0.7", "0.1", "0.8"], "regionBoundary": {"x2": 284.0, "y1": 63.115516662597656, "x1": 78.46151733398438, "y2": 157.8900146484375}, "caption": "Figure 3: Multi-head attention for relation identification. At left is the attention matrix, where each column corresponds to a unique attention head, and each row corresponds to an existing node.", "page": 5}, {"figType": "Figure", "name": "1", "captionBoundary": {"x2": 527.1954345703125, "y1": 255.44955444335938, "x1": 307.2760009765625, "y2": 297.31707763671875}, "imageText": ["The", "boy", "must", "not", "go", "The", "current", "partial", "(solid)", "and", "full", "(solid", "+", "dashed)", "AMR", "graphs", "for", "the", "sentence", "\u201cThe", "boy", "must", "no", "go\u201d", "(b)", "(a)", "polarity", "ARG2", "?", "obligate-01", "go-02", "RG", "0", "go-02", "ARG2", "A", "?", "obligate-01", "polarity", "ARG2", "A", "RG", "0", "-", "go-02", "boy", "obligate-01", "The", "boy", "must", "not", "go", "The", "boy", "must", "not", "go"], "regionBoundary": {"x2": 497.2513427734375, "y1": 62.8900146484375, "x1": 335.665283203125, "y2": 242.8900146484375}, "caption": "Figure 1: AMR graph construction given the partially constructed graph: (a) one possible expansion resulting in the boy concept. (b) another possible expansion resulting in the - (negation) concept.", "page": 1}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 525.547119140625, "y1": 248.09054565429688, "x1": 71.99990844726562, "y2": 289.95806884765625}, "imageText": ["graph", "memory", "W", "Gi", "y1", "x1", "y2", "g", "(W,", "y1)", "g", "(W,", "y2)", "f", "(Gi,", "x0)", "f", "(Gi,", "x1)", "xtyt+", "1", "\u2026initial", "state", "x0", "believe", "him.", "attention", "The", "boy", "wants", "the", "girl", "to", "Concept", "Solver", "\u2026", "yt", "xt", "attention", "Relation", "Solver", "\u2026", "text", "memory", "Sequence", "Encoder", "Graph", "Encoder", "believe", "him.", "The", "boy", "wants", "the", "girl", "to", "(Input", "Sequence)", "(Current", "Graph)"], "regionBoundary": {"x2": 495.0, "y1": 62.8900146484375, "x1": 103.0, "y2": 234.8900146484375}, "caption": "Figure 2: Overview of the dual graph-sequence iterative inference for AMR parsing. Given the current graph Gi and input sequence W . The inference starts with an initial concept decision x0 and follows the inference chain x0 \u2192 f(Gi, x0) \u2192 y1 \u2192 g(W, y1) \u2192 x1 \u2192 f(Gi, x1) \u2192 y2 \u2192 g(W, y2) \u2192 \u00b7 \u00b7 \u00b7 . The details of f and g are shown in red and blue boxes, where nodes in graph and tokens in sequence are selected via attention mechanisms.", "page": 2}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 525.849365234375, "y1": 218.67153930664062, "x1": 71.69100189208984, "y2": 236.6290283203125}, "imageText": ["\u00d7", "\u00d7", "74.5", "77.8", "75.1", "85.9", "68.5", "57.7", "65.0", "82.9", "81.1", "X", "\u00d7", "77.3", "80.1", "77.9", "86.4", "69.4", "58.5", "75.6", "78.4", "86.1", "\u00d7", "X", "78.7", "81.5", "79.2", "88.1", "74.5", "63.8", "66.1", "87.1", "81.3", "X", "X", "80.2", "82.8", "80.8", "88.1", "74.2", "64.6", "78.9", "81.1", "86.3", "Ours", "Lyu", "and", "Titov", "(2018)", "X", "\u00d7", "74.4", "77.1", "75.5", "85.9", "69.8", "52.3", "58.4", "86.0", "75.7", "Cai", "and", "Lam", "(2019)", "\u00d7", "\u00d7", "73.2", "77.0", "74.2", "84.4", "66.7", "55.3", "62.9", "82.0", "73.2", "Lindemann", "et", "al.", "(2019)", "X", "X", "75.3", "-", "-", "-", "-", "-", "-", "-", "-", "Naseem", "et", "al.", "(2019)", "X", "X", "75.5", "80", "76", "86", "72", "56", "67", "83", "80", "Zhang", "et", "al.", "(2019a)", "X", "\u00d7", "74.6", "-", "-", "-", "-", "-", "-", "-", "-", "Zhang", "et", "al.", "(2019a)", "X", "X", "76.3", "79.0", "76.8", "84.8", "69.7", "60.0", "75.2", "77.9", "85.8", "Zhang", "et", "al.", "(2019b)", "X", "X", "77.0", "80", "78", "86", "71", "61", "77", "79", "86", "van", "Noord", "and", "Bos", "(2017)", "\u00d7", "\u00d7", "71.0", "74", "72", "82", "66", "52", "62", "79", "65", "Groschwitz", "et", "al.", "(2018)", "X", "\u00d7", "71.0", "74", "72", "84", "64", "49", "57", "78", "71", "Model", "G.", "R.", "BERT", "SMATCH", "\ufb01ne-grained", "evaluation", "Unlabeled", "No", "WSD", "Concept", "SRL", "Reent.", "Neg.", "NER", "Wiki"], "regionBoundary": {"x2": 518.0, "y1": 63.8900146484375, "x1": 82.0, "y2": 206.8900146484375}, "caption": "Table 1: SMATCH scores (%) (left) and fine-grained evaluations (%) (right) on the test set of AMR 2.0. G. R./BERT indicates whether or not the results use Graph Re-categorization/BERT respectively.", "page": 7}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 285.68707275390625, "y1": 383.6525573730469, "x1": 76.27300262451172, "y2": 389.655029296875}, "imageText": ["\u00d7", "\u00d7", "68.8", "X", "\u00d7", "71.2", "\u00d7", "X", "74.0", "X", "X", "75.4", "Ours", "Guo", "and", "Lu", "(2018)", "X", "\u00d7", "68.3", "Zhang", "et", "al.", "(2019a)", "X", "X", "70.2", "Zhang", "et", "al.", "(2019b)", "X", "X", "71.3", "Pust", "et", "al.", "(2015)", "\u00d7", "\u00d7", "67.1", "Wang", "and", "Xue", "(2017)", "X", "\u00d7", "68.1", "Model", "G.", "R.", "BERT", "SMATCH", "Flanigan", "et", "al.", "(2016)", "\u00d7", "\u00d7", "66.0"], "regionBoundary": {"x2": 274.0, "y1": 259.8900146484375, "x1": 88.0, "y2": 371.8900146484375}, "caption": "Table 2: SMATCH scores on the test set of AMR 1.0.", "page": 7}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 525.5394897460938, "y1": 411.9765319824219, "x1": 307.2760009765625, "y2": 429.9339904785156}, "imageText": ["All", "(0,", "15]", "(15,", "30]", "(30,", "\u221e)", "1", "2", "3", "4", "5", "6", "Number", "of", "Inference", "Steps", "85.0", "77.5", "70.0", "62.5", "55.0", ")", "ch", "(%", "Sm", "at"], "regionBoundary": {"x2": 498.1168518066406, "y1": 260.6488952636719, "x1": 338.9202880859375, "y2": 396.9270324707031}, "caption": "Figure 4: SMATCH scores with different numbers of inference steps. Sentences are grouped by length.", "page": 7}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 487.43853759765625, "y1": 585.41455078125, "x1": 345.072998046875, "y2": 591.4169921875}, "imageText": ["Embeddings", "lemma", "300", "POS", "tag", "32", "NER", "tag", "16", "concept", "300", "char", "32", "Char-level", "CNN", "#\ufb01lters", "256", "ngram", "\ufb01lter", "size", "[3]", "output", "size", "128", "Sentence", "Encoder", "#transformer", "layers", "4", "Graph", "Encoder", "#transformer", "layers", "2", "Transformer", "Layer", "#heads", "8", "hidden", "size", "512", "feed-forward", "hidden", "size", "1024", "Concept", "Solver", "feed-forward", "hidden", "size", "1024", "Relation", "Solver", "#heads", "8", "feed-forward", "hidden", "size", "1024", "Deep", "biaf\ufb01ne", "classi\ufb01er", "hidden", "size", "100"], "regionBoundary": {"x2": 494.0, "y1": 230.8900146484375, "x1": 339.0, "y2": 572.8900146484375}, "caption": "Table 3: Hyper-parameters settings.", "page": 11}, {"figType": "Figure", "name": "5", "captionBoundary": {"x2": 509.97021484375, "y1": 191.09451293945312, "x1": 87.57599639892578, "y2": 197.09698486328125}, "imageText": ["1.00.50.0", "\u03b11", "\u03b12", "\u03b13", "\u03b14", "cccc", "cccc", "c", "cccc", "cccc", "c", "cccc", "cccc", "c", "cccc", "cccc", "cI", "have", "little", "or", "no", "pity", "for", "you", ".", "I", "have", "little", "or", "no", "pity", "for", "you", ".", "I", "have", "little", "or", "no", "pity", "for", "you", ".", "I", "have", "little", "or", "no", "pity", "for", "you", ".", "or", "or", "or", "or\u03b21", "\u03b22", "\u03b23", "\u03b24", "you", "I", "or", "pity-01", "pity-01", "-", "Golden", "AMR", "little", "Predicted", "Expansion", "you", "I", "or", "pity-01", "pity-01", "you", "pity-01", "pity-01", "pity-01", "pity-01", "you", "pity-01", "pity-01", "you", "pity-01", "pity-01", "you"], "regionBoundary": {"x2": 513.0, "y1": 61.8900146484375, "x1": 85.0, "y2": 180.11273193359375}, "caption": "Figure 5: Case study (viewed in color). Color shading intensity represents the value of the attention score.", "page": 8}, {"figType": "Figure", "name": "6", "captionBoundary": {"x2": 520.5537109375, "y1": 370.19354248046875, "x1": 312.2659912109375, "y2": 376.1960144042969}, "imageText": ["1", "2", "3", "4", "5", "6", "7", "8", "Beam", "Size", "81.0", "80.0", "ch", "(%", ")", "77.0", "78.0", "79.0", "Sm", "at"], "regionBoundary": {"x2": 498.3619384765625, "y1": 220.80068969726562, "x1": 339.10205078125, "y2": 357.07879638671875}, "caption": "Figure 6: SMATCH scores with different beam sizes.", "page": 8}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 732.2158813476562, "y1": 354.79104783799914, "x1": 426.772223578559, "y2": 412.94038560655383}, "name": "1", "caption_text": "Figure 1: AMR graph construction given the partially constructed graph: (a) one possible expansion resulting in the boy concept. (b) another possible expansion resulting in the - (negation) concept.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 690.0, "y1": 87.0, "x1": 466.0, "y2": 354.0}, "page": 1, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9265543619791, "y1": 344.5702022976345, "x1": 99.99987284342447, "y2": 402.7195400661892}, "name": "2", "caption_text": "Figure 2: Overview of the dual graph-sequence iterative inference for AMR parsing. Given the current graph Gi and input sequence W . The inference starts with an initial concept decision x0 and follows the inference chain x0 \u2192 f(Gi, x0) \u2192 y1 \u2192 g(W, y1) \u2192 x1 \u2192 f(Gi, x1) \u2192 y2 \u2192 g(W, y2) \u2192 \u00b7 \u00b7 \u00b7 . The details of f and g are shown in red and blue boxes, where nodes in graph and tokens in sequence are selected via attention mechanisms.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 688.0, "y1": 86.0, "x1": 143.0, "y2": 327.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.4505666097005, "y1": 235.88133917914496, "x1": 100.0, "y2": 294.0320332845052}, "name": "3", "caption_text": "Figure 3: Multi-head attention for relation identification. At left is the attention matrix, where each column corresponds to a unique attention head, and each row corresponds to an existing node.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 394.0, "y1": 87.0, "x1": 100.0, "y2": 235.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 730.3463406032986, "y1": 303.7104712592231, "x1": 99.57083596123589, "y2": 328.65142822265625}, "name": "1", "caption_text": "Table 1: SMATCH scores (%) (left) and fine-grained evaluations (%) (right) on the test set of AMR 2.0. G. R./BERT indicates whether or not the results use Graph Re-categorization/BERT respectively.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 724.0, "y1": 86.0, "x1": 103.0, "y2": 304.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 396.787601047092, "y1": 532.8507741292318, "x1": 105.93472586737738, "y2": 541.1875406901041}, "name": "2", "caption_text": "Table 2: SMATCH scores on the test set of AMR 1.0.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 392.0, "y1": 361.0, "x1": 106.0, "y2": 533.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9159579806858, "y1": 572.1896277533637, "x1": 426.772223578559, "y2": 597.1305423312717}, "name": "4", "caption_text": "Figure 4: SMATCH scores with different numbers of inference steps. Sentences are grouped by length.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 692.0, "y1": 361.0, "x1": 465.0, "y2": 554.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 708.2919650607639, "y1": 265.4090457492404, "x1": 121.63332833184135, "y2": 273.74581231011285}, "name": "5", "caption_text": "Figure 5: Case study (viewed in color). Color shading intensity represents the value of the attention score.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 712.0, "y1": 87.0, "x1": 119.0, "y2": 247.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 722.9912651909722, "y1": 514.1576978895399, "x1": 433.7027655707465, "y2": 522.4944644504124}, "name": "6", "caption_text": "Figure 6: SMATCH scores with different beam sizes.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 701.0, "y1": 306.0, "x1": 465.0, "y2": 513.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 676.9979688856337, "y1": 813.0757649739583, "x1": 479.2680528428819, "y2": 821.4124891493055}, "name": "3", "caption_text": "Table 3: Hyper-parameters settings.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 686.0, "y1": 320.0, "x1": 470.0, "y2": 796.0}, "page": 11, "dpi": 0}], "error": null, "pdf": "/work/host-output/76e23fa764e16cd4d0dd4943cf960ed98ae61c72/2020.acl-main.119.pdf", "dpi": 100}