{"raw_detected_boxes": [[], [], [], [], [], [{"x2": 720.0, "y1": 93.0, "x1": 107.0, "y2": 415.0}, {"x2": 717.0, "y1": 546.0, "x1": 113.0, "y2": 681.0}], [{"x2": 715.0, "y1": 89.0, "x1": 113.0, "y2": 225.0}], [{"x2": 382.0, "y1": 89.0, "x1": 119.0, "y2": 335.0}, {"x2": 723.0, "y1": 92.0, "x1": 435.0, "y2": 189.0}, {"x2": 692.0, "y1": 287.0, "x1": 464.0, "y2": 389.0}], [], [], []], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Table", "name": "1", "captionBoundary": {"x2": 527.2906494140625, "y1": 310.7385559082031, "x1": 71.64099884033203, "y2": 376.5169982910156}, "imageText": ["Perplexity", "78.6", "49.5", "(\u00b10.2)", "56.4", "(\u00b10.5)", "50.4", "(\u00b10.6)", "49.6", "(\u00b10.3)", "50.3", "(\u00b10.2)", "56.7", "(\u00b10.2)", "NPI:", "Simple", "40.0", "99.2", "(\u00b10.7)", "98.7", "(\u00b11.6)", "97.7", "(\u00b12.0)", "98.0", "(\u00b13.1)", "98.2", "(\u00b11.2)", "94.0", "(\u00b14.0)", "Across", "an", "ORC", "41.0", "63.5", "(\u00b115.0)", "56.8", "(\u00b16.0)", "64.1", "(\u00b113.8)", "64.5", "(\u00b114.0)", "48.5", "(\u00b16.4)", "91.0", "(\u00b17.0)", "REFLEXIVE:", "Simple", "83.0", "94.1", "(\u00b11.9)", "99.4", "(\u00b11.1)", "99.9", "(\u00b10.2)", "91.8", "(\u00b12.9)", "98.0", "(\u00b11.1)", "91.0", "(\u00b14.0)", "In", "a", "sent.", "complement", "86.0", "80.8", "(\u00b11.7)", "99.2", "(\u00b10.6)", "97.9", "(\u00b10.8)", "79.0", "(\u00b13.1)", "92.6", "(\u00b12.9)", "82.0", "(\u00b13.0)", "Across", "an", "ORC", "55.0", "74.9", "(\u00b15.0)", "72.8", "(\u00b12.4)", "73.9", "(\u00b11.3)", "72.3", "(\u00b13.0)", "78.9", "(\u00b18.6)", "67.0", "(\u00b13.0)", "AGREEMENT:", "Simple", "94.0", "98.1", "(\u00b11.3)", "100.0", "(\u00b10.0)", "100.0", "(\u00b10.0)", "99.1", "(\u00b11.2)", "99.7", "(\u00b10.6)", "100.0", "(\u00b10.0)", "In", "a", "sent.", "complement", "99.0", "96.1", "(\u00b12.0)", "95.8", "(\u00b10.7)", "99.3", "(\u00b10.4)", "96.9", "(\u00b12.4)", "92.7", "(\u00b13.1)", "98.0", "(\u00b12.0)", "Short", "VP", "coordination", "90.0", "93.6", "(\u00b13.0)", "100.0", "(\u00b10.0)", "99.4", "(\u00b11.1)", "93.8", "(\u00b13.3)", "95.6", "(\u00b13.0)", "99.0", "(\u00b12.0)", "Long", "VP", "coordination", "61.0", "82.2", "(\u00b13.4)", "94.5", "(\u00b11.0)", "99.0", "(\u00b10.8)", "83.9", "(\u00b13.2)", "90.0", "(\u00b12.4)", "80.0", "(\u00b12.0)", "Across", "a", "PP", "57.0", "92.6", "(\u00b11.4)", "98.8", "(\u00b10.4)", "98.6", "(\u00b10.3)", "92.7", "(\u00b11.3)", "95.2", "(\u00b11.2)", "91.0", "(\u00b13.0)", "Across", "a", "SRC", "56.0", "91.5", "(\u00b13.4)", "99.6", "(\u00b10.4)", "99.8", "(\u00b10.2)", "91.9", "(\u00b12.5)", "97.1", "(\u00b10.7)", "90.0", "(\u00b12.0)", "Across", "an", "ORC", "50.0", "84.5", "(\u00b13.1)", "93.5", "(\u00b14.0)", "93.7", "(\u00b12.0)", "86.3", "(\u00b13.2)", "88.7", "(\u00b14.1)", "84.0", "(\u00b13.0)", "Across", "an", "ORC", "(no", "that)", "52.0", "75.7", "(\u00b13.3)", "86.7", "(\u00b14.2)", "89.4", "(\u00b12.7)", "78.6", "(\u00b14.0)", "86.4", "(\u00b13.5)", "77.0", "(\u00b12.0)", "In", "an", "ORC", "84.0", "84.3", "(\u00b15.5)", "99.8", "(\u00b10.2)", "99.9", "(\u00b10.1)", "89.3", "(\u00b16.2)", "92.4", "(\u00b13.5)", "92.0", "(\u00b14.0)", "In", "an", "ORC", "(no", "that)", "71.0", "81.8", "(\u00b12.3)", "97.0", "(\u00b11.0)", "98.6", "(\u00b10.9)", "83.0", "(\u00b15.1)", "88.9", "(\u00b12.4)", "92.0", "(\u00b12.0)", "M&L18", "Ours", "Sentence-level", "Token-level", "Binary-pred.", "Unlike.", "K19", "LSTM-LM", "Additional", "margin", "loss", "(\u03b4", "=", "10)", "Additional", "loss", "(\u03b1", "=", "1000,", "\u03b2", "=", "1)", "Distilled"], "regionBoundary": {"x2": 525.0, "y1": 67.86872100830078, "x1": 74.0, "y2": 298.8900146484375}, "caption": "Table 1: Comparison of syntactic dependency evaluation accuracies across different types of dependencies and perplexities. Numbers in parentheses are standard deviations. M&L18 is the result of two-layer LSTM-LM in Marvin and Linzen (2018). K19 is the result of distilled two-layer LSTM-LM from RNNGs (Kuncoro et al., 2019). VP: verb phrase; PP: prepositional phrase; SRC: subject relative clause; and ORC: object-relative clause. Margin values are set to 10, which works better according to Figure 1. Perplexity values are calculated on the test set of the Wikipedia dataset. The values of M&L18 and K19 are copied from Kuncoro et al. (2019).", "page": 5}, {"figType": "Figure", "name": "1", "captionBoundary": {"x2": 525.5472412109375, "y1": 505.9115295410156, "x1": 72.0, "y2": 523.8690185546875}, "imageText": ["58", "Perplexity", "56", "54", "52", "50", "0", "1", "5", "10", "15", "margin", "100", "NPI", "90", "80", "70", "60", "0", "1", "5", "10", "15", "margin", "100", "Reflexive", "95", "90", "85", "80", "0", "1", "5", "10", "15", "margin", "sentence-level", "token-level", "Agreement", "xi", "ty", "rp", "le", "/", "Pe", "ra", "cy", "Ac", "cu", "100", "95", "90", "85", "80", "0", "1", "5", "10", "15", "margin"], "regionBoundary": {"x2": 516.172607421875, "y1": 392.889404296875, "x1": 84.53671264648438, "y2": 490.5938415527344}, "caption": "Figure 1: Margin value vs. macro average accuracy over the same type of constructions, or perplexity, with standard deviation for the sentence and token-level margin losses. \u03b4 = 0 is the baseline LSTM-LM without additional loss.", "page": 5}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 527.2005004882812, "y1": 148.02755737304688, "x1": 306.9670104980469, "y2": 189.89605712890625}, "imageText": ["-TOKEN", "90.8", "(\u00b13.3)", "51.0", "(\u00b129.9)", "95.2", "(\u00b12.6)", "-PATTERN", "90.1", "(\u00b14.6)", "50.0", "(\u00b130.6)", "94.6", "(\u00b12.2)", "LSTM-LM", "82.2", "(\u00b13.4)", "13.0", "(\u00b112.2)", "89.9", "(\u00b13.6)", "Margin", "(token)", "99.0", "(\u00b10.8)", "94.0", "(\u00b16.5)", "99.6", "(\u00b10.5)", "Second", "verb", "(V1", "and", "V2)", "Models", "All", "verbs", "like", "other", "verbs"], "regionBoundary": {"x2": 523.0, "y1": 62.8900146484375, "x1": 309.0, "y2": 135.8900146484375}, "caption": "Table 2: Accuracies on long VP coordinations by the models with/without ablations. \u201cAll verbs\u201d scores are overall accuracies. \u201clike\u201d scores are accuracies on examples on which the second verb (target verb) is like.", "page": 7}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 291.924072265625, "y1": 255.20651245117188, "x1": 71.6709976196289, "y2": 309.029052734375}, "imageText": ["LSTM-LM", "margin", "(token)", "margin", "(token)", "w/o", "negative", "examples", "on", "target", "verbs", "margin", "(token)", "w/o", "negative", "examples", "on", "a", "construction", "ra", "cy", "Ac", "cu", "100", "95", "90", "85", "Long", "VP", "coord.", "80", "Across", "a", "SRC", "Across", "an", "ORC", "ra", "cy", "Ac", "cu", "100", "95", "90", "85", "Across", "a", "PP", "80"], "regionBoundary": {"x2": 278.0, "y1": 63.920352935791016, "x1": 88.3441390991211, "y2": 240.192138671875}, "caption": "Figure 3: An ablation study to see the performance of models trained with reduced explicit negative examples (token-level and construction-level). One color represents the same models across plots, except the last bar (construction-level), which is different for each plot.", "page": 7}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 525.7158203125, "y1": 291.9045715332031, "x1": 306.9670104980469, "y2": 333.7720642089844}, "imageText": ["-TOKEN", "63.5", "(\u00b118.5)", "99.2", "(\u00b11.1)", "-PATTERN", "67.0", "(\u00b121.2)", "98.0", "(\u00b11.4)", "LSTM-LM", "61.5", "(\u00b120.0)", "93.5", "(\u00b13.4)", "Margin", "(token)", "97.0", "(\u00b14.5)", "99.9", "(\u00b10.1)", "First", "verb", "(V1", "and", "V2)", "Models", "likes", "other", "verbs"], "regionBoundary": {"x2": 499.0, "y1": 206.8900146484375, "x1": 334.0, "y2": 279.8900146484375}, "caption": "Table 3: Further analysis of accuracies on the \u201cother verbs\u201d cases of Table 2. Among these cases, the second column (\u201clikes\u201d) shows accuracies on examples where the first verb (not target) is likes.", "page": 7}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 527.2905883789062, "y1": 176.66152954101562, "x1": 70.67500305175781, "y2": 218.530029296875}, "imageText": ["no", "that", "(animate", "only)", "0.1M", "0.37M", "0.5M", "0.8M", "#", "ORCs", "no", "that", "(all", "cases)", "0.1M", "0.37M", "0.5M", "0.8M", "#", "ORCs", "with", "that", "(animate", "only)", "0.1M", "0.37M", "0.5M", "0.8M", "#", "ORCs", "LSTM-LM", "margin", "(sent.)", "margin", "(token)", "OR", "C'", "with", "that", "(all", "cases)", "95", "100", "Ac", "cu", "ra", "cy", "o", "n", "'A", "cr", "os", "s", "a", "n", "90", "85", "80", "75", "0.1M", "0.37M", "0.5M", "0.8M", "#", "ORCs"], "regionBoundary": {"x2": 518.55517578125, "y1": 60.62882614135742, "x1": 84.17684936523438, "y2": 161.55682373046875}, "caption": "Figure 2: Accuracies on \u201cAcross an ORC\u201d (with and without complementizer \u201cthat\u201d) by models trained on augmented data with additional sentences containing an object RC. Margin is set to 10. X-axis denotes the total number of object RCs in the training data. 0.37M roughly equals the number of subject RCs in the original data. \u201canimate only\u201d is a subset of examples (see body). Error bars are standard deviations across 5 different runs.", "page": 6}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 732.3481241861979, "y1": 431.5813276502821, "x1": 99.50138727823892, "y2": 522.9402754041884}, "name": "1", "caption_text": "Table 1: Comparison of syntactic dependency evaluation accuracies across different types of dependencies and perplexities. Numbers in parentheses are standard deviations. M&L18 is the result of two-layer LSTM-LM in Marvin and Linzen (2018). K19 is the result of distilled two-layer LSTM-LM from RNNGs (Kuncoro et al., 2019). VP: verb phrase; PP: prepositional phrase; SRC: subject relative clause; and ORC: object-relative clause. Margin values are set to 10, which works better according to Figure 1. Perplexity values are calculated on the test set of the Wikipedia dataset. The values of M&L18 and K19 are copied from Kuncoro et al. (2019).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 729.0, "y1": 86.0, "x1": 100.0, "y2": 432.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9267239040798, "y1": 702.6549021402994, "x1": 100.0, "y2": 727.5958591037327}, "name": "1", "caption_text": "Figure 1: Margin value vs. macro average accuracy over the same type of constructions, or perplexity, with standard deviation for the sentence and token-level margin losses. \u03b4 = 0 is the baseline LSTM-LM without additional loss.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 717.0, "y1": 546.0, "x1": 113.0, "y2": 683.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.3480394151476, "y1": 245.3632354736328, "x1": 98.15972646077473, "y2": 303.51392957899304}, "name": "2", "caption_text": "Figure 2: Accuracies on \u201cAcross an ORC\u201d (with and without complementizer \u201cthat\u201d) by models trained on augmented data with additional sentences containing an object RC. Margin is set to 10. X-axis denotes the total number of object RCs in the training data. 0.37M roughly equals the number of subject RCs in the original data. \u201canimate only\u201d is a subset of examples (see body). Error bars are standard deviations across 5 different runs.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 720.0, "y1": 89.0, "x1": 113.0, "y2": 225.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.4501003689236, "y1": 354.4534895155165, "x1": 99.54305224948459, "y2": 429.20701768663196}, "name": "3", "caption_text": "Figure 3: An ablation study to see the performance of models trained with reduced explicit negative examples (token-level and construction-level). One color represents the same models across plots, except the last bar (construction-level), which is different for each plot.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 386.0, "y1": 89.0, "x1": 119.0, "y2": 335.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2229173448351, "y1": 205.59382968478732, "x1": 426.3430701361762, "y2": 263.7445237901476}, "name": "2", "caption_text": "Table 2: Accuracies on long VP coordinations by the models with/without ablations. \u201cAll verbs\u201d scores are overall accuracies. \u201clike\u201d scores are accuracies on examples on which the second verb (target verb) is like.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 727.0, "y1": 86.0, "x1": 426.0, "y2": 206.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 730.1608615451389, "y1": 405.42301601833765, "x1": 426.3430701361762, "y2": 463.5723114013672}, "name": "3", "caption_text": "Table 3: Further analysis of accuracies on the \u201cother verbs\u201d cases of Table 2. Among these cases, the second column (\u201clikes\u201d) shows accuracies on examples where the first verb (not target) is likes.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 701.0, "y1": 286.0, "x1": 447.0, "y2": 406.0}, "page": 7, "dpi": 0}], "error": null, "pdf": "/work/host-output/89bb215d2abaf4d1e3701e0cc6d0325f9d94f8f3/2020.acl-main.309.pdf", "dpi": 100}