{"raw_detected_boxes": [[], [{"x2": 729.0, "y1": 761.0, "x1": 427.0, "y2": 947.0}], [], [], [{"x2": 730.0, "y1": 221.0, "x1": 421.0, "y2": 309.0}, {"x2": 725.0, "y1": 432.0, "x1": 421.0, "y2": 563.0}, {"x2": 398.0, "y1": 805.0, "x1": 95.0, "y2": 959.0}, {"x2": 717.0, "y1": 916.0, "x1": 421.0, "y2": 1000.0}], [{"x2": 730.0, "y1": 288.0, "x1": 427.0, "y2": 604.0}, {"x2": 393.0, "y1": 436.0, "x1": 96.0, "y2": 653.0}, {"x2": 724.0, "y1": 657.0, "x1": 426.0, "y2": 1016.0}], [{"x2": 397.0, "y1": 163.0, "x1": 94.0, "y2": 476.0}]], "raw_pdffigures_output": {"regionless-captions": [{"figType": "Table", "boundary": {"x2": 285.9061584472656, "y1": 303.6402893066406, "x1": 68.12000274658203, "y2": 458.24224853515625}, "text": "Table 6-2 shows complex cases where a combination of two or more English phonemes are mapped to multiple candidates of a composite Korean phonetic unit. This phonetic mapping table is carefully constructed so as to produce a unique candidate in syllabification and alignment in the training stage. When a given English pronunciation can be syllabificated into serveral units or a single composite unit, we adopt a heuristic that only the composite unit consisting of longer phonetic units is considered. For example, the English phonetic unit \u201cu@\u201d can be mapped to a Korean phonetic unit \u201c", "name": "6-2", "page": 1}, {"figType": "Figure", "boundary": {"x2": 286.4580383300781, "y1": 99.63764953613281, "x1": 68.12000274658203, "y2": 192.91912841796875}, "text": "Figure 1. shows an example of syllabification and alignment. To take the English word \"computer\" as an example, the English pronunciation notation \u201ck@mpu@R\u201d is retrieved from the Oxford dictionary. In the first stage, it is segmented in front of the consonants \u201ck\u201d, \u201cm\u201d, \u201cp\u201d and \u201ct\u201d which are aligned with the corresponding Korean consonants \u201c [k]\u201d, \u201c", "name": "1", "page": 2}, {"figType": "Figure", "boundary": {"x2": 522.6810913085938, "y1": 369.6380615234375, "x1": 303.5594482421875, "y2": 429.08099365234375}, "text": "Figure 3 pictorially summarizes the final information sources that our statistical tagger utilizes. It can be thought of as a generalized case of prevalent Part-of-Speech tagging model. When _ LLL", "name": "3", "page": 3}], "figures": [{"figType": "Table", "name": "4", "captionBoundary": {"x2": 286.2261962890625, "y1": 492.1796875, "x1": 68.12000274658203, "y2": 657.9420166015625}, "imageText": ["<Table", "4>", "comparison", "with", "other", "models", "\u2013", "word", "accuracy", "(precision)", "of", "1-best", "candidate.", "Average:(", "train", "+", "untrained", ")", "/", "2Lee\u2019s", "\u2013", "modified", "Direct", "[9]", "38.1%", "WFSA", "[19]", "?", "64%", "Neural", "Net", "[10]", "37.7%", "26.2%", "MBRtalk", "[16]", "(100%)", "43%", "(no", "training)", "36.4%", "transcription", "automata", "64.0%", "54.9%", "Extended", "Markov", "window", "Trained", "Untrained"], "regionBoundary": {"x2": 291.0, "y1": 312.0, "x1": 67.0, "y2": 473.96380615234375}, "caption": "Table 4 shows the comparison with MBRtalk [16], Neural Network [10], Weighted finite-state acceptor (WFSA) [19] and direct transliteration [9] even though they are based on different problem domains. MBRtalk and Neural Network models are based on English word\u2019s pronunciation generation. WFSA is for Englishto-Japanese transliteration. Experiment for training data in MBRtalk makes no sense, since it finds the most similar word in a database that stores all the training data; thus the result would always produce the exact answer. The results show that the model we propose indicates the good performance.", "page": 5}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 522.7372436523438, "y1": 124.02021789550781, "x1": 303.5601501464844, "y2": 142.54095458984375}, "imageText": ["<Table", "1>", "estimated", "recall", "result", "on", "10-best", "results", "Transcription", "automata", "used", "as", "well", "0.939", "0.875", "Pure", "statistical", "tagger", "(eq.", "7)", "0.925", "0.850", "Trained", "Test"], "regionBoundary": {"x2": 526.0, "y1": 157.0, "x1": 303.0, "y2": 223.76397705078125}, "caption": "Figure 4 shows recall values given a number of candidates.", "page": 4}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 397.09188673231336, "y1": 421.72262403700086, "x1": 94.61111492580838, "y2": 636.4475674099392}, "name": "6-2", "caption_text": "Table 6-2 shows complex cases where a combination of two or more English phonemes are mapped to multiple candidates of a composite Korean phonetic unit. This phonetic mapping table is carefully constructed so as to produce a unique candidate in syllabification and alignment in the training stage. When a given English pronunciation can be syllabificated into serveral units or a single composite unit, we adopt a heuristic that only the composite unit consisting of longer phonetic units is considered. For example, the English phonetic unit \u201cu@\u201d can be mapped to a Korean phonetic unit \u201c", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 729.0, "y1": 744.0, "x1": 422.0, "y2": 964.0}, "page": 1, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 726.023949517144, "y1": 172.25030263264972, "x1": 421.61131964789496, "y2": 197.97354804144965}, "name": "4", "caption_text": "Figure 4 shows recall values given a number of candidates.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 731.0, "y1": 218.0, "x1": 421.0, "y2": 311.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 397.536383734809, "y1": 683.5828993055555, "x1": 94.61111492580838, "y2": 913.808356391059}, "name": "4", "caption_text": "Table 4 shows the comparison with MBRtalk [16], Neural Network [10], Weighted finite-state acceptor (WFSA) [19] and direct transliteration [9] even though they are based on different problem domains. MBRtalk and Neural Network models are based on English word\u2019s pronunciation generation. WFSA is for Englishto-Japanese transliteration. Experiment for training data in MBRtalk makes no sense, since it finds the most similar word in a database that stores all the training data; thus the result would always produce the exact answer. The results show that the model we propose indicates the good performance.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 725.0, "y1": 652.0, "x1": 421.0, "y2": 1027.0}, "page": 5, "dpi": 0}], "error": null, "pdf": "/work/host-output/c6bfaef1519b3d0ded2eec38dd52b9cc733431a9/C00-1056.pdf", "dpi": 100}