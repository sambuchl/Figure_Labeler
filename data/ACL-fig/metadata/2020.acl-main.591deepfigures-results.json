{"raw_detected_boxes": [[], [], [{"x2": 386.0, "y1": 87.0, "x1": 115.0, "y2": 234.0}, {"x2": 703.0, "y1": 99.0, "x1": 435.0, "y2": 241.0}], [], [{"x2": 727.0, "y1": 86.0, "x1": 430.0, "y2": 239.0}], [{"x2": 398.0, "y1": 86.0, "x1": 107.0, "y2": 242.0}], [{"x2": 723.0, "y1": 96.0, "x1": 108.0, "y2": 283.0}], [{"x2": 720.0, "y1": 92.0, "x1": 107.0, "y2": 301.0}, {"x2": 400.0, "y1": 387.0, "x1": 107.0, "y2": 763.0}], [{"x2": 400.0, "y1": 86.0, "x1": 104.0, "y2": 237.0}], [], [], [{"x2": 387.0, "y1": 875.0, "x1": 103.0, "y2": 1060.0}], [], [{"x2": 673.0, "y1": 111.0, "x1": 158.0, "y2": 357.0}, {"x2": 666.0, "y1": 458.0, "x1": 167.0, "y2": 678.0}, {"x2": 724.0, "y1": 790.0, "x1": 110.0, "y2": 989.0}], [{"x2": 681.0, "y1": 128.0, "x1": 153.0, "y2": 345.0}, {"x2": 692.0, "y1": 481.0, "x1": 136.0, "y2": 673.0}, {"x2": 703.0, "y1": 813.0, "x1": 136.0, "y2": 970.0}], [{"x2": 547.0, "y1": 102.0, "x1": 286.0, "y2": 305.0}, {"x2": 709.0, "y1": 395.0, "x1": 120.0, "y2": 668.0}, {"x2": 673.0, "y1": 764.0, "x1": 158.0, "y2": 996.0}], [{"x2": 644.0, "y1": 121.0, "x1": 189.0, "y2": 324.0}, {"x2": 698.0, "y1": 467.0, "x1": 129.0, "y2": 629.0}, {"x2": 649.0, "y1": 765.0, "x1": 183.0, "y2": 974.0}], [{"x2": 541.0, "y1": 215.0, "x1": 291.0, "y2": 412.0}, {"x2": 534.0, "y1": 724.0, "x1": 299.0, "y2": 883.0}]], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Table", "name": "2", "captionBoundary": {"x2": 290.8664245605469, "y1": 188.82955932617188, "x1": 71.69100189208984, "y2": 234.68304443359375}, "imageText": ["Model", "PTB-Sepsent", "CTB-", "Sepsent", "Kim", "et", "al.", "(2019b)", "-", "RNNLM", "93.2", "201.3", "Kim", "et", "al.", "(2019b)", "-", "RNNG", "88.7", "193.1", "Kim", "et", "al.", "(2019b)", "-", "URNNG", "90.6", "195.7", "Kim", "et", "al.", "(2019b)", "-", "RNNG-URNNG", "85.9", "181.1", "Kim", "et", "al.", "(2019b)", "-", "PRPN", "(default)", "126.2", "290.9", "Kim", "et", "al.", "(2019b)", "-", "PRPN", "(\ufb01netuned)", "96.7", "216.0", "ONLSTM-noAWD", "69.0", "167.7", "ONLSTM", "60.0", "145.7", "ONLSTM-SYD-noAWD", "67.6", "163.1", "ONLSTM-SYD", "59.6", "140.5"], "regionBoundary": {"x2": 287.0, "y1": 62.8900146484375, "x1": 77.0, "y2": 176.8900146484375}, "caption": "Table 2: Language modeling perplexity on PTB-Sepsent and CTB-Sepsent. Kim et al. (2019b) report two results of PRPN, the default one using settings in Shen et al. (2017) and another one finetuned by themselves. Our models use the same hyperparameter settings as in Section 5.1.", "page": 5}, {"figType": "Figure", "name": "8", "captionBoundary": {"x2": 526.79248046875, "y1": 264.1345520019531, "x1": 72.0, "y2": 282.092041015625}, "imageText": ["beginning", "in", "mid-N", "prices", "began", "accelerating", "as", "a", "growing", "u.s.", "economy", "and", "the", "weak", "dollar", "spurred", "demand", "beginning", "in", "mid-N", "prices", "began", "accelerating", "as", "a", "growing", "u.s.", "economy", "and", "the", "weak", "dollar", "spurred", "demand", "beginning", "in", "mid-N", "prices", "began", "accelerating", "as", "a", "growing", "u.s.", "economy", "and", "the", "weak", "dollar", "spurred", "demand"], "regionBoundary": {"x2": 490.39715576171875, "y1": 86.8900146484375, "x1": 109.32740783691406, "y2": 246.426025390625}, "caption": "Figure 8: Sentence 4. Trees induced from the syntactic task (top) and language model task (middle) set of distances, as well as the gold-standard trees (bottom).", "page": 14}, {"figType": "Figure", "name": "10", "captionBoundary": {"x2": 525.5481567382812, "y1": 717.4345092773438, "x1": 72.0009994506836, "y2": 735.3929443359375}, "imageText": ["total", "advertising", "linage", "was", "modestly", "lower", "as", "classi\ufb01ed-ad", "volume", "increased", "while", "there", "was", "softer", "demand", "for", "retail", "and", "national", "ad", "linage", "said", "john", "curley", "gannett", "\u2019s", "chief", "executive", "of\ufb01cer", "total", "advertising", "linage", "was", "modestly", "lower", "as", "classi\ufb01ed-ad", "volume", "increased", "while", "there", "was", "softer", "demand", "for", "retail", "and", "national", "ad", "linage", "said", "john", "curley", "gannett", "\u2019s", "chief", "executive", "of\ufb01cer", "total", "advertising", "linage", "was", "modestly", "lower", "as", "classi\ufb01ed-ad", "volume", "increased", "while", "there", "was", "softer", "demand", "for", "retail", "and", "national", "ad", "linage", "said", "john", "curley", "gannett", "\u2019s", "chief", "executive", "of\ufb01cer"], "regionBoundary": {"x2": 505.7710266113281, "y1": 577.8900146484375, "x1": 93.13751220703125, "y2": 701.7434692382812}, "caption": "Figure 10: Sentence 6. Trees induced from the syntactic task (top) and language model task (middle) set of distances, as well as the gold-standard trees (bottom).", "page": 14}, {"figType": "Figure", "name": "9", "captionBoundary": {"x2": 526.79248046875, "y1": 501.1255187988281, "x1": 72.0, "y2": 519.0830078125}, "imageText": ["however", "as", "expected", "brazil", "waited", "for", "the", "crop", "estimate", "to", "come", "out", "and", "then", "cut", "the", "export", "price", "of", "its", "juice", "concentrate", "to", "about", "N.N", "a", "pound", "from", "around", "N.N", "however", "as", "expected", "brazil", "waited", "for", "the", "crop", "estimate", "to", "come", "out", "and", "then", "cut", "the", "export", "price", "of", "its", "juice", "concentrate", "to", "about", "N.N", "a", "pound", "from", "around", "N.N", "however", "as", "expected", "brazil", "waited", "for", "the", "crop", "estimate", "to", "come", "out", "and", "then", "cut", "the", "export", "price", "of", "its", "juice", "concentrate", "to", "about", "N.N", "a", "pound", "from", "around", "N.N"], "regionBoundary": {"x2": 501.4355773925781, "y1": 340.8900146484375, "x1": 97.68833923339844, "y2": 484.896484375}, "caption": "Figure 9: Sentence 5. Trees induced from the syntactic task (top) and language model task (middle) set of distances, as well as the gold-standard trees (bottom).", "page": 14}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 525.8584594726562, "y1": 216.07852172851562, "x1": 71.4530029296875, "y2": 271.89404296875}, "imageText": ["ON-LSTM", "LM", "Unbiased", "63.2", "39.0", "4.9", "37.9", "42.8", "49.6", "54.2", "1.08", "ON-LSTM", "LM", "Biased", "69.5", "44.2", "5.5", "57.0", "53.0", "52.4", "49.6", "2.09", "ONLSTM-SYDsyd", "LM+SYD", "Unbiased", "77.6", "61.3", "7.3", "38.2", "73.2", "69.6", "72.9", "2.81", "ONLSTM-SYDsyd", "LM+SYD", "Biased", "65.7", "45.5", "5.5", "30.4", "40.6", "70.7", "43.9", "5.07", "ONLSTM-SYDlm", "LM+SYD", "Unbiased", "55.1", "34.5", "4.8", "14.9", "42.2", "16.7", "67.4", "0.83", "ONLSTM-SYDlm", "LM+SYD", "Biased", "58.0", "36.3", "5.3", "41.1", "53.9", "52.4", "43.0", "1.70", "Binary", "Gold", "Standard", "Trees", "\u2013", "\u2013", "88.1", "85.6", "6.4", "100", "100", "100", "100", "2.92", "Gold", "standard", "Trees", "\u2013", "\u2013", "100", "100", "5.0", "100", "100", "100", "100", "2.22", "Random", "Trees", "(Htut", "et", "al.,", "2018)", "\u2013", "\u2013", "32.2", "18.6", "5.3", "17.4", "22.3", "\u2013", "16.0", "\u2013", "Balanced", "Trees", "(Htut", "et", "al.,", "2018)", "\u2013", "\u2013", "43.4", "24.5", "4.6", "22.1", "20.2", "\u2013", "9.3", "\u2013", "Left", "Branching", "Trees", "\u2013", "\u2013", "19.6", "9.0", "12.4", "\u2013", "\u2013", "\u2013", "\u2013", "\u2013", "Right", "Branching", "Trees", "\u2013", "\u2013", "56.6", "39.8", "12.4", "\u2013", "\u2013", "\u2013", "\u2013", "\u2013", "Parsing", "F1", "Depth", "WSJ", "Accuracy", "on", "WSJ", "by", "Tag", "R/L", "Ratio", "on", "WSJModel", "WSJ10", "WSJ", "ADJP", "NP", "VP", "PP", "Training", "Objective", "Induction", "Algorithm"], "regionBoundary": {"x2": 523.0, "y1": 62.8900146484375, "x1": 76.0, "y2": 203.8900146484375}, "caption": "Table 3: Unlabeled parsing results evaluated on the WSJ10 and the full WSJ test set. Numbers in bold font indicate that they are the best compared to those computed from the other parts of the model (i.e., within the same section in the table). The Algorithm column represents whether bias or unbiased algorithm is performed. ONLSTM-SYDsyd and ONLSTM-SYDlm represent two sets of trees induced from loss Lsyd and Llm respectively. The Accuracy columns represent the fraction of ground truth constituents of a given type that correspond to constituents in the model parses. The R/L Ratio column represents the ratio between the number of words that are left children of its parent, and those that are right children.", "page": 6}, {"figType": "Figure", "name": "5", "captionBoundary": {"x2": 526.79248046875, "y1": 273.1335754394531, "x1": 72.0, "y2": 291.092041015625}, "imageText": ["boeing", "is", "also", "supposed", "to", "send", "to", "america", "west", "another", "N", "twin-engine", "aircraft", "as", "well", "as", "a", "N", "by", "year", "\u2019s", "end", "boeing", "is", "also", "supposed", "to", "send", "to", "america", "west", "another", "N", "twin-engine", "aircraft", "as", "well", "as", "a", "N", "by", "year", "\u2019s", "end", "boeing", "is", "also", "supposed", "to", "send", "to", "america", "west", "another", "N", "twin-engine", "aircraft", "as", "well", "as", "a", "N", "by", "year", "\u2019s", "end"], "regionBoundary": {"x2": 486.8859558105469, "y1": 73.8900146484375, "x1": 112.83739471435547, "y2": 255.425048828125}, "caption": "Figure 5: Sentence 1. Trees induced from the syntactic task (top) and language model task (middle) set of distances, as well as the gold-standard trees (bottom).", "page": 13}, {"figType": "Figure", "name": "6", "captionBoundary": {"x2": 526.79345703125, "y1": 504.00958251953125, "x1": 72.0009994506836, "y2": 521.968017578125}, "imageText": ["that", "discrepancy", "hurts", "quantum", "badly", "because", "its", "own", "plants", "cover", "only", "about", "half", "of", "its", "ethylene", "needs", "that", "discrepancy", "hurts", "quantum", "badly", "because", "its", "own", "plants", "cover", "only", "about", "half", "of", "its", "ethylene", "needs", "that", "discrepancy", "hurts", "quantum", "badly", "because", "its", "own", "plants", "cover", "only", "about", "half", "of", "its", "ethylene", "needs"], "regionBoundary": {"x2": 479.4395751953125, "y1": 325.8900146484375, "x1": 120.2835922241211, "y2": 486.3010559082031}, "caption": "Figure 6: Sentence 2. Trees induced from the syntactic task (top) and language model task (middle) set of distances, as well as the gold-standard trees (bottom).", "page": 13}, {"figType": "Figure", "name": "7", "captionBoundary": {"x2": 526.79248046875, "y1": 729.5205688476562, "x1": 72.0, "y2": 747.47802734375}, "imageText": ["britain", "\u2019s", "retail", "price", "index", "rose", "N.N", "%", "in", "september", "from", "august", "and", "was", "up", "N.N", "%", "for", "the", "year", "the", "central", "statistical", "of\ufb01ce", "said", "britain", "\u2019s", "retail", "price", "index", "rose", "N.N", "%", "in", "september", "from", "august", "and", "was", "up", "N.N", "%", "for", "the", "year", "the", "central", "statistical", "of\ufb01ce", "said", "britain", "\u2019s", "retail", "price", "index", "rose", "N.N", "%", "in", "september", "from", "august", "and", "was", "up", "N.N", "%", "for", "the", "year", "the", "central", "statistical", "of\ufb01ce", "said"], "regionBoundary": {"x2": 522.7308349609375, "y1": 556.8900146484375, "x1": 76.99440002441406, "y2": 711.81103515625}, "caption": "Figure 7: Sentence 3. Trees induced from the syntactic task (top) and language model task (middle) set of distances, as well as the gold-standard trees (bottom).", "page": 13}, {"figType": "Figure", "name": "1", "captionBoundary": {"x2": 290.4169616699219, "y1": 181.10055541992188, "x1": 72.0, "y2": 197.0660400390625}, "imageText": ["hft", "Linear", "Layer", "xt", "ht-1", "cumax", "Llm", "Lsyd", "cumax", "hwt", "Linear", "Layer"], "regionBoundary": {"x2": 278.3475036621094, "y1": 62.8900146484375, "x1": 83.0, "y2": 168.8900146484375}, "caption": "Figure 1: Split-head approach of constructing the two master forget gates in the multi-task setting.", "page": 2}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 527.0333251953125, "y1": 194.09652709960938, "x1": 306.9800109863281, "y2": 339.5760192871094}, "imageText": [], "regionBoundary": {"x2": 515.0, "y1": 61.8900146484375, "x1": 307.0, "y2": 181.8900146484375}, "caption": "Figure 2: Binarized grammar tree and its corresponding syntactic distances. The heights of the bars stand for the values of the distances. To convert this tree to syntactic distances, we first assign all the words an initial value of 1, and then the non-leaf nodes are assigned distances in the order of d3 \u2192 d2 \u2192 d1 \u2192 d4, according to the procedures in the second part of Model section. On the other hand, given the distances, the tree can be recovered in a top-down process by setting up the split boundaries in descending order of distances (i.e., d4 \u2192 d1 \u2192 d2 \u2192 d3). Syntactically, a shorter distance between a pair of words indicates a closer relationship between the constituents on the two sides of the distance. Note that since only the relative order of the distances could affect the structure of the trees, valid values of these distances are not unique.", "page": 2}, {"figType": "Figure", "name": "17", "captionBoundary": {"x2": 525.5472412109375, "y1": 312.2426452636719, "x1": 72.0, "y2": 330.2001037597656}, "imageText": ["and", "i", "think", "institutions", "are", "going", "to", "come", "in", "and", "buy", "and", "i", "think", "institutions", "are", "going", "to", "come", "in", "and", "buy", "and", "i", "think", "institutions", "are", "going", "to", "come", "in", "and", "buy"], "regionBoundary": {"x2": 390.3580017089844, "y1": 151.8900146484375, "x1": 209.3664093017578, "y2": 294.53228759765625}, "caption": "Figure 17: Sentence 13. Trees induced from the syntactic task (top) and language model task (middle) set of distances, as well as the gold-standard trees (bottom).", "page": 17}, {"figType": "Figure", "name": "18", "captionBoundary": {"x2": 525.5472412109375, "y1": 652.2446899414062, "x1": 72.0, "y2": 670.2021484375}, "imageText": ["there", "\u2019s", "nothing", "rational", "about", "this", "kind", "of", "action", "there", "\u2019s", "nothing", "rational", "about", "this", "kind", "of", "action", "there", "\u2019s", "nothing", "rational", "about", "this", "kind", "of", "action"], "regionBoundary": {"x2": 384.483642578125, "y1": 519.8900146484375, "x1": 215.2406005859375, "y2": 634.5343017578125}, "caption": "Figure 18: Sentence 14. Trees induced from the syntactic task (top) and language model task (middle) set of distances, as well as the gold-standard trees (bottom).", "page": 17}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 291.8368225097656, "y1": 580.1715087890625, "x1": 71.6780014038086, "y2": 655.9119873046875}, "imageText": [], "regionBoundary": {"x2": 288.0, "y1": 278.8900146484375, "x1": 77.0, "y2": 549.8900146484375}, "caption": "Figure 4: Accuracy breakdown w.r.t. constituent height in unbiased trees derived from the syntactic task distances in our model (top) and the language modeling distances (bottom). A constituent is considered as correct if its boundaries correspond to a true constituent. The constituents\u2019 heights are those in the predicted tree. Since constituents that represent the whole sentence always have correct boundaries, they are excluded from the calculation.", "page": 7}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 525.5482177734375, "y1": 230.57150268554688, "x1": 71.6719970703125, "y2": 248.52899169921875}, "imageText": ["The", "company", "which", "issued", "a", "statement", "on", "the", "agreement", "late", "Friday", "said", "that", "1", "million", "of", "the", "payment", "was", "previously", "provided", "for", "in", "its", "\ufb01nancial", "statements", "and", "that", "500,000", "will", "be", "recognized", "in", "its", "1989", "third-quarter", "statement", "the", "company", "which", "issued", "a", "statement", "on", "the", "agreement", "late", "friday", "said", "that", "N", "million", "of", "the", "payment", "was", "previously", "provided", "for", "in", "its", "\ufb01nancial", "statements", "and", "that", "NN", "will", "be", "recognized", "in", "its", "N", "third-quarter", "statement", "the", "company", "which", "issued", "a", "statement", "on", "the", "agreement", "late", "friday", "said", "that", "N", "million", "of", "the", "payment", "was", "previously", "provided", "for", "in", "its", "\ufb01nancial", "statements", "and", "that", "NN", "will", "be", "recognized", "in", "its", "N", "third-quarter", "statement"], "regionBoundary": {"x2": 522.5799560546875, "y1": 62.8900146484375, "x1": 76.19538879394531, "y2": 215.21673583984375}, "caption": "Figure 3: Trees induced from the syntactic task distances in our model (top), the language modeling task distances (middle) as well as the gold-standard trees (bottom).", "page": 7}, {"figType": "Figure", "name": "16", "captionBoundary": {"x2": 525.5472412109375, "y1": 716.8585205078125, "x1": 71.99998474121094, "y2": 734.8169555664062}, "imageText": ["it", "also", "drops", "a", "provision", "that", "would", "have", "permitted", "corporations", "to", "use", "excess", "pension", "funds", "to", "pay", "health", "bene\ufb01ts", "for", "current", "retirees", "it", "also", "drops", "a", "provision", "that", "would", "have", "permitted", "corporations", "to", "use", "excess", "pension", "funds", "to", "pay", "health", "bene\ufb01ts", "for", "current", "retirees", "it", "also", "drops", "a", "provision", "that", "would", "have", "permitted", "corporations", "to", "use", "excess", "pension", "funds", "to", "pay", "health", "bene\ufb01ts", "for", "current", "retirees"], "regionBoundary": {"x2": 467.2482604980469, "y1": 548.8900146484375, "x1": 131.8762969970703, "y2": 700.6294555664062}, "caption": "Figure 16: Sentence 12. Trees induced from the syntactic task (top) and language model task (middle) set of distances, as well as the gold-standard trees (bottom).", "page": 16}, {"figType": "Figure", "name": "14", "captionBoundary": {"x2": 525.5482177734375, "y1": 250.93350219726562, "x1": 72.00099182128906, "y2": 268.8909912109375}, "imageText": ["when", "you", "suggest", "otherwise", "you", "leave", "the", "realm", "of", "reporting", "and", "enter", "the", "orbit", "of", "speculation", "when", "you", "suggest", "otherwise", "you", "leave", "the", "realm", "of", "reporting", "and", "enter", "the", "orbit", "of", "speculation", "when", "you", "suggest", "otherwise", "you", "leave", "the", "realm", "of", "reporting", "and", "enter", "the", "orbit", "of", "speculation"], "regionBoundary": {"x2": 464.0478515625, "y1": 86.8900146484375, "x1": 135.6776123046875, "y2": 233.2249755859375}, "caption": "Figure 14: Sentence 10. Trees induced from the syntactic task (top) and language model task (middle) set of distances, as well as the gold-standard trees (bottom).", "page": 16}, {"figType": "Figure", "name": "15", "captionBoundary": {"x2": 525.5472412109375, "y1": 470.2555236816406, "x1": 71.99999237060547, "y2": 488.2129821777344}, "imageText": ["but", "not", "much", "money", "was", "spent", "on", "the", "shows", "either", "a", "situation", "that", "encouraged", "cheap-to-make", "talk", "and", "game", "shows", "while", "discouraging", "expensive-to-produce", "dramas", "but", "not", "much", "money", "was", "spent", "on", "the", "shows", "either", "a", "situation", "that", "encouraged", "cheap-to-make", "talk", "and", "game", "shows", "while", "discouraging", "expensive-to-produce", "dramas", "but", "not", "much", "money", "was", "spent", "on", "the", "shows", "either", "a", "situation", "that", "encouraged", "cheap-to-make", "talk", "and", "game", "shows", "while", "discouraging", "expensive-to-produce", "dramas"], "regionBoundary": {"x2": 507.4359130859375, "y1": 328.8900146484375, "x1": 91.68933868408203, "y2": 454.0264587402344}, "caption": "Figure 15: Sentence 11. Trees induced from the syntactic task (top) and language model task (middle) set of distances, as well as the gold-standard trees (bottom).", "page": 16}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 290.41827392578125, "y1": 184.20755004882812, "x1": 71.69100189208984, "y2": 240.02301025390625}, "imageText": ["Vanilla", "Multitasking", "60.9", "58.5", "24.9", "One", "set", "of", "trees", "58.5", "55.9", "54.4", "Two", "sets", "of", "trees", "57.8", "55.7", "61.3", "Multitask", "Variants", "No", "Parse", "Tree", "58.3", "55.9", "39.0", "Random", "Tree", "60.2", "57.5", "32.4", "Gold", "Parse", "Tree", "57.8", "55.7", "61.3", "Tree", "Structure", "Layer", "for", "Supervision", "1st", "layer", "58.0", "55.6", "57.7", "2nd", "layer", "57.8", "55.5", "59.7", "3rd", "layer", "57.8", "55.7", "61.3", "Ablation", "Experiment", "Validation", "Test", "WSJ", "Study", "Detail", "PPL", "PPL", "F1"], "regionBoundary": {"x2": 289.0, "y1": 62.8900146484375, "x1": 75.0, "y2": 171.8900146484375}, "caption": "Table 4: Perplexity and unlabeled parsing F1 in ablation studies. We choose unbiased algorithm and the layer with supervision injected. For the unsupervised models, we report the layer with best F1 score. (Top) When supervising on different layers. (Middle) Using different tree structures for supervision. (Bottom) Different multitasking strategies.", "page": 8}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 527.0320434570312, "y1": 184.82553100585938, "x1": 306.9670104980469, "y2": 230.67901611328125}, "imageText": ["Model", "Param", "Dev", "Test", "Gal", "and", "Ghahramani", "(2016)", "-", "Variational", "LSTM", "66M", "\u2212", "73.4", "Kim", "et", "al.", "(2016)", "-", "CharCNN", "19M", "\u2212", "78.9", "Merity", "et", "al.", "(2016)", "-", "Pointer", "Sentinel-LSTM", "21M", "72.4", "70.9", "Grave", "et", "al.", "(2016)", "-", "LSTM", "\u2212", "\u2212", "82.3", "Zoph", "and", "Le", "(2016)", "-", "NAS", "Cell", "54M", "\u2212", "62.4", "Zilly", "et", "al.", "(2017)", "-", "Variational", "RHN", "23M", "67.9", "65.4", "Shen", "et", "al.", "(2017)", "-", "PRPN", "\u2212", "\u2212", "62.0", "Merity", "et", "al.", "(2017)", "-", "3-layer", "AWD-LSTM", "24M", "60.0", "57.3", "Zolna", "et", "al.", "(2018)", "-", "Fraternal", "dropout", "24M", "58.9", "56.8", "Shen", "et", "al.", "(2018)", "-", "3-layer", "ON-LSTM", "25M", "58.3", "56.2", "ONLSTM-SYD", "25M", "57.8", "55.7", "Yang", "et", "al.", "(2018)", "-", "AWD-LSTM-MoS", "22M", "56.5", "54.4", "Takase", "et", "al.", "(2018)", "-", "AWD-LSTM-DOC", "23M", "54.1", "52.4"], "regionBoundary": {"x2": 524.0, "y1": 62.8900146484375, "x1": 309.0, "y2": 172.8900146484375}, "caption": "Table 1: Various language models evaluated on validation and test sets on PTB-Concat. Our model is denoted as ONLSTMSYD, which incorporates tree structures during training. Yang et al. (2018) and Takase et al. (2018) focus on improving the softmax module of LSTM LM, which are orthogonal to ours.", "page": 4}, {"figType": "Figure", "name": "11", "captionBoundary": {"x2": 525.5462036132812, "y1": 235.35653686523438, "x1": 71.99900817871094, "y2": 253.31402587890625}, "imageText": ["it", "\u2019s", "turning", "out", "to", "be", "a", "real", "blockbuster", "mr.", "sweig", "said", "it", "\u2019s", "turning", "out", "to", "be", "a", "real", "blockbuster", "mr.", "sweig", "said", "it", "\u2019s", "turning", "out", "to", "be", "a", "real", "blockbuster", "mr.", "sweig", "said"], "regionBoundary": {"x2": 393.94403076171875, "y1": 69.8900146484375, "x1": 205.78038024902344, "y2": 217.6480712890625}, "caption": "Figure 11: Sentence 7. Trees induced from the syntactic task (top) and language model task (middle) set of distances, as well as the gold-standard trees (bottom).", "page": 15}, {"figType": "Figure", "name": "12", "captionBoundary": {"x2": 525.5462036132812, "y1": 499.2005310058594, "x1": 71.99900817871094, "y2": 517.157958984375}, "imageText": ["the", "fact", "that", "this", "happened", "two", "years", "ago", "and", "there", "was", "a", "recovery", "gives", "people", "some", "comfort", "that", "this", "wo", "n\u2019t", "be", "a", "problem", "the", "fact", "that", "this", "happened", "two", "years", "ago", "and", "there", "was", "a", "recovery", "gives", "people", "some", "comfort", "that", "this", "wo", "n\u2019t", "be", "a", "problem", "the", "fact", "that", "this", "happened", "two", "years", "ago", "and", "there", "was", "a", "recovery", "gives", "people", "some", "comfort", "that", "this", "wo", "n\u2019t", "be", "a", "problem"], "regionBoundary": {"x2": 513.1019897460938, "y1": 278.8900146484375, "x1": 86.6196060180664, "y2": 481.4912109375}, "caption": "Figure 12: Sentence 8. Trees induced from the syntactic task (top) and language model task (middle) set of distances, as well as the gold-standard trees (bottom).", "page": 15}, {"figType": "Figure", "name": "13", "captionBoundary": {"x2": 525.5462036132812, "y1": 734.12255859375, "x1": 71.99900817871094, "y2": 752.0800170898438}, "imageText": ["ncnb", "will", "also", "acquire", "N", "million", "of", "freedom", "\u2019s", "assets", "from", "the", "rtc", "which", "will", "require", "N", "million", "in", "assistance", "ncnb", "will", "also", "acquire", "N", "million", "of", "freedom", "\u2019s", "assets", "from", "the", "rtc", "which", "will", "require", "N", "million", "in", "assistance", "ncnb", "will", "also", "acquire", "N", "million", "of", "freedom", "\u2019s", "assets", "from", "the", "rtc", "which", "will", "require", "N", "million", "in", "assistance"], "regionBoundary": {"x2": 486.5141296386719, "y1": 542.8900146484375, "x1": 113.20741271972656, "y2": 716.4130249023438}, "caption": "Figure 13: Sentence 9. Trees induced from the syntactic task (top) and language model task (middle) set of distances, as well as the gold-standard trees (bottom).", "page": 15}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 403.3568912082248, "y1": 251.52854919433594, "x1": 100.0, "y2": 273.7028333875868}, "name": "1", "caption_text": "Figure 1: Split-head approach of constructing the two master forget gates in the multi-task setting.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 386.0, "y1": 87.0, "x1": 100.0, "y2": 251.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 731.990729437934, "y1": 269.57850986056854, "x1": 426.36112636990015, "y2": 471.63336012098523}, "name": "2", "caption_text": "Figure 2: Binarized grammar tree and its corresponding syntactic distances. The heights of the bars stand for the values of the distances. To convert this tree to syntactic distances, we first assign all the words an initial value of 1, and then the non-leaf nodes are assigned distances in the order of d3 \u2192 d2 \u2192 d1 \u2192 d4, according to the procedures in the second part of Model section. On the other hand, given the distances, the tree can be recovered in a top-down process by setting up the split boundaries in descending order of distances (i.e., d4 \u2192 d1 \u2192 d2 \u2192 d3). Syntactically, a shorter distance between a pair of words indicates a closer relationship between the constituents on the two sides of the distance. Note that since only the relative order of the distances could affect the structure of the trees, valid values of these distances are not unique.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 703.0, "y1": 99.0, "x1": 435.0, "y2": 242.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 731.9889492458767, "y1": 256.7021263970269, "x1": 426.3430701361762, "y2": 320.38752237955725}, "name": "1", "caption_text": "Table 1: Various language models evaluated on validation and test sets on PTB-Concat. Our model is denoted as ONLSTMSYD, which incorporates tree structures during training. Yang et al. (2018) and Takase et al. (2018) focus on improving the softmax module of LSTM LM, which are orthogonal to ours.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 727.0, "y1": 86.0, "x1": 426.0, "y2": 256.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.98114522298175, "y1": 262.2632768419054, "x1": 99.57083596123589, "y2": 325.94867282443573}, "name": "2", "caption_text": "Table 2: Language modeling perplexity on PTB-Sepsent and CTB-Sepsent. Kim et al. (2019b) report two results of PRPN, the default one using settings in Shen et al. (2017) and another one finetuned by themselves. Our models use the same hyperparameter settings as in Section 5.1.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 399.0, "y1": 86.0, "x1": 107.0, "y2": 245.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 730.3589714898003, "y1": 300.10905795627167, "x1": 99.24028184678819, "y2": 377.630615234375}, "name": "3", "caption_text": "Table 3: Unlabeled parsing results evaluated on the WSJ10 and the full WSJ test set. Numbers in bold font indicate that they are the best compared to those computed from the other parts of the model (i.e., within the same section in the table). The Algorithm column represents whether bias or unbiased algorithm is performed. ONLSTM-SYDsyd and ONLSTM-SYDlm represent two sets of trees induced from loss Lsyd and Llm respectively. The Accuracy columns represent the fraction of ground truth constituents of a given type that correspond to constituents in the model parses. The R/L Ratio column represents the ratio between the number of words that are left children of its parent, and those that are right children.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 727.0, "y1": 86.0, "x1": 100.0, "y2": 300.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9280802408854, "y1": 320.23819817437067, "x1": 99.54444037543402, "y2": 345.17915513780383}, "name": "3", "caption_text": "Figure 3: Trees induced from the syntactic task distances in our model (top), the language modeling task distances (middle) as well as the gold-standard trees (bottom).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 726.0, "y1": 86.0, "x1": 106.0, "y2": 301.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.32892015245227, "y1": 805.7937622070312, "x1": 99.55277972751193, "y2": 910.9888712565104}, "name": "4", "caption_text": "Figure 4: Accuracy breakdown w.r.t. constituent height in unbiased trees derived from the syntactic task distances in our model (top) and the language modeling distances (bottom). A constituent is considered as correct if its boundaries correspond to a true constituent. The constituents\u2019 heights are those in the predicted tree. Since constituents that represent the whole sentence always have correct boundaries, they are excluded from the calculation.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 400.0, "y1": 387.0, "x1": 107.0, "y2": 763.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.35871378580725, "y1": 255.84381951226126, "x1": 99.57083596123589, "y2": 333.3652920193142}, "name": "4", "caption_text": "Table 4: Perplexity and unlabeled parsing F1 in ablation studies. We choose unbiased algorithm and the layer with supervision injected. For the unsupervised models, we report the layer with best F1 score. (Top) When supervising on different layers. (Middle) Using different tree structures for supervision. (Bottom) Different multitasking strategies.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 402.0, "y1": 86.0, "x1": 104.0, "y2": 239.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 731.6562228732639, "y1": 379.35218811035156, "x1": 100.0, "y2": 404.29450141059027}, "name": "5", "caption_text": "Figure 5: Sentence 1. Trees induced from the syntactic task (top) and language model task (middle) set of distances, as well as the gold-standard trees (bottom).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 676.0, "y1": 103.0, "x1": 157.0, "y2": 357.0}, "page": 13, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 731.6575792100695, "y1": 700.0133090549045, "x1": 100.00138812594943, "y2": 724.955579969618}, "name": "6", "caption_text": "Figure 6: Sentence 2. Trees induced from the syntactic task (top) and language model task (middle) set of distances, as well as the gold-standard trees (bottom).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 666.0, "y1": 453.0, "x1": 167.0, "y2": 678.0}, "page": 13, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 731.6562228732639, "y1": 1013.2230122884114, "x1": 100.0, "y2": 1038.1639268663193}, "name": "7", "caption_text": "Figure 7: Sentence 3. Trees induced from the syntactic task (top) and language model task (middle) set of distances, as well as the gold-standard trees (bottom).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 726.0, "y1": 773.0, "x1": 107.0, "y2": 991.0}, "page": 13, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 731.6562228732639, "y1": 366.8535444471571, "x1": 100.0, "y2": 391.79450141059027}, "name": "8", "caption_text": "Figure 8: Sentence 4. Trees induced from the syntactic task (top) and language model task (middle) set of distances, as well as the gold-standard trees (bottom).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 681.0, "y1": 120.0, "x1": 152.0, "y2": 345.0}, "page": 14, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 731.6562228732639, "y1": 696.0076649983723, "x1": 100.0, "y2": 720.9486219618055}, "name": "9", "caption_text": "Figure 9: Sentence 5. Trees induced from the syntactic task (top) and language model task (middle) set of distances, as well as the gold-standard trees (bottom).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 697.0, "y1": 474.0, "x1": 136.0, "y2": 675.0}, "page": 14, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9279954698351, "y1": 996.4368184407551, "x1": 100.00138812594943, "y2": 1021.3790893554688}, "name": "10", "caption_text": "Figure 10: Sentence 6. Trees induced from the syntactic task (top) and language model task (middle) set of distances, as well as the gold-standard trees (bottom).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 703.0, "y1": 803.0, "x1": 129.0, "y2": 976.0}, "page": 14, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9252827962239, "y1": 326.8840789794922, "x1": 99.99862247043185, "y2": 351.82503594292535}, "name": "11", "caption_text": "Figure 11: Sentence 7. Trees induced from the syntactic task (top) and language model task (middle) set of distances, as well as the gold-standard trees (bottom).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 547.0, "y1": 97.0, "x1": 286.0, "y2": 305.0}, "page": 15, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9252827962239, "y1": 693.3340708414713, "x1": 99.99862247043185, "y2": 718.2749430338541}, "name": "12", "caption_text": "Figure 12: Sentence 8. Trees induced from the syntactic task (top) and language model task (middle) set of distances, as well as the gold-standard trees (bottom).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 713.0, "y1": 387.0, "x1": 120.0, "y2": 671.0}, "page": 15, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9252827962239, "y1": 1019.6146647135416, "x1": 99.99862247043185, "y2": 1044.5555792914497}, "name": "13", "caption_text": "Figure 13: Sentence 9. Trees induced from the syntactic task (top) and language model task (middle) set of distances, as well as the gold-standard trees (bottom).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 675.0, "y1": 754.0, "x1": 157.0, "y2": 998.0}, "page": 15, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9280802408854, "y1": 348.5187530517578, "x1": 100.00137752956815, "y2": 373.459710015191}, "name": "14", "caption_text": "Figure 14: Sentence 10. Trees induced from the syntactic task (top) and language model task (middle) set of distances, as well as the gold-standard trees (bottom).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 644.0, "y1": 121.0, "x1": 188.0, "y2": 327.0}, "page": 16, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9267239040798, "y1": 653.1326717800564, "x1": 99.9999894036187, "y2": 678.0735863579644}, "name": "15", "caption_text": "Figure 15: Sentence 11. Trees induced from the syntactic task (top) and language model task (middle) set of distances, as well as the gold-standard trees (bottom).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 704.0, "y1": 457.0, "x1": 127.0, "y2": 632.0}, "page": 16, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9267239040798, "y1": 995.6368340386284, "x1": 99.99997880723741, "y2": 1020.5791049533419}, "name": "16", "caption_text": "Figure 16: Sentence 12. Trees induced from the syntactic task (top) and language model task (middle) set of distances, as well as the gold-standard trees (bottom).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 649.0, "y1": 762.0, "x1": 183.0, "y2": 975.0}, "page": 16, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9267239040798, "y1": 433.6703406439887, "x1": 100.0, "y2": 458.61125522189667}, "name": "17", "caption_text": "Figure 17: Sentence 13. Trees induced from the syntactic task (top) and language model task (middle) set of distances, as well as the gold-standard trees (bottom).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 542.0, "y1": 210.0, "x1": 291.0, "y2": 412.0}, "page": 17, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9267239040798, "y1": 905.8954026963976, "x1": 100.0, "y2": 930.8363172743055}, "name": "18", "caption_text": "Figure 18: Sentence 14. Trees induced from the syntactic task (top) and language model task (middle) set of distances, as well as the gold-standard trees (bottom).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 534.0, "y1": 722.0, "x1": 299.0, "y2": 884.0}, "page": 17, "dpi": 0}], "error": null, "pdf": "/work/host-output/3b0ee392aa7c4cefbbdd8a841f363c972aad5fa0/2020.acl-main.591.pdf", "dpi": 100}