{"raw_detected_boxes": [[{"x2": 721.0, "y1": 313.0, "x1": 435.0, "y2": 432.0}], [{"x2": 719.0, "y1": 86.0, "x1": 438.0, "y2": 163.0}], [{"x2": 718.0, "y1": 91.0, "x1": 109.0, "y2": 391.0}], [{"x2": 695.0, "y1": 95.0, "x1": 141.0, "y2": 248.0}, {"x2": 714.0, "y1": 317.0, "x1": 438.0, "y2": 417.0}], [{"x2": 725.0, "y1": 90.0, "x1": 429.0, "y2": 257.0}, {"x2": 407.0, "y1": 96.0, "x1": 103.0, "y2": 193.0}], [], [], [{"x2": 710.0, "y1": 86.0, "x1": 447.0, "y2": 232.0}, {"x2": 395.0, "y1": 98.0, "x1": 105.0, "y2": 349.0}, {"x2": 700.0, "y1": 373.0, "x1": 457.0, "y2": 459.0}], [{"x2": 692.0, "y1": 90.0, "x1": 139.0, "y2": 364.0}, {"x2": 368.0, "y1": 485.0, "x1": 139.0, "y2": 552.0}, {"x2": 720.0, "y1": 476.0, "x1": 438.0, "y2": 599.0}], [{"x2": 696.0, "y1": 88.0, "x1": 134.0, "y2": 350.0}, {"x2": 388.0, "y1": 431.0, "x1": 113.0, "y2": 688.0}], [{"x2": 376.0, "y1": 95.0, "x1": 127.0, "y2": 180.0}, {"x2": 393.0, "y1": 289.0, "x1": 107.0, "y2": 428.0}]], "raw_pdffigures_output": {"regionless-captions": [{"figType": "Figure", "boundary": {"x2": 509.7303161621094, "y1": 295.7472229003906, "x1": 78.65100860595703, "y2": 312.8018798828125}, "text": "Figure 2: Examples from Ref-Easy, Ref-Hard, and Ref-Adv splits. As seen, Ref-Hard and Ref-Adv have several words in common but differ in their linguistic structure and the target object of interest.", "name": "2", "page": 2}, {"figType": "Figure", "boundary": {"x2": 525.5424194335938, "y1": 293.8845520019531, "x1": 71.64096069335938, "y2": 311.842041015625}, "text": "Figure 2: Examples fro ef- sy, ef- rd, and Ref-Adv splits. As seen, Ref-Hard and Ref-Adv have several words in common but dif er i t i li istic structure and the target object of interest.", "name": "2", "page": 2}, {"figType": "Table", "boundary": {"x2": 289.0441589355469, "y1": 517.3442993164062, "x1": 92.00698852539062, "y2": 533.4404296875}, "text": "Table 8: Distribution of object categories in Ref-Easy, Ref-Hard, and Ref-Adv splits.", "name": "8", "page": 8}, {"figType": "Figure", "boundary": {"x2": 525.7157592773438, "y1": 445.8825378417969, "x1": 306.9670104980469, "y2": 463.8399963378906}, "text": "Figure 6: Referring expression length distribution for Ref-Easy, Ref-Hard, Ref-Adv datasets.", "name": "6", "page": 8}, {"figType": "Table", "boundary": {"x2": 448.2665710449219, "y1": 390.4587707519531, "x1": 353.4481201171875, "y2": 395.8390808105469}, "text": "Table 7: Ref-Adv Statistics", "name": "7", "page": 8}], "figures": [{"figType": "Figure", "name": "1", "captionBoundary": {"x2": 525.7955322265625, "y1": 322.0465393066406, "x1": 307.2760009765625, "y2": 375.8699645996094}, "imageText": ["Model", "Region", "Proposals", "(r1,", "r2)", "Input", "Image", "Original", "Expression", "Adversarial", "Modification", "r1", "r1", "r1r2", "a", "blue", "fork", "next", "to", "the", "pastry", "plate", "Modelpastry", "on", "the", "plate", "next", "to", "a", "blue", "fork"], "regionBoundary": {"x2": 526.0, "y1": 221.8900146484375, "x1": 307.0, "y2": 312.8900146484375}, "caption": "Figure 1: An example of the visual referring expression recognition task. If the word pastry is present in the referring expression, models prefer the bounding box r1 (highlighted in green) irrespective of the change in linguistic structure (word order).", "page": 0}, {"figType": "Table", "name": "10", "captionBoundary": {"x2": 291.92169189453125, "y1": 320.5725402832031, "x1": 71.64100646972656, "y2": 362.44097900390625}, "imageText": ["Without", "TL", "and", "MTL", "83.39", "83.63", "70.90", "TL", "with", "VQA", "82.26", "84.14", "72.96", "TL", "with", "GQA", "80.60", "82.08", "70.41", "TL", "with", "GQA-Rel", "81.05", "83.12", "70.78", "MTL", "with", "VQA", "81.20", "82.10", "70.82", "MTL", "with", "GQA-Rel", "83.45", "84.30", "73.92", "ViLBERT", "Ref-Dev", "Ref-Test", "Ref-Adv"], "regionBoundary": {"x2": 288.0, "y1": 200.8900146484375, "x1": 75.0, "y2": 307.8900146484375}, "caption": "Table 10: Comparing ViLBERT\u2019s Multi-task Learning (MTL) with Transfer Learning (TL) experiments. RefDev and Ref-Test correspond to: RefCOCOg-Dev and RefCOCOg-Test splits respectively.", "page": 10}, {"figType": "Table", "name": "9", "captionBoundary": {"x2": 290.2703857421875, "y1": 141.48855590820312, "x1": 71.69100189208984, "y2": 183.3560791015625}, "imageText": ["GQA-Rel", "Dev", "53.7%", "56.0%", "GQA", "Dev", "40.24%", "42.1%", "GQA", "Test", "36.64%", "39.2%", "Split", "Before", "MTL", "After", "MTL"], "regionBoundary": {"x2": 271.0, "y1": 62.8900146484375, "x1": 91.0, "y2": 128.8900146484375}, "caption": "Table 9: Performance on GQA-Rel Dev, GQA-Dev and GQA-Test splits before and after MTL training with RefCOCOg (Note: MTL training for all the three rows is performed using GQA-Rel and RefCOCOg).", "page": 10}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 527.2866821289062, "y1": 133.74154663085938, "x1": 306.9670104980469, "y2": 175.61004638671875}, "imageText": ["8.0", "10.2", "11.4", "avg.", "length", "in", "words", "data", "size", "8034", "(83.7%", "of", "RefCOCOg)", "1568", "(16.3%", "of", "RefCOCOg)", "3704", "Ref-Easy", "Ref-Hard", "Ref-Adv"], "regionBoundary": {"x2": 518.0, "y1": 62.8900146484375, "x1": 315.0, "y2": 121.8900146484375}, "caption": "Table 1: Statistics of Ref-Easy, Ref-Hard and Ref-Adv. Ref-Easy and Ref-Hard indicate the proportion of samples in RefCOCOg test set that are insensitive and sensitive to linguistic structure respectively.", "page": 1}, {"figType": "Table", "name": "8", "captionBoundary": {"x2": 291.5126647949219, "y1": 513.9615478515625, "x1": 71.69100189208984, "y2": 531.9190063476562}, "imageText": ["Outdoor", "1.21%", "1.90%", "1.97%", "Food", "7.94%", "9.80%", "9.63%", "Indoor", "2.81%", "2.83%", "2.76%", "Appliance", "0.80%", "1.07%", "1.11%", "Kitchen", "4.52%", "5.73%", "5.77%", "Accessory", "3.20%", "5.44%", "5.29%", "Person", "37.26%", "20.88%", "21.01%", "Animal", "15.95%", "13.92%", "13.90%", "Vehicle", "10.91%", "10.40%", "10.26%", "Sports", "1.45%", "5.04%", "5.13%", "Electronic", "2.62%", "3.20%", "3.31%", "Furniture", "11.28%", "19.73%", "19.83%", "Ref-Easy", "8034", "samples", "Ref-Hard", "1568", "samples", "Ref-Adv", "3704", "samples"], "regionBoundary": {"x2": 281.0, "y1": 304.8900146484375, "x1": 81.0, "y2": 501.8900146484375}, "caption": "Table 8: Distribution of object categories in Ref-Easy, Ref-Hard, and Ref-Adv splits.", "page": 9}, {"figType": "Figure", "name": "7", "captionBoundary": {"x2": 510.9685363769531, "y1": 265.6045227050781, "x1": 86.57499694824219, "y2": 271.60699462890625}, "imageText": [], "regionBoundary": {"x2": 503.0, "y1": 61.8900146484375, "x1": 94.0, "y2": 253.8900146484375}, "caption": "Figure 7: Relative frequency of the most frequent spatial relationships in Ref-Easy, Ref-Hard, and Ref-Adv", "page": 9}, {"figType": "Table", "name": "6", "captionBoundary": {"x2": 527.2007446289062, "y1": 347.8065490722656, "x1": 306.9670104980469, "y2": 401.6299743652344}, "imageText": ["Ref-Easy", "86.40", "75.06", "76.00", "Ref-Hard", "72.73", "51.13", "56.60", "Ref-Adv", "71.08", "50.23", "57.40", "Test", "Original", "Shuf", "N+J"], "regionBoundary": {"x2": 504.0, "y1": 268.8900146484375, "x1": 329.0, "y2": 335.8900146484375}, "caption": "Table 6: Ref-Easy, Ref-Hard, and Ref-Adv test accuracies of ViLBERT on (a) original undistorted split, (b) after randomly shuffling words (Shuf) in the referring expression, and (c) after deleting all the words except for nouns and adjectives (N+J).", "page": 7}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 291.5126647949219, "y1": 263.4305725097656, "x1": 71.69100189208984, "y2": 281.3890380859375}, "imageText": ["Outdoor", "0.89%", "0.88%", "1.65%", "Food", "10.16%", "10.07%", "8.10%", "Indoor", "3.10%", "3.09%", "2.59%", "Appliance", "0.67%", "0.68%", "1.03%", "Kitchen", "3.95%", "3.95%", "5.40%", "Accessory", "2.33%", "2.33%", "2.85%", "Person", "49.50%", "49.70%", "37.02%", "Animal", "13.26%", "13.27%", "15.05%", "Vehicle", "7.23%", "7.22%", "10.71%", "Sports", "0.73%", "0.74%", "1.91%", "Electronic", "1.94%", "1.95%", "2.56%", "Furniture", "6.14%", "6.12%", "11.09%", "RefCOCO", "RefCOCO+", "RefCOCOg"], "regionBoundary": {"x2": 288.0, "y1": 62.8900146484375, "x1": 74.0, "y2": 250.8900146484375}, "caption": "Table 4: Distribution of object categories in RefCOCO, RefCOCO+, and RefCOCOg datasets.", "page": 7}, {"figType": "Table", "name": "5", "captionBoundary": {"x2": 527.2000732421875, "y1": 184.92556762695312, "x1": 306.9469909667969, "y2": 250.7041015625}, "imageText": ["ViLBERT", "(Lu", "et", "al.,", "2019)", "83.6", "71.4", "73.6", "MattNet", "(Yu", "et", "al.,", "2018)", "78.5", "75.3", "76.1", "CMN", "(Hu", "et", "al.,", "2017)", "69.4", "66.4", "67.4", "GroundNet", "(Cirik", "et", "al.,", "2018a)", "65.8", "57.6", "62.8", "Model", "Original", "Shuf", "N+J"], "regionBoundary": {"x2": 511.0, "y1": 62.8900146484375, "x1": 321.0, "y2": 172.8900146484375}, "caption": "Table 5: RefCOCOg test accuracies of SOTA models on (a) original undistorted split, (b) after randomly shuffling words (Shuf) in the referring expression, and (c) after deleting all the words except for nouns and adjectives (N+J). ViLBERT is relatively more robust than other baselines.", "page": 7}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 525.5433349609375, "y1": 312.6275329589844, "x1": 306.9670104980469, "y2": 330.5849914550781}, "imageText": ["GroundNet", "66.50", "65.80", "67.11", "54.47", "42.90", "CMN", "70.00", "69.40", "69.55", "68.63", "49.50", "MattNet", "79.21", "78.51", "80.96", "65.94", "54.64", "ViLBERT", "83.39", "83.63", "85.93", "72.00", "70.90", "Model", "Dev", "Test", "Easy", "Hard", "Adv"], "regionBoundary": {"x2": 518.0, "y1": 219.8900146484375, "x1": 315.0, "y2": 299.8900146484375}, "caption": "Table 2: Accuracy of models on RefCOCOg standard splits and our splits Ref-Easy, Ref-Hard and Ref-Adv.", "page": 3}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 464.73974609375, "y1": 191.83053588867188, "x1": 132.80599975585938, "y2": 197.8330078125}, "imageText": ["Task-Specific", "Layers", "Input", "question/", "Referring", "expression", "Shared", "ViLBERT", "Layers", "Input", "image", "Co-TRM", "TRM", "Co-TRM", "TRM"], "regionBoundary": {"x2": 502.0, "y1": 61.8900146484375, "x1": 93.0, "y2": 190.8900146484375}, "caption": "Figure 3: Multi-task learning model for referring expression recognition with GQA", "page": 3}, {"figType": "Table", "name": "7", "captionBoundary": {"x2": 233.87095642089844, "y1": 409.4715576171875, "x1": 128.08900451660156, "y2": 415.4740295410156}, "imageText": ["Referring", "Expressions", "3704", "Unique", "Images", "976", "Vocabulary", "2319", "Avg.", "Length", "of", "Expression", "11.4", "Outdoor", "1.21%", "1.90%", "1.97%", "Food", "7.94%", "9.80%", "9.63%", "Indoor", "2.81%", "2.83%", "2.76%", "Appliance", "0.80%", "1.07%", "1.11%", "Ref-Easy", "8034", "samples", "Ref-Hard", "1568", "samples", "Ref-Adv", "3704", "samples"], "regionBoundary": {"x2": 277.1441955566406, "y1": 336.2966003417969, "x1": 95.0, "y2": 403.0251159667969}, "caption": "Table 7: Ref-Adv Statistics", "page": 8}, {"figType": "Figure", "name": "5", "captionBoundary": {"x2": 527.1937255859375, "y1": 278.7615661621094, "x1": 71.99996948242188, "y2": 308.674072265625}, "imageText": [], "regionBoundary": {"x2": 499.0, "y1": 64.8900146484375, "x1": 94.0, "y2": 262.8900146484375}, "caption": "Figure 5: Overview of our three-stage Ref-Adv construction process. Given the image, referring expression, groundtruth bounding boxes for all the samples in RefCOCOg test split, we first filter out the hard samples and then construct adversarial expressions using them. Please refer to section 2 for further detail.", "page": 8}, {"figType": "Figure", "name": "5", "captionBoundary": {"x2": 500.296875, "y1": 273.29681396484375, "x1": 92.283935546875, "y2": 300.10888671875}, "imageText": [], "regionBoundary": {"x2": 527.1937255859375, "y1": 278.7615661621094, "x1": 500.296875, "y2": 390.4587707519531}, "caption": "Figure 5: Overview of our three-stage Ref-Adv construction process. Given the image, referring expression, groundtruth bounding boxes for all the samples in RefCOCOg test split, we first filter out the hard samples and then construct adversarial expressions using them. Please refer to section 2 for further detail.", "page": 8}, {"figType": "Figure", "name": "6", "captionBoundary": {"x2": 288.08258056640625, "y1": 656.2767333984375, "x1": 92.00698852539062, "y2": 672.3729248046875}, "imageText": ["Visualization"], "regionBoundary": {"x2": 262.3092346191406, "y1": 672.3729248046875, "x1": 72.00000762939453, "y2": 690.6780395507812}, "caption": "Figure 6: Referring expression length distribution for Ref-Easy, Ref-Hard, Ref-Adv datasets.", "page": 8}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 525.5465698242188, "y1": 199.88052368164062, "x1": 306.9469909667969, "y2": 229.79302978515625}, "imageText": ["GT", "MTL", "ViLBERT", "e2:", "A", "wooden", "boat", "carries", "5", "boys", "with", "skis", "e1\u2019:", "The", "ladder", "in", "front", "of", "the", "raised", "ladder", "e2\u2019:", "A", "pair", "of", "skis", "in", "the", "boat", "e1:", "The", "ladder", "that", "is", "raised", "the", "tallest"], "regionBoundary": {"x2": 525.0, "y1": 61.8900146484375, "x1": 306.83056640625, "y2": 185.8900146484375}, "caption": "Figure 4: Predictions of ViLBERT and MTL model (GT denotes ground-truth). e1\u2032 and e2\u2032 are adversarial expressions of e1 and e2 respectively.", "page": 4}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 281.3324890136719, "y1": 156.03353881835938, "x1": 80.6259994506836, "y2": 162.0360107421875}, "imageText": ["ViLBERT", "(VB)", "83.39", "83.63", "85.93", "72.00", "70.90", "VB+Sum-H", "81.61", "83.00", "85.93", "70.60", "72.30", "VB+Max-H", "82.93", "82.70", "86.58", "70.46", "73.35", "VB+MTL", "(GQA)", "83.45", "84.30", "86.23", "73.79", "73.92", "Model", "Dev", "Test", "Easy", "Hard", "Adv"], "regionBoundary": {"x2": 293.0, "y1": 63.8900146484375, "x1": 72.0, "y2": 143.8900146484375}, "caption": "Table 3: Accuracy of enhanced ViLBERT models.", "page": 4}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 730.2715725368923, "y1": 447.28686014811194, "x1": 426.772223578559, "y2": 522.0416174994574}, "name": "1", "caption_text": "Figure 1: An example of the visual referring expression recognition task. If the word pastry is present in the referring expression, models prefer the bounding box r1 (highlighted in green) irrespective of the change in linguistic structure (word order).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 728.0, "y1": 313.0, "x1": 427.0, "y2": 449.0}, "page": 0, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.3426140679253, "y1": 185.7521480984158, "x1": 426.3430701361762, "y2": 243.90284220377603}, "name": "1", "caption_text": "Table 1: Statistics of Ref-Easy, Ref-Hard and Ref-Adv. Ref-Easy and Ref-Hard indicate the proportion of samples in RefCOCOg test set that are insensitive and sensitive to linguistic structure respectively.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 719.0, "y1": 86.0, "x1": 438.0, "y2": 169.0}, "page": 1, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9200269911024, "y1": 408.17298889160156, "x1": 99.50133429633246, "y2": 433.11394585503473}, "name": "2", "caption_text": "Figure 2: Examples fro ef- sy, ef- rd, and Ref-Adv splits. As seen, Ref-Hard and Ref-Adv have several words in common but dif er i t i li istic structure and the target object of interest.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 729.0, "y1": 91.0, "x1": 100.0, "y2": 408.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 645.4718695746527, "y1": 266.4312998453776, "x1": 184.45277743869357, "y2": 274.76806640625}, "name": "3", "caption_text": "Figure 3: Multi-task learning model for referring expression recognition with GQA", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 695.0, "y1": 91.0, "x1": 137.0, "y2": 265.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9212985568577, "y1": 434.2049068874783, "x1": 426.3430701361762, "y2": 459.1458214653863}, "name": "2", "caption_text": "Table 2: Accuracy of models on RefCOCOg standard splits and our splits Ref-Easy, Ref-Hard and Ref-Adv.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 729.0, "y1": 305.0, "x1": 426.0, "y2": 434.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.925791422526, "y1": 277.6118384467231, "x1": 426.3152652316623, "y2": 319.156985812717}, "name": "4", "caption_text": "Figure 4: Predictions of ViLBERT and MTL model (GT denotes ground-truth). e1\u2032 and e2\u2032 are adversarial expressions of e1 and e2 respectively.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 729.0, "y1": 90.0, "x1": 426.0, "y2": 258.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 390.73956807454425, "y1": 216.71324835883246, "x1": 111.9805547926161, "y2": 225.05001491970486}, "name": "3", "caption_text": "Table 3: Accuracy of enhanced ViLBERT models.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 407.0, "y1": 88.0, "x1": 100.0, "y2": 200.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2223239474827, "y1": 256.841066148546, "x1": 426.3152652316623, "y2": 348.20014105902777}, "name": "5", "caption_text": "Table 5: RefCOCOg test accuracies of SOTA models on (a) original undistorted split, (b) after randomly shuffling words (Shuf) in the referring expression, and (c) after deleting all the words except for nouns and adjectives (N+J). ViLBERT is relatively more robust than other baselines.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 710.0, "y1": 86.0, "x1": 447.0, "y2": 240.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 404.87870110405817, "y1": 365.87579515245227, "x1": 99.57083596123589, "y2": 390.818108452691}, "name": "4", "caption_text": "Table 4: Distribution of object categories in RefCOCO, RefCOCO+, and RefCOCOg datasets.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 400.0, "y1": 86.0, "x1": 100.0, "y2": 366.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2232564290364, "y1": 483.0646514892578, "x1": 426.3430701361762, "y2": 557.8194088406033}, "name": "6", "caption_text": "Table 6: Ref-Easy, Ref-Hard, and Ref-Adv test accuracies of ViLBERT on (a) original undistorted split, (b) after randomly shuffling words (Shuf) in the referring expression, and (c) after deleting all the words except for nouns and adjectives (N+J).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 700.0, "y1": 373.0, "x1": 457.0, "y2": 466.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 694.8567708333333, "y1": 379.5789082845052, "x1": 128.17213270399304, "y2": 416.8178982204861}, "name": "5", "caption_text": "Figure 5: Overview of our three-stage Ref-Adv construction process. Given the image, referring expression, groundtruth bounding boxes for all the samples in RefCOCOg test split, we first filter out the hard samples and then construct adversarial expressions using them. Please refer to section 2 for further detail.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 692.0, "y1": 90.0, "x1": 131.0, "y2": 364.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 324.82077280680335, "y1": 568.7104966905382, "x1": 177.9013951619466, "y2": 577.0472632514105}, "name": "7", "caption_text": "Table 7: Ref-Adv Statistics", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 371.0, "y1": 474.0, "x1": 132.0, "y2": 569.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 622.5924597846137, "y1": 542.3038482666016, "x1": 490.90016682942706, "y2": 549.7765011257595}, "name": "7", "caption_text": "Table 7: Ref-Adv Statistics", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 721.0, "y1": 476.0, "x1": 436.0, "y2": 599.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 709.6785227457682, "y1": 368.8951704237196, "x1": 120.24305131700304, "y2": 377.231936984592}, "name": "7", "caption_text": "Figure 7: Relative frequency of the most frequent spatial relationships in Ref-Easy, Ref-Hard, and Ref-Adv", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 697.0, "y1": 88.0, "x1": 134.0, "y2": 350.0}, "page": 9, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 404.87870110405817, "y1": 713.8354831271702, "x1": 99.57083596123589, "y2": 738.7763977050781}, "name": "8", "caption_text": "Table 8: Distribution of object categories in Ref-Easy, Ref-Hard, and Ref-Adv splits.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 390.0, "y1": 423.0, "x1": 113.0, "y2": 697.0}, "page": 9, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.153313530816, "y1": 196.51188320583768, "x1": 99.57083596123589, "y2": 254.66122097439236}, "name": "9", "caption_text": "Table 9: Performance on GQA-Rel Dev, GQA-Dev and GQA-Test splits before and after MTL training with RefCOCOg (Note: MTL training for all the three rows is performed using GQA-Rel and RefCOCOg).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 377.0, "y1": 86.0, "x1": 113.0, "y2": 197.0}, "page": 10, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.4467942979601, "y1": 445.23963928222656, "x1": 99.50139787462022, "y2": 503.39024861653644}, "name": "10", "caption_text": "Table 10: Comparing ViLBERT\u2019s Multi-task Learning (MTL) with Transfer Learning (TL) experiments. RefDev and Ref-Test correspond to: RefCOCOg-Dev and RefCOCOg-Test splits respectively.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 399.0, "y1": 279.0, "x1": 100.0, "y2": 445.0}, "page": 10, "dpi": 0}], "error": null, "pdf": "/work/host-output/32ad25290ac827214f0c822ebe4e6df67b7ee7ba/2020.acl-main.586.pdf", "dpi": 100}