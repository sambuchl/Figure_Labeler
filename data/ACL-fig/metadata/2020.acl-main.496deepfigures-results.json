{"raw_detected_boxes": [[{"x2": 723.0, "y1": 314.0, "x1": 433.0, "y2": 462.0}], [], [], [{"x2": 718.0, "y1": 89.0, "x1": 435.0, "y2": 352.0}], [{"x2": 602.0, "y1": 88.0, "x1": 236.0, "y2": 210.0}], [{"x2": 396.0, "y1": 88.0, "x1": 103.0, "y2": 152.0}, {"x2": 714.0, "y1": 92.0, "x1": 440.0, "y2": 201.0}], [{"x2": 681.0, "y1": 88.0, "x1": 152.0, "y2": 183.0}, {"x2": 715.0, "y1": 273.0, "x1": 442.0, "y2": 478.0}], [{"x2": 709.0, "y1": 93.0, "x1": 121.0, "y2": 258.0}, {"x2": 722.0, "y1": 352.0, "x1": 434.0, "y2": 532.0}], [{"x2": 687.0, "y1": 92.0, "x1": 140.0, "y2": 309.0}, {"x2": 395.0, "y1": 416.0, "x1": 104.0, "y2": 705.0}], [], [], [], [{"x2": 696.0, "y1": 92.0, "x1": 458.0, "y2": 410.0}], [{"x2": 716.0, "y1": 91.0, "x1": 118.0, "y2": 486.0}, {"x2": 640.0, "y1": 554.0, "x1": 190.0, "y2": 987.0}], [{"x2": 394.0, "y1": 87.0, "x1": 103.0, "y2": 322.0}], [{"x2": 724.0, "y1": 86.0, "x1": 434.0, "y2": 142.0}], [], []], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Figure", "name": "1", "captionBoundary": {"x2": 525.54638671875, "y1": 349.6295471191406, "x1": 307.2760009765625, "y2": 367.5870056152344}, "imageText": ["How", "to", "\ufb01nd", "(and", "delete)", "duplicate", "\ufb01les?", "I", "have", "a", "largish", "music", "collection", "and", "there", "are", "some", "duplicates", "in", "there.", "Is", "there", "any", "way", "to", "find", "duplicate", "files.", "At", "a", "minimum", "by", "doing", "a", "hash", "and", "seeing", "if", "two", "files", "have", "the", "same", "hash.", "\u2026", "I\u2019m", "happy", "using", "the", "command", "line", "if", "that", "is", "the", "easiest", "way.", "Can", "I", "find", "duplicate", "songs", "with", "different", "names?", "I", "have", "so", "many", "duplicate", "songs", "but", "they", "have", "different", "names.", "Is", "there", "an", "application", "I", "can", "use", "to", "\ufb01nd", "and", "delete", "the", "duplicates?"], "regionBoundary": {"x2": 522.0, "y1": 226.8900146484375, "x1": 312.0, "y2": 333.8900146484375}, "caption": "Figure 1: An illustration of a text matching rationale for detecting similar forum posts.", "page": 0}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 291.9242248535156, "y1": 124.1735610961914, "x1": 71.69100189208984, "y2": 166.04107666015625}, "imageText": ["Vanilla", "1", "0", "0", "s", "\u2264", "n", "+", "m\u2212", "1", "One-to-k", "k", "m\u2212", "kn", "0", "s", "=", "kn", "\u2264", "m", "R", "one-to-k", "k", "m", "kn", "s", "\u2264", "kn", "\u2264", "m", "Exact-k", "1", "m\u2212", "k", "n\u2212", "k", "s", "=", "k", "\u2264", "n", "Constraint", "#", "R", "of", "X", "#", "D", "in", "X\u2032", "#", "D", "in", "Y", "\u2032", "Sparsity", "(s)"], "regionBoundary": {"x2": 286.0, "y1": 62.8900146484375, "x1": 74.0, "y2": 111.8900146484375}, "caption": "Table 1: Summary of constrained alignment construction and sparsity. # R is the number of replicas, # D is the number of dummy points, R one-to-k is the relaxed one-to-k assignment, and n = |X| \u2264 |Y | = m.", "page": 5}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 521.8590698242188, "y1": 157.01455688476562, "x1": 310.6520080566406, "y2": 163.01702880859375}, "imageText": ["#", "docs", "730,818", "10,130", "#", "similar", "doc", "pairs", "187,377", "22,623", "Avg", "sents", "per", "doc", "3.7", "31", "Max", "sents", "per", "doc", "54", "1,632", "Avg", "words", "per", "doc", "87", "680", "Vocab", "size", "603,801", "299,732", "Metric", "StackExchange", "MultiNews"], "regionBoundary": {"x2": 517.0, "y1": 62.8900146484375, "x1": 316.0, "y2": 144.8900146484375}, "caption": "Table 2: Statistics for the document ranking datasets.", "page": 5}, {"figType": "Table", "name": "6", "captionBoundary": {"x2": 291.9244384765625, "y1": 247.83554077148438, "x1": 71.69100189208984, "y2": 313.61407470703125}, "imageText": ["OT", "(1:1)", "(+S)", "62.1", "20.5", "48.1", "OT", "(1:2)", "(+S)", "60.0", "31.3", "46.0", "OT", "(relaxed", "1:1)", "(+S)", "60.3", "18.2", "46.2", "OT", "(relaxed", "1:2)", "(+S)", "60.6", "25.2", "44.9", "OT", "(exact", "k", "=", "2)", "(+S)", "61.2", "16.7", "48.7", "Attention", "61.4", "33.2", "15.7", "Attention", "(T", "=", "0.1)", "61.0", "34.7", "17.5", "Attention", "(T", "=", "0.01)", "61.0", "34.4", "18.5", "Sparse", "Attention", "60.7", "37.5", "25.0", "OT", "(1:1)", "59.5", "20.3", "24.2", "OT", "(1:2)", "60.1", "28.0", "26.5", "OT", "(relaxed", "1:1)", "59.7", "13.6", "19.5", "OT", "(relaxed", "1:2)", "60.2", "24.7", "29.1", "OT", "(exact", "k", "=", "2)", "61.0", "15.2", "22.7", "Model", "Task", "F1", "%", "Token", "R.", "F1"], "regionBoundary": {"x2": 286.0, "y1": 62.8900146484375, "x1": 74.0, "y2": 235.8900146484375}, "caption": "Table 6: MultiRC macro-averaged task F1, percentage of tokens used in active alignments, and token-level F1 of the model-selected rationales compared to humanannotated rationales (R. F1). (+S) denotes supervised learning of rationales. All models use a simplified recurrent unit (Lei et al., 2018) encoder.", "page": 14}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 525.5473022460938, "y1": 151.50155639648438, "x1": 72.0, "y2": 169.45904541015625}, "imageText": [], "regionBoundary": {"x2": 492.0, "y1": 61.8900146484375, "x1": 106.0, "y2": 139.8900146484375}, "caption": "Figure 4: An illustration of our constrained OT model applied to two text documents. The final output of the model depends on a combination of the encodings, the cost matrix, and the alignment matrix.", "page": 6}, {"figType": "Figure", "name": "5", "captionBoundary": {"x2": 525.5464477539062, "y1": 360.81854248046875, "x1": 307.2760009765625, "y2": 378.7760009765625}, "imageText": ["OT", "(exact", "k=4)", "0", "10", "OT", "(relaxed", "1:1)", "0", "10", "OT", "(1:1)", "0", "10", "OT", "10", "15", "20", "25", "0", "5", "0", "10", "cost", "attention", "sparse", "attention", "attention(T=0.01)", "10", "15", "20", "25", "0", "5"], "regionBoundary": {"x2": 514.0, "y1": 196.62034606933594, "x1": 318.0169372558594, "y2": 343.52447509765625}, "caption": "Figure 5: Attention or alignment heatmaps generated by different methods on a synthetic 30\u00d720 cost matrix.", "page": 6}, {"figType": "Figure", "name": "8", "captionBoundary": {"x2": 527.2855224609375, "y1": 367.66656494140625, "x1": 72.0, "y2": 385.6240234375}, "imageText": [], "regionBoundary": {"x2": 517.0, "y1": 61.8900146484375, "x1": 77.0, "y2": 351.8900146484375}, "caption": "Figure 8: Examples of extracted rationales from the StackExchange dataset using the OT (exact k = 2) model. Each rationale alignment is displayed visually as lines connecting pairs of sentences from the two text documents.", "page": 13}, {"figType": "Figure", "name": "9", "captionBoundary": {"x2": 525.905029296875, "y1": 728.381591796875, "x1": 72.0, "y2": 746.3390502929688}, "imageText": [], "regionBoundary": {"x2": 461.0, "y1": 398.8900146484375, "x1": 136.0, "y2": 712.8900146484375}, "caption": "Figure 9: Examples of extracted rationales from the e-SNLI dataset using the OT (exact k = 3) model. We show two examples of entailment (left column), neutral (middle column) and contradiction (right column).", "page": 13}, {"figType": "Figure", "name": "7", "captionBoundary": {"x2": 527.2899169921875, "y1": 308.5825500488281, "x1": 306.9670104980469, "y2": 386.31597900390625}, "imageText": ["(c)", "Overall", "quality", "(b)", "Relevance", "(a)", "Redundancy"], "regionBoundary": {"x2": 504.0, "y1": 61.8900146484375, "x1": 329.0, "y2": 293.2650146484375}, "caption": "Figure 7: Human evaluation of rationales extracted from StackExchange document pairs using metrics of redundancy, relevance, and overall quality. Scores are either 0 (red), 1 (gray), or 2 (blue) and higher is better. The length of each bar segment indicates the proportion of examples with that score, and the number to the right of each bar is the average score.", "page": 12}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 526.7872924804688, "y1": 197.84854125976562, "x1": 71.6409912109375, "y2": 227.76104736328125}, "imageText": ["Attention", "98.2", "92.4", "92.5", "88.0", "23", "97.8", "96.4", "97.6", "96.3", "637", "Attention", "(T", "=", "0.1)", "98.2", "92.4", "92.5", "87.7", "22", "98.0", "97.0", "98.1", "97.1", "634", "Attention", "(T", "=", "0.01)", "97.9", "89.7", "89.9", "83.5", "8", "97.9", "96.9", "98.0", "97.0", "594", "Sparse", "Attention", "98.0", "92.5", "92.6", "88.3", "19", "98.2", "97.7", "98.1", "97.1", "330", "OT", "98.0", "91.2", "91.5", "86.1", "8", "97.5", "96.8", "98.1", "97.2", "48", "OT", "(1:1)", "97.7", "89.7", "90.0", "83.9", "4", "97.8", "96.7", "97.9", "96.8", "19", "OT", "(relaxed", "1:1)", "97.8", "88.5", "88.9", "81.8", "3", "93.1", "93.2", "96.0", "94.1", "19", "OT", "(exact", "k)", "98.1", "92.3", "92.5", "87.8", "2", "96.4", "96.3", "97.7", "96.6", "6", "Model", "AUC", "MAP", "MRR", "P@1", "#", "Align.", "AUC", "MAP", "MRR", "P@1", "#", "Align.", "StackExchange", "MultiNews"], "regionBoundary": {"x2": 511.0, "y1": 65.8900146484375, "x1": 87.0, "y2": 185.8900146484375}, "caption": "Table 3: Performance of all models on the StackExchange and MultiNews datasets. We report ranking results and the average number of active alignments (# Align.) used. For our method with the exact k alignment constraint, we set k = 2 for StackExchange and k = 6 for MultiNews, respectively.", "page": 7}, {"figType": "Figure", "name": "6", "captionBoundary": {"x2": 527.1939086914062, "y1": 396.2415466308594, "x1": 307.2760009765625, "y2": 450.0649719238281}, "imageText": [], "regionBoundary": {"x2": 521.0, "y1": 250.8900146484375, "x1": 312.0, "y2": 384.8900146484375}, "caption": "Figure 6: Model accuracy on the e-SNLI dataset when using different percentages of tokens as rationales. The attention model values are obtained using different thresholds \u03bb to clip the attention weights while the values for our exact-k model correspond to k = 1, 2, 3, 4.", "page": 7}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 527.2012329101562, "y1": 269.9915466308594, "x1": 307.27593994140625, "y2": 335.7700500488281}, "imageText": ["(c)", "Alignment", "1", "(matrix)", "(d)", "Alignment", "2", "(matrix)", "(a)", "Alignment", "1", "(graph)", "(b)", "Alignment", "2", "(graph)"], "regionBoundary": {"x2": 519.490234375, "y1": 61.8900146484375, "x1": 313.2850036621094, "y2": 254.67401123046875}, "caption": "Figure 2: An illustration of two different alignments between the points of X and Y , displayed both as a graph (top) and as an (unnormalized) alignment matrix P (bottom). Alignment 2 (right) corresponds to the special case where P is a permutation matrix, which produces an assignment between points in X and Y .", "page": 3}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 525.54736328125, "y1": 234.61557006835938, "x1": 71.6709976196289, "y2": 276.48309326171875}, "imageText": ["Thorne", "et", "al.", "(2019)", "-", "(81.0)", "-", "-", "22.2", "57.8", "-", "\u2020Lei", "et", "al.", "(2016)", "-", "90.3", "-", "-", "-", "37.9", "\u2020Lei", "et", "al.", "(2016)", "(+S)", "-", "91.7", "-", "-", "-", "69.2", "\u2020Bert-To-Bert", "(+S)", "-", "73.3", "-", "-", "-", "70.1", "Attention", "76.3", "(82.1)", "76.2", "37.9", "26.6", "37.6", "32.2", "Attention", "(T", "=", "0.1)", "73.9", "(81.5)", "73.9", "33.0", "28.4", "44.1", "36.5", "Attention", "(T", "=", "0.01)", "70.2", "(81.4)", "69.9", "30.6", "26.1", "38.0", "32.2", "Sparse", "Attention", "63.5", "(75.0)", "63.1", "12.5", "8.8", "24.5", "17.2", "OT", "(relaxed", "1:1)", "82.4", "82.4", "69.1", "25.1", "43.7", "34.6", "OT", "(exact", "k", "=", "4)", "81.4", "81.4", "38.7", "24.3", "45.0", "35.4", "OT", "(exact", "k", "=", "3)", "81.3", "81.4", "29.6", "28.6", "50.0", "39.8", "OT", "(exact", "k", "=", "2)", "81.3", "81.3", "21.6", "24.8", "30.6", "27.8", "Model", "Accuracy", "Task", "F1", "%", "Token", "Premise", "F1", "Hypothesis", "F1", "P&H", "F1"], "regionBoundary": {"x2": 497.0, "y1": 62.8900146484375, "x1": 101.0, "y2": 221.8900146484375}, "caption": "Table 4: e-SNLI accuracy, macro-averaged task F1, percentage of tokens in active alignments, and token-level F1 of the model-selected rationales compared to human-annotated rationales for the premise, hypothesis, and both (P&H F1). Accuracy numbers in parentheses use all attention weights, not just active ones. (+S) denotes supervised learning of rationales. \u2020 denotes results from DeYoung et al. (2019).", "page": 8}, {"figType": "Table", "name": "5", "captionBoundary": {"x2": 291.9244384765625, "y1": 522.1566162109375, "x1": 71.69100189208984, "y2": 587.9339599609375}, "imageText": ["OT", "(1:1)", "(+S)", "61.5", "19.0", "50.0", "OT", "(relaxed", "1:1)", "(+S)", "60.6", "19.4", "45.4", "OT", "(relaxed", "1:2)", "(+S)", "61.5", "28.7", "46.8", "OT", "(exact", "k", "=", "2)", "(+S)", "61.0", "18.9", "51.3", "OT", "(exact", "k", "=", "3)", "(+S)", "60.9", "23.1", "49.3", "\u2020Lei", "et", "al.", "(2016)", "(+S)", "65.5", "-", "45.6", "\u2020Lehman", "et", "al.", "(2019)", "(+S)", "61.4", "-", "14.0", "\u2020Bert-To-Bert", "(+S)", "63.3", "-", "41.2", "Attention", "62.6", "44.7", "21.3", "Attention", "(T", "=", "0.1)", "62.6", "34.7", "18.2", "Attention", "(T", "=", "0.01)", "62.7", "30.1", "17.3", "Sparse", "Attention", "59.3", "31.3", "21.2", "\u2020Lei", "et", "al.", "(2016)", "64.8", "-", "0.0", "OT", "(1:1)", "62.3", "21.6", "33.7", "OT", "(relaxed", "1:1)", "62.0", "23.1", "32.1", "OT", "(relaxed", "1:2)", "62.2", "24.0", "35.9", "OT", "(exact", "k", "=", "2)", "62.5", "25.8", "34.7", "OT", "(exact", "k", "=", "3)", "62.0", "24.6", "37.3", "Model", "Task", "F1", "%", "Token", "R.", "F1"], "regionBoundary": {"x2": 286.0, "y1": 299.8900146484375, "x1": 74.0, "y2": 509.8900146484375}, "caption": "Table 5: MultiRC macro-averaged task F1, percentage of tokens used in active alignments, and token-level F1 of the model-selected rationales compared to humanannotated rationales (R. F1). (+S) denotes supervised learning of rationales. \u2020 denotes results from DeYoung et al. (2019).", "page": 8}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 526.2116088867188, "y1": 164.91952514648438, "x1": 71.69097900390625, "y2": 218.7430419921875}, "imageText": [], "regionBoundary": {"x2": 436.0, "y1": 61.8900146484375, "x1": 161.0, "y2": 152.8900146484375}, "caption": "Figure 3: An illustration of the process of computing a one-to-two assignment between the points of X and Y . (a) The original points of X and Y . (b) X\u0302 and Y\u0302 are constructed so that X\u0302 has two copies of each point in X and one dummy point and Y\u0302 = Y . (c) OT is applied to X\u0302 and Y\u0302 using uniform distributions a and b, which produces a one-to-one assignment between X\u0302 and Y\u0302 . (d) A one-to-two assignment between X and Y is extracted from the one-to-one assignment between X\u0302 and Y\u0302 .", "page": 4}, {"figType": "Table", "name": "7", "captionBoundary": {"x2": 527.2899780273438, "y1": 114.5015640258789, "x1": 306.9670104980469, "y2": 180.28009033203125}, "imageText": ["OT", "2.0M", "600", "8.0e-3", "Attention", "2.4M", "180", "4.9e-3", "Model", "#", "Parameters", "Train", "time", "(s)", "Infer", "time", "(s)"], "regionBoundary": {"x2": 523.0, "y1": 62.8900146484375, "x1": 312.0, "y2": 101.8900146484375}, "caption": "Table 7: Number of parameters, training time, and inference time for models on the StackExchange dataset. Training time represents training time per epoch while inference time represents the average time to encode and align one pair of documents. All models use an NVIDIA Tesla V100 GPU.", "page": 15}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 729.925537109375, "y1": 485.5965932210286, "x1": 426.772223578559, "y2": 510.5375077989366}, "name": "1", "caption_text": "Figure 1: An illustration of a text matching rationale for detecting similar forum posts.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 725.0, "y1": 314.0, "x1": 433.0, "y2": 463.0}, "page": 0, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2239345974392, "y1": 374.9882592095269, "x1": 426.77213880750867, "y2": 466.34729173448346}, "name": "2", "caption_text": "Figure 2: An illustration of two different alignments between the points of X and Y , displayed both as a graph (top) and as an (unnormalized) alignment matrix P (bottom). Alignment 2 (right) corresponds to the special case where P is a permutation matrix, which produces an assignment between points in X and Y .", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 721.0, "y1": 89.0, "x1": 435.0, "y2": 356.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 730.8494567871094, "y1": 229.05489603678384, "x1": 99.57080417209201, "y2": 303.80978054470484}, "name": "3", "caption_text": "Figure 3: An illustration of the process of computing a one-to-two assignment between the points of X and Y . (a) The original points of X and Y . (b) X\u0302 and Y\u0302 are constructed so that X\u0302 has two copies of each point in X and one dummy point and Y\u0302 = Y . (c) OT is applied to X\u0302 and Y\u0302 using uniform distributions a and b, which produces a one-to-one assignment between X\u0302 and Y\u0302 . (d) A one-to-two assignment between X and Y is extracted from the one-to-one assignment between X\u0302 and Y\u0302 .", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 602.0, "y1": 88.0, "x1": 228.0, "y2": 210.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.45031229654944, "y1": 172.46327930026584, "x1": 99.57083596123589, "y2": 230.61260647243924}, "name": "1", "caption_text": "Table 1: Summary of constrained alignment construction and sparsity. # R is the number of replicas, # D is the number of dummy points, R one-to-k is the relaxed one-to-k assignment, and n = |X| \u2264 |Y | = m.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 397.0, "y1": 86.0, "x1": 103.0, "y2": 156.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 724.8042636447483, "y1": 218.07577345106336, "x1": 431.46112230088977, "y2": 226.41254001193576}, "name": "2", "caption_text": "Table 2: Statistics for the document ranking datasets.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 718.0, "y1": 86.0, "x1": 431.0, "y2": 218.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9268086751301, "y1": 210.4188283284505, "x1": 100.0, "y2": 235.35978529188367}, "name": "4", "caption_text": "Figure 4: An illustration of our constrained OT model applied to two text documents. The final output of the model depends on a combination of the encodings, the cost matrix, and the alignment matrix.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 681.0, "y1": 88.0, "x1": 148.0, "y2": 191.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9256218804253, "y1": 501.1368645562066, "x1": 426.772223578559, "y2": 526.0777791341146}, "name": "5", "caption_text": "Figure 5: Attention or alignment heatmaps generated by different methods on a synthetic 30\u00d720 cost matrix.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 715.0, "y1": 273.0, "x1": 442.0, "y2": 478.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 731.6490173339844, "y1": 274.78964063856336, "x1": 99.50137668185764, "y2": 316.33478800455725}, "name": "3", "caption_text": "Table 3: Performance of all models on the StackExchange and MultiNews datasets. We report ranking results and the average number of active alignments (# Align.) used. For our method with the exact k alignment constraint, we set k = 2 for StackExchange and k = 6 for MultiNews, respectively.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 709.0, "y1": 86.0, "x1": 104.0, "y2": 275.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2137620713976, "y1": 550.3354814317491, "x1": 426.772223578559, "y2": 625.0902387830946}, "name": "6", "caption_text": "Figure 6: Model accuracy on the e-SNLI dataset when using different percentages of tokens as rationales. The attention model values are obtained using different thresholds \u03bb to clip the attention weights while the values for our exact-k model correspond to k = 1, 2, 3, 4.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 722.0, "y1": 352.0, "x1": 427.0, "y2": 549.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9268934461805, "y1": 325.8549584282769, "x1": 99.54305224948459, "y2": 384.0042961968316}, "name": "4", "caption_text": "Table 4: e-SNLI accuracy, macro-averaged task F1, percentage of tokens in active alignments, and token-level F1 of the model-selected rationales compared to human-annotated rationales for the premise, hypothesis, and both (P&H F1). Accuracy numbers in parentheses use all attention weights, not just active ones. (+S) denotes supervised learning of rationales. \u2020 denotes results from DeYoung et al. (2019).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 704.0, "y1": 86.0, "x1": 138.0, "y2": 326.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.4506089952257, "y1": 725.2175225151909, "x1": 99.57083596123589, "y2": 816.5749443901909}, "name": "5", "caption_text": "Table 5: MultiRC macro-averaged task F1, percentage of tokens used in active alignments, and token-level F1 of the model-selected rationales compared to humanannotated rationales (R. F1). (+S) denotes supervised learning of rationales. \u2020 denotes results from DeYoung et al. (2019).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 397.0, "y1": 416.0, "x1": 103.0, "y2": 708.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.3471069335938, "y1": 428.58687506781683, "x1": 426.3430701361762, "y2": 536.5499708387587}, "name": "7", "caption_text": "Figure 7: Human evaluation of rationales extracted from StackExchange document pairs using metrics of redundancy, relevance, and overall quality. Scores are either 0 (red), 1 (gray), or 2 (blue) and higher is better. The length of each bar segment indicates the proportion of examples with that score, and the number to the right of each bar is the average score.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 698.0, "y1": 92.0, "x1": 458.0, "y2": 410.0}, "page": 12, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.3410034179688, "y1": 510.6480068630642, "x1": 100.0, "y2": 535.5889214409722}, "name": "8", "caption_text": "Figure 8: Examples of extracted rationales from the StackExchange dataset using the OT (exact k = 2) model. Each rationale alignment is displayed visually as lines connecting pairs of sentences from the two text documents.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 718.0, "y1": 87.0, "x1": 108.0, "y2": 488.0}, "page": 13, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 730.4236518012152, "y1": 1011.641099717882, "x1": 100.0, "y2": 1036.5820142957898}, "name": "9", "caption_text": "Figure 9: Examples of extracted rationales from the e-SNLI dataset using the OT (exact k = 3) model. We show two examples of entailment (left column), neutral (middle column) and contradiction (right column).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 640.0, "y1": 537.0, "x1": 190.0, "y2": 989.0}, "page": 13, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.4506089952257, "y1": 344.2160288492838, "x1": 99.57083596123589, "y2": 435.5751037597656}, "name": "6", "caption_text": "Table 6: MultiRC macro-averaged task F1, percentage of tokens used in active alignments, and token-level F1 of the model-selected rationales compared to humanannotated rationales (R. F1). (+S) denotes supervised learning of rationales. All models use a simplified recurrent unit (Lei et al., 2018) encoder.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 397.0, "y1": 86.0, "x1": 103.0, "y2": 327.0}, "page": 14, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.347191704644, "y1": 159.02995003594293, "x1": 426.3430701361762, "y2": 250.3890143500434}, "name": "7", "caption_text": "Table 7: Number of parameters, training time, and inference time for models on the StackExchange dataset. Training time represents training time per epoch while inference time represents the average time to encode and align one pair of documents. All models use an NVIDIA Tesla V100 GPU.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 727.0, "y1": 86.0, "x1": 426.0, "y2": 159.0}, "page": 15, "dpi": 0}], "error": null, "pdf": "/work/host-output/df0facf78616f4a60f8dec37be7b83ec557bae54/2020.acl-main.496.pdf", "dpi": 100}