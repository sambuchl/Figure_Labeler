{"raw_detected_boxes": [[], [{"x2": 710.0, "y1": 88.0, "x1": 117.0, "y2": 355.0}], [], [], [{"x2": 706.0, "y1": 104.0, "x1": 146.0, "y2": 407.0}, {"x2": 720.0, "y1": 478.0, "x1": 434.0, "y2": 616.0}], [{"x2": 367.0, "y1": 88.0, "x1": 133.0, "y2": 165.0}, {"x2": 722.0, "y1": 86.0, "x1": 431.0, "y2": 199.0}, {"x2": 722.0, "y1": 336.0, "x1": 431.0, "y2": 397.0}], [{"x2": 720.0, "y1": 89.0, "x1": 107.0, "y2": 282.0}], [{"x2": 720.0, "y1": 92.0, "x1": 430.0, "y2": 260.0}], [], [], [], [{"x2": 721.0, "y1": 129.0, "x1": 103.0, "y2": 346.0}, {"x2": 724.0, "y1": 402.0, "x1": 103.0, "y2": 617.0}, {"x2": 617.0, "y1": 693.0, "x1": 213.0, "y2": 895.0}], [{"x2": 716.0, "y1": 110.0, "x1": 116.0, "y2": 305.0}, {"x2": 716.0, "y1": 629.0, "x1": 118.0, "y2": 731.0}, {"x2": 712.0, "y1": 827.0, "x1": 114.0, "y2": 927.0}], [{"x2": 696.0, "y1": 403.0, "x1": 131.0, "y2": 501.0}, {"x2": 613.0, "y1": 583.0, "x1": 214.0, "y2": 673.0}], [], [], [], [{"x2": 706.0, "y1": 97.0, "x1": 123.0, "y2": 1034.0}]], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Table", "name": "6", "captionBoundary": {"x2": 526.7916870117188, "y1": 300.3315734863281, "x1": 306.967041015625, "y2": 330.2440490722656}, "imageText": ["Task", "1", "2", "3", "5", "N", "=", "3", "p", "=0.3", "ConvAI2", "20.0", "21.0", "21.3", "21.2", "21.3", "18.7", "WoW", "35.9", "37.4", "37.8", "37.9", "37.9", "31.1", "N-gram", "Beam", "Size", "Block", "Nucleus"], "regionBoundary": {"x2": 520.0, "y1": 239.36282348632812, "x1": 310.0, "y2": 285.0140380859375}, "caption": "Table 6: Impact of the decoding strategy on select tasks, reporting validation F1 score for the All Tasks MT model. N-gram block is for best beam size.", "page": 5}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 291.9241638183594, "y1": 135.31155395507812, "x1": 71.69100952148438, "y2": 153.26904296875}, "imageText": ["Knowledge", "grounding", "Without", "With", "Wiz.", "of", "Wikipedia", "16.8", "8.7", "ELI5", "21.3", "21.2", "Image", "grounding", "Image", "Chat", "19.5", "18.3", "IGC", "10.1", "10.1"], "regionBoundary": {"x2": 269.0, "y1": 64.3797836303711, "x1": 94.0, "y2": 119.9940185546875}, "caption": "Table 4: The impact of knowledge and image grounding in dodecaDialogue (validation perplexity).", "page": 5}, {"figType": "Table", "name": "5", "captionBoundary": {"x2": 527.1972045898438, "y1": 155.63552856445312, "x1": 306.9469909667969, "y2": 221.4140625}, "imageText": ["Cornell", "21.9", "21.5", "20.6", "20.1", "19.9", "19.8", "-", "Fine-tuned", "20.1", "20.0", "20.0", "19.9", "19.8", "19.8", "20.0", "ELI5", "25.0", "24.1", "22.8", "22.2", "21.6", "21.3", "-", "Fine-tuned", "21.8", "21.6", "21.4", "21.3", "21.1", "21.1", "21.2", "Ubuntu", "23.1", "22.2", "20.6", "19.6", "18.6", "17.4", "-", "Fine-tuned", "18.2", "18.1", "17.8", "17.7", "17.2", "17.2", "17.3", "Relative", "Task", "Weighting", "1", "2", "5", "10", "20", "50", "\u221e"], "regionBoundary": {"x2": 520.0, "y1": 61.8900146484375, "x1": 310.0, "y2": 143.8900146484375}, "caption": "Table 5: Validation perplexity on select dodecaDialogue tasks comparing relative weights of tasks during multi-tasking, followed by fine-tuning (row below). The relative task weight is the ratio of examples from that task compared to others presented during multitasking. \u221e indicates single-task training.", "page": 5}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 525.7119750976562, "y1": 272.4425354003906, "x1": 71.69100189208984, "y2": 290.4000244140625}, "imageText": ["ConvAI2", "X", "X", "X", "X", "131,438", "7,801", "6,634", "14.8", "11.9", "DailyDialog", "X", "X", "X", "87,170", "8,069", "7,740", "7.9", "14.6", "Wiz.", "of", "Wikipedia", "X", "X", "X", "X", "74,092", "3,939", "3,865", "9.0", "21.6", "Empathetic", "Dialog", "X", "X", "X", "X", "40,252", "5,736", "5,257", "4.3", "15.2", "Cornell", "Movie", "X", "X", "X", "309,987", "38,974", "38,636", "4.0", "15.0", "LIGHT", "X", "X", "X", "X", "X", "110,877", "6,623", "13,272", "13.0", "18.3", "ELI5", "X", "X", "231,410", "9,828", "24,560", "2.0", "130.6", "Ubuntu", "X", "X", "X", "1,000,000", "19,560", "18,920", "2.0", "18.9", "Twitter", "X", "X", "X", "2,580,428", "10,405", "10,405", "2.0", "15.7", "pushshift.io", "Reddit", "X", "X", "X", "\u223c", "2200", "M", "10,000", "10,000", "2.0", "35.0", "Image", "Chat", "X", "X", "X", "X", "X", "355,862", "15,000", "29,991", "3.0", "11.4", "IGC", "X", "X", "X", "4,353", "486", "7,773", "3.0", "8.6", "Resp.", "Name", "Train", "Valid", "Test", "#", "Turns", "Length", "nd", "ing", "e", "G", "rou", "Im", "ag", "ing", "rou", "nd", "tio", "n", "G", "Sit", "ua", "nd", "ing", "Gr", "ou", "led", "ge", "Kn", "ow", "ing", "ou", "nd", "na", "Gr", "Pe", "rso", "ts", "em", "en", "o", "S", "tat", "on", "d", "t", "Re", "sp", "s", "est", "ion", "er", "Qu", "An", "sw", "on", "s", "ue", "sti", "As", "k", "Q"], "regionBoundary": {"x2": 511.0, "y1": 63.401031494140625, "x1": 84.0, "y2": 259.8900146484375}, "caption": "Table 1: The 12 dodecaDialogue subtasks, their sizes (number of train, valid, test utterances), and average number of turns and response length (words).", "page": 1}, {"figType": "Table", "name": "7", "captionBoundary": {"x2": 527.19873046875, "y1": 215.01254272460938, "x1": 71.69100189208984, "y2": 244.925048828125}, "imageText": ["ConvAI2", "(Lewis", "et", "al.,", "2019)", "*11.9", "*20.7", "F1", "11.1", "21.6", "10.8", "21.7", "DailyDialog", "(He", "et", "al.,", "2019)", "11.1", "-", "F1", "10.4", "18.2", "12.0", "16.2", "Wiz.", "of", "Wikipedia", "(Dinan", "et", "al.,", "2019)", "23.1", "35.5", "F1", "8.3", "38.4", "8.4", "38.4", "Empathetic", "Dialog", "(Rashkin", "et", "al.,", "2019)", "21.2", "6.27", "Avg-BLEU", "11.4", "8.1", "11.5", "8.4", "Cornell", "Movie", "(He", "et", "al.,", "2019)", "27.5", "-", "F1", "20.2", "12.4", "22.2", "11.9", "LIGHT", "(Urbanek", "et", "al.,", "2019)", "\u221727.1", "\u221713.9", "F1", "18.9", "16.2", "19.3", "16.1", "ELI5", "(Lewis", "et", "al.,", "2019)", "24.2", "20.4", "Avg-ROUGE", "21.0", "22.6", "24.9", "20.7", "Ubuntu", "(Luan", "et", "al.,", "2016)", "46.8", "-", "F1", "17.1", "12.7", "23.1", "12.1", "Twitter", "-", "-", "F1", "30.7", "9.9", "38.2", "9.8", "pushshift.io", "Reddit", "-", "-", "F1", "25.6", "13.6", "27.8", "13.5", "Image", "Chat", "(Shuster", "et", "al.,", "2018)", "-", "27.4", "ROUGE-L", "(1st", "turn)", "18.8", "43.8", "22.3", "39.7", "IGC", "(Mostafazadeh", "et", "al.,", "2017)", "-", "1.57", "BLEU", "(responses)", "11.9", "9.9", "12.0", "8.2", "Existing", "Approaches", "(independent)", "MT", "+", "FT", "All", "Tasks", "MT", "Approach", "PPL", "Score", "(Metric)", "PPL", "Score", "PPL", "Score"], "regionBoundary": {"x2": 522.0, "y1": 61.8900146484375, "x1": 73.0, "y2": 202.8900146484375}, "caption": "Table 7: Test performance for various metrics on the dodecaDialogue tasks comparing our multi-task and multitask + fine-tuned methods to existing approaches (cited). Dashes mean metric was not provided. \u2217 was reported on validation only. Score is defined on a per-task basis in the metric column.", "page": 6}, {"figType": "Table", "name": "15", "captionBoundary": {"x2": 527.200927734375, "y1": 501.7285461425781, "x1": 71.69100189208984, "y2": 531.6419677734375}, "imageText": ["Image+Seq2Seq", "(All", "Tasks", "MT)", "Beam", "3.42", "(1.10)", "3.33", "(1.09)", "Image+Seq2Seq", "(All", "Tasks", "MT)", "Nucleus", "2.95", "(1.08)", "3.43", "(1.05)", "(Dinan", "et", "al.,", "2019)", "Beam", "2.92", "(1.33)", "2.93", "(1.30)", "Human", "Performance", "4.13", "(1.08)", "4.34", "(0.98)", "Method", "Decode", "Method", "Seen", "Unseen"], "regionBoundary": {"x2": 448.0, "y1": 414.8900146484375, "x1": 150.0, "y2": 484.8900146484375}, "caption": "Table 15: Human evaluations on Wizard of Wikipedia, comparing various decoding schemes for our Image+Seq2Seq model trained on all tasks MT, as well as comparisons with human outputs, in terms of Likert Scores. Ratings are reported as mean (stddev).", "page": 13}, {"figType": "Table", "name": "14", "captionBoundary": {"x2": 527.2010498046875, "y1": 375.8015441894531, "x1": 71.69100189208984, "y2": 405.7139892578125}, "imageText": ["Human", "24.2", "27.2", "39.5", "-", "(Dinan", "et", "al.,", "2019)", "-", "62.3", "64.1", "75.8", "Image+Seq2Seq", "Nucleus", "37.7", "-", "-", "72.8", "Image+Seq2Seq", "Beam", "35.9", "-", "-", "60.5", "(Dinan", "et", "al.,", "2019)", "Image+Seq2Seq", "Image+Seq2Seq", "Human", "Nucleus", "Beam", "Lose", "Percentage", "Win", "Percentage"], "regionBoundary": {"x2": 513.0, "y1": 289.8900146484375, "x1": 82.0, "y2": 363.8900146484375}, "caption": "Table 14: Human evaluations on Wizard of Wikipedia (unseen) test set, comparing various decoding schemes for our Image+Seq2Seq model trained on all tasks MT, as well as comparisons with human outputs, using ACUTEEval. All scores are statistically significant (binomial test, p < .05).", "page": 13}, {"figType": "Table", "name": "13", "captionBoundary": {"x2": 527.290771484375, "y1": 681.49658203125, "x1": 71.64099884033203, "y2": 711.4100341796875}, "imageText": ["Human", "28.1", "29.6", "40.0", "-", "(Dinan", "et", "al.,", "2019)", "-", "59.1", "62.1", "71.9", "Image+Seq2Seq", "Nucleus", "40.1", "-", "-", "70.4", "Image+Seq2Seq", "Beam", "37.9", "-", "-", "60.0", "(Dinan", "et", "al.,", "2019)", "Image+Seq2Seq", "Image+Seq2Seq", "Human", "Nucleus", "Beam", "Lose", "Percentage", "Win", "Percentage"], "regionBoundary": {"x2": 513.0, "y1": 595.8900146484375, "x1": 82.0, "y2": 669.8900146484375}, "caption": "Table 13: Human evaluations on Wizard of Wikipedia (seen) test set, comparing various decoding schemes for our Image+Seq2Seq model trained on all tasks MT, as well as comparisons with human outputs, using ACUTE-Eval. All scores are statistically significant (binomial test, p < .05).", "page": 12}, {"figType": "Table", "name": "11", "captionBoundary": {"x2": 527.2010498046875, "y1": 232.02053833007812, "x1": 71.69100189208984, "y2": 261.93304443359375}, "imageText": ["ConvAI2", "5.7", "10", "10", "128", "3", "31.6", "10", "50", "128", "3", "21.1", "3", "10", "128", "3", "DailyDialog", "4.4", "10", "5", "128", "3", "32.0", "3", "50", "128", "3", "18.8", "5", "10", "128", "3", "Wiz.", "of", "Wikipedia", "20.8", "10", "5", "128", "0", "44.8", "10", "50", "128", "3", "37.9", "10", "10", "128", "3", "Empathetic", "Dialog", "3.6", "10", "5", "128", "3", "32.7", "5", "50", "128", "3", "19.7", "5", "10", "128", "3", "Cornell", "Movie", "2.5", "10", "5", "128", "3", "25.6", "10", "50", "128", "3", "12.3", "10", "20", "128", "3", "LIGHT", "2.6", "3", "5", "128", "3", "25.2", "5", "50", "128", "3", "16.5", "5", "20", "128", "3", "ELI5", "3.7", "10", "200", "256", "3", "22.1", "5", "200", "256", "3", "23.2", "10", "200", "256", "3", "Ubuntu", "2.4", "10", "5", "128", "0", "22.9", "10", "40", "128", "3", "12.8", "2", "10", "128", "3", "Twitter", "3.2", "10", "20", "128", "3", "14.5", "5", "50", "128", "3", "10.1", "10", "20", "128", "3", "pushshift.io", "Reddit", "2.2", "10", "10", "128", "0", "18.7", "5", "50", "128", "3", "13.4", "5", "50", "128", "3", "Image", "Chat", "(all", "turns)", "2.4", "10", "5", "128", "3", "26.4", "3", "50", "128", "3", "14.3", "5", "1", "128", "3", "IGC", "10.6", "10", "5", "128", "3", "64.5", "3", "50", "128", "3", "45.1", "10", "5", "128", "3", "BLEU", "ROUGE-L", "F1", "Score", "Beam", "Min", "L", "Max", "L", "N-gram", "Block", "Score", "Beam", "Min", "L", "Max", "L", "N-gram", "Block", "Score", "Beam", "Min", "L", "Max", "L", "N-gram", "Block"], "regionBoundary": {"x2": 516.0, "y1": 78.8900146484375, "x1": 79.0, "y2": 219.8900146484375}, "caption": "Table 11: Best decoding parameters for each task, based on metric. Scores are from the best performing taskspecific multi-task + fine-tuned model on validation sets. \u201dMin L\u201d and \u201dMax L\u201d refer to the minimum and maximum decoding length, where \u201dL\u201d is the number of tokens.", "page": 12}, {"figType": "Table", "name": "12", "captionBoundary": {"x2": 525.547119140625, "y1": 538.83154296875, "x1": 71.6710205078125, "y2": 568.7449951171875}, "imageText": ["Human", "\u221720.7", "\u221726.2", "\u221720.6", "-", "(Shuster", "et", "al.,", "2018)", "-", "50.8", "\u221760.7", "\u221779.3", "Image+Seq2Seq", "Nucleus", "49.2", "-", "52.1", "\u221773.8", "Image+Seq2Seq", "Beam", "\u221739.3", "47.9", "-", "\u221779.4", "(Shuster", "et", "al.,", "2018)", "Image+Seq2Seq", "Image+Seq2Seq", "Human", "Nucleus", "Beam", "Lose", "Percentage", "Win", "Percentage"], "regionBoundary": {"x2": 516.0, "y1": 452.8900146484375, "x1": 80.0, "y2": 526.8900146484375}, "caption": "Table 12: Human evaluations on Image Chat, comparing various decoding schemes for our Image+Seq2Seq model trained on all tasks MT, as well as comparisons with human outputs. Scores with \u2217 are statistically significant (binomial test, p < .05).", "page": 12}, {"figType": "Figure", "name": "1", "captionBoundary": {"x2": 527.2003173828125, "y1": 203.40255737304688, "x1": 307.2760009765625, "y2": 257.22607421875}, "imageText": [], "regionBoundary": {"x2": 526.0, "y1": 61.8900146484375, "x1": 307.0, "y2": 191.8900146484375}, "caption": "Figure 1: Human evaluations on Image Chat and Wizard of Wikipedia (WoW), comparing existing state of the art models with our All Tasks MT conversational agent. Engagingness win rates are statistically significant in all three matchups (binomial test, p < .05).", "page": 7}, {"figType": "Table", "name": "10", "captionBoundary": {"x2": 525.546875, "y1": 661.7685546875, "x1": 71.6409912109375, "y2": 691.6810302734375}, "imageText": ["ConvAI2", "11.3", "5.6", "22.2", "7.0", "20.4", "21.3", "DailyDialog", "11.8", "4.8", "18.9", "5.6", "17.6", "16.6", "Wiz.", "of", "Wikipedia", "8.7", "19.7", "40.9", "22.6", "36.9", "37.7", "Empathetic", "Dialog", "11.2", "4.8", "20.9", "5.6", "19.0", "19.3", "Cornell", "Movie", "21.9", "3.3", "14.2", "3.2", "13.4", "11.3", "LIGHT", "19.0", "2.9", "17.0", "3.4", "15.0", "16.2", "ELI5", "25.0", "1.6", "14.2", "2.6", "9.6", "16.2", "Ubuntu", "23.3", "2.3", "12.5", "1.9", "11.6", "11.2", "Twitter", "37.0", "2.3", "9.5", "1.7", "8.7", "8.9", "pushshift.io", "Reddit", "28.0", "1.8", "12.1", "2.2", "10.4", "11.3", "Image", "Chat", "(all", "turns)", "21.8", "2.1", "14.7", "2.5", "13.6", "13.1", "IGC", "10.2", "5.5", "50.7", "25.3", "49.1", "36.0", "dodecaScore", "19.1", "4.7", "20.7", "7.0", "18.8", "18.3", "PPL", "BLEU", "ROUGE", "f1", "4", "1", "2", "L"], "regionBoundary": {"x2": 445.0, "y1": 498.8900146484375, "x1": 151.0, "y2": 649.8900146484375}, "caption": "Table 10: All Tasks Multi-Tasking (MT) validation performance for various metrics on the dodecaDialogue tasks with one set of decoding parameters: a beam size of 3, minimum response length of 10, and blocking repeated tri-grams.", "page": 11}, {"figType": "Table", "name": "8", "captionBoundary": {"x2": 527.19873046875, "y1": 265.3825378417969, "x1": 71.69100952148438, "y2": 283.34100341796875}, "imageText": ["4", "1", "2", "L", "4", "1", "2", "L", "ConvAI2", "11.1", "6.6", "37.0", "11.6", "31.8", "21.6", "10.8", "5.5", "39.4", "12.5", "33.7", "21.7", "DailyDialog", "10.4", "4.0", "35.6", "10.0", "30.8", "18.2", "12.0", "2.9", "33.9", "8.7", "29.2", "16.2", "Wiz.", "of", "Wikipedia", "8.3", "21.5", "55.3", "28.4", "44.9", "38.4", "8.4", "21.0", "53.2", "28.0", "45.4", "38.4", "Empathetic", "Dialog", "11.4", "3.5", "38.0", "9.5", "32.3", "19.5", "11.5", "3.7", "37.2", "8.9", "31.4", "19.3", "Cornell", "Movie", "20.2", "2.5", "29.5", "6.7", "25.7", "12.4", "22.2", "2.1", "29.1", "6.5", "25.6", "11.9", "LIGHT", "18.9", "2.6", "30.8", "5.8", "24.8", "16.2", "19.3", "2.4", "30.5", "5.6", "24.6", "16.1", "ELI5", "21.0", "3.7", "38.6", "7.2", "22.1", "23.1", "24.9", "3.2", "35.2", "6.3", "20.5", "21.3", "Ubuntu", "17.1", "2.5", "27.0", "5.0", "22.8", "12.7", "23.1", "3.7", "26.0", "4.3", "22.0", "12.1", "Twitter", "30.7", "3.2", "16.5", "3.3", "14.3", "9.9", "38.2", "2.6", "19.4", "3.3", "16.5", "9.8", "pushshift.io", "Reddit", "25.6", "2.1", "24.1", "4.5", "18.7", "13.6", "27.8", "1.6", "23.4", "4.2", "18.1", "13.5", "Image", "Chat", "18.8", "2.4", "30.1", "5.7", "26.0", "13.0", "22.3", "2.1", "28.4", "4.9", "24.6", "12.9", "IGC", "11.9", "8.6", "65.0", "34.1", "60.5", "38.4", "12.0", "8.0", "61.3", "28.3", "56.8", "41.4", "dodecaScore", "17.1", "5.3", "35.6", "11.0", "29.6", "19.8", "19.4", "4.9", "34.8", "10.1", "29.0", "19.6", "MT", "+", "FT", "All", "Tasks", "MT", "PPL", "BLEU", "ROUGE", "F1", "PPL", "BLEU", "ROUGE", "F1"], "regionBoundary": {"x2": 523.0, "y1": 92.8900146484375, "x1": 72.0, "y2": 253.8900146484375}, "caption": "Table 8: Test performance for various metrics on the dodecaDialogue tasks comparing our multi-task and multitask + fine-tuned methods.", "page": 11}, {"figType": "Table", "name": "9", "captionBoundary": {"x2": 525.5449829101562, "y1": 460.05255126953125, "x1": 71.69100952148438, "y2": 478.0110168457031}, "imageText": ["4", "1", "2", "L", "4", "1", "2", "L", "ConvAI2", "11.2", "5.7", "36.7", "10.9", "31.6", "21.1", "11.3", "5.3", "38.7", "11.6", "32.9", "21.3", "DailyDialog", "10.2", "4.4", "36.8", "10.7", "32", "18.8", "11.8", "3.1", "34.8", "9.3", "30.2", "17.1", "Wiz.", "of", "Wikipedia", "8.5", "20.8", "54.9", "28.0", "44.8", "37.9", "8.7", "20.2", "55.2", "28.2", "45.0", "37.9", "Empathetic", "Dialog", "11.1", "3.6", "38.6", "9.8", "32.7", "19.7", "11.2", "3.5", "37.5", "9.1", "31.8", "19.3", "Cornell", "Movie", "19.8", "2.5", "29.3", "6.7", "25.6", "12.3", "21.9", "2.1", "29.0", "6.5", "25.6", "11.8", "LIGHT", "18.7", "2.6", "31.2", "6.2", "25.2", "16.5", "19.0", "2.5", "30.9", "6.1", "25.0", "16.4", "ELI5", "21.1", "3.7", "38.7", "7.3", "22.1", "23.2", "25.0", "3.2", "35.3", "6.3", "20.6", "21.2", "Ubuntu", "17.2", "2.4", "27.1", "5.0", "22.9", "12.8", "23.3", "3.5", "26.4", "4.6", "22.3", "12.2", "Twitter", "29.8", "3.2", "16.7", "3.5", "14.5", "10.1", "37.0", "2.6", "19.7", "3.6", "16.8", "9.9", "pushshift.io", "Reddit", "25.8", "2.2", "24.2", "4.5", "18.7", "13.4", "28.0", "1.7", "23.4", "4.1", "18.2", "13.3", "Image", "Chat", "18.3", "2.4", "30.7", "6.2", "26.3", "14.3", "21.8", "2.1", "28.6", "5.3", "24.7", "13.1", "IGC", "10.0", "10.6", "67.9", "38.2", "64.5", "45.1", "10.2", "11.0", "66.3", "34.8", "61.4", "45.3", "dodecaScore", "16.8", "5.3", "36.1", "11.4", "30.1", "20.4", "19.1", "5.1", "35.5", "10.8", "29.5", "19.9", "MT", "+", "FT", "All", "Tasks", "MT", "PPL", "BLEU", "ROUGE", "F1", "PPL", "BLEU", "ROUGE", "F1"], "regionBoundary": {"x2": 523.0, "y1": 287.8900146484375, "x1": 72.0, "y2": 447.8900146484375}, "caption": "Table 9: Validation performance for various metrics on the dodecaDialogue tasks comparing our multi-task and multi-task + fine-tuned methods.", "page": 11}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 456.18072509765625, "y1": 305.5935363769531, "x1": 141.05499267578125, "y2": 311.59600830078125}, "imageText": ["dodecaScore", "N/A", "49.5", "45.7", "35.6", "26.5", "17.1", "16.8", "19.1", "31.1", "ConvAI2", "19.4", "43.3", "38.9", "28.7", "18.3", "11.4", "11.2", "11.3", "16.4", "DailyDialog", "15.2", "37.8", "32.8", "20.8", "18.2", "10.4", "10.2", "11.8", "15.5", "Wiz.", "of", "Wikipedia", "14.1", "40.7", "36.0", "37.3", "15.3", "8.7", "8.5", "8.7", "13.2", "Empathetic", "Dialog", "23.2", "47.1", "40.5", "23.1", "14.4", "11.3", "11.1", "11.2", "13.0", "Cornell", "Movie", "29.4", "46.2", "44.8", "34.2", "27.8", "20.0", "19.8", "22.3", "25.4", "LIGHT", "29.7", "63.6", "57.5", "40.0", "32.9", "18.7", "18.7", "19.0", "26.9", "ELI5", "28.1", "62.9", "58.8", "63.8", "31.2", "21.2", "21.1", "25.0", "31.1", "Ubuntu", "20.7", "35.8", "34.5", "38.5", "31.1", "17.3", "17.2", "23.3", "30.8", "Twitter", "37.0", "61.9", "59.3", "59.3", "53.6", "29.8", "29.8", "37.0", "52.8", "pushshift.io", "Reddit", "39.0", "27.8", "27.8", "27.8", "27.8", "27.8", "25.8", "28.0", "106.3", "Image", "Chat", "N/A", "40.1", "37.4", "31.1", "32.5", "18.3", "18.3", "21.8", "29.3", "IGC", "N/A", "86.3", "79.5", "23.1", "14.6", "10.0", "10.0", "10.2", "12.2", "Sh", "ot", "Ze", "ro-", "-O", "ut", "e-O", "ne", "Le", "av", "T", "ask", "s", "M", "Al", "l", "T", "sk", "gle", "Ta", "FT", "Sin", "ask", "s", "+", "Al", "l", "T", "M", "T", "sk", "gle", "Ta", "it", "+", "Sin", "Re", "dd", "it", "O", "nly", "Re", "dd", "sk", "gle", "Ta", "r", "+", "Sin", "Tw", "itte", "it)", "ex", "t", "in", "(fa", "stT", "Ta", "sk", "Sin", "gle", "h)", "scr", "atc", "(fr", "om", "Ta", "sk", "Sin", "gle", "-ba", "sed", "BE", "RT"], "regionBoundary": {"x2": 510.7560119628906, "y1": 64.89926147460938, "x1": 99.0, "y2": 292.8900146484375}, "caption": "Table 2: Validation perplexity for the dodecaDialogue tasks in various settings.", "page": 4}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 525.7954711914062, "y1": 457.487548828125, "x1": 306.9670104980469, "y2": 475.44500732421875}, "imageText": ["Reddit", "18.3", "15.3", "14.4", "Reddit+ConvAI2", "11.4", "14.2", "14.7", "Reddit+Wiz.", "of", "Wikipedia", "16.3", "8.7", "14.0", "Reddit+Empathetic", "Dialog", "17.9", "15.3", "11.3", "Multi-Tasking", "All", "4", "Tasks", "11.6", "8.7", "11.2", "the", "tic", "Di", "alo", "g", "Em", "pa", "dia", "iki", "pe", "of", "W", "W", "iz.", "AI", "2", "Model", "Co", "nv"], "regionBoundary": {"x2": 519.609619140625, "y1": 336.8116149902344, "x1": 309.0, "y2": 441.8818359375}, "caption": "Table 3: Transfer performance of various multi-task models (validation perplexity).", "page": 4}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 730.155520968967, "y1": 378.3924102783203, "x1": 99.57083596123589, "y2": 403.3333672417535}, "name": "1", "caption_text": "Table 1: The 12 dodecaDialogue subtasks, their sizes (number of train, valid, test utterances), and average number of turns and response length (words).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 710.0, "y1": 88.0, "x1": 117.0, "y2": 361.0}, "page": 1, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 633.5843404134114, "y1": 424.43546719021265, "x1": 195.90971204969617, "y2": 432.7722337510851}, "name": "2", "caption_text": "Table 2: Validation perplexity for the dodecaDialogue tasks in various settings.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 709.0, "y1": 89.0, "x1": 137.0, "y2": 424.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 730.271487765842, "y1": 635.3993733723958, "x1": 426.3430701361762, "y2": 660.3402879503038}, "name": "3", "caption_text": "Table 3: Transfer performance of various multi-task models (validation perplexity).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 722.0, "y1": 468.0, "x1": 429.0, "y2": 616.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.45022752549914, "y1": 187.93271382649738, "x1": 99.57084655761719, "y2": 212.87367078993054}, "name": "4", "caption_text": "Table 4: The impact of knowledge and image grounding in dodecaDialogue (validation perplexity).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 373.0, "y1": 88.0, "x1": 130.0, "y2": 167.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2183397081163, "y1": 216.16045633951822, "x1": 426.3152652316623, "y2": 307.51953125}, "name": "5", "caption_text": "Table 5: Validation perplexity on select dodecaDialogue tasks comparing relative weights of tasks during multi-tasking, followed by fine-tuning (row below). The relative task weight is the ratio of examples from that task compared to others presented during multitasking. \u221e indicates single-task training.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 722.0, "y1": 86.0, "x1": 426.0, "y2": 216.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 731.6551208496094, "y1": 417.1271853976779, "x1": 426.34311252170136, "y2": 458.67229037814667}, "name": "6", "caption_text": "Table 6: Impact of the decoding strategy on select tasks, reporting validation F1 score for the All Tasks MT model. N-gram block is for best beam size.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 722.0, "y1": 331.0, "x1": 431.0, "y2": 397.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.220458984375, "y1": 298.62853156195746, "x1": 99.57083596123589, "y2": 340.17367892795136}, "name": "7", "caption_text": "Table 7: Test performance for various metrics on the dodecaDialogue tasks comparing our multi-task and multitask + fine-tuned methods to existing approaches (cited). Dashes mean metric was not provided. \u2217 was reported on validation only. Score is defined on a per-task basis in the metric column.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 727.0, "y1": 86.0, "x1": 100.0, "y2": 299.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.222663031684, "y1": 282.5035519070095, "x1": 426.772223578559, "y2": 357.25843641493054}, "name": "1", "caption_text": "Figure 1: Human evaluations on Image Chat and Wizard of Wikipedia (WoW), comparing existing state of the art models with our All Tasks MT conversational agent. Engagingness win rates are statistically significant in all three matchups (binomial test, p < .05).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 726.0, "y1": 92.0, "x1": 430.0, "y2": 260.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.220458984375, "y1": 368.58685811360675, "x1": 99.57084655761719, "y2": 393.52917141384546}, "name": "8", "caption_text": "Table 8: Test performance for various metrics on the dodecaDialogue tasks comparing our multi-task and multitask + fine-tuned methods.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 726.0, "y1": 129.0, "x1": 100.0, "y2": 352.0}, "page": 11, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.923587375217, "y1": 638.9618767632378, "x1": 99.57084655761719, "y2": 663.9041900634766}, "name": "9", "caption_text": "Table 9: Validation performance for various metrics on the dodecaDialogue tasks comparing our multi-task and multi-task + fine-tuned methods.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 726.0, "y1": 385.0, "x1": 100.0, "y2": 622.0}, "page": 11, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9262152777777, "y1": 919.1229926215277, "x1": 99.50137668185764, "y2": 960.6680976019965}, "name": "10", "caption_text": "Table 10: All Tasks Multi-Tasking (MT) validation performance for various metrics on the dodecaDialogue tasks with one set of decoding parameters: a beam size of 3, minimum response length of 10, and blocking repeated tri-grams.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 617.0, "y1": 693.0, "x1": 210.0, "y2": 902.0}, "page": 11, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2236802842882, "y1": 322.25074768066406, "x1": 99.57083596123589, "y2": 363.79589504665796}, "name": "11", "caption_text": "Table 11: Best decoding parameters for each task, based on metric. Scores are from the best performing taskspecific multi-task + fine-tuned model on validation sets. \u201dMin L\u201d and \u201dMax L\u201d refer to the minimum and maximum decoding length, where \u201dL\u201d is the number of tokens.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 724.0, "y1": 110.0, "x1": 100.0, "y2": 322.0}, "page": 12, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9265543619791, "y1": 748.3771430121527, "x1": 99.54308403862846, "y2": 789.923604329427}, "name": "12", "caption_text": "Table 12: Human evaluations on Image Chat, comparing various decoding schemes for our Image+Seq2Seq model trained on all tasks MT, as well as comparisons with human outputs. Scores with \u2217 are statistically significant (binomial test, p < .05).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 729.0, "y1": 612.0, "x1": 101.0, "y2": 748.0}, "page": 12, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.3482937282986, "y1": 946.5230305989583, "x1": 99.50138727823892, "y2": 988.0694919162326}, "name": "13", "caption_text": "Table 13: Human evaluations on Wizard of Wikipedia (seen) test set, comparing various decoding schemes for our Image+Seq2Seq model trained on all tasks MT, as well as comparisons with human outputs, using ACUTE-Eval. All scores are statistically significant (binomial test, p < .05).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 712.0, "y1": 827.0, "x1": 114.0, "y2": 930.0}, "page": 12, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2236802842882, "y1": 521.9465891520182, "x1": 99.57083596123589, "y2": 563.4916517469618}, "name": "14", "caption_text": "Table 14: Human evaluations on Wizard of Wikipedia (unseen) test set, comparing various decoding schemes for our Image+Seq2Seq model trained on all tasks MT, as well as comparisons with human outputs, using ACUTEEval. All scores are statistically significant (binomial test, p < .05).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 712.0, "y1": 403.0, "x1": 114.0, "y2": 505.0}, "page": 13, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2235107421875, "y1": 696.845202975803, "x1": 99.57083596123589, "y2": 738.391621907552}, "name": "15", "caption_text": "Table 15: Human evaluations on Wizard of Wikipedia, comparing various decoding schemes for our Image+Seq2Seq model trained on all tasks MT, as well as comparisons with human outputs, in terms of Likert Scores. Ratings are reported as mean (stddev).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 622.0, "y1": 576.0, "x1": 208.0, "y2": 673.0}, "page": 13, "dpi": 0}], "error": null, "pdf": "/work/host-output/058fd4869bd21fbb5a3a3d840998c0c7e29f6506/2020.acl-main.222.pdf", "dpi": 100}