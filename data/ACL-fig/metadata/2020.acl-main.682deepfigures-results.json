{"raw_detected_boxes": [[{"x2": 694.0, "y1": 310.0, "x1": 463.0, "y2": 534.0}], [{"x2": 381.0, "y1": 88.0, "x1": 122.0, "y2": 226.0}], [], [], [], [{"x2": 717.0, "y1": 95.0, "x1": 114.0, "y2": 340.0}], [], [{"x2": 656.0, "y1": 89.0, "x1": 171.0, "y2": 263.0}], [], [], [], [], [{"x2": 398.0, "y1": 461.0, "x1": 102.0, "y2": 520.0}], [{"x2": 718.0, "y1": 89.0, "x1": 102.0, "y2": 201.0}, {"x2": 361.0, "y1": 275.0, "x1": 142.0, "y2": 451.0}], [{"x2": 629.0, "y1": 108.0, "x1": 203.0, "y2": 519.0}, {"x2": 629.0, "y1": 598.0, "x1": 202.0, "y2": 1007.0}], [{"x2": 724.0, "y1": 89.0, "x1": 103.0, "y2": 251.0}, {"x2": 359.0, "y1": 329.0, "x1": 144.0, "y2": 453.0}], [{"x2": 719.0, "y1": 254.0, "x1": 104.0, "y2": 370.0}, {"x2": 716.0, "y1": 746.0, "x1": 104.0, "y2": 861.0}]], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Figure", "name": "1", "captionBoundary": {"x2": 527.19775390625, "y1": 398.28253173828125, "x1": 306.44903564453125, "y2": 440.14996337890625}, "imageText": ["Legend", "abstract", "situated", "is", "appliance", "has", "buttons", "usedto_blend", "is", "white", "is", "white", "is", "appliance", "has", "buttons", "is", "white", "is", "appliance", "has", "buttons", "food", "mixer", "oven", "left-of", "left-of", "microwave", "1", "is", "it", "an", "appliance?", "yes", "2", "does", "it", "blend?", "no", "3", "is", "it", "the", "oven?", "no", "4", "is", "it", "the", "microwave?", "yes", "Turn", "Question", "Answer"], "regionBoundary": {"x2": 500.0, "y1": 222.8900146484375, "x1": 333.0, "y2": 384.8900146484375}, "caption": "Figure 1: Every gameplay in the CompGuessWhat?! benchmark has a reference scene that is mapped to a scene graph composed of objects represented in terms of abstract and situated attributes.", "page": 0}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 527.1971435546875, "y1": 258.0235290527344, "x1": 71.69100189208984, "y2": 313.590087890625}, "imageText": ["GDSE-SL", "49.1%", "59.9", "47.6", "60.1", "48.3", "29.8%", "22.3%", "43.0", "GDSE-CL", "59.8%", "59.5", "47.6", "59.8", "48.1", "43.4%", "29.8%", "50.1", "DeVries-SL", "41.5%", "46.8", "39.1", "48.5", "42.7", "31.3%", "28.4%", "38.5", "DeVries-RL", "53.5%", "45.2", "38.9", "47.2", "42.5", "43.9%", "38.7%", "46.2", "GDSE-SL-text", "-", "57.0", "45.3", "57.5", "46", "-", "-", "-", "GDSE-CL-text", "-", "56.9", "45.0", "57.3", "45", "-", "-", "-", "GloVe", "-", "34.6", "29.7", "36.4", "33.6", "-", "-", "-", "ResNet", "-", "24.5", "31.7", "27.9", "43.4", "-", "-", "-", "Random", "15.81%", "15.1", "0.1", "7.8", "2.8", "16.9%", "18.6%", "13.3", "Gameplay", "Attribute", "Prediction", "Zero-shot", "Gameplay", "GroLLA", "Accuracy", "A-F1", "S-F1", "AS-F1", "L-F1", "ND-Acc", "OD-Acc"], "regionBoundary": {"x2": 517.0, "y1": 69.19353485107422, "x1": 81.0, "y2": 245.8900146484375}, "caption": "Table 1: Results for state-of-the-art models on the CompGuessWhat?! suite of evaluation tasks. We assess model quality in terms of gameplay accuracy, the attribute prediction quality, measured in terms of F1 for the abstract (AF1), situated (S-F1), abstract+situated (AS-F1) and location (L-F1) prediction scenario, as well as zero-shot learning gameplay. The final score GROLLA is a macro-average of the individual scores. We use the models GloVe, ResNet and GDSE-*-text only as a baseline for the attribute prediction tasks.", "page": 5}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 460.076904296875, "y1": 388.0775451660156, "x1": 137.46800231933594, "y2": 394.08001708984375}, "imageText": [], "regionBoundary": {"x2": 455.0, "y1": 73.8900146484375, "x1": 142.0, "y2": 375.8900146484375}, "caption": "Figure 4: Object category distribution in the near-domain reference set of games.", "page": 14}, {"figType": "Figure", "name": "5", "captionBoundary": {"x2": 463.778076171875, "y1": 742.1416015625, "x1": 133.76699829101562, "y2": 748.14404296875}, "imageText": [], "regionBoundary": {"x2": 455.0, "y1": 427.8900146484375, "x1": 142.0, "y2": 729.8900146484375}, "caption": "Figure 5: Object category distribution in the out-of-domain reference set of games.", "page": 14}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 291.9177551269531, "y1": 179.42056274414062, "x1": 71.67100524902344, "y2": 233.24407958984375}, "imageText": ["Person", "little", "girl", "has", "eyes", "has", "2", "legs", "has", "2", "arms", "has", "mouth", "Umbrella", "center", "has", "handle", "open", "black", "red", "type", "accessories", "has", "shaft"], "regionBoundary": {"x2": 275.0, "y1": 61.8900146484375, "x1": 86.0, "y2": 166.8900146484375}, "caption": "Figure 2: CompGuessWhat?!: Detailed description of the attributes of two different objects in the reference scene. Both the objects have a set of abstract attributes (indicated in blue) and a set of situated attributes (indicated in green).", "page": 1}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 291.92431640625, "y1": 336.5565490722656, "x1": 71.69100189208984, "y2": 354.5140075683594}, "imageText": ["Model", "Hidden", "size", "DeVries-SL", "512", "DeVries-RL", "512", "GDSE-SL", "512", "GDSE-CL", "512", "GDSE-SL-text", "1024", "GDSE-CL-text", "1024", "GloVe", "300", "ResNet", "2048"], "regionBoundary": {"x2": 260.0, "y1": 197.8900146484375, "x1": 102.0, "y2": 324.8900146484375}, "caption": "Table 4: Summary of hidden state sizes for all the models considered in the attribute prediction evaluation.", "page": 13}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 525.5410766601562, "y1": 156.83053588867188, "x1": 71.69100189208984, "y2": 174.78802490234375}, "imageText": ["Split", "#", "GuessWhat?!dialogues", "#", "CompGuessWhat?!", "dialogues", "Vocab.", "size", "Avg.", "dialogue", "length", "Successful", "dialogues", "Failed", "dialogues", "Incomplete", "dialogues", "Train", "113221", "46277(40.92%)", "7090", "5.128", "84.06%", "(38901)", "10.35%", "(4790)", "5.59%", "(2586)", "Valid", "23739", "9716(41.02%)", "3605", "5.106", "83.97%", "(8159)", "11.03%", "(1069)", "5.03%", "(488)", "Test", "23785", "9619(40.44%)", "3552", "5.146", "84.10%", "(8090)", "10.74%", "(1034)", "5.14%", "(495)"], "regionBoundary": {"x2": 519.0, "y1": 63.8900146484375, "x1": 72.0, "y2": 144.8900146484375}, "caption": "Table 3: Comparison between the original GuessWhat?! dataset and CompGuessWhat?! dataset. We report the percentage of dialogues that we retain after the filtering procedure based on the VisualGenome images.", "page": 13}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 291.92425537109375, "y1": 386.695556640625, "x1": 71.5320053100586, "y2": 404.65301513671875}, "imageText": ["Split", "GuessWhat?!", "images", "Mapped", "images", "Train", "46794", "19117", "Validation", "9844", "4049", "Test", "9899", "3989"], "regionBoundary": {"x2": 289.0, "y1": 331.8900146484375, "x1": 74.0, "y2": 374.8900146484375}, "caption": "Table 2: Statistics of the mapping between GuessWhat?! images and VisualGenome images.", "page": 12}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 525.5472412109375, "y1": 202.54751586914062, "x1": 72.0, "y2": 232.46002197265625}, "imageText": ["Failure", "DeVries-RL", "is", "it", "in", "the", "top?", "yes", "is", "it", "in", "the", "middle?", "yes", "is", "it", "in", "the", "left?", "no", "is", "it", "tie?", "no", "is", "it", "a", "person?", "no", "is", "it", "a", "person?", "yes", "Question", "Answer", "is", "it", "an", "animal?", "no", "is", "it", "a", "picture?", "no", "is", "it", "a", "toilet?", "no", "Failure", "GDSE-CL", "is", "the", "person", "holding", "it?", "yes", "is", "person", "wearing", "it?", "no", "is", "it", "to", "the", "right", "of", "the", "person?", "no", "is", "it", "wood?", "no", "is", "it", "a", "light?", "no", "is", "it", "near", "the", "person?", "yes", "is", "it", "a", "vehicle?", "no", "is", "it", "a", "person?", "no", "Question", "Answer", "is", "it", "a", "bird?", "no", "Target", "object:", "crocodile"], "regionBoundary": {"x2": 475.0, "y1": 63.94525909423828, "x1": 123.0, "y2": 189.3603515625}, "caption": "Figure 3: Example gameplay in the out-of-domain scenario of the two best performing systems GDSE-RL and DeVries-RL. The models have to play the game considering the crocodile as target object. This is a zero-shot scenario because the object category crocodile is not among the MSCOCO categories.", "page": 7}, {"figType": "Table", "name": "7", "captionBoundary": {"x2": 457.477294921875, "y1": 278.4075622558594, "x1": 139.75999450683594, "y2": 284.4100341796875}, "imageText": ["Model", "Lexicaldiversity", "Question", "diversity", "%", "games", "repeated", "questions", "Super-cat", "->", "obj/attr", "Object", "->", "attribute", "%", "turns", "location", "questions", "Vocab.", "size", "Accuracy", "DeVries-SL", "0.76", "44.64", "12.54%", "97.33%", "73%", "29.34%", "2668", "31.33%", "DeVries-RL", "0.13", "1.77", "99.48%", "96.43%", "98.63%", "78.07%", "702", "43.92%", "GDSE-SL", "0.13", "6.10", "92.38%", "95.60%", "52.35%", "15.74%", "862", "29.78%", "GDSE-CL", "0.17", "13.74", "66.76%", "99.48%", "67.25%", "31.23%", "1260", "43.42%"], "regionBoundary": {"x2": 524.0, "y1": 182.8900146484375, "x1": 72.0, "y2": 266.8900146484375}, "caption": "Table 7: Gameplay quality analysis on Near-domain zero-shot reference games.", "page": 16}, {"figType": "Table", "name": "8", "captionBoundary": {"x2": 461.17742919921875, "y1": 632.4705810546875, "x1": 136.05799865722656, "y2": 638.4730224609375}, "imageText": ["Model", "Lexicaldiversity", "Question", "diversity", "%", "games", "repeated", "questions", "Super-cat", "->", "obj/attr", "Object", "->", "attribute", "%", "turns", "location", "questions", "Vocab.", "size", "Accuracy", "DeVries-SL", "0.83", "45.86", "11.58", "97.87%", "76.50%", "29.64%", "2604", "28.37%", "DeVries-RL", "0.24", "2.96", "98.49%", "91.83%", "98.58%", "75.84%", "1275", "38.73%", "GDSE-SL", "0.09", "1.31", "97.19%", "100%", "67.45%", "7.90%", "519", "22.32%", "GDSE-CL", "0.14", "7.86", "66.32%", "100%", "71.14%", "26.03%", "1002", "29.83%"], "regionBoundary": {"x2": 524.0, "y1": 537.8900146484375, "x1": 72.0, "y2": 620.8900146484375}, "caption": "Table 8: Gameplay quality analysis on Out-of-domain zero-shot reference games.", "page": 16}, {"figType": "Table", "name": "6", "captionBoundary": {"x2": 290.2705078125, "y1": 339.4755554199219, "x1": 71.69100189208984, "y2": 369.3890075683594}, "imageText": ["#", "images", "#", "games", "Near-domain", "validation", "1208", "5343", "Out-of-domain", "validation", "1306", "5372", "Near-domain", "test", "3097", "13836", "Out-of-domain", "test", "3212", "13300"], "regionBoundary": {"x2": 258.0, "y1": 234.8900146484375, "x1": 104.0, "y2": 327.8900146484375}, "caption": "Table 6: Statistics for the CompGuessWhat?! zero-shot scenario. We provide near-domain and out-of-domain splits using specific nocaps images as reference scenes.", "page": 15}, {"figType": "Table", "name": "5", "captionBoundary": {"x2": 525.547119140625, "y1": 194.02151489257812, "x1": 71.69100189208984, "y2": 211.97900390625}, "imageText": ["Abstract", "Situated-only", "Abstract+situated", "Location", "Models", "F1", "Precision", "Recall", "F1", "Precision", "Recall", "F1", "Precision", "Recall", "F1", "Precision", "Recall", "DeVries-SL", "46.8", "46.2", "53.4", "39.1", "34.8", "51.2", "48.5", "50.8", "57.8", "42.7", "42.8", "42.9", "DeVries-RL", "45.2", "44.4", "52.5", "38.9", "34.4", "51", "47.2", "49.4", "57.4", "43.5", "43.6", "43.6", "GDSE-SL", "59.9", "59.8", "64.1", "47.6", "44", "58.3", "60.1", "63.8", "65.9", "48.3", "48.6", "48.6", "GDSE-CL", "59.5", "59.3", "63.6", "47.6", "43.8", "58.6", "59.8", "63.3", "65.6", "48.1", "48.1", "48.6", "GDSE-SL-text", "57", "56.7", "61.5", "45.3", "41.3", "56.5", "57.5", "60.6", "60.6", "46", "46.1", "46.4", "GDSE-CL-text", "56.9", "56.9", "61.4", "45", "40.9", "56.4", "57.3", "60.5", "60.5", "45", "45", "45.4", "GloVe", "34.6", "33.6", "45.9", "29.7", "25.1", "42.1", "36.4", "37.4", "52.9", "33.6", "33.6", "33.7", "ResNet", "24.5", "24.3", "37.9", "31.7", "27.5", "43.8", "27.9", "30.3", "47.1", "43.4", "43.5", "43.6", "Random", "15.1", "40.8", "16.3", "0.1", "50.6", "0.1", "7.8", "50.3", "5.4", "27.5", "49.7", "20.3"], "regionBoundary": {"x2": 526.0, "y1": 61.8900146484375, "x1": 72.0, "y2": 181.8900146484375}, "caption": "Table 5: Full set of attribute prediction task metrics. We evaluate F1, Precision and Recall for all the tasks. All the metrics are computed as macro-average.", "page": 15}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 732.2191026475695, "y1": 553.1701829698351, "x1": 425.6236606174045, "y2": 611.3193935818142}, "name": "1", "caption_text": "Figure 1: Every gameplay in the CompGuessWhat?! benchmark has a reference scene that is mapped to a scene graph composed of objects represented in terms of abstract and situated attributes.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 694.0, "y1": 310.0, "x1": 463.0, "y2": 534.0}, "page": 0, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.44132656521265, "y1": 249.19522603352863, "x1": 99.54306284586588, "y2": 323.95011054144965}, "name": "2", "caption_text": "Figure 2: CompGuessWhat?!: Detailed description of the attributes of two different objects in the reference scene. Both the objects have a set of abstract attributes (indicated in blue) and a set of situated attributes (indicated in green).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 382.0, "y1": 88.0, "x1": 121.0, "y2": 229.0}, "page": 1, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2182549370659, "y1": 358.3660125732422, "x1": 99.57083596123589, "y2": 435.54178873697913}, "name": "1", "caption_text": "Table 1: Results for state-of-the-art models on the CompGuessWhat?! suite of evaluation tasks. We assess model quality in terms of gameplay accuracy, the attribute prediction quality, measured in terms of F1 for the abstract (AF1), situated (S-F1), abstract+situated (AS-F1) and location (L-F1) prediction scenario, as well as zero-shot learning gameplay. The final score GROLLA is a macro-average of the individual scores. We use the models GloVe, ResNet and GDSE-*-text only as a baseline for the attribute prediction tasks.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 729.0, "y1": 86.0, "x1": 100.0, "y2": 357.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9267239040798, "y1": 281.3159942626953, "x1": 100.0, "y2": 322.8611416286892}, "name": "3", "caption_text": "Figure 3: Example gameplay in the out-of-domain scenario of the two best performing systems GDSE-RL and DeVries-RL. The models have to play the game considering the crocodile as target object. This is a zero-shot scenario because the object category crocodile is not among the MSCOCO categories.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 673.0, "y1": 89.0, "x1": 158.0, "y2": 280.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.45035468207465, "y1": 537.077162000868, "x1": 99.35000737508138, "y2": 562.018076578776}, "name": "2", "caption_text": "Table 2: Statistics of the mapping between GuessWhat?! images and VisualGenome images.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 401.0, "y1": 461.0, "x1": 100.0, "y2": 537.0}, "page": 12, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9181620279948, "y1": 217.82018873426648, "x1": 99.57083596123589, "y2": 242.76114569769965}, "name": "3", "caption_text": "Table 3: Comparison between the original GuessWhat?! dataset and CompGuessWhat?! dataset. We report the percentage of dialogues that we retain after the filtering procedure based on the VisualGenome images.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 720.0, "y1": 86.0, "x1": 99.0, "y2": 218.0}, "page": 13, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.450439453125, "y1": 467.4396514892578, "x1": 99.57083596123589, "y2": 492.3805660671658}, "name": "4", "caption_text": "Table 4: Summary of hidden state sizes for all the models considered in the attribute prediction evaluation.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 363.0, "y1": 275.0, "x1": 136.0, "y2": 468.0}, "page": 13, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 638.9957004123264, "y1": 538.996590508355, "x1": 190.9277809990777, "y2": 547.3333570692274}, "name": "4", "caption_text": "Figure 4: Object category distribution in the near-domain reference set of games.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 629.0, "y1": 106.0, "x1": 203.0, "y2": 519.0}, "page": 14, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 644.136216905382, "y1": 1030.752224392361, "x1": 185.78749762641058, "y2": 1039.0889485677083}, "name": "5", "caption_text": "Figure 5: Object category distribution in the out-of-domain reference set of games.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 629.0, "y1": 598.0, "x1": 202.0, "y2": 1010.0}, "page": 14, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9265543619791, "y1": 269.47432623969183, "x1": 99.57083596123589, "y2": 294.415283203125}, "name": "5", "caption_text": "Table 5: Full set of attribute prediction task metrics. We evaluate F1, Precision and Recall for all the tasks. All the metrics are computed as macro-average.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 731.0, "y1": 86.0, "x1": 99.0, "y2": 253.0}, "page": 15, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.15348307291663, "y1": 471.4938269721137, "x1": 99.57083596123589, "y2": 513.0402882893881}, "name": "6", "caption_text": "Table 6: Statistics for the CompGuessWhat?! zero-shot scenario. We provide near-domain and out-of-domain splits using specific nocaps images as reference scenes.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 359.0, "y1": 326.0, "x1": 144.0, "y2": 455.0}, "page": 15, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 635.3851318359375, "y1": 386.6771697998047, "x1": 194.11110348171658, "y2": 395.01393636067706}, "name": "7", "caption_text": "Table 7: Gameplay quality analysis on Near-domain zero-shot reference games.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 727.0, "y1": 254.0, "x1": 99.0, "y2": 387.0}, "page": 16, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 640.5242072211371, "y1": 878.4313625759548, "x1": 188.96944257948132, "y2": 886.768086751302}, "name": "8", "caption_text": "Table 8: Gameplay quality analysis on Out-of-domain zero-shot reference games.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 727.0, "y1": 746.0, "x1": 99.0, "y2": 878.0}, "page": 16, "dpi": 0}], "error": null, "pdf": "/work/host-output/85c12c83c8ecb0cd5e96e3c352bb66d64080b64c/2020.acl-main.682.pdf", "dpi": 100}