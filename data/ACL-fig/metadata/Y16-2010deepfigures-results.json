{"raw_detected_boxes": [[], [{"x2": 376.0, "y1": 150.0, "x1": 118.0, "y2": 226.0}], [{"x2": 662.0, "y1": 151.0, "x1": 161.0, "y2": 362.0}, {"x2": 379.0, "y1": 481.0, "x1": 121.0, "y2": 582.0}], [{"x2": 389.0, "y1": 153.0, "x1": 105.0, "y2": 363.0}, {"x2": 722.0, "y1": 184.0, "x1": 435.0, "y2": 269.0}], [{"x2": 690.0, "y1": 152.0, "x1": 130.0, "y2": 287.0}], [{"x2": 695.0, "y1": 153.0, "x1": 132.0, "y2": 308.0}, {"x2": 704.0, "y1": 428.0, "x1": 448.0, "y2": 519.0}], [], [{"x2": 678.0, "y1": 148.0, "x1": 147.0, "y2": 351.0}], [], []], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Table", "name": "1", "captionBoundary": {"x2": 525.0263671875, "y1": 387.072021484375, "x1": 304.5108947753906, "y2": 404.5311584472656}, "imageText": ["Train", "330,000", "6.09M", "5.91M", "Tune", "1,235", "34.4", "k", "30.8", "k", "Test", "1,160", "28.5", "k", "26.7", "k", "Sentence", "Pairs", "Words", "Japanese", "English"], "regionBoundary": {"x2": 507.0, "y1": 308.0, "x1": 322.0, "y2": 376.0}, "caption": "Table 1: Number of sentences and words in the training, tuning and test sets of the KFTT corpus.", "page": 5}, {"figType": "Figure", "name": "6", "captionBoundary": {"x2": 525.040283203125, "y1": 246.44189453125, "x1": 70.00241088867188, "y2": 287.1488037109375}, "imageText": [], "regionBoundary": {"x2": 503.0, "y1": 105.0, "x1": 92.0, "y2": 231.0}, "caption": "Figure 6: Example of building and applying preordering model using tree structures as the reference. (a), (b) and the arrow from (a) to (b) are the same with Figure 5. The difference is that both (a) and (b) generating from only a training set. (c) a sentence from test set becomes a target-like source sentence in the solid line and in dotted line it shows corresponding target sentence. The arrow from (b) to (c) represents building preordering model.", "page": 5}, {"figType": "Figure", "name": "1", "captionBoundary": {"x2": 246.86497497558594, "y1": 185.17413330078125, "x1": 113.65489959716797, "y2": 191.01031494140625}, "imageText": [], "regionBoundary": {"x2": 280.0, "y1": 105.0, "x1": 81.0, "y2": 169.0}, "caption": "Figure 1: Example of preordering.", "page": 1}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 290.5179443359375, "y1": 438.1943054199219, "x1": 70.00248718261719, "y2": 455.6534423828125}, "imageText": [], "regionBoundary": {"x2": 280.0, "y1": 343.0, "x1": 81.0, "y2": 423.0}, "caption": "Figure 3: Tree structures related to bracketing transduction grammar.", "page": 2}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 525.0401000976562, "y1": 281.15924072265625, "x1": 70.00238037109375, "y2": 321.86419677734375}, "imageText": [], "regionBoundary": {"x2": 480.0, "y1": 105.0, "x1": 115.0, "y2": 266.0}, "caption": "Figure 2: The difference between previous methods (Neubig et al., 2012; Nakagawa, 2015) and our proposed method when building a preordering model. In previous work, the tree structures and the preordering model should be deduced at the same time from the parallel text. Our work firstly produces the tree structures from parallel text, and then computes a preordering model.", "page": 2}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 525.0586547851562, "y1": 263.79949951171875, "x1": 70.00149536132812, "y2": 339.3742370605469}, "imageText": ["ja-en", "59.41", "72.98", "64.87", "65.87", "16.01", "18.10", "Tree-based", "64.87", "80.14", "66.23", "66.63", "17.55", "18.76", "Top-down", "66.40", "81.45", "68.53", "68.69", "19.10", "19.07", "Oracle", "68.18", "85.81", "75.44", "75.18", "23.20", "23.87", "en-ja", "51.12", "73.99", "65.83", "68.10", "19.45", "21.51", "Tree-based", "66.12", "83.08", "69.31", "70.11", "20.43", "21.97", "Top-down", "75.59", "87.68", "71.56", "72.28", "22.56", "23.31", "Oracle", "66.60", "87.39", "75.17", "74.74", "23.75", "24.23", "Baseline", "dl", "=", "0", "dl", "=", "6", "dl", "=", "0", "dl", "=", "6", "Baseline", "Language", "pair", "Intrinsic", "Intrinsic", "&", "Extrinsic", "Extrinsic", "mod", "FRS", "norm", "\u03c4", "RIBES", "BLEU"], "regionBoundary": {"x2": 489.0, "y1": 106.0, "x1": 106.0, "y2": 253.0}, "caption": "Table 2: Intrinsic and extrinsic evaluation scores in English to Japanese and Japanese to English (mod FRS is the modified Fuzzy Reordering Score; norm \u03c4 is normalized Kendall\u2019s \u03c4 ; dl stands for distortion limits). Baseline is a default PB-SMT system; Tree-based is our proposed preordering model; Top-down is the top-down BTG parsingbased reordering model; Oracle is an oracle system that uses HSSA tree structures obtained for the test set. The gray cells indicate the results to compare in translation: systems with preordering methods and with a distortion limit of 0 should be compared with the corresponding baseline system with a default distortion limit of 6; other results are given for completeness.", "page": 7}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 290.5179443359375, "y1": 280.67413330078125, "x1": 70.00247192382812, "y2": 356.2489318847656}, "imageText": [], "regionBoundary": {"x2": 285.0, "y1": 105.0, "x1": 75.0, "y2": 266.0}, "caption": "Figure 4: Hierarchical sub-sentential alignment and generation of tree structures. (a) a best segmentation according to the second diagonal in the soft alignment matrix using the HSSA method coresponds to an Inverted rule in the BTG formalism; (b) a best segmentation according to the main diagonal corresponds to a Straight rule. (b) is a sub-part in (a) to illustrate recursivity.", "page": 3}, {"figType": "Figure", "name": "5", "captionBoundary": {"x2": 525.0595092773438, "y1": 226.3385009765625, "x1": 70.00241088867188, "y2": 290.289306640625}, "imageText": [], "regionBoundary": {"x2": 503.0, "y1": 105.0, "x1": 92.0, "y2": 211.0}, "caption": "Figure 5: Example for oracle experiment. (a) a soft alignment matrix between a source sentence (left) and a target sentence (above); (b) a tree structure with Straight or Inverted nodes; (c) the alignment between the reordered source sentence and the target sentence. The arrow from (a) to (b) represents the generation of tree structures from word-toword associations by use of the HSSA method; the arrow from (b) to (c) is reordering. In the oracle experiment, this is applied on test data. In a real experiment, this is applied on test data and development data, while the scheme given in Figure 6 is applied on the test data.", "page": 4}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 342.8680207994249, "y1": 257.1862962510851, "x1": 157.85402721828885, "y2": 265.29210408528644}, "name": "1", "caption_text": "Figure 1: Example of preordering.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 376.0, "y1": 150.0, "x1": 118.0, "y2": 228.0}, "page": 1, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.2223612467448, "y1": 390.49894544813367, "x1": 97.22552829318576, "y2": 447.03360663519965}, "name": "2", "caption_text": "Figure 2: The difference between previous methods (Neubig et al., 2012; Nakagawa, 2015) and our proposed method when building a preordering model. In previous work, the tree structures and the preordering model should be deduced at the same time from the parallel text. Our work firstly produces the tree structures from parallel text, and then computes a preordering model.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 665.0, "y1": 150.0, "x1": 160.0, "y2": 362.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.4971449110243, "y1": 608.6032019721137, "x1": 97.22567664252387, "y2": 632.8520033094618}, "name": "3", "caption_text": "Figure 3: Tree structures related to bracketing transduction grammar.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 384.0, "y1": 481.0, "x1": 118.0, "y2": 582.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.4971449110243, "y1": 389.82518513997394, "x1": 97.22565544976128, "y2": 494.7901831732856}, "name": "4", "caption_text": "Figure 4: Hierarchical sub-sentential alignment and generation of tree structures. (a) a best segmentation according to the second diagonal in the soft alignment matrix using the HSSA method coresponds to an Inverted rule in the BTG formalism; (b) a best segmentation according to the main diagonal corresponds to a Straight rule. (b) is a sub-part in (a) to illustrate recursivity.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 393.0, "y1": 147.0, "x1": 105.0, "y2": 365.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.2493184407551, "y1": 314.35902913411456, "x1": 97.22557067871094, "y2": 403.1795925564236}, "name": "5", "caption_text": "Figure 5: Example for oracle experiment. (a) a soft alignment matrix between a source sentence (left) and a target sentence (above); (b) a tree structure with Straight or Inverted nodes; (c) the alignment between the reordered source sentence and the target sentence. The arrow from (a) to (b) represents the generation of tree structures from word-toword associations by use of the HSSA method; the arrow from (b) to (c) is reordering. In the oracle experiment, this is applied on test data. In a real experiment, this is applied on test data and development data, while the scheme given in Figure 6 is applied on the test data.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 693.0, "y1": 152.0, "x1": 129.0, "y2": 291.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.2226155598958, "y1": 342.28040907118054, "x1": 97.22557067871094, "y2": 398.8177829318576}, "name": "6", "caption_text": "Figure 6: Example of building and applying preordering model using tree structures as the reference. (a), (b) and the arrow from (a) to (b) are the same with Figure 5. The difference is that both (a) and (b) generating from only a training set. (c) a sentence from test set becomes a target-like source sentence in the solid line and in dotted line it shows corresponding target sentence. The arrow from (b) to (c) represents building preordering model.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 698.0, "y1": 152.0, "x1": 129.0, "y2": 317.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.2032877604166, "y1": 537.6000298394097, "x1": 422.9317982991536, "y2": 561.8488311767578}, "name": "1", "caption_text": "Table 1: Number of sentences and words in the training, tuning and test sets of the KFTT corpus.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 704.0, "y1": 428.0, "x1": 448.0, "y2": 522.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.2481316460503, "y1": 366.388193766276, "x1": 97.22429911295572, "y2": 471.3531070285373}, "name": "2", "caption_text": "Table 2: Intrinsic and extrinsic evaluation scores in English to Japanese and Japanese to English (mod FRS is the modified Fuzzy Reordering Score; norm \u03c4 is normalized Kendall\u2019s \u03c4 ; dl stands for distortion limits). Baseline is a default PB-SMT system; Tree-based is our proposed preordering model; Top-down is the top-down BTG parsingbased reordering model; Oracle is an oracle system that uses HSSA tree structures obtained for the test set. The gray cells indicate the results to compare in translation: systems with preordering methods and with a distortion limit of 0 should be compared with the corresponding baseline system with a default distortion limit of 6; other results are given for completeness.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 694.0, "y1": 146.0, "x1": 131.0, "y2": 368.0}, "page": 7, "dpi": 0}], "error": null, "pdf": "/work/host-output/53a1afcc9dadf37700dbf361b8bc2234ddccbfd9/Y16-2010.pdf", "dpi": 100}