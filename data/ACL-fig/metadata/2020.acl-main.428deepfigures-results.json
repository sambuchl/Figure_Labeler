{"raw_detected_boxes": [[{"x2": 718.0, "y1": 311.0, "x1": 430.0, "y2": 416.0}], [], [], [{"x2": 727.0, "y1": 94.0, "x1": 432.0, "y2": 227.0}, {"x2": 720.0, "y1": 367.0, "x1": 438.0, "y2": 508.0}], [{"x2": 393.0, "y1": 94.0, "x1": 111.0, "y2": 221.0}, {"x2": 726.0, "y1": 91.0, "x1": 431.0, "y2": 278.0}, {"x2": 726.0, "y1": 354.0, "x1": 431.0, "y2": 540.0}], [{"x2": 725.0, "y1": 92.0, "x1": 431.0, "y2": 282.0}, {"x2": 727.0, "y1": 387.0, "x1": 433.0, "y2": 529.0}, {"x2": 726.0, "y1": 710.0, "x1": 431.0, "y2": 890.0}], [{"x2": 395.0, "y1": 90.0, "x1": 102.0, "y2": 245.0}, {"x2": 387.0, "y1": 297.0, "x1": 117.0, "y2": 409.0}], [{"x2": 675.0, "y1": 93.0, "x1": 159.0, "y2": 167.0}, {"x2": 680.0, "y1": 336.0, "x1": 147.0, "y2": 410.0}, {"x2": 710.0, "y1": 496.0, "x1": 121.0, "y2": 652.0}], [], [], [{"x2": 393.0, "y1": 87.0, "x1": 110.0, "y2": 227.0}, {"x2": 724.0, "y1": 93.0, "x1": 433.0, "y2": 312.0}], [], [{"x2": 695.0, "y1": 219.0, "x1": 111.0, "y2": 861.0}], [{"x2": 720.0, "y1": 192.0, "x1": 110.0, "y2": 455.0}, {"x2": 567.0, "y1": 706.0, "x1": 264.0, "y2": 906.0}]], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Figure", "name": "1", "captionBoundary": {"x2": 525.5465087890625, "y1": 315.0435485839844, "x1": 307.2760009765625, "y2": 333.0010070800781}, "imageText": [], "regionBoundary": {"x2": 526.0, "y1": 221.8900146484375, "x1": 307.0, "y2": 302.8900146484375}, "caption": "Figure 1: GPT-2 345M model completions can show lack of coherence, e.g. direct contradictions.", "page": 0}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 525.5467529296875, "y1": 400.1995544433594, "x1": 307.2760009765625, "y2": 489.887939453125}, "imageText": ["Human", "-", "-", ".400", ".300", ".200", ".100", "MLE", "Baseline", "11.4", ".199", ".491", ".282", ".157", ".068", "UL,", "\u03b1", "=", "100", "11.4", ".200", ".483", ".289", ".163", ".063", "UL,", "\u03b1", "=", "101", "11.9", ".201", ".459", ".328", ".154", ".058", "UL,", "\u03b1", "=", "102", "12.5", ".190", ".430", ".335", ".163", ".071", "UL,", "\u03b1", "=", "103", "14.4", ".174", ".399", ".339", ".188", ".073", "Model", "PPL", "F1", "Freq", "Med", "Rare", "Rarest", "Token", "frequency", "classes"], "regionBoundary": {"x2": 524.0, "y1": 278.8900146484375, "x1": 309.0, "y2": 387.8900146484375}, "caption": "Table 4: Unlikelihood loss applied to vocabulary distributions. Stronger \u03b1 terms greatly shift probability mass from the most Frequent words to Medium and Rare words, at a small cost to PPL and F1. Frequent, medium, rare and rarest token classes are defined as the sets of tokens whose cumulative masses account for the top 40%, the next 30%, the next 20% and final 10% of tokens empirically generated by humans, respectively.", "page": 5}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 525.5465698242188, "y1": 218.35153198242188, "x1": 307.2760009765625, "y2": 260.22003173828125}, "imageText": ["MLE", "Baseline", "Unlikelihood", "Repetition", "(ELI5)", "Vocabulary", "(ConvAI2)", "ag", "e", "ce", "nt", "P", "er", "ni", "ng", "W", "in", "100%", "75%", "50%", "25%", "\u03b1", "=", "101", "\u03b1", "=", "102", "0%"], "regionBoundary": {"x2": 523.0, "y1": 66.97338104248047, "x1": 312.4180603027344, "y2": 201.39752197265625}, "caption": "Figure 4: Human evaluation experiments for label unlikelihood on ELI5 (left), and vocabulary unlikelihood on ConvAI2 for two values of \u03b1 (right). Unlikelihood significantly outperforms the MLE baselines.", "page": 5}, {"figType": "Figure", "name": "5", "captionBoundary": {"x2": 525.5465087890625, "y1": 657.9575805664062, "x1": 307.2759704589844, "y2": 711.781005859375}, "imageText": ["1000", "100", "10", "1", "Human", "Baseline", "as", "s", "e", "m", "at", "iv", "um", "ul", "ds", "c", "w", "or", "Ra", "re", "0.21", "0.19", "0.17", "0.15", "0.37", "0.39", "0.41", "0.43", "0.45", "0.47", "0.49", "Frequent", "words", "cumulative", "mass"], "regionBoundary": {"x2": 523.0, "y1": 510.8900146484375, "x1": 312.8147277832031, "y2": 641.3585205078125}, "caption": "Figure 5: Vocabulary control with unlikelihood training: more probability mass is transferred from Frequent words to Rare words as we increase the \u03b1weighting parameter. The maximum likelihood baseline is far from the human distribution.", "page": 5}, {"figType": "Table", "name": "8", "captionBoundary": {"x2": 290.27056884765625, "y1": 175.88253784179688, "x1": 72.0, "y2": 217.75103759765625}, "imageText": ["UL", "(Context", "only)", "8.8", ".345", ".270", ".001", "UL", "(Label", "only)", "8.3", ".371", ".645", ".000", "UL", "(Context", "+", "Label)", "8.5", ".358", ".445", ".003", "Human", "-", "-", ".160", ".0006", "MLE", "Baseline", "8.3", ".373", ".582", ".002", "Model", "PPL", "F1", "Context", "Label", "Repetition"], "regionBoundary": {"x2": 283.0, "y1": 62.8900146484375, "x1": 79.0, "y2": 163.8900146484375}, "caption": "Table 8: Evaluation on the Wizard of Wikipedia task test set, comparing standard likelihood (MLE) with repetition unlikelihood loss training, where both methods use beam search (beam size of 5).", "page": 10}, {"figType": "Table", "name": "9", "captionBoundary": {"x2": 525.5467529296875, "y1": 236.97152709960938, "x1": 307.2760009765625, "y2": 362.5260314941406}, "imageText": ["Nucleus", "p", "=", "0.3", "11.4", ".180", ".452", ".315", ".168", ".064", "Nucleus", "p", "=", "0.4", "11.4", ".171", ".440", ".320", ".172", ".068", "Nucleus", "p", "=", "0.5", "11.4", ".160", ".425", ".322", ".180", ".072", "Nucleus", "p", "=", "0.6", "11.4", ".151", ".411", ".318", ".192", ".078", "Nucleus", "p", "=", "1.0", "11.4", ".141", ".394", ".302", ".201", ".101", "UL,", "\u03b1", "=", "100", "11.4", ".200", ".483", ".289", ".163", ".063", "UL,", "\u03b1", "=", "101", "11.9", ".201", ".459", ".328", ".154", ".058", "UL,", "\u03b1", "=", "102", "12.5", ".190", ".430", ".335", ".163", ".071", "UL,", "\u03b1", "=", "103", "14.4", ".174", ".399", ".339", ".188", ".073", "Human", "-", "-", ".400", ".300", ".200", ".100", "MLE", "Baseline", "11.4", ".199", ".491", ".282", ".157", ".068", "Model", "PPL", "F1", "Freq", "Med", "Rare", "Rarest", "Token", "frequency", "classes"], "regionBoundary": {"x2": 524.0, "y1": 62.8900146484375, "x1": 309.0, "y2": 224.8900146484375}, "caption": "Table 9: Unlikelihood loss applied to vocabulary distributions. Stronger \u03b1 terms greatly shift probability mass from the most Frequent words to Medium and Rare words, at a small cost to PPL and F1. Frequent, medium, rare and rarest token classes are defined as the sets of tokens whose cumulative masses account for the top 40%, the next 30%, the next 20% and final 10% of tokens empirically generated by humans, respectively. Nucleus sampling can also produce a distribution close to human with parameter p close to 1, but with larger losses in F1.", "page": 10}, {"figType": "Figure", "name": "6", "captionBoundary": {"x2": 291.6719970703125, "y1": 192.02957153320312, "x1": 79.0999984741211, "y2": 198.03204345703125}, "imageText": [], "regionBoundary": {"x2": 288.0, "y1": 61.8900146484375, "x1": 72.0, "y2": 179.8900146484375}, "caption": "Figure 6: Dialogue NLI from (Welleck et al., 2019b).", "page": 6}, {"figType": "Table", "name": "5", "captionBoundary": {"x2": 290.2705993652344, "y1": 306.7425231933594, "x1": 72.0, "y2": 324.70001220703125}, "imageText": ["Entailment", "95k", "4613", "4959", "Triple-Entailment", "105k", "5285", "5481", "Neutral", "110k", "5500", "5700", "Negatives", "110k", "5500", "5700", "Train", "Test", "Valid"], "regionBoundary": {"x2": 278.0, "y1": 214.8900146484375, "x1": 84.0, "y2": 294.8900146484375}, "caption": "Table 5: Dialogue NLI two utterance generation task dataset statistics.", "page": 6}, {"figType": "Figure", "name": "10", "captionBoundary": {"x2": 525.54296875, "y1": 667.3665771484375, "x1": 71.99993896484375, "y2": 685.3240356445312}, "imageText": ["MLE", "Baseline", "Unlikelihood", "Repetition", "(ELI5)", "Vocabulary", "(ConvAI2)", "ag", "e", "ce", "nt", "P", "er", "ni", "ng", "W", "in", "100%", "75%", "50%", "25%", "\u03b1", "=", "100", "\u03b1", "=", "101", "\u03b1", "=", "102", "\u03b1", "=", "103", "0%"], "regionBoundary": {"x2": 410.0, "y1": 510.570556640625, "x1": 190.80633544921875, "y2": 650.2322998046875}, "caption": "Figure 10: Complete Human Evaluation results. Human evaluators do not significantly prefer the \u03b1 = 100 and \u03b1 = 103 models over the baseline model.", "page": 13}, {"figType": "Figure", "name": "9", "captionBoundary": {"x2": 397.1731872558594, "y1": 340.5745544433594, "x1": 200.3719940185547, "y2": 346.5770263671875}, "imageText": [], "regionBoundary": {"x2": 526.0, "y1": 135.8900146484375, "x1": 72.0, "y2": 328.8900146484375}, "caption": "Figure 9: Screenshot of the Human Evaluator UI.", "page": 13}, {"figType": "Figure", "name": "8", "captionBoundary": {"x2": 525.5473022460938, "y1": 639.4425659179688, "x1": 72.0, "y2": 669.35498046875}, "imageText": [], "regionBoundary": {"x2": 526.0, "y1": 151.8900146484375, "x1": 72.0, "y2": 627.8900146484375}, "caption": "Figure 8: Examples of model-human conversations collected during human evaluation of the vocab unlikelihood models. Human utterances are in blue bubbles, model utterances are in white. Conversations (a) and (b) are from the baseline. Conversations (c) and (d) are from the \u03b1 = 102 model and more frequently employ rarer words.", "page": 12}, {"figType": "Table", "name": "6", "captionBoundary": {"x2": 525.5471801757812, "y1": 132.40151977539062, "x1": 72.0, "y2": 222.090087890625}, "imageText": ["MLE", "Baseline", "72%", "41%", "18%", "8.54", "17.5", "36.7", "12.5", "11.4", "UL", "(Dialogue", "NLI)", "96%", "85%", "78%", "9.1", "26.6", "39.4", "248.9", "11.9", "Data", "+", "Model", "Entail", "Tr.-E", "Neutral", "Entail", "Tr.-E", "Neutral", "Contradict", "ConvAI2", "Selection", "Accuracy", "Perplexity"], "regionBoundary": {"x2": 489.0, "y1": 62.8900146484375, "x1": 106.0, "y2": 119.8900146484375}, "caption": "Table 6: Test evaluation on the Dialogue NLI two utterance generation task, comparing standard likelihood (MLE) models trained on pushshift.io Reddit and ConvAI2 with unlikelihood loss NLI training. Results are broken down according to whether the premise and positive candidate are entailing, triple-entailing, or neutral (Entail, Tr.-E, Neutral). Selection Accuracy measures how often the model assigns lower perplexity to the positive candidate than to the negative candidate in the pair. Top two rows: for standard maximum likelihood models, the perplexity of contradicting utterances is lower compared to neutral or triple-entailing utterances (albeit higher compared to entailing utterances), showing partial failure at the coherence task. Bottom row: NLI Unlikelihood training yields large improvements on all coherence metrics, while minimally increasing overall perplexity.", "page": 7}, {"figType": "Table", "name": "7", "captionBoundary": {"x2": 525.5471801757812, "y1": 307.2245178222656, "x1": 72.0, "y2": 337.1369934082031}, "imageText": ["MLE", "Baseline", "66.5%", "36.8%", "23.3", "45.1", "35.9", "11.4", "UL", "(Dialogue", "NLI)", "89.0%", "69.8%", "21.5", "40.3", "63.5", "11.8", "Data", "+", "Model", "Triple-Entail", "Neutral", "Triple-Entail", "Neutral", "Contradict", "ConvAI2", "Selection", "Accuracy", "(vs.", "Neg)", "Perplexity"], "regionBoundary": {"x2": 494.0, "y1": 237.8900146484375, "x1": 101.0, "y2": 294.8900146484375}, "caption": "Table 7: Test evaluation on the Full Dialogue NLI generation task. NLI unlikelihood training improves coherence metrics compared to likelihood (MLE) training. For UL, the triple-entailing or neutral candidates are assigned relatively lower perplexity compared to contradicting candidates, with higher selection accuracy for coherent labels.", "page": 7}, {"figType": "Figure", "name": "7", "captionBoundary": {"x2": 525.539306640625, "y1": 487.7425537109375, "x1": 72.0, "y2": 517.656005859375}, "imageText": ["(C)", "We", "did", "too", "but", "working", "in", "real", "estate", "for", "\ufb01fteen", "years", "sucked", "up", "a", "lot", "of", "time.", "3.1", "17.6", "We", "did", "too", "but", "working", "in", "real", "estate", "for", "12", "years", ".", "(E)", "I", "have", "been", "working", "as", "a", "real", "estate", "sucked", "up", "a", "lot", "of", "time", "agent", "for", "the", "past", "12", "years.", "3.9", "3.8", "Yes,", "I", "love", "watching", "baseball", "and", "basketball.", "I", "do", "like", "(E)", "I", "love", "running.", "26.2", "3.1", "running", "though.", "(C)", "I", "despise", "running.", "42.8", "247.1", "Yes,", "I", "love", "watching", "baseball", "and", "basketball.", "I", "do", "not", "(C)", "I", "love", "running.", "25.5", "226.9", "like", "running", "though.", "(E)", "I", "despise", "running.", "29.9", "9.4", "LMLE", "LUL", "Premise", "Hypothesis", "PPL", "PPL"], "regionBoundary": {"x2": 512.0, "y1": 352.8900146484375, "x1": 84.0, "y2": 475.8900146484375}, "caption": "Figure 7: Example perplexities of a baseline maximum likelihood model (LMLE) and our unlikelihood trained model (LUL ) when generating the provided hypotheses, given the premise. The maximum likelihood trained model assigns high probability (low perplexity) to contradictory generations, while unlikelihood does not.", "page": 7}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 525.5465087890625, "y1": 180.02554321289062, "x1": 307.2760009765625, "y2": 245.8040771484375}, "imageText": ["UL", "(Context", "only)", "11.8", ".194", ".0330", ".0069", "UL", "(Label", "only)", "11.4", ".203", ".0984", ".0005", "UL", "(Context", "&", "Label)", "11.9", ".193", ".0352", ".0023", "Human", "-", "-", ".0223", ".0004", "MLE", "Baseline", "11.4", ".199", ".1131", ".0210", "Model", "PPL", "F1", "Context", "Label", "Repetition"], "regionBoundary": {"x2": 524.0, "y1": 62.8900146484375, "x1": 309.0, "y2": 167.8900146484375}, "caption": "Table 1: Evaluation on the ConvAI2 task valid set (test set is hidden), comparing standard likelihood (MLE) with context and label repetition unlikelihood loss training. The repetition types can be decreased depending on which type of unlikelihood loss is used, with minimal changes in perplexity and F1.", "page": 3}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 525.5465087890625, "y1": 377.633544921875, "x1": 307.2760009765625, "y2": 431.45697021484375}, "imageText": ["UL", "(Context", "only)", "8.8", ".346", ".229", ".037", "UL", "(Label", "only)", "8.3", ".371", ".426", ".001", "UL", "(Context", "+", "Label)", "8.5", ".358", ".313", ".009", "Human", "-", "-", ".160", ".001", "MLE", "Baseline", "8.3", ".368", ".441", ".014", "Model", "PPL", "F1", "Context", "Label", "Repetition"], "regionBoundary": {"x2": 518.0, "y1": 264.8900146484375, "x1": 315.0, "y2": 365.8900146484375}, "caption": "Table 2: Evaluation on the Wizard of Wikipedia test set, comparing standard likelihood (MLE) with context and label repetition unlikelihood loss training. The repetition types can be decreased depending on the type of unlikelihood loss used, while minimally impacting F1.", "page": 3}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 525.5465087890625, "y1": 403.820556640625, "x1": 307.2760009765625, "y2": 421.77801513671875}, "imageText": ["1.00", "0.75", "0.50", "0.25", "0.00", "ve", "l", "an", "le", "Hu", "m", "PP", "L", "22.8", "22.6", "22.4", "22.2", "22.0", "21.8", "21.6", "21.4", "0.00", "0.01", "0.02", "0.03", "ELI5", "Context", "Repeats"], "regionBoundary": {"x2": 523.0, "y1": 255.04310607910156, "x1": 312.797607421875, "y2": 387.231201171875}, "caption": "Figure 3: ELI5: Perplexity vs. context repeats as a function of \u03b1 in the context unlikelihood objective.", "page": 4}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 290.2706604003906, "y1": 178.60751342773438, "x1": 72.0, "y2": 232.4310302734375}, "imageText": ["UL", "(Context", "only)", "21.4", ".163", ".008", ".322", "UL", "(Label", "only)", "21.4", ".183", ".015", ".055", "UL", "(Context", "+", "Label)", "21.8", ".184", ".009", ".078", "Human", "-", "-", ".009", ".010", "MLE", "Baseline", "21.0", ".130", ".033", ".617", "Model", "PPL", "F1", "Context", "Label", "Repetition"], "regionBoundary": {"x2": 283.0, "y1": 62.8900146484375, "x1": 79.0, "y2": 166.8900146484375}, "caption": "Table 3: Evaluation on the ELI5 task test set, comparing standard likelihood (MLE) with context and label repetition unlikelihood loss training. The repetition types can be decreased depending on which type of unlikelihood loss is used, while improving F1.", "page": 4}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 525.5465087890625, "y1": 215.81253051757812, "x1": 307.2760009765625, "y2": 233.77099609375}, "imageText": ["0.0", "0.5", "1.0", "1.5", "2.0", "2.5", "3.0", "ve", "l", "an", "le", "Hu", "m", "PP", "L", "32", "30", "28", "26", "24", "22", "0.00", "0.02", "0.04", "0.06", "0.08", "0.10", "0.12", "ELI5", "Label", "Repeats"], "regionBoundary": {"x2": 523.0, "y1": 64.64190673828125, "x1": 313.01507568359375, "y2": 199.10003662109375}, "caption": "Figure 2: ELI5: Perplexity vs. label repeats as a function of \u03b1 in the label unlikelihood objective.", "page": 4}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 729.9257066514757, "y1": 437.56048414442273, "x1": 426.772223578559, "y2": 462.5013987223307}, "name": "1", "caption_text": "Figure 1: GPT-2 345M model completions can show lack of coherence, e.g. direct contradictions.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 724.0, "y1": 311.0, "x1": 430.0, "y2": 416.0}, "page": 0, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9257066514757, "y1": 250.0354766845703, "x1": 426.772223578559, "y2": 341.39455159505206}, "name": "1", "caption_text": "Table 1: Evaluation on the ConvAI2 task valid set (test set is hidden), comparing standard likelihood (MLE) with context and label repetition unlikelihood loss training. The repetition types can be decreased depending on which type of unlikelihood loss is used, with minimal changes in perplexity and F1.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 727.0, "y1": 86.0, "x1": 430.0, "y2": 233.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9257066514757, "y1": 524.4910346137152, "x1": 426.772223578559, "y2": 599.2457919650608}, "name": "2", "caption_text": "Table 2: Evaluation on the Wizard of Wikipedia test set, comparing standard likelihood (MLE) with context and label repetition unlikelihood loss training. The repetition types can be decreased depending on the type of unlikelihood loss used, while minimally impacting F1.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 720.0, "y1": 367.0, "x1": 427.0, "y2": 525.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.15369500054254, "y1": 248.0659908718533, "x1": 100.0, "y2": 322.8208753797743}, "name": "3", "caption_text": "Table 3: Evaluation on the ELI5 task test set, comparing standard likelihood (MLE) with context and label repetition unlikelihood loss training. The repetition types can be decreased depending on which type of unlikelihood loss is used, while improving F1.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 393.0, "y1": 86.0, "x1": 110.0, "y2": 231.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9257066514757, "y1": 299.7396257188585, "x1": 426.772223578559, "y2": 324.68193901909723}, "name": "2", "caption_text": "Figure 2: ELI5: Perplexity vs. label repeats as a function of \u03b1 in the label unlikelihood objective.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 726.0, "y1": 91.0, "x1": 431.0, "y2": 278.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9257066514757, "y1": 560.8618842230902, "x1": 426.772223578559, "y2": 585.8027988009983}, "name": "3", "caption_text": "Figure 3: ELI5: Perplexity vs. context repeats as a function of \u03b1 in the context unlikelihood objective.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 726.0, "y1": 354.0, "x1": 431.0, "y2": 540.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.925791422526, "y1": 303.26601664225257, "x1": 426.772223578559, "y2": 361.41671074761285}, "name": "4", "caption_text": "Figure 4: Human evaluation experiments for label unlikelihood on ELI5 (left), and vocabulary unlikelihood on ConvAI2 for two values of \u03b1 (right). Unlikelihood significantly outperforms the MLE baselines.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 726.0, "y1": 90.0, "x1": 431.0, "y2": 282.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.926045735677, "y1": 555.8327145046658, "x1": 426.772223578559, "y2": 680.399915907118}, "name": "4", "caption_text": "Table 4: Unlikelihood loss applied to vocabulary distributions. Stronger \u03b1 terms greatly shift probability mass from the most Frequent words to Medium and Rare words, at a small cost to PPL and F1. Frequent, medium, rare and rarest token classes are defined as the sets of tokens whose cumulative masses account for the top 40%, the next 30%, the next 20% and final 10% of tokens empirically generated by humans, respectively.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 727.0, "y1": 387.0, "x1": 429.0, "y2": 539.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9257066514757, "y1": 913.8299730088976, "x1": 426.7721811930338, "y2": 988.584730360243}, "name": "5", "caption_text": "Figure 5: Vocabulary control with unlikelihood training: more probability mass is transferred from Frequent words to Rare words as we increase the \u03b1weighting parameter. The maximum likelihood baseline is far from the human distribution.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 726.0, "y1": 710.0, "x1": 431.0, "y2": 892.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.09999593098956, "y1": 266.7077382405599, "x1": 109.86110899183485, "y2": 275.04450480143225}, "name": "6", "caption_text": "Figure 6: Dialogue NLI from (Welleck et al., 2019b).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 396.0, "y1": 90.0, "x1": 101.0, "y2": 245.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.1536102294922, "y1": 426.03128221299914, "x1": 100.0, "y2": 450.97223917643225}, "name": "5", "caption_text": "Table 5: Dialogue NLI two utterance generation task dataset statistics.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 399.0, "y1": 297.0, "x1": 100.0, "y2": 426.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9266391330295, "y1": 183.89099968804254, "x1": 100.0, "y2": 308.4584554036458}, "name": "6", "caption_text": "Table 6: Test evaluation on the Dialogue NLI two utterance generation task, comparing standard likelihood (MLE) models trained on pushshift.io Reddit and ConvAI2 with unlikelihood loss NLI training. Results are broken down according to whether the premise and positive candidate are entailing, triple-entailing, or neutral (Entail, Tr.-E, Neutral). Selection Accuracy measures how often the model assigns lower perplexity to the positive candidate than to the negative candidate in the pair. Top two rows: for standard maximum likelihood models, the perplexity of contradicting utterances is lower compared to neutral or triple-entailing utterances (albeit higher compared to entailing utterances), showing partial failure at the coherence task. Bottom row: NLI Unlikelihood training yields large improvements on all coherence metrics, while minimally increasing overall perplexity.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 687.0, "y1": 86.0, "x1": 148.0, "y2": 184.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9266391330295, "y1": 426.7007191975911, "x1": 100.0, "y2": 468.2458241780599}, "name": "7", "caption_text": "Table 7: Test evaluation on the Full Dialogue NLI generation task. NLI unlikelihood training improves coherence metrics compared to likelihood (MLE) training. For UL, the triple-entailing or neutral candidates are assigned relatively lower perplexity compared to contradicting candidates, with higher selection accuracy for coherent labels.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 690.0, "y1": 329.0, "x1": 134.0, "y2": 427.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9157036675347, "y1": 677.4202134874132, "x1": 100.0, "y2": 718.9666748046875}, "name": "7", "caption_text": "Figure 7: Example perplexities of a baseline maximum likelihood model (LMLE) and our unlikelihood trained model (LUL ) when generating the provided hypotheses, given the premise. The maximum likelihood trained model assigns high probability (low perplexity) to contradictory generations, while unlikelihood does not.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 710.0, "y1": 489.0, "x1": 116.0, "y2": 661.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.153567843967, "y1": 244.2813025580512, "x1": 100.0, "y2": 302.43199666341144}, "name": "8", "caption_text": "Table 8: Evaluation on the Wizard of Wikipedia task test set, comparing standard likelihood (MLE) with repetition unlikelihood loss training, where both methods use beam search (beam size of 5).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 399.0, "y1": 86.0, "x1": 100.0, "y2": 244.0}, "page": 10, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.926045735677, "y1": 329.1271209716797, "x1": 426.772223578559, "y2": 503.5083770751953}, "name": "9", "caption_text": "Table 9: Unlikelihood loss applied to vocabulary distributions. Stronger \u03b1 terms greatly shift probability mass from the most Frequent words to Medium and Rare words, at a small cost to PPL and F1. Frequent, medium, rare and rarest token classes are defined as the sets of tokens whose cumulative masses account for the top 40%, the next 30%, the next 20% and final 10% of tokens empirically generated by humans, respectively. Nucleus sampling can also produce a distribution close to human with parameter p close to 1, but with larger losses in F1.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 727.0, "y1": 86.0, "x1": 427.0, "y2": 329.0}, "page": 10, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9268086751301, "y1": 888.1146748860676, "x1": 100.0, "y2": 929.6596950954861}, "name": "8", "caption_text": "Figure 8: Examples of model-human conversations collected during human evaluation of the vocab unlikelihood models. Human utterances are in blue bubbles, model utterances are in white. Conversations (a) and (b) are from the baseline. Conversations (c) and (d) are from the \u03b1 = 102 model and more frequently employ rarer words.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 696.0, "y1": 219.0, "x1": 110.0, "y2": 864.0}, "page": 12, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 551.6294267442491, "y1": 473.0202145046658, "x1": 278.2944361368815, "y2": 481.35698106553815}, "name": "9", "caption_text": "Figure 9: Screenshot of the Human Evaluator UI.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 730.0, "y1": 190.0, "x1": 100.0, "y2": 472.0}, "page": 13, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9207899305555, "y1": 926.8980238172743, "x1": 99.99991522894965, "y2": 951.8389383951823}, "name": "10", "caption_text": "Figure 10: Complete Human Evaluation results. Human evaluators do not significantly prefer the \u03b1 = 100 and \u03b1 = 103 models over the baseline model.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 569.0, "y1": 706.0, "x1": 262.0, "y2": 906.0}, "page": 13, "dpi": 0}], "error": null, "pdf": "/work/host-output/49794a2b7005719058547f079cf111d874cb3cb9/2020.acl-main.428.pdf", "dpi": 100}