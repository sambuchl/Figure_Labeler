{"raw_detected_boxes": [[], [], [{"x2": 402.0, "y1": 74.0, "x1": 101.0, "y2": 211.0}, {"x2": 729.0, "y1": 73.0, "x1": 429.0, "y2": 358.0}, {"x2": 727.0, "y1": 464.0, "x1": 427.0, "y2": 749.0}, {"x2": 394.0, "y1": 800.0, "x1": 109.0, "y2": 851.0}], [{"x2": 726.0, "y1": 59.0, "x1": 103.0, "y2": 267.0}], [{"x2": 394.0, "y1": 105.0, "x1": 101.0, "y2": 354.0}], [], [], []], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Table", "name": "1", "captionBoundary": {"x2": 290.27056884765625, "y1": 635.4895629882812, "x1": 71.69100189208984, "y2": 665.4020385742188}, "imageText": ["BERT", "81.4%", "82.3%", "80.8%", "Face-to-Face", "(OOD)", "Letters", "(OOD)", "Model", "Telephone", "(IID)"], "regionBoundary": {"x2": 291.0, "y1": 576.8900146484375, "x1": 72.0, "y2": 618.8900146484375}, "caption": "Table 1: Accuracy of a BERT Base MNLI model trained on Telephone data and tested on three different distributions. Accuracy only slightly fluctuates.", "page": 2}, {"figType": "Figure", "name": "1", "captionBoundary": {"x2": 290.44000244140625, "y1": 164.35855102539062, "x1": 72.0, "y2": 182.3160400390625}, "imageText": ["Semantic", "Textual", "Similarity", "(STS-B)", "Generalization", "IID", "Data", "(Images)", "OOD", "Data", "(MSRvid)", ")", "n", "(%", "la", "tio", "Co", "rre", "so", "n", "Pe", "ar", "100", "80", "60", "40", "20", "RoBERTa", "0", "Avg.", "BoW", "Avg.", "w2v", "ConvNet", "w2v", "LSTM", "w2v", "BERT", "Base", "BERT", "Large"], "regionBoundary": {"x2": 290.0, "y1": 52.222633361816406, "x1": 74.67975616455078, "y2": 152.22076416015625}, "caption": "Figure 1: Pretrained Transformers often have smaller IID/OOD generalization gaps than previous models.", "page": 2}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 527.2002563476562, "y1": 552.7335815429688, "x1": 307.2760009765625, "y2": 570.6910400390625}, "imageText": [")", "SST-2", "Model", "Size", "vs.", "Accuracy", "Drop", "y", "(%", "ur", "ac", "A", "cc", "IM", "Db", "cy", "-", "cu", "ra", "2", "Ac", "SS", "T-", "10", "8", "6", "4", "2", "rge", "0", "RT", "x", "xla", "AL", "BE", "ge", "RT", "x", "lar", "AL", "BE", "e", "RT", "l", "arg", "AL", "BE", "RT", "b", "ase", "AL", "BE", "arg", "e", "BE", "RT", "l", "ase", "BE", "RT", "b"], "regionBoundary": {"x2": 523.3062744140625, "y1": 329.8349609375, "x1": 311.7263488769531, "y2": 539.3587036132812}, "caption": "Figure 3: The IID/OOD generalization gap is not improved with larger models, unlike in computer vision.", "page": 2}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 527.2003173828125, "y1": 270.9445495605469, "x1": 307.2760009765625, "y2": 312.81304931640625}, "imageText": ["ReCoRD", "Reading", "Comprehension", "Generalization", "IID", "Data", "(CNN)", "OOD", "Data", "(Daily", "Mail)", ")", "ch", "(%", "t", "M", "at", "Ex", "ac", "80", "70", "60", "50", "40", "30", "DocQA", "DistilBERT", "BERT", "Base", "BERT", "Large", "RoBERTa", "20", "IMDb", "Sentiment", "Classifier", "Generalization", "IID", "Data", "(IMDb)", "OOD", "Data", "(SST-2)", "(%", ")", "ra", "cy", "Ac", "cu", "100", "90", "80", "70", "RoBERTa", "60", "Avg.", "BoW", "Avg.", "w2v", "ConvNet", "w2v", "LSTM", "w2v", "BERT", "Base", "BERT", "Large"], "regionBoundary": {"x2": 526.0, "y1": 52.222633361816406, "x1": 309.95574951171875, "y2": 257.7958984375}, "caption": "Figure 2: Generalization results for sentiment analysis and reading comprehension. While IID accuracy does not vary much for IMDb sentiment analysis, OOD accuracy does. Here pretrained Transformers do best.", "page": 2}, {"figType": "Figure", "name": "5", "captionBoundary": {"x2": 292.0106201171875, "y1": 269.2255554199219, "x1": 71.53199768066406, "y2": 346.95904541015625}, "imageText": ["y", "SST", "Classifier", "Confidence", "Distribution", "SST", "(IID)", "WMT16", "(OOD)", "ue", "nc", "Fr", "eq", "0.5", "0.6", "0.7", "0.8", "0.9", "1.0", "Maximum", "Softmax", "Probability", "(Confidence)"], "regionBoundary": {"x2": 289.9817199707031, "y1": 61.873538970947266, "x1": 77.03651428222656, "y2": 255.04913330078125}, "caption": "Figure 5: The confidence distribution for a RoBERTa SST-2 classifier on examples from the SST-2 test set and the English side of WMT16 English-German. The WMT16 histogram is translucent and overlays the SST histogram. The minimum prediction confidence is 0.5. Although RoBERTa is better than previous models at OOD detection, there is clearly room for future work.", "page": 4}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 527.5396728515625, "y1": 204.90652465820312, "x1": 72.0, "y2": 270.68505859375}, "imageText": ["Random", "Detector", "Bag", "of", "Words", "Avg.", "word2vec", "LSTM", "word2vec", "ConvNet", "word2vec", "BERT", "Large", "Detecting", "OOD", "Examples", "for", "an", "SST-2", "Sentiment", "Classifier", "Model", "Type", "te", "r)", "B", "et", "er", "Is", "(L", "ow", "(%", ")", "at", "e", "rm", "R", "A", "la", "Fa", "lse", "100", "80", "60", "40", "20", "20", "NG", "Multi30K", "RTE", "SNLI", "WMT16", "Average", "0"], "regionBoundary": {"x2": 524.0, "y1": 41.996334075927734, "x1": 78.19400024414062, "y2": 191.18426513671875}, "caption": "Figure 4: We feed in OOD examples from out-of-distribution datasets (20 Newsgroups, Multi30K, etc.) to SST-2 sentiment classifiers and report the False Alarm Rate at 95% Recall. A lower False Alarm Rate is better. Classifiers are repurposed as anomaly detectors by using their negative maximum softmax probability as the anomaly score\u2014 OOD examples should be predicted with less confidence than IID examples. Models such as BoW, word2vec averages, and LSTMs are near random chance; that is, previous NLP models are frequently more confident when classifying OOD examples than when classifying IID test examples.", "page": 3}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 403.3888922797309, "y1": 228.2757653130425, "x1": 100.0, "y2": 253.21672227647568}, "name": "1", "caption_text": "Figure 1: Pretrained Transformers often have smaller IID/OOD generalization gaps than previous models.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 403.0, "y1": 73.0, "x1": 100.0, "y2": 228.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.222663031684, "y1": 376.31187438964844, "x1": 426.772223578559, "y2": 434.46256849500867}, "name": "2", "caption_text": "Figure 2: Generalization results for sentiment analysis and reading comprehension. While IID accuracy does not vary much for IMDb sentiment analysis, OOD accuracy does. Here pretrained Transformers do best.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 730.0, "y1": 73.0, "x1": 427.0, "y2": 375.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2225782606337, "y1": 767.6855299207899, "x1": 426.772223578559, "y2": 792.6264444986979}, "name": "3", "caption_text": "Figure 3: The IID/OOD generalization gap is not improved with larger models, unlike in computer vision.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 727.0, "y1": 458.0, "x1": 427.0, "y2": 749.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.153567843967, "y1": 882.6243930392795, "x1": 99.57083596123589, "y2": 924.1694980197482}, "name": "1", "caption_text": "Table 1: Accuracy of a BERT Base MNLI model trained on Telephone data and tested on three different distributions. Accuracy only slightly fluctuates.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 404.0, "y1": 800.0, "x1": 100.0, "y2": 859.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.6939900716145, "y1": 284.5923953586154, "x1": 100.0, "y2": 375.95147026909723}, "name": "4", "caption_text": "Figure 4: We feed in OOD examples from out-of-distribution datasets (20 Newsgroups, Multi30K, etc.) to SST-2 sentiment classifiers and report the False Alarm Rate at 95% Recall. A lower False Alarm Rate is better. Classifiers are repurposed as anomaly detectors by using their negative maximum softmax probability as the anomaly score\u2014 OOD examples should be predicted with less confidence than IID examples. Models such as BoW, word2vec averages, and LSTMs are near random chance; that is, previous NLP models are frequently more confident when classifying OOD examples than when classifying IID test examples.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 728.0, "y1": 59.0, "x1": 100.0, "y2": 284.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.570305718316, "y1": 373.92438252766925, "x1": 99.34999677870009, "y2": 481.88756306966144}, "name": "5", "caption_text": "Figure 5: The confidence distribution for a RoBERTa SST-2 classifier on examples from the SST-2 test set and the English side of WMT16 English-German. The WMT16 histogram is translucent and overlays the SST histogram. The minimum prediction confidence is 0.5. Although RoBERTa is better than previous models at OOD detection, there is clearly room for future work.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 402.0, "y1": 88.0, "x1": 101.0, "y2": 357.0}, "page": 4, "dpi": 0}], "error": null, "pdf": "/work/host-output/6bb51447dd634a6e822cd77683a1662f163c3002/2020.acl-main.244.pdf", "dpi": 100}