{"raw_detected_boxes": [[], [{"x2": 396.0, "y1": 95.0, "x1": 100.0, "y2": 305.0}], [{"x2": 714.0, "y1": 258.0, "x1": 443.0, "y2": 364.0}, {"x2": 714.0, "y1": 623.0, "x1": 442.0, "y2": 827.0}], [], [], [{"x2": 389.0, "y1": 86.0, "x1": 116.0, "y2": 208.0}], [{"x2": 693.0, "y1": 92.0, "x1": 140.0, "y2": 287.0}, {"x2": 706.0, "y1": 412.0, "x1": 454.0, "y2": 538.0}], [{"x2": 400.0, "y1": 87.0, "x1": 101.0, "y2": 219.0}, {"x2": 709.0, "y1": 773.0, "x1": 449.0, "y2": 953.0}], [{"x2": 388.0, "y1": 88.0, "x1": 114.0, "y2": 141.0}], [], [{"x2": 380.0, "y1": 389.0, "x1": 116.0, "y2": 596.0}], [{"x2": 659.0, "y1": 817.0, "x1": 175.0, "y2": 987.0}]], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Table", "name": "2", "captionBoundary": {"x2": 291.9244079589844, "y1": 167.81253051757812, "x1": 71.69100189208984, "y2": 233.591064453125}, "imageText": ["Model", "/", "Dataset", "CoQA", "MARCO", "BoolQ", "BERT-MLP", "78.0", "70.8", "71.6", "BERT-HA", "78.8", "71.3", "72.9", "BERT-HA+RL", "79.3", "70.3", "70.4", "BERT-HA+Rule", "78.1", "70.4", "73.8", "BERT-HA+STM", "80.5\u2020", "72.3\u2021", "75.2\u2020", "BERT-HA+Gold", "82.0", "N/A", "N/A"], "regionBoundary": {"x2": 282.0, "y1": 62.8900146484375, "x1": 83.0, "y2": 155.8900146484375}, "caption": "Table 2: Classification accuracy on three Yes/No question answering datasets. N/A means there is no golden evidence label. Significance tests were conducted between BERT-HA+STM and the best baseline of each column (t-test). \u2021 means p-value < 0.01, and \u2020 means p-value < 0.05.", "page": 5}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 290.2705993652344, "y1": 449.03253173828125, "x1": 72.0, "y2": 466.989990234375}, "imageText": [], "regionBoundary": {"x2": 280.0, "y1": 275.8900146484375, "x1": 82.0, "y2": 436.8900146484375}, "caption": "Figure 4: Weight distribution of the two cases from the sentence-level attention.", "page": 10}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 291.9242248535156, "y1": 231.50051879882812, "x1": 71.69100189208984, "y2": 261.41400146484375}, "imageText": ["A:", "Yes", "lady,", "small", "and", "slight,", "holding", "the", "hand", "of", "a", "little", "boy.", "In", "her", "other", "hand,", "she", "holds", "a", "paper", "carrier", "bag.", "...", "A:", "No", "Q:", "Is", "she", "carrying", "something?", "D:", "...On", "the", "step,", "I", "\ufb01nd", "the", "elderly", "Chinese", "wants", "to", "be", "your", "friend.", "If", "you", "want", "to", "be", "her", "friend,", "...", "Q:", "Did", "a", "little", "boy", "write", "the", "note?", "D:", "...This", "note", "is", "from", "a", "little", "girl.", "She"], "regionBoundary": {"x2": 290.0, "y1": 62.8900146484375, "x1": 72.0, "y2": 218.8900146484375}, "caption": "Table 1: Examples of Yes/No question answering. Evidential sentences in bold in reference documents are crucial to answer the questions.", "page": 1}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 527.2003173828125, "y1": 400.7385559082031, "x1": 306.9670104980469, "y2": 454.5619812011719}, "imageText": ["+distant", "supervision", "41.7", "48.7", "+STM", "41.8\u2020", "49.2\u2020", "Model", "EM", "F1", "GA", "(Dhingra", "et", "al.,", "2017a)", "26.4", "26.4", "BiDAF", "(Seo", "et", "al.,", "2017)", "25.9", "28.5", "R3", "(Wang", "et", "al.,", "2018)", "35.3", "41.7", "DSQA", "(Lin", "et", "al.,", "2018)", "40.7", "47.6"], "regionBoundary": {"x2": 508.0, "y1": 295.8900146484375, "x1": 327.0, "y2": 388.8900146484375}, "caption": "Table 4: Experimental results on the test set of QuasarT. R3 is a RL-based method. Results of GA, BiDAF and R3 are copied from (Lin et al., 2018). DSQA+STM outperforms the best baseline (DSQA+DS) significantly (t-test, p-value< 0.05, DS=distant supervision).", "page": 6}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 527.290771484375, "y1": 218.75753784179688, "x1": 71.69100189208984, "y2": 272.5810546875}, "imageText": ["GPT+DPL", "64.2", "62.4", "58.5", "60.2", "70.5", "67.8", "13.3", "57.3", "57.7", "BERT-MLP", "66.2", "65.5", "61.6", "59.5", "71.8", "69.1", "21.2", "63.9", "63.2", "BERT-HA", "67.8", "68.2", "62.6", "60.4", "70.1", "68.1", "19.9", "64.2", "62.8", "BERT-HA+RL", "68.5", "66.9", "62.5", "60.0", "72.1", "69.5", "21.1", "63.1", "63.4", "BERT-HA+Rule", "66.6", "66.4", "61.6", "59.0", "69.5", "66.7", "17.9", "62.5", "63.0", "BERT-HA+STM", "69.3\u2021", "69.2\u2020", "64.7\u2021", "62.6\u2021", "74.0\u2021", "70.9\u2021", "22.0\u2020", "65.3\u2021", "65.8\u2020", "BERT-HA+Gold", "N/A", "N/A", "N/A", "N/A", "73.7", "70.9", "27.2", "N/A", "N/A", "Dev", "Test", "Dev", "Test", "Dev", "Dev", "Test", "Acc", "Acc", "Acc", "Acc", "F1m", "F1a", "EM0", "Acc", "Acc", "Model", "/", "Dataset", "RACE-M", "RACE-H", "MultiRC", "DREAM"], "regionBoundary": {"x2": 501.0, "y1": 62.8900146484375, "x1": 96.0, "y2": 206.8900146484375}, "caption": "Table 3: Results on three multiple-choice reading comprehension datasets. (F1a: F1 score on all answer-options; F1m: macro-average F1 score of all questions; EM0: exact match.) Note that there is no golden evidence label on RACE and DREAM. The results for DPL (deep programming logic) are copied from (Wang et al., 2019). Significance tests were conducted between BERT-HA+STM and the best baseline of each column (t-test). \u2021means p-value < 0.01, and \u2020 means p-value < 0.05.", "page": 6}, {"figType": "Figure", "name": "1", "captionBoundary": {"x2": 526.783935546875, "y1": 275.2305603027344, "x1": 307.2760009765625, "y2": 352.96405029296875}, "imageText": ["\u2461\u2460", "\u2462", "\u2460", "labeling", "training", "\ud835\udc73", "moving", "\ud835\udc7c", "Base", "Model", "Selector"], "regionBoundary": {"x2": 514.0, "y1": 182.8900146484375, "x1": 320.0, "y2": 260.8900146484375}, "caption": "Figure 1: Overview of Self-Training MRC (STM). The base model is trained on both L and U . After training, the base model is used to generate evidence labels for the data from U , and then Selector chooses the most confident samples, which will be used to supervise the evidence extractor at the next iteration. The selected data is moved from U to L at each iteration.", "page": 2}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 527.2003784179688, "y1": 610.9696044921875, "x1": 307.2760009765625, "y2": 664.7919921875}, "imageText": ["EncoderQ", "\ud835\udc89\ud835\udfcf", "\ud835\udc6b", "\u2026", "\ud835\udc89\ud835\udc8e", "\ud835\udc6b\ud835\udc89\ud835\udfd0", "\ud835\udc6b", "\ud835\udc89", "\ud835\udc78", "\ud835\udc89\ud835\udc6b", "\ud835\udc68\ud835\udc8f\ud835\udc94\ud835\udc98\ud835\udc86\ud835\udc93", "\ud835\udc6b\ud835\udc90\ud835\udc84\ud835\udc96\ud835\udc8e\ud835\udc86\ud835\udc8f\ud835\udc95", "\ud835\udc78\ud835\udc96\ud835\udc86\ud835\udc94\ud835\udc95\ud835\udc8a\ud835\udc90\ud835\udc8f", "Predictor", "Evidence", "Extractor", "EncoderD"], "regionBoundary": {"x2": 514.0, "y1": 451.2400817871094, "x1": 319.0, "y2": 596.266845703125}, "caption": "Figure 2: Overall structure of a base model that consists of an encoder layer, an evidence extractor, and an answer predictor. The encoders will obtain hQ for the question, and hDi for each sentence in a document. The summary vector hD will be used to predict the answer.", "page": 2}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 527.2003173828125, "y1": 705.0595703125, "x1": 307.0270080566406, "y2": 746.927001953125}, "imageText": [], "regionBoundary": {"x2": 515.0, "y1": 551.8900146484375, "x1": 318.0, "y2": 692.8900146484375}, "caption": "Figure 3: Evolution of evidence predictions on the development set of CoQA. From the inside to the outside, the four rings correspond to BERT-HA (iteration 0) and BERT-HA+STM (iteration 1, 2, 3), respectively.", "page": 7}, {"figType": "Table", "name": "5", "captionBoundary": {"x2": 291.9242248535156, "y1": 172.52255249023438, "x1": 71.69100189208984, "y2": 214.39007568359375}, "imageText": ["+RL", "5.2", "10.5", "22.3", "32.9", "24.0", "25.3", "24.7", "+Rule", "38.4", "32.4", "53.6", "65.1", "71.8", "59.6", "48.7", "+STM", "(iter", "1)", "32.7", "32.8", "57.1", "70.1", "72.2", "63.3", "52.5", "+STM", "(iter", "2)", "37.3", "32.9", "58.0", "71.3", "72.7", "64.4", "53.5", "+STM", "(iter", "3)", "39.9", "31.4", "55.3", "68.8", "69.5", "61.6", "51.6", "BERT-HA+Gold", "53.6", "33.7", "59.5", "73.4", "74.5", "65.9", "54.8", "Model/Dataset", "CoQA", "MultiRCP@1", "R@1", "R@2", "R@3", "P@1", "P@2", "P@3", "BERT-HA", "20.0", "28.2", "49.8", "62.5", "62.3", "55.2", "46.6"], "regionBoundary": {"x2": 290.0, "y1": 62.8900146484375, "x1": 73.0, "y2": 159.8900146484375}, "caption": "Table 5: Evidence extraction evaluation on the development sets of CoQA and MultiRC. P@k / R@k represent precision / recall of the generated evidence labels, respectively for top k predicted evidence sentences.", "page": 7}, {"figType": "Table", "name": "7", "captionBoundary": {"x2": 525.547119140625, "y1": 515.0255737304688, "x1": 71.69100189208984, "y2": 532.9830322265625}, "imageText": ["(Case", "1)", "Passage:", "...(3)\u201cWhy", "don\u2019t", "you", "tackle", "Indian", "River,", "Daylight?\u201d", "(4)Harper", "advised,", "at", "parting.", "(5)There\u2019s", "whole", "slathers", "of", "creeks", "and", "draws", "draining", "in", "up", "there,", "and", "somewhere", "gold", "just", "crying", "to", "be", "found.", "(6)That\u2019s", "my", "hunch.", "(7)There\u2019s", "a", "big", "strike", "coming,", "and", "Indian", "River", "ain\u2019t", "going", "to", "be", "a", "million", "miles", "away.", "(8)\u201cAnd", "the", "place", "is", "swarming", "with", "moose,\u201d", "Joe", "Ladue", "added.", "(9)\u201cBob", "Henderson\u2019s", "up", "there", "somewhere,", "been", "there", "three", "years", "now,", "swearing", "something", "big", "is", "going", "to", "happen,", "living", "off\u2019n", "straight", "moose", "and", "prospecting", "around", "like", "a", "crazy", "man.\u201d", "(10)Daylight", "decided", "to", "go", "Indian", "River", "a", "\ufb02utter,", "as", "he", "expressed", "it;", "but", "Elijah", "could", "not", "be", "persuaded", "into", "accompanying", "him.", "Elijah\u2019s", "soul", "had", "been", "seared", "by", "famine,", "and", "he", "was", "obsessed", "by", "fear", "of", "repeating", "the", "experience.", "(11)\u201cI", "jest", "can\u2019t", "bear", "to", "separate", "from", "grub,\u201d", "he", "explained.", "(12)\u201cI", "know", "it\u2019s", "downright", "foolishness,", "but", "I", "jest", "can\u2019t", "help", "it...\u201d", "Question:", "Are", "there", "many", "bodies", "of", "water", "there?", "Answer:", "No", "(Case", "2)", "Passage:", "(1)If", "you", "live", "in", "the", "United", "States,", "you", "can\u2019t", "have", "a", "full-time", "job", "until", "you", "are", "16", "years", "old.", "(2)At", "14", "or", "15,", "you", "work", "part-time", "after", "school", "or", "on", "weekends,", "and", "during", "summer", "vacation", "you", "can", "work", "40", "hours", "each", "week.", "(3)Does", "all", "that", "mean", "that", "if", "you", "are", "younger", "than", "14,", "you", "can\u2019t", "make", "your", "own", "money?", "(4)Of", "course", "not!", "(5)Kids", "from", "10-13", "years", "of", "age", "can", "make", "money", "by", "doing", "lots", "of", "things.", "(6)Valerie,", "11,", "told", "us", "that", "she", "made", "money", "by", "cleaning", "up", "other", "people\u2019s", "yards.", "...(11)Kids", "can", "learn", "lots", "of", "things", "from", "making", "money.", "(12)By", "working", "to", "make", "your", "own", "money,", "you", "are", "learning", "the", "skills", "you", "will", "need", "in", "life.", "(13)These", "skills", "can", "include", "things", "like", "how", "to", "get", "along", "with", "others,", "how", "to", "use", "technology", "and", "how", "to", "use", "your", "time", "wisely.", "(14)Some", "people", "think", "that", "asking", "for", "money", "is", "a", "lot", "easier", "than", "making", "it;", "however,", "if", "you", "can", "make", "your", "own", "money,", "you", "don\u2019t", "have", "to", "depend", "on", "anyone", "else...", "Question:", "Can", "they", "learn", "time", "management?", "Answer:", "No"], "regionBoundary": {"x2": 487.0, "y1": 80.8900146484375, "x1": 111.0, "y2": 502.8900146484375}, "caption": "Table 7: Examples from the development set of CoQA. Evidential sentences in red in reference passages are crucial to answer the questions. Sentences in blue are distracting as Figure 4 shows.", "page": 11}, {"figType": "Table", "name": "8", "captionBoundary": {"x2": 527.19384765625, "y1": 722.7535400390625, "x1": 71.69100189208984, "y2": 740.7109985351562}, "imageText": ["Dataset", "RACE-H", "RACE-M", "DREAM", "MultiRC", "CoQA", "MARCO", "BoolQ", "Lmax", "380", "380", "512", "512", "512", "480", "512", "learning", "rate", "5e-5\u2663/4e-5\u2660", "5e-5\u2663/4e-5\u2660", "2e-5", "2e-5", "2e-5", "2e-5", "3e-5", "epoch", "3", "3", "5", "8", "3", "2\u2663/3\u2660", "4", "\u03b7", "0.8", "0.8", "0.8", "0.8", "0.8", "0.8", "0.8", "batch", "size", "32", "32", "32", "32", "6", "8", "6", "0.5", "0.5", "0.5", "0.5", "0.6", "0.5", "0.5", "\u03b4", "0.9", "0.9", "0.8", "0.8", "0.9", "0.9", "0.7", "n", "40000", "10000", "3000", "2000", "1500", "1000", "500", "Kmax", "2", "3", "4", "3", "1", "1", "1"], "regionBoundary": {"x2": 476.0, "y1": 581.8900146484375, "x1": 126.0, "y2": 710.8900146484375}, "caption": "Table 8: Hyper-parameters marked with \u2663/\u2660 are used in BERT-HA/BERT-HA+STM, respectively. Other unmarked hyper-parameters are shared by these two models.", "page": 11}, {"figType": "Table", "name": "6", "captionBoundary": {"x2": 291.9244079589844, "y1": 118.3105239868164, "x1": 71.69100189208984, "y2": 148.2230224609375}, "imageText": ["Model/Metric", "Ans.", "Acc", "Evi.", "Acc", "RoBERTa-HA", "92.6", "13.8", "RoBERTa-HA+STM", "92.7", "19.3(+40%)"], "regionBoundary": {"x2": 283.0, "y1": 62.8900146484375, "x1": 82.0, "y2": 105.8900146484375}, "caption": "Table 6: Answer prediction accuracy (Ans. Acc) and evidence extraction accuracy (Evi. Acc) on the development set of CoQA.", "page": 8}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 405.45031229654944, "y1": 321.5284983317057, "x1": 99.57083596123589, "y2": 363.0750020345052}, "name": "1", "caption_text": "Table 1: Examples of Yes/No question answering. Evidential sentences in bold in reference documents are crucial to answer the questions.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 403.0, "y1": 86.0, "x1": 100.0, "y2": 322.0}, "page": 1, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 731.6443549262152, "y1": 382.26466708713104, "x1": 426.772223578559, "y2": 490.22784762912323}, "name": "1", "caption_text": "Figure 1: Overview of Self-Training MRC (STM). The base model is trained on both L and U . After training, the base model is used to generate evidence labels for the data from U , and then Selector chooses the most confident samples, which will be used to supervise the evidence extractor at the next iteration. The selected data is moved from U to L at each iteration.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 720.0, "y1": 254.0, "x1": 427.0, "y2": 381.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2227478027344, "y1": 848.5688951280382, "x1": 426.772223578559, "y2": 923.3222113715277}, "name": "2", "caption_text": "Figure 2: Overall structure of a base model that consists of an encoder layer, an evidence extractor, and an answer predictor. The encoders will obtain hQ for the question, and hDi for each sentence in a document. The summary vector hD will be used to predict the answer.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 714.0, "y1": 606.0, "x1": 442.0, "y2": 830.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.4505666097005, "y1": 233.07295905219183, "x1": 99.57083596123589, "y2": 324.4320339626736}, "name": "2", "caption_text": "Table 2: Classification accuracy on three Yes/No question answering datasets. N/A means there is no golden evidence label. Significance tests were conducted between BERT-HA+STM and the best baseline of each column (t-test). \u2021 means p-value < 0.01, and \u2020 means p-value < 0.05.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 391.0, "y1": 86.0, "x1": 116.0, "y2": 216.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.3482937282986, "y1": 303.8299136691623, "x1": 99.57083596123589, "y2": 378.5847981770833}, "name": "3", "caption_text": "Table 3: Results on three multiple-choice reading comprehension datasets. (F1a: F1 score on all answer-options; F1m: macro-average F1 score of all questions; EM0: exact match.) Note that there is no golden evidence label on RACE and DREAM. The results for DPL (deep programming logic) are copied from (Wang et al., 2019). Significance tests were conducted between BERT-HA+STM and the best baseline of each column (t-test). \u2021means p-value < 0.01, and \u2020 means p-value < 0.05.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 707.0, "y1": 86.0, "x1": 134.0, "y2": 304.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.222663031684, "y1": 556.5813276502821, "x1": 426.3430701361762, "y2": 631.3360850016276}, "name": "4", "caption_text": "Table 4: Experimental results on the test set of QuasarT. R3 is a RL-based method. Results of GA, BiDAF and R3 are copied from (Lin et al., 2018). DSQA+STM outperforms the best baseline (DSQA+DS) significantly (t-test, p-value< 0.05, DS=distant supervision).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 706.0, "y1": 410.0, "x1": 454.0, "y2": 540.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.45031229654944, "y1": 239.61465623643662, "x1": 99.57083596123589, "y2": 297.76399400499133}, "name": "5", "caption_text": "Table 5: Evidence extraction evaluation on the development sets of CoQA and MultiRC. P@k / R@k represent precision / recall of the generated evidence labels, respectively for top k predicted evidence sentences.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 402.0, "y1": 86.0, "x1": 101.0, "y2": 223.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.222663031684, "y1": 979.2494032118055, "x1": 426.42640007866754, "y2": 1037.3986138237847}, "name": "3", "caption_text": "Figure 3: Evolution of evidence predictions on the development set of CoQA. From the inside to the outside, the four rings correspond to BERT-HA (iteration 0) and BERT-HA+STM (iteration 1, 2, 3), respectively.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 709.0, "y1": 773.0, "x1": 449.0, "y2": 953.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.4505666097005, "y1": 164.32017220391168, "x1": 99.57083596123589, "y2": 205.8653089735243}, "name": "6", "caption_text": "Table 6: Answer prediction accuracy (Ans. Acc) and evidence extraction accuracy (Evi. Acc) on the development set of CoQA.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 392.0, "y1": 86.0, "x1": 114.0, "y2": 147.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.1536102294922, "y1": 623.6562940809462, "x1": 100.0, "y2": 648.5972086588541}, "name": "4", "caption_text": "Figure 4: Weight distribution of the two cases from the sentence-level attention.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 384.0, "y1": 389.0, "x1": 116.0, "y2": 596.0}, "page": 10, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2136773003472, "y1": 1003.8243611653645, "x1": 99.57083596123589, "y2": 1028.7652757432725}, "name": "8", "caption_text": "Table 8: Hyper-parameters marked with \u2663/\u2660 are used in BERT-HA/BERT-HA+STM, respectively. Other unmarked hyper-parameters are shared by these two models.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 661.0, "y1": 807.0, "x1": 159.0, "y2": 1004.0}, "page": 11, "dpi": 0}], "error": null, "pdf": "/work/host-output/6c6b4433d32ae1511ff09ce307c80059dd95b27a/2020.acl-main.361.pdf", "dpi": 100}