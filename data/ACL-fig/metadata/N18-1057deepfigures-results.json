{"raw_detected_boxes": [[{"x2": 707.0, "y1": 321.0, "x1": 446.0, "y2": 538.0}], [], [{"x2": 338.0, "y1": 95.0, "x1": 165.0, "y2": 359.0}], [{"x2": 687.0, "y1": 87.0, "x1": 140.0, "y2": 315.0}], [{"x2": 403.0, "y1": 88.0, "x1": 100.0, "y2": 234.0}], [{"x2": 714.0, "y1": 92.0, "x1": 117.0, "y2": 391.0}], [{"x2": 383.0, "y1": 95.0, "x1": 118.0, "y2": 382.0}, {"x2": 699.0, "y1": 89.0, "x1": 457.0, "y2": 262.0}, {"x2": 720.0, "y1": 383.0, "x1": 436.0, "y2": 531.0}], [{"x2": 649.0, "y1": 93.0, "x1": 178.0, "y2": 405.0}, {"x2": 385.0, "y1": 484.0, "x1": 103.0, "y2": 616.0}], [], []], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Figure", "name": "1", "captionBoundary": {"x2": 525.5474853515625, "y1": 395.134521484375, "x1": 307.2770080566406, "y2": 437.0029602050781}, "imageText": ["\"louiseana\"", "\"conferance\"", "\"Louisiana\"", "\"conference\"", "Noise", "(Train)", "1.", "Train", "noising", "model", "3.", "Train/decode", "denoising", "model", "\"New", "Orleans\"", "\"NLP\"", "\"new", "Orleens\"", "\"nlp\"", "2.", "Synthesize", "data", "\"new", "Orleens\"", "\"nlp\"", "\"New", "Orleans\"", "\"NLP\"", "Denoise", "Noise", "(Decode)"], "regionBoundary": {"x2": 517.0, "y1": 222.8900146484375, "x1": 313.0, "y2": 386.8900146484375}, "caption": "Figure 1: Overview of method. We first train a noise model on a seed corpus, then apply noise during decoding to synthesize data that is in turn used to train the denoising model.", "page": 0}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 525.5481567382812, "y1": 292.8325500488281, "x1": 72.0009994506836, "y2": 322.74603271484375}, "imageText": ["Yuan", "and", "Briscoe", "(2016)", "\u2014", "\u2014", "\u2014", "\u2014", "\u2014", "\u2014", "\u2014", "\u2014", "39.9", "Ji", "et", "al.", "(2017)", "\u2014", "\u2014", "28.6", "\u2014", "\u2014", "33.5", "\u2014", "\u2014", "45.2", "Junczys-Dowmunt", "et", "al.", "(2016)", "\u2014", "\u2014", "\u2014", "\u2014", "\u2014", "\u2014", "61.3", "28.0", "49.5", "Chollampatt", "and", "Ng", "(2018)", "\u2014", "\u2014", "\u2014", "\u2014", "\u2014", "\u2014", "65.5", "33.1", "54.8", "expanded", "64.4", "11.2", "33.0", "54.9", "20.0", "40.7", "57.2", "32.0", "49.4", "none", "50.7", "10.5", "28.7", "48.4", "17.2", "35.5", "52.7", "27.5", "44.5", "clean", "56.1", "9.4", "28.1", "47.5", "16.9", "34.8", "52.3", "27.5", "44.3", "token", "49.7", "11.9", "30.4", "47.7", "18.7", "36.4", "51.4", "30.3", "45.1", "reverse", "53.1", "13.0", "32.8", "50.5", "19.1", "38.0", "54.7", "29.6", "46.8", "rank", "51.3", "12.3", "31.4", "51.0", "18.3", "37.6", "54.3", "29.3", "46.4", "top", "49.1", "17.4", "36.0", "47.7", "23.9", "39.8", "50.9", "34.7", "46.6", "random", "50.0", "17.9", "36.8", "48.9", "23.0", "39.9", "54.2", "35.4", "49.0", "P", "R", "F0.5", "P", "R", "F0.5", "P", "R", "F0.5", "Method", "Dev", "(no", "LM)", "Dev", "Test"], "regionBoundary": {"x2": 514.0, "y1": 65.8900146484375, "x1": 84.0, "y2": 280.8900146484375}, "caption": "Table 2: Results on CoNLL 2013 (Dev) and CoNLL 2014 (Test) sets. All results use the \u201cbase\u201d parallel corpus of 1.3M sentence pairs along with additional synthesized data (totaling 2.3M sentence pairs) except for \u201cexpanded\u201d, which uses 3.3M nonsynthesized sentence pairs (and no synthesized data).", "page": 5}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 525.5474853515625, "y1": 204.50155639648438, "x1": 307.2770080566406, "y2": 258.3250732421875}, "imageText": ["14.8", "33.0", "33.6", "48.0", "42.8", "\ufb01", "ed", "as", "si", "is", "cl", "%", "M", "50", "40", "30", "20", "10", "0", "token", "reverse", "rank", "top", "random", "Noising", "Method"], "regionBoundary": {"x2": 503.0, "y1": 62.8900146484375, "x1": 333.4603271484375, "y2": 186.89703369140625}, "caption": "Figure 4: Percentage of time human evaluators misclassified synthesized noisy sentence Y\u0303 (vs. X) when using each noising scheme, along with 95% confidence intervals. The best we can expect any scheme to do is 50%.", "page": 6}, {"figType": "Figure", "name": "5", "captionBoundary": {"x2": 525.5474853515625, "y1": 399.5435791015625, "x1": 307.2770080566406, "y2": 453.3659973144531}, "imageText": [")", "iz", "ed", "m", "al", "(N", "or", "n", "ce", "is", "ta", "it", "D", "E", "d", "0.15", "0.10", "0.05", "0.00", "none", "clean", "token", "reverse", "rank", "top", "random", "Noising", "Method"], "regionBoundary": {"x2": 518.0, "y1": 270.803955078125, "x1": 318.68450927734375, "y2": 381.88726806640625}, "caption": "Figure 5: Mean edit distance between sentence pairs in X and Y after augmentation with noised sentences. none contains no synthesized examples while clean refers to the baseline of simply appending clean examples (source = target).", "page": 6}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 290.2716369628906, "y1": 292.3465881347656, "x1": 72.0009994506836, "y2": 334.21405029296875}, "imageText": ["Sakaguchi", "et", "al.", "(2017)\u2020", "54.0", "Ji", "et", "al.", "(2017)", "53.4", "Yuan", "and", "Briscoe", "(2016)", "52.1", "Junczys-Dowmunt", "et", "al.", "(2016)", "51.5", "Chollampatt", "and", "Ng", "(2018)", "57.5", "expanded", "72.7", "45.9", "65.1", "56.2", "none", "68.9", "44.2", "62.0", "53.9", "clean", "69.2", "42.8", "61.6", "54.1", "token", "69.2", "47.6", "63.5", "55.9", "reverse", "69.1", "42.1", "61.3", "53.8", "rank", "68.3", "43.3", "61.2", "54.4", "top", "67.3", "48.2", "62.4", "55.5", "random", "69.1", "48.5", "63.7", "56.6", "Scheme", "P", "R", "F0.5", "GLEU"], "regionBoundary": {"x2": 277.0, "y1": 62.8900146484375, "x1": 85.0, "y2": 275.8900146484375}, "caption": "Table 3: Results on the JFLEG test set (we use best hyperparameter settings from CoNLL dev set). GLEU is a variant of BLEU developed for this task; higher is better (Napoles et al., 2017). \u2020Tuned to JFLEG dev set.", "page": 6}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 290.2716064453125, "y1": 268.6605529785156, "x1": 72.0009994506836, "y2": 286.6190185546875}, "imageText": ["Decode", "target", "Input", "source", "Attention", "CNN", "Decoder", "CNN", "Encoder"], "regionBoundary": {"x2": 248.0, "y1": 62.8900146484375, "x1": 112.0, "y2": 266.8900146484375}, "caption": "Figure 2: Model architecture used for both noising and denoising networks.", "page": 2}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 525.5482177734375, "y1": 308.3705749511719, "x1": 72.0009994506836, "y2": 326.32806396484375}, "imageText": ["clean", "But", "at", "the", "beginning", ",", "I", "suffered", "from", "a", "horrible", "pain", "in", "my", "jaw", ".", "1", "But", "at", "the", "\ufb01rst", "time", ",", "I", "suffer", "from", "a", "horrible", "pain", "on", "my", "jaw", ".", "2", "But", "at", "the", "beginning", ",", "I", "suffered", "from", "a", "horrible", "pain", "in", "my", "jaw", "joint", ".", "clean", "There", "is", "one", "child", "who", "is", "15", "years", "old", "and", "a", "mother", "who", "is", "around", "50", ".", "1", "There", "are", "one", "child", "who", "is", "15", "years", "old", "and", "mother", "is", "around", "50", ".", "2", "It", "has", "one", "child", ",", "15", "years", "old", "and", "the", "mother", "who", "is", "around", "50", "years", "old", ".", "clean", "Currently", ",", "I", "\u2019m", "studying", "to", "take", "the", "TOEIC", "exam", "for", "my", "future", "career", ".", "1", "I", "am", "studying", "to", "take", "TOEIC", "exam", "for", "career", "of", "my", "future", ".", "2", "Currently", ",", "I", "will", "have", "take", "TOEIC", "exam", "for", "future", "career", ".", "clean", "After", "I", "practiced", ",", "I", "could", "play", "the", "song", "perfectly", ".", "1", "After", "the", "results", ",", "I", "could", "accomplish", "without", "a", "fault", ".", "2", "When", "I", "tried", "that", ",", "I", "could", "play", "the", "song", "perfectly", ".", "clean", "Thanks", "Giving", "Day", "in", "Korea", "is", "coming", "soon", ".", "1", "In", "Korea", ",", "it", "\u2019s", "coming", "soon", ",", "thanks", "Giving", "day", ".", "2", "Thanks", "Giving", "Day", "in", "korea", "is", "coming", "soon", ".", "clean", "Day", "after", "day", ",", "I", "get", "up", "at", "8", "o\u2018clock", ".", "1", "I", "got", "up", "at", "8", "o\u2018clock", "day", "after", "day", ".", "2", "Day", "after", "day", ",", "I", "get", "up", "8", "o\u2018clock", "in", "the", "week", ".", "Sentence", "1", "or", "2"], "regionBoundary": {"x2": 470.0, "y1": 62.8900146484375, "x1": 128.0, "y2": 291.8900146484375}, "caption": "Table 4: Examples of nonsynthesized and synthesized sentences from validation set. Which example (1 or 2) was synthesized?", "page": 7}, {"figType": "Figure", "name": "6", "captionBoundary": {"x2": 290.2716064453125, "y1": 459.30059814453125, "x1": 72.00101470947266, "y2": 501.16802978515625}, "imageText": ["random", "none", "al", "l", "R", "ec", "0.4", "0.3", "0.2", "0.1", "0.0", "Error", "Type", "Art/Det", "Wci", "Nn", "Prep", "Wform", "Mec", "Vt", "Trans", "Vform", "Rloc-"], "regionBoundary": {"x2": 277.0, "y1": 348.8900146484375, "x1": 75.66023254394531, "y2": 442.1096496582031}, "caption": "Figure 6: Recall vs. error type for the ten most frequent error types in our dev set. Noising improves recall uniformly across error types (See Ng et al. (2014) for a description of error types).", "page": 7}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 525.5481567382812, "y1": 250.80154418945312, "x1": 72.0009994506836, "y2": 292.6690673828125}, "imageText": ["scor", "es", "penal", "t", "y", "he", "you", "i", "s", "i", "s", "you", "up", "he", "you", "=", "+", "=", "+", "you", "up", "he", "you", "ar", "e", "i", "s", "i", "s", "ar", "e", "=", "+", "=", "+", "Expansi", "on", "St", "ep", "Sel", "ect", "i", "on", "St", "ep", "ar", "e", "i", "s", "i", "s", "ar", "e", "What", "How"], "regionBoundary": {"x2": 497.0, "y1": 62.8900146484375, "x1": 101.0, "y2": 233.8900146484375}, "caption": "Figure 3: Illustration of random noising with beam width 2. Darker shading indicates less probable expansions. In this example, greedy decoding would yield \u201cHow are you\u201d. Applying noise penalties, however, results in the hypotheses \u201cHow is you/he\u201d. Note that applying a penalty does not always result in an expansion falling off the beam.", "page": 3}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 257.95721435546875, "y1": 187.53457641601562, "x1": 104.31400299072266, "y2": 193.53704833984375}, "imageText": ["CoNLL", "2014", "60K", "Lang-8", "1.3M", "Lang-8", "expanded", "3.3M", "synthesized", "(NYT", "2007)", "1.0M", "base", "(CoNLL", "+", "L8)", "1.3M", "expanded", "(CoNLL", "+", "L8", "expanded)", "3.3M", "Corpus", "Sent.", "Pairs"], "regionBoundary": {"x2": 297.0, "y1": 63.8900146484375, "x1": 72.0, "y2": 170.8900146484375}, "caption": "Table 1: Summary of training corpora.", "page": 4}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 729.9270629882812, "y1": 548.7979465060764, "x1": 426.77362230088977, "y2": 606.9485558403862}, "name": "1", "caption_text": "Figure 1: Overview of method. We first train a noise model on a seed corpus, then apply noise during decoding to synthesize data that is in turn used to train the denoising model.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 723.0, "y1": 321.0, "x1": 429.0, "y2": 555.0}, "page": 0, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.1550089518229, "y1": 373.13965691460504, "x1": 100.00138812594943, "y2": 398.08197021484375}, "name": "2", "caption_text": "Figure 2: Model architecture used for both noising and denoising networks.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 355.0, "y1": 95.0, "x1": 148.0, "y2": 376.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9279954698351, "y1": 348.3354780409071, "x1": 100.00138812594943, "y2": 406.4848158094618}, "name": "3", "caption_text": "Figure 3: Illustration of random noising with beam width 2. Darker shading indicates less probable expansions. In this example, greedy decoding would yield \u201cHow are you\u201d. Applying noise penalties, however, results in the hypotheses \u201cHow is you/he\u201d. Note that applying a penalty does not always result in an expansion falling off the beam.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 691.0, "y1": 87.0, "x1": 140.0, "y2": 316.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 358.2739088270399, "y1": 260.46468946668836, "x1": 144.880559709337, "y2": 268.80145602756073}, "name": "1", "caption_text": "Table 1: Summary of training corpora.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 413.0, "y1": 88.0, "x1": 100.0, "y2": 237.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9279954698351, "y1": 406.71187506781683, "x1": 100.00138812594943, "y2": 448.2583787706163}, "name": "2", "caption_text": "Table 2: Results on CoNLL 2013 (Dev) and CoNLL 2014 (Test) sets. All results use the \u201cbase\u201d parallel corpus of 1.3M sentence pairs along with additional synthesized data (totaling 2.3M sentence pairs) except for \u201cexpanded\u201d, which uses 3.3M nonsynthesized sentence pairs (and no synthesized data).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 731.0, "y1": 86.0, "x1": 100.0, "y2": 408.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.1550513373481, "y1": 406.03692796495227, "x1": 100.00138812594943, "y2": 464.1861809624566}, "name": "3", "caption_text": "Table 3: Results on the JFLEG test set (we use best hyperparameter settings from CoNLL dev set). GLEU is a variant of BLEU developed for this task; higher is better (Napoles et al., 2017). \u2020Tuned to JFLEG dev set.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 385.0, "y1": 86.0, "x1": 118.0, "y2": 383.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9270629882812, "y1": 284.02993943956164, "x1": 426.77362230088977, "y2": 358.7848239474826}, "name": "4", "caption_text": "Figure 4: Percentage of time human evaluators misclassified synthesized noisy sentence Y\u0303 (vs. X) when using each noising scheme, along with 95% confidence intervals. The best we can expect any scheme to do is 50%.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 699.0, "y1": 87.0, "x1": 457.0, "y2": 262.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9270629882812, "y1": 554.921637641059, "x1": 426.77362230088977, "y2": 629.6749962700737}, "name": "5", "caption_text": "Figure 5: Mean edit distance between sentence pairs in X and Y after augmentation with noised sentences. none contains no synthesized examples while clean refers to the baseline of simply appending clean examples (source = target).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 720.0, "y1": 378.0, "x1": 436.0, "y2": 533.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9280802408854, "y1": 428.29246520996094, "x1": 100.00138812594943, "y2": 453.2334221733941}, "name": "4", "caption_text": "Table 4: Examples of nonsynthesized and synthesized sentences from validation set. Which example (1 or 2) was synthesized?", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 652.0, "y1": 86.0, "x1": 178.0, "y2": 405.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.1550089518229, "y1": 637.9174974229601, "x1": 100.00140931871202, "y2": 696.0667080349392}, "name": "6", "caption_text": "Figure 6: Recall vs. error type for the ten most frequent error types in our dev set. Noising improves recall uniformly across error types (See Ng et al. (2014) for a description of error types).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 385.0, "y1": 484.0, "x1": 102.0, "y2": 616.0}, "page": 7, "dpi": 0}], "error": null, "pdf": "/work/host-output/be70e163473c1c6e42d02b5c4711d0faa493a49b/N18-1057.pdf", "dpi": 100}