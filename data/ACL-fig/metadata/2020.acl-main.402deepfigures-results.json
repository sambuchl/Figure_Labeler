{"raw_detected_boxes": [[], [], [], [{"x2": 693.0, "y1": 86.0, "x1": 137.0, "y2": 185.0}, {"x2": 699.0, "y1": 244.0, "x1": 140.0, "y2": 450.0}, {"x2": 392.0, "y1": 602.0, "x1": 114.0, "y2": 757.0}], [], [{"x2": 722.0, "y1": 96.0, "x1": 102.0, "y2": 463.0}], [{"x2": 731.0, "y1": 88.0, "x1": 426.0, "y2": 164.0}], [{"x2": 717.0, "y1": 91.0, "x1": 110.0, "y2": 250.0}, {"x2": 379.0, "y1": 298.0, "x1": 125.0, "y2": 459.0}, {"x2": 691.0, "y1": 302.0, "x1": 459.0, "y2": 459.0}], [{"x2": 705.0, "y1": 92.0, "x1": 119.0, "y2": 207.0}, {"x2": 403.0, "y1": 256.0, "x1": 106.0, "y2": 377.0}], [], [], []], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Figure", "name": "4", "captionBoundary": {"x2": 525.54736328125, "y1": 354.1185302734375, "x1": 72.0, "y2": 372.07598876953125}, "imageText": [], "regionBoundary": {"x2": 526.0, "y1": 61.8900146484375, "x1": 72.0, "y2": 338.8900146484375}, "caption": "Figure 4: The architectural diagram of the proposed network. SA, IMA, ITA represent self, inter-modal and inter-task attentions, respectively.", "page": 5}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 527.2002563476562, "y1": 106.98954010009766, "x1": 306.9670104980469, "y2": 124.947021484375}, "imageText": ["Train", "7497", "242", "7489", "831", "Test", "1879", "60", "2500", "208", "IEMOCAP", "MELD", "#", "Utterance", "#", "Dialogue", "#", "Utterance", "#", "Dialogue"], "regionBoundary": {"x2": 524.0, "y1": 62.8900146484375, "x1": 309.0, "y2": 103.8900146484375}, "caption": "Table 2: Statistics of the train and test set of the EMOTyDA dataset from different sources", "page": 6}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 290.2703857421875, "y1": 342.4685363769531, "x1": 71.69100189208984, "y2": 360.4259948730469}, "imageText": ["T", "+", "V", "(SA)", "56.76", "49.84", "T", "+", "V", "(IMA)", "56.62", "52.79", "T", "+", "V", "(ITA)", "56.99", "52.23", "T", "+", "V", "(SA", "+", "IMA)", "56.62", "51.70", "T", "+", "V", "(SA", "+", "ITA)", "58.48", "52.62", "T", "+", "V", "(IMA", "+", "ITA)", "57.74", "52.85", "T", "+", "V", "60.88", "57.96", "Hidden-state", "level", "(late", "fusion)", "53.27", "49.80", "Hypothesis", "level", "50.93", "47.31", "Model", "EMOTyDA", "(DA", "+", "ER)Acc.", "F1-score", "Feature", "level", "(early", "fusion)", "51.20", "48.09"], "regionBoundary": {"x2": 273.0, "y1": 214.8900146484375, "x1": 90.0, "y2": 330.8900146484375}, "caption": "Table 4: Results of various baseline models for the multi-task framework for the EMOTyDA dataset", "page": 7}, {"figType": "Figure", "name": "5", "captionBoundary": {"x2": 527.0308227539062, "y1": 335.26654052734375, "x1": 307.2760009765625, "y2": 402.041015625}, "imageText": [], "regionBoundary": {"x2": 501.0, "y1": 214.8900146484375, "x1": 326.0, "y2": 333.8900146484375}, "caption": "Figure 5: The visualization of the attention scores for 5 sample utterances for the tri-modal variant. V, A and T represent attention scores of video, audio and textual features, respectively. Sample utterance - u1:\u201cI am not in the least bit drunk.\u201d, u2: \u201cThere\u2019s a lot of people looking for jobs.\u201d, u3: \u201cIt was ridiculous. Completely ridiculous.\u201d, u4: \u201cYou don\u2019t have to explain.\u201d, u5: \u201cNo, Rachel doesn\u2019t want me to...\u201d", "page": 7}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 525.547119140625, "y1": 183.39352416992188, "x1": 71.69100189208984, "y2": 201.35101318359375}, "imageText": ["T", "+", "A", "65.43", "60.67", "66.98", "62.08", "47.17", "40.30", "49.42", "41.69", "54.12", "50.00", "56.62", "51.99", "A", "+", "V", "38.59", "34.98", "40.07", "36.00", "27.91", "22.76", "28.95", "23.89", "32.09", "28.86", "33.76", "29.13", "T", "+", "V", "67.12", "64.14", "70.55", "68.12", "49.80", "41.90", "51.00", "44.52", "57.31", "53.20", "60.88", "57.96", "T", "+", "A", "+", "V", "66.35", "62.30", "69.45", "67.00", "49.02", "41.00", "50.65", "44.00", "56.77", "52.09", "59.86", "56.05", "T", "+", "V", "(emotional", "cue)", "65.26", "60.20", "-", "-", "46.88", "39.70", "-", "-", "54.31", "50.02", "-", "-", "DA", "DA", "+", "ER", "DA", "DA", "+", "ER", "DA", "DA", "+", "ER", "Modality", "Acc.", "F1-score", "Acc.", "F1-score", "Acc.", "F1-score", "Acc.", "F1-score", "Acc.", "F1-score", "Acc.", "F1-score", "Text", "(T)", "63.75", "60.67", "65.23", "62.35", "46.20", "39.23", "48.90", "41.10", "53.56", "49.17", "53.02", "50.22", "Audio", "(A)", "32.06", "24.95", "35.42", "38.92", "25.76", "19.45", "26.58", "21.01", "27.13", "23.09", "28.65", "24.87", "Video", "(V)", "35.94", "29.71", "36.88", "30.34", "27.23", "20.26", "28.12", "21.03", "30.16", "26.85", "32.09", "27.73", "Dataset", "EMOTyDA:dyadic", "EMOTyDA:multiparty", "EMOTyDA"], "regionBoundary": {"x2": 524.0, "y1": 61.8900146484375, "x1": 73.0, "y2": 180.8900146484375}, "caption": "Table 3: Results of all the baselines and the proposed models in terms of accuracy and F1-score. All the reported results are statistically significant", "page": 7}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 514.373046875, "y1": 145.97451782226562, "x1": 82.86299896240234, "y2": 151.97698974609375}, "imageText": ["Rachel", "Well,", "I", "just", "checked", "our", "messages", "and", "Joshua", "didn\u2019t", "call.", "s", "sad", "M", "1", "That\u2019s", "very", "amusing", "indeed.", "dag", "ang", "Chandler", "Come", "on,", "pick", "up,", "pick", "up", "c", "fear", "Speaker", "Utterance", "DA", "Emotion", "M", "4", "Adders", "don\u2019t", "snap,", "they", "sting.", "dag", "ang"], "regionBoundary": {"x2": 501.0, "y1": 62.8900146484375, "x1": 97.0, "y2": 133.8900146484375}, "caption": "Table 1: Example utterances from the EMOTyDA dataset with its corresponding DA and emotion categories", "page": 3}, {"figType": "Figure", "name": "1", "captionBoundary": {"x2": 506.51318359375, "y1": 281.3935241699219, "x1": 91.03298950195312, "y2": 287.39599609375}, "imageText": ["(a)", "(b)"], "regionBoundary": {"x2": 479.0, "y1": 166.8900146484375, "x1": 115.0, "y2": 276.3380126953125}, "caption": "Figure 1: Statistics across the datasets : (a) Distribution of DA labels, (b) Distribution of emotion labels.", "page": 3}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 290.9381408691406, "y1": 548.1035766601562, "x1": 72.0, "y2": 566.06103515625}, "imageText": ["(a)", "(b)"], "regionBoundary": {"x2": 284.0, "y1": 428.8900146484375, "x1": 76.0, "y2": 543.0479736328125}, "caption": "Figure 3: Statistics : (a) Source across the dataset, (b) Overall speaker distribution.", "page": 3}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 460.69915771484375, "y1": 404.8625183105469, "x1": 136.84498596191406, "y2": 410.864990234375}, "imageText": ["(a)", "(b)"], "regionBoundary": {"x2": 479.0, "y1": 293.8900146484375, "x1": 115.0, "y2": 399.8059997558594}, "caption": "Figure 2: (a) Incongruent modalities in DAC, (b) Importance of emotion in DAC.", "page": 3}, {"figType": "Figure", "name": "6", "captionBoundary": {"x2": 292.03607177734375, "y1": 287.6115417480469, "x1": 71.67100524902344, "y2": 329.48004150390625}, "imageText": [], "regionBoundary": {"x2": 291.0, "y1": 180.8900146484375, "x1": 72.0, "y2": 271.8900146484375}, "caption": "Figure 6: The visualization of the learned weights for an utterance - u1: \u201cOh yes, yes I am, you can\u2019t stop me.\u201d for the best performing model (T+V), single task DAC (baseline) and multi-task DAC+ER (proposed) model", "page": 8}, {"figType": "Table", "name": "5", "captionBoundary": {"x2": 525.5473022460938, "y1": 145.13552856445312, "x1": 71.69100189208984, "y2": 175.04803466796875}, "imageText": ["God,", "I,feel", "so", "guilty", "about", "Ross.", "ap", "ap", "s", "o", "s", "q", "Then", "why", "is", "she", "still", "single?,New", "York", "is", "full", "of", "men.,Why", "hasn\u2019t", "she", "married?", "Probably", "a", "hundred", "people", "told", "her", "she\u2019s", "foolish,", "but", "she\u2019s", "waited.", "She", "is", "not", "Larry\u2019s", "girl", "dag", "dag", "s", "I", "know,", "it", "was", "amazing!", "I", "mean,", "we", "totally", "nailed", "it,", "it", "was", "beautiful.", "ag", "ag", "o", "Utterance", "TrueLabel", "MT(T+V)", "ST", "(T+V)"], "regionBoundary": {"x2": 510.0, "y1": 63.8900146484375, "x1": 82.0, "y2": 142.8900146484375}, "caption": "Table 5: Sample utterances with its predicted labels for the best performing multi-task (MT) (T+V) model and its single task (ST) DAC variants; These examples show that ER as an auxiliary task helps DAC for better performance in MT.", "page": 8}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 714.4070095486111, "y1": 202.7423858642578, "x1": 115.08749855889214, "y2": 211.0791524251302}, "name": "1", "caption_text": "Table 1: Example utterances from the EMOTyDA dataset with its corresponding DA and emotion categories", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 702.0, "y1": 86.0, "x1": 120.0, "y2": 202.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 703.4905327690972, "y1": 390.8243391248915, "x1": 126.43470764160156, "y2": 399.16110568576386}, "name": "1", "caption_text": "Figure 1: Statistics across the datasets : (a) Distribution of DA labels, (b) Distribution of emotion labels.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 703.0, "y1": 234.0, "x1": 126.0, "y2": 467.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 404.08075120713977, "y1": 761.2549675835503, "x1": 100.0, "y2": 786.1958821614583}, "name": "3", "caption_text": "Figure 3: Statistics : (a) Source across the dataset, (b) Overall speaker distribution.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 403.0, "y1": 598.0, "x1": 100.0, "y2": 772.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9268934461805, "y1": 491.831292046441, "x1": 100.0, "y2": 516.772206624349}, "name": "4", "caption_text": "Figure 4: The architectural diagram of the proposed network. SA, IMA, ITA represent self, inter-modal and inter-task attentions, respectively.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 726.0, "y1": 95.0, "x1": 102.0, "y2": 465.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2225782606337, "y1": 148.59658347235785, "x1": 426.3430701361762, "y2": 173.53752983940973}, "name": "2", "caption_text": "Table 2: Statistics of the train and test set of the EMOTyDA dataset from different sources", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 732.0, "y1": 86.0, "x1": 426.0, "y2": 176.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9265543619791, "y1": 254.71322801378037, "x1": 99.57083596123589, "y2": 279.6541849772135}, "name": "3", "caption_text": "Table 3: Results of all the baselines and the proposed models in terms of accuracy and F1-score. All the reported results are statistically significant", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 730.0, "y1": 86.0, "x1": 100.0, "y2": 266.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.153313530816, "y1": 475.6507449679904, "x1": 99.57083596123589, "y2": 500.59165954589844}, "name": "4", "caption_text": "Table 4: Results of various baseline models for the multi-task framework for the EMOTyDA dataset", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 393.0, "y1": 281.0, "x1": 108.0, "y2": 476.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 731.9872538248698, "y1": 465.6479729546441, "x1": 426.772223578559, "y2": 558.3902994791666}, "name": "5", "caption_text": "Figure 5: The visualization of the attention scores for 5 sample utterances for the tri-modal variant. V, A and T represent attention scores of video, audio and textual features, respectively. Sample utterance - u1:\u201cI am not in the least bit drunk.\u201d, u2: \u201cThere\u2019s a lot of people looking for jobs.\u201d, u3: \u201cIt was ridiculous. Completely ridiculous.\u201d, u4: \u201cYou don\u2019t have to explain.\u201d, u5: \u201cNo, Rachel doesn\u2019t want me to...\u201d", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 705.0, "y1": 302.0, "x1": 442.0, "y2": 476.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9268086751301, "y1": 201.57712300618488, "x1": 99.57083596123589, "y2": 243.1222703721788}, "name": "5", "caption_text": "Table 5: Sample utterances with its predicted labels for the best performing multi-task (MT) (T+V) model and its single task (ST) DAC variants; These examples show that ER as an auxiliary task helps DAC for better performance in MT.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 722.0, "y1": 86.0, "x1": 102.0, "y2": 224.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.60565524631073, "y1": 399.46047465006507, "x1": 99.54306284586588, "y2": 457.61116875542535}, "name": "6", "caption_text": "Figure 6: The visualization of the learned weights for an utterance - u1: \u201cOh yes, yes I am, you can\u2019t stop me.\u201d for the best performing model (T+V), single task DAC (baseline) and multi-task DAC+ER (proposed) model", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 403.0, "y1": 239.0, "x1": 100.0, "y2": 377.0}, "page": 8, "dpi": 0}], "error": null, "pdf": "/work/host-output/3686d078b62ead76b23d913804ec76d7528a13c2/2020.acl-main.402.pdf", "dpi": 100}