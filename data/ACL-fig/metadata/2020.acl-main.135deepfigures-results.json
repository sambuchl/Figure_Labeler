{"raw_detected_boxes": [[{"x2": 726.0, "y1": 309.0, "x1": 431.0, "y2": 491.0}], [], [{"x2": 721.0, "y1": 89.0, "x1": 106.0, "y2": 313.0}], [{"x2": 716.0, "y1": 870.0, "x1": 479.0, "y2": 971.0}], [{"x2": 395.0, "y1": 137.0, "x1": 125.0, "y2": 181.0}], [], [{"x2": 723.0, "y1": 86.0, "x1": 107.0, "y2": 277.0}, {"x2": 731.0, "y1": 331.0, "x1": 100.0, "y2": 452.0}], [{"x2": 724.0, "y1": 86.0, "x1": 107.0, "y2": 250.0}], [{"x2": 711.0, "y1": 98.0, "x1": 104.0, "y2": 388.0}], [], [], [{"x2": 397.0, "y1": 381.0, "x1": 103.0, "y2": 861.0}], [{"x2": 729.0, "y1": 92.0, "x1": 105.0, "y2": 304.0}]], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Figure", "name": "1", "captionBoundary": {"x2": 525.546630859375, "y1": 366.91253662109375, "x1": 307.2760009765625, "y2": 384.8699951171875}, "imageText": ["b)", "Example", "of", "Deep", "Question", "Generation", "a)", "Example", "of", "Shallow", "Question", "Generation", "Input", "Sentence:", "Oxygen", "is", "used", "in", "cellular", "respiration", "and", "released", "by", "photosynthesis,", "which", "uses", "the", "energy", "of", "sunlight", "to", "produce", "oxygen", "from", "water.", "Question:", "What", "life", "process", "produces", "oxygen", "in", "the", "presence", "of", "light?", "Answer:", "Photosynthesis", "Input", "Paragraph", "A:", "Pago", "Pago", "International", "Airport", "Pago", "Pago", "International", "Airport,", "also", "known", "as", "Tafuna", "Airport,", "is", "a", "public", "airport", "located", "7", "miles", "(11.3", "km)", "southwest", "of", "the", "central", "business", "district", "of", "Pago", "Pago,", "in", "the", "village", "and", "plains", "of", "Tafuna", "on", "the", "island", "of", "Tutuila", "in", "American", "Samoa,", "an", "unincorporated", "territory", "of", "the", "United", "States.", "Input", "Paragraph", "B:", "Hoonah", "Airport", "Hoonah", "Airport", "is", "a", "state-owned", "public-use", "airport", "located", "one", "nautical", "mile", "(2", "km)", "southeast", "of", "the", "central", "business", "district", "of", "Hoonah,", "Alaska.", "Question:", "Are", "Pago", "Pago", "International", "Airport", "and", "Hoonah", "Airport", "both", "on", "American", "territory?", "Answer:", "Yes"], "regionBoundary": {"x2": 522.0, "y1": 222.8900146484375, "x1": 310.0, "y2": 352.0308532714844}, "caption": "Figure 1: Examples of shallow/deep QG. The evidence needed to generate the question are highlighted.", "page": 0}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 507.8133239746094, "y1": 216.40756225585938, "x1": 89.42400360107422, "y2": 222.4100341796875}, "imageText": ["Ablation", "A1.", "-w/o", "Contexts", "36.48", "20.56", "12.89", "8.46", "15.43", "30.86", "A2.", "-w/o", "Semantic", "Graph", "37.63", "24.81", "18.14", "13.85", "19.24", "34.93", "A3.", "-w/o", "Multi-Relation", "&", "Attention", "38.50", "25.37", "18.54", "14.15", "19.15", "35.12", "A4.", "-w/o", "Multi-Task", "39.43", "26.10", "19.14", "14.66", "19.25", "35.76", "Baselines", "B1.", "Seq2Seq", "+", "Attn", "32.97", "21.11", "15.41", "11.81", "18.19", "33.48", "B2.", "NQG++", "35.31", "22.12", "15.53", "11.50", "16.96", "32.01", "B3.", "ASs2s", "34.60", "22.77", "15.21", "11.29", "16.78", "32.88", "B4.", "S2s-at-mp-gsa", "35.36", "22.38", "15.88", "11.85", "17.63", "33.02", "B5.", "S2s-at-mp-gsa", "(+cov,", "+ans)", "38.74", "24.89", "17.88", "13.48", "18.39", "34.51", "B6.", "CGC-QG", "31.18", "22.55", "17.69", "14.36", "25.20", "40.94", "Proposed", "P1.", "SRL-Graph", "40.40", "26.83", "19.66", "15.03", "19.73", "36.24P2.", "DP-Graph", "40.55", "27.21", "20.13", "15.53", "20.15", "36.94", "Model", "BLEU1", "BLEU2", "BLEU3", "BLEU4", "Meteor", "Rouge-L"], "regionBoundary": {"x2": 520.0, "y1": 63.8900146484375, "x1": 77.0, "y2": 199.8900146484375}, "caption": "Table 1: Performance comparison with baselines and the ablation study. The best performance is in bold.", "page": 6}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 527.2864379882812, "y1": 336.6655578613281, "x1": 71.69100189208984, "y2": 354.6230163574219}, "imageText": ["Model", "Short", "Contexts", "Medium", "Contexts", "Long", "Contexts", "AverageFlu.", "Rel.", "Cpx.", "Flu.", "Rel.", "Cpx.", "Flu.", "Rel.", "Cpx.", "Flu.", "Rel.", "Cpx.", "B4.", "S2sa-at-mp-gsa", "3.76", "4.25", "3.98", "3.43", "4.35", "4.13", "3.17", "3.86", "3.57", "3.45", "4.15", "3.89", "B6.", "CGC-QG", "3.91", "4.43", "3.60", "3.63", "4.17", "4.10", "3.69", "3.85", "4.13", "3.75", "4.15", "3.94", "A2.", "-w/o", "Semantic", "Graph", "4.01", "4.43", "4.15", "3.65", "4.41", "4.12", "3.54", "3.88", "3.55", "3.73", "4.24", "3.94", "A4.", "-w/o", "Multi-Task", "4.11", "4.58", "4.28", "3.81", "4.27", "4.38", "3.44", "3.91", "3.84", "3.79", "4.25", "4.17", "P2.", "DP-Graph", "4.34", "4.64", "4.33", "3.83", "4.51", "4.28", "3.55", "4.08", "4.04", "3.91", "4.41", "4.22", "G1.", "Ground", "Truth", "4.75", "4.87", "4.74", "4.65", "4.73", "4.73", "4.46", "4.61", "4.55", "4.62", "4.74", "4.67"], "regionBoundary": {"x2": 526.0, "y1": 238.8900146484375, "x1": 72.0, "y2": 324.8900146484375}, "caption": "Table 2: Human evaluation results for different methods on inputs with different lengths. Flu., Rel., and Cpx. denote the Fluency, Relevance, and Complexity, respectively. Each metric is rated on a 1\u20135 scale (5 for the best).", "page": 6}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 525.8909912109375, "y1": 243.68154907226562, "x1": 71.99996948242188, "y2": 309.4591064453125}, "imageText": ["x", "K", "\u2026", "\u2026", "\u2026", "\u2026", "Softmax", "Previous", "word", "Copy", "Switch", "Vocabulary", "Source", "\u2026", "\u2026", "Content", "Selection", "Context", "vector", "Structure-aware", "node", "representations", "\u2026", "[SOS]", "\u2026", "Answer", "Encoder", "+", "Answer", "tags", "+", "POS", "features", "Node", "embeddings", "\u2026", "Encoder", "(Att-GGNN)", "Semantic", "Graph", "Document", "Encoder", "Question", "Decoder", "Aggregator", "Semantic-enriched", "document", "representationsFeature", "Document", "QuestionAnswer", "\u2026", "Cross", "Attention", "\u2026\u2026", "Prediction", "Layer", "\ud835\udc34\ud835\udc34", "\ud835\udc40\ud835\udc40", "\ud835\udc49\ud835\udc49", "\ud835\udc49\ud835\udc49", "Word-to-Node", "Attention", "SIMILAR", "SIMILAR", "dep", "conj", "pobj", "pobj", "pobj", "partmod", "nsubj", "cop", "cop", "pobj", "pobj", "cop", "pobj", "nsubj", "by", "Dick", "Ebersol", "developed", "by", "Lome", "Michaels", "created", "an", "American", "late", "-", "night", "live", "television", "sketch", "comedy", "is", "at", "SNL", "abbreviated", "Saturday", "Night", "Live", "on", "Saturday", "Night", "Live", "of", "parody", "advertisements", "variety", "show", "the", "subject", "The", "Happy", "Fun", "Ball", "of", "a", "series", "is", "Question", "The", "\"Happy", "Fun", "Ball\"", "was", "the", "subject", "of", "a", "series", "of", "parody", "advertisements", "on", "a", "show", "created", "by", "who?", "Answer", "Lorne", "Michaels", "Evidence", "#1", "The", "\u201cHappy", "Fun", "Ball\u201d", "was", "the", "subject", "of", "a", "series", "of", "parody", "advertisements", "on", "\u201cSaturday", "Night", "Live\u201d", ".", "Evidence", "#2", "Saturday", "Night", "Live", "(", "abbreviated", "as", "SNL", ")", "is", "an", "American", "late", "-", "night", "live", "television", "sketch", "comedy", "and", "variety", "show", "created", "by", "Lorne", "Michaels", "and", "developed", "by", "Dick", "Ebersol", "."], "regionBoundary": {"x2": 523.0, "y1": 63.8900146484375, "x1": 74.0, "y2": 229.8900146484375}, "caption": "Figure 2: The framework of our proposed model (on the right) together with an input example (on the left). The model consists of four parts: (1) a document encoder to encode the input document, (2) a semantic graph encoder to embed the document-level semantic graph via Att-GGNN, (3) a content selector to select relevant question-worthy contents from the semantic graph, and (4) a question decoder to generate question from the semantic-enriched document representation. The left figure shows an input example and its semantic graph. Dark-colored nodes in the semantic graph are question-worthy nodes that are labeled to train the content selection task.", "page": 2}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 526.7894287109375, "y1": 231.50955200195312, "x1": 72.0, "y2": 249.467041015625}, "imageText": [".", "SRL-based", "Semantic", "Graph", "SIMILAR", "SIMILAR", "dobj", "conj", "dobj", "nsubj", "mark", "pobj", "partmod", "dobj", "advcl", "xcomp", "nsubjpass", "pobj", "cop", "nsubj", "the", "reservation", "system", "established", "since", "treaties", "the", "U.S.", "government", "enacted", "created", "the", "paternalistic", "policies", "upon", "Native", "American", "tribes", "to", "reverse", "is", "meant", "Self", "determination", "a", "leading", "member", "is", "John", "E.", "EchoHawk", "(Pawnee)", "movement", "of", "the", "Native", "American", "self", "determination", "ARG0", "ARG2", "ARG1", "member", "leading", "a", "leading", "member", "of", "the", "Native", "American", "self", "-", "determination", "movement", "is", "John", "E.", "EchoHawk", "(Pawnee)", "ARG1ARG1", "ARG0", "ARG0", "ARG1", "ARG0", "ARG1", "ARGM-TMP", "ARG2", "ARG1", "R-ARG1", "created", "the", "paternalistic", "policies", "upon", "Native", "American", "tribes", "the", "paternalistic", "policies", "enacted", "upon", "Native", "American", "tribes", "the", "reservation", "system", "the", "U.S.", "government", "treaties", "enacted", "established", "meant", "to", "reverse", "the", "paternalistic", "policies", "enacted", "upon", "Native", "American", "tribes", "since", "the", "U.S.", "government", "created", "treaties", "and", "established", "the", "reservation", "system", "reverse", "reservation", "system", "Self", "-", "determination", "since", "the", "U.S.", "government", "created", "treaties", "and", "established", "the", "DP-based", "Semantic", "Graph", "Document", "1)", "John", "E.", "EchoHawk", "(Pawnee)", "is", "a", "leading", "member", "of", "the", "Native", "American", "self", "-", "determination", "movement", ".", "2)", "Self", "-", "determination", "\u201c", "is", "meant", "to", "reverse", "the", "paternalistic", "policies", "enacted", "upon", "Native", "American", "tribes", "since", "the", "U.S.", "government", "created", "treaties", "and", "established", "the", "reservation", "system", "."], "regionBoundary": {"x2": 526.0, "y1": 61.8900146484375, "x1": 72.0, "y2": 219.8900146484375}, "caption": "Figure 4: An example of constructed DP- and SRL- based semantic graphs, where 99K indicates CHILD relation, and rectangular, rhombic and circular nodes represent arguments, verbs and modifiers respectively.", "page": 12}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 527.2864990234375, "y1": 197.27957153320312, "x1": 71.69100189208984, "y2": 251.10211181640625}, "imageText": ["Correct", "(Pred.)", "Between", "Kemess", "Mine", "and", "Colomac", "Mine,", "which", "mine", "was", "operated", "earlier?", "56.5%", "52.9%", "67.4%(G.T.)", "What", "mine", "was", "operated", "at", "an", "earlier", "date,", "Kemess", "Mine", "or", "Colomac", "Mine?", "Semantic", "(Pred.)", "Lawrence", "Ferlinghetti", "is", "an", "American", "poet,", "he", "is", "a", "short", "story", "written", "by", "who?", "17.7%", "26.4%", "8.3%Error", "(G.T.)", "Lawrence", "Ferlinghetti", "is", "an", "American", "poet,", "he", "wrote", "a", "short", "story", "named", "what", "?", "Answer", "(Pred.)", "What", "is", "the", "release", "date", "of", "this", "game", "released", "on", "17", "October", "2006?", "2.1%", "5.7%", "1.4%Revealing", "(G.T.)", "What", "is", "the", "release", "date", "of", "this", "game", "named", "Hurricane?", "Ghost", "(Pred.)", "When", "was", "the", "video", "game", "on", "which", "Michael", "Gelling", "plays", "Dr.", "Promoter?", "6.8%", "0.7%", "4.9%Entity", "(G.T.)", "When", "was", "the", "video", "game", "on", "which", "Drew", "Gelling", "plays", "Dr.", "Promoter?", "Redundant", "(Pred.)", "What", "town", "did", "Walcha", "and", "Walcha", "belong", "to?", "16.3%", "14.3%", "13.9%(G.T.)", "What", "town", "did", "Walcha", "belong", "to?", "Unanswerable", "(Pred.)", "What", "is", "the", "population", "of", "the", "city", "Barack", "Obama", "was", "born?", "8.2%", "18.6%", "8.3%(G.T.)", "What", "was", "the", "ranking", "of", "the", "population", "of", "the", "city", "Barack", "Obama", "was", "born", "in", "1999?", "Types", "Examples", "S2sa-at-", "CGC-QG", "DP-Graphmp-gsa"], "regionBoundary": {"x2": 523.0, "y1": 63.8900146484375, "x1": 74.0, "y2": 179.8900146484375}, "caption": "Table 3: Error analysis on 3 different methods, with respects to 5 major error types (excluding the \u201cCorrect\u201d). Pred. and G.T. show the example of the predicted question and the ground-truth question, respectively. Semantic Error: the question has logic or commonsense error; Answer Revealing: the question reveals the answer; Ghost Entity: the question refers to entities that do not occur in the document; Redundant: the question contains unnecessary repetition; Unanswerable: the question does not have the above errors but cannot be answered by the document.", "page": 7}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 525.547119140625, "y1": 293.4985656738281, "x1": 72.0, "y2": 311.4560546875}, "imageText": ["Question(Ours)", "What", "is", "the", "name", "of", "the", "American", "teen", "musical", "comedy", "in", "which", "the", "second", "studio", "album", "by", "the", "Christian", "rock", "band", "Superchic[k].", "\u201d", "Na", "Na", "appeared", "?", "Question(Humans)", "Which", "song", "by", "Last", "One", "Picked", "appeared", "in", "a", "2004", "American", "teen", "musical", "comedy", "film", "directed", "by", "Sara", "Sugarman", "?", "Question(Baseline)", "Who", "directed", "the", "2004", "American", "musical", "comedy", "Na", "in", "the", "film", "confessions", "\u201d", "Na", "\u201d", "?", "Question", "(CGC)", "Last", "One", "Picked", "is", "the", "second", "studio", "album", "by", "which", "2004", "American", "teen", "musical", "comedy", "film", "directed", "by", "Sara", "Sugarman", "and", "produced", "by", "Robert", "Shapiro", "and", "Matthew", "Hart", "for", "Walt", "Disney", "Pictures", "?", "Semantic", "Graph", "Matthew", "Hart", "for", "Walt", "Disney", "Pictures", ".", "3)", "Confessions", "of", "a", "Teenage", "Drama", "Queen", "is", "a", "2004", "American", "teen", "musical", "comedy", "film", "directed", "by", "Sara", "Sugarman", "and", "produced", "by", "Robert", "Shapiro", "and", "Passage", "1)", "Last", "One", "Picked", "is", "the", "second", "studio", "album", "by", "the", "Christian", "rock", "band", "Superchic[k].", "2)", "\u201d", "Na", "Na", "\u201d", "appeared", "on", "the", "Disney", "film", ",", "\u201d", "Confessions", "of", "a", "Teenage", "Drama", "Queen", "\u201d", ".", "Picked", "SIMILARdobj", "SIMILAR", "SIMILAR", "SIMILAR", "prep", "conj", "cop", "dep", "pobj", "pobj", "pobj", "pobj", "pobj", "pobj", "pobj", "nsubj", "nsubj", "nsubj", "pobj", "dep", "nsubj", "produced", "directed", "is", "appeared", "is", "by", "Sara", "Sugarman", "by", "for", "Walt", "Disney", "Pictures", "of", "a", "Teenage", "Drama", "Queen", "of", "a", "Teenage", "Drama", "Queen", "Disney", "film", "on", "the", "by", "the", "Christian", "rock", "band", "Superchic[k].", "a", "2004", "American", "teen", "musical", "comedy", "film", "Robert", "ShapiroMatthew", "Hart", "Confessions", "Confessions", "Na", "Na", "the", "second", "studio", "album", "Last", "One"], "regionBoundary": {"x2": 517.0, "y1": 61.8900146484375, "x1": 72.0, "y2": 281.8900146484375}, "caption": "Figure 3: An example of generated questions and average attention distribution on the semantic graph, with nodes colored darker for more attention (best viewed in color).", "page": 8}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 729.9258761935764, "y1": 509.60074530707465, "x1": 426.772223578559, "y2": 534.5416598849827}, "name": "1", "caption_text": "Figure 1: Examples of shallow/deep QG. The evidence needed to generate the question are highlighted.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 726.0, "y1": 309.0, "x1": 431.0, "y2": 491.0}, "page": 0, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 730.4041544596354, "y1": 338.44659593370227, "x1": 99.99995761447482, "y2": 429.8043145073785}, "name": "2", "caption_text": "Figure 2: The framework of our proposed model (on the right) together with an input example (on the left). The model consists of four parts: (1) a document encoder to encode the input document, (2) a semantic graph encoder to embed the document-level semantic graph via Att-GGNN, (3) a content selector to select relevant question-worthy contents from the semantic graph, and (4) a question decoder to generate question from the semantic-enriched document representation. The left figure shows an input example and its semantic graph. Dark-colored nodes in the semantic graph are question-worthy nodes that are labeled to train the content selection task.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 724.0, "y1": 89.0, "x1": 102.0, "y2": 318.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 705.2962832980686, "y1": 300.56605868869354, "x1": 124.20000500149196, "y2": 308.902825249566}, "name": "1", "caption_text": "Table 1: Performance comparison with baselines and the ablation study. The best performance is in bold.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 723.0, "y1": 86.0, "x1": 107.0, "y2": 277.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.3422749837239, "y1": 467.5910525851779, "x1": 99.57083596123589, "y2": 492.53196716308594}, "name": "2", "caption_text": "Table 2: Human evaluation results for different methods on inputs with different lengths. Flu., Rel., and Cpx. denote the Fluency, Relevance, and Complexity, respectively. Each metric is rated on a 1\u20135 scale (5 for the best).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 731.0, "y1": 331.0, "x1": 100.0, "y2": 469.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.3423597547743, "y1": 273.99940490722656, "x1": 99.57083596123589, "y2": 348.752933078342}, "name": "3", "caption_text": "Table 3: Error analysis on 3 different methods, with respects to 5 major error types (excluding the \u201cCorrect\u201d). Pred. and G.T. show the example of the predicted question and the ground-truth question, respectively. Semantic Error: the question has logic or commonsense error; Answer Revealing: the question reveals the answer; Ghost Entity: the question refers to entities that do not occur in the document; Redundant: the question contains unnecessary repetition; Unanswerable: the question does not have the above errors but cannot be answered by the document.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 726.0, "y1": 86.0, "x1": 104.0, "y2": 250.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9265543619791, "y1": 407.6368967692057, "x1": 100.0, "y2": 432.57785373263886}, "name": "3", "caption_text": "Figure 3: An example of generated questions and average attention distribution on the semantic graph, with nodes colored darker for more attention (best viewed in color).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 712.0, "y1": 91.0, "x1": 104.0, "y2": 389.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 731.6519843207465, "y1": 321.5410444471571, "x1": 100.0, "y2": 346.48200141059027}, "name": "4", "caption_text": "Figure 4: An example of constructed DP- and SRL- based semantic graphs, where 99K indicates CHILD relation, and rectangular, rhombic and circular nodes represent arguments, verbs and modifiers respectively.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 730.0, "y1": 90.0, "x1": 100.0, "y2": 321.0}, "page": 12, "dpi": 0}], "error": null, "pdf": "/work/host-output/0af664417e36daf344533310af7ba3737a92a87a/2020.acl-main.135.pdf", "dpi": 100}