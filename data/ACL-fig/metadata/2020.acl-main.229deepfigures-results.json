{"raw_detected_boxes": [[], [{"x2": 389.0, "y1": 98.0, "x1": 112.0, "y2": 269.0}], [], [{"x2": 398.0, "y1": 89.0, "x1": 105.0, "y2": 195.0}], [{"x2": 725.0, "y1": 89.0, "x1": 112.0, "y2": 280.0}, {"x2": 725.0, "y1": 449.0, "x1": 432.0, "y2": 537.0}, {"x2": 725.0, "y1": 590.0, "x1": 437.0, "y2": 698.0}], [], [{"x2": 725.0, "y1": 100.0, "x1": 105.0, "y2": 281.0}], [{"x2": 398.0, "y1": 88.0, "x1": 103.0, "y2": 261.0}, {"x2": 725.0, "y1": 94.0, "x1": 429.0, "y2": 242.0}, {"x2": 397.0, "y1": 337.0, "x1": 103.0, "y2": 506.0}, {"x2": 727.0, "y1": 347.0, "x1": 430.0, "y2": 487.0}], [{"x2": 715.0, "y1": 87.0, "x1": 112.0, "y2": 355.0}], [], [{"x2": 719.0, "y1": 94.0, "x1": 459.0, "y2": 274.0}], [], [{"x2": 719.0, "y1": 98.0, "x1": 111.0, "y2": 317.0}], [{"x2": 727.0, "y1": 405.0, "x1": 431.0, "y2": 628.0}], [{"x2": 716.0, "y1": 96.0, "x1": 437.0, "y2": 684.0}, {"x2": 382.0, "y1": 96.0, "x1": 118.0, "y2": 708.0}], [{"x2": 391.0, "y1": 98.0, "x1": 112.0, "y2": 629.0}], [{"x2": 718.0, "y1": 98.0, "x1": 140.0, "y2": 984.0}], [{"x2": 714.0, "y1": 169.0, "x1": 113.0, "y2": 938.0}]], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Table", "name": "7", "captionBoundary": {"x2": 290.2273254394531, "y1": 531.5452880859375, "x1": 71.6530990600586, "y2": 549.5028076171875}, "imageText": ["A", "ve", "ra", "ge", "PL", "51.8", "26.8", "27.9", "30.6", "25.1", "22.1", "NE\u2193", "8.5", "8.7", "8.5", "8.5", "8.1", "8.3", "SR\u2191", "24.7", "25.5", "24.6", "27.0", "27.5", "28.1", "SPL\u2191", "8.6", "13.1", "12.9", "13.9", "15.1", "16.5", "CLS\u2191", "26.6", "44.5", "43.9", "44.6", "46.2", "48.6", "NDTW\u2191", "23.0", "33.9", "32.2", "32.4", "37.4", "39.0", "SDTW\u2191", "11.0", "14.8", "14.4", "15.7", "17.3", "18.4", "R", "8R", "PL", "93.1", "47.5", "50.0", "55.3", "45.2", "39.9", "NE\u2193", "10.0", "10.2", "10.2", "10.1", "9.3", "10.1", "SR\u2191", "21.9", "21.4", "20.4", "22.1", "23.1", "23.1", "SPL\u2191", "4.3", "6.1", "5.5", "6.1", "6.8", "7.4", "CLS\u2191", "24.1", "42.1", "41.0", "41.5", "43.9", "46.0", "NDTW\u2191", "15.5", "24.6", "22.9", "23.8", "27.7", "28.2", "SDTW\u2191", "6.4", "8.3", "7.9", "9.2", "10.5", "11.1", "R", "6R", "PL", "68.8", "35.3", "37.0", "40.6", "33.2", "28.7", "NE\u2193", "9.4", "9.5", "9.4", "9.4", "8.9", "9.2", "SR\u2191", "22.7", "23.7", "21.9", "23.4", "24.7", "25.5", "SPL\u2191", "4.2", "7.2", "6.4", "6.8", "8.1", "9.2", "CLS\u2191", "24.4", "43.0", "41.8", "42.3", "44.2", "47.2", "NDTW\u2191", "17.8", "28.1", "26.0", "26.9", "30.9", "32.7", "SDTW\u2191", "7.7", "10.8", "9.7", "11.0", "12.7", "13.6", "R", "4R", "PL", "43.4", "22.8", "23.9", "25.5", "21.4", "19.0", "NE\u2193", "8.4", "8.6", "8.5", "8.4", "8.0", "8.2", "SR\u2191", "24.7", "25.0", "24.1", "26.7", "27.9", "27.3", "SPL\u2191", "8.2", "11.2", "11.0", "12.3", "13.7", "14.7", "CLS\u2191", "27.9", "45.5", "44.8", "45.9", "47.4", "49.4", "NDTW\u2191", "24.3", "34.4", "32.8", "33.7", "38.4", "39.6", "SDTW\u2191", "11.1", "13.6", "13.5", "15.2", "17.0", "17.3", "R", "2R", "PL", "22.4", "12.0", "11.6", "13.2", "10.6", "9.6", "NE\u2193", "6.8", "7.1", "6.8", "6.8", "6.7", "6.6", "SR\u2191", "28.1", "29.8", "29.9", "33.2", "32.2", "34.1", "SPL\u2191", "15.7", "24.3", "24.9", "26.6", "27.5", "30.2", "CLS\u2191", "28.9", "46.2", "46.6", "47.2", "48.1", "50.4", "NDTW\u2191", "30.6", "43.8", "42.5", "41.0", "47.7", "50.0", "SDTW\u2191", "16.5", "23.2", "23.1", "24.3", "25.7", "27.8", "4t", "h", "3r", "d", "2n", "d", "1s", "t", "L", "IL", "+R", "IL", "s", "M", "et", "ric", "D", "at", "as", "et", "s", "IL+", "CRL", "w/", "LECTURE", "#"], "regionBoundary": {"x2": 278.0, "y1": 62.0, "x1": 85.0, "y2": 520.0}, "caption": "Table 7: Ablation on BABYWALK after each learning stage (trained on R4R).", "page": 14}, {"figType": "Table", "name": "8", "captionBoundary": {"x2": 527.1621704101562, "y1": 511.5022888183594, "x1": 306.9295959472656, "y2": 541.4158935546875}, "imageText": ["Average", "PL", "24.2", "22.1", "NE\u2193", "8.3", "8.3", "SR\u2191", "25.2", "28.1", "SPL\u2191", "13.8", "16.5", "CLS\u2191", "45.9", "48.6", "NDTW\u2191", "34.6", "39.0", "SDTW\u2191", "15.4", "18.4", "PL", "42.9", "39.9", "NE\u2193", "9.8", "10.1", "SR\u2191", "21.2", "23.1", "SPL\u2191", "6.3", "7.4", "CLS\u2191", "43.2", "46.0", "NDTW\u2191", "25.5", "28.2", "SDTW\u2191", "9.3", "11.1", "R8R", "PL", "32.1", "28.7", "NE\u2193", "9.0", "9.2", "SR\u2191", "22.5", "25.5", "SPL\u2191", "7.5", "9.2", "CLS\u2191", "44.2", "47.2", "NDTW\u2191", "29.3", "32.7", "SDTW\u2191", "11.1", "13.6", "R6R", "PL", "20.9", "19.0", "NE\u2193", "8.2", "8.2", "SR\u2191", "26.3", "27.3", "SPL\u2191", "12.7", "14.7", "CLS\u2191", "46.4", "49.4", "NDTW\u2191", "35.5", "39.6", "SDTW\u2191", "15.9", "17.3", "R4R", "PL", "10.3", "9.6", "NE\u2193", "6.8", "6.6", "SR\u2191", "28.7", "34.1", "SPL\u2191", "24.9", "30.2", "CLS\u2191", "48.3", "50.4", "NDTW\u2191", "43.6", "50.0", "SDTW\u2191", "22.4", "27.8", "R2R", "Datasets", "Metrics", "Sentence-wise", "Template", "based"], "regionBoundary": {"x2": 518.0, "y1": 62.0, "x1": 314.0, "y2": 500.0}, "caption": "Table 8: BABYWALK Agent performances between different segmentation rules (trained on R4R). Refer to text for more details.", "page": 14}, {"figType": "Figure", "name": "1", "captionBoundary": {"x2": 291.986083984375, "y1": 209.79660034179688, "x1": 71.65325927734375, "y2": 311.44073486328125}, "imageText": [], "regionBoundary": {"x2": 282.0, "y1": 69.0, "x1": 80.0, "y2": 194.0}, "caption": "Figure 1: Performance of various VLN agents on generalizing from shorter navigation tasks to longer ones. The vertical axis is the newly proposed path-following metric SDTW (Magalhaes et al., 2019), the higher the better. BABYWALK generalizes better than other approaches across different lengths of navigation tasks. Meanwhile, it get very close to the performances of the in-domain agents (the dashed line). Please refer to the texts for details.", "page": 1}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 525.5038452148438, "y1": 214.35964965820312, "x1": 71.60368347167969, "y2": 256.22833251953125}, "imageText": ["BABYWALK", "29.6", "47.8", "18.1", "35.2", "48.5", "27.2", "26.4", "44.9", "13.1", "26.3", "44.7", "11.5", "29.3", "46.0", "17.3", "BABYWALK", "+", "27.3", "49.4", "17.3", "34.1", "50.4", "27.8", "25.5", "47.2", "13.6", "23.1", "46.0", "11.1", "27.6", "47.9", "17.5", "In-domain", "Generalization", "to", "other", "datasets", "Setting", "R4R", "\u2192", "R4R", "R4R", "\u2192", "R2R", "R4R", "\u2192", "R6R", "R4R", "\u2192", "R8R", "Average", "Metrics", "SR\u2191", "CLS\u2191", "SDTW\u2191", "SR\u2191", "CLS\u2191", "SDTW\u2191", "SR\u2191", "CLS\u2191", "SDTW\u2191", "SR\u2191", "CLS\u2191", "SDTW\u2191", "SR\u2191", "CLS\u2191", "SDTW\u2191", "SEQ2SEQ", "25.7", "20.7", "9.0", "16.3", "27.1", "10.6", "14.4", "17.7", "4.6", "20.7", "15.0", "4.7", "17.1", "19.9", "6.6", "SF+", "24.9", "23.6", "9.2", "22.5", "29.5", "14.8", "15.5", "20.4", "5.2", "21.6", "17.2", "5.0", "19.9", "22.4", "8.3", "RCM(GOAL)+", "28.7", "36.3", "13.2", "25.9", "44.2", "20.2", "19.3", "31.8", "7.3", "22.8", "27.6", "5.1", "22.7", "34.5", "10.9", "RCM(FIDELITY)+", "24.7", "39.2", "13.7", "29.1", "34.3", "18.3", "20.5", "38.3", "7.9", "20.9", "34.6", "6.1", "23.5", "35.7", "10.8", "REGRETFUL+", "30.1", "34.1", "13.5", "22.8", "32.6", "13.4", "18.0", "31.7", "7.5", "18.7", "29.3", "5.6", "19.8", "31.2", "8.8", "FAST+", "36.2", "34.0", "15.5", "25.1", "33.9", "14.2", "22.1", "31.5", "7.7", "27.7", "29.6", "6.3", "25.0", "31.7", "9.4"], "regionBoundary": {"x2": 524.0, "y1": 73.3530502319336, "x1": 74.0, "y2": 203.0}, "caption": "Table 2: VLN agents trained on the R4R dataset and evaluated on the unseen portion of the R4R (in-domain) and the other 3 out-of-the-domain datasets: R2R, R6R and R8R with different distributions in instruction length. The Appendix has more comparisons. (+: pre-trained with data augmentation. : reimplemented or adapted from the original authors\u2019 public codes).", "page": 6}, {"figType": "Table", "name": "6", "captionBoundary": {"x2": 525.5096435546875, "y1": 464.484130859375, "x1": 306.88037109375, "y2": 506.3519287109375}, "imageText": ["Re-implemented", "Version", "SEQ2SEQ", "15.76", "6.71", "33.6", "25.5", "SF+", "15.55", "6.52", "35.8", "27.6", "RCM+", "11.15", "6.18", "42.4", "38.6", "REGRETFUL+", "13.74", "5.38", "48.7", "39.7", "FAST+", "20.45", "4.97", "56.6", "43.7", "Data", "Splits", "R2R", "Validation", "Unseen", "Perf.", "Measures", "PL", "NE\u2193", "SR\u2191", "SPL", "Reported", "Results", "SEQ2SEQ", "(Fried", "et", "al.,", "2018)", "-", "7.07", "31.2", "-", "SF+", "(Fried", "et", "al.,", "2018)", "-", "6.62", "35.5", "-", "RCM+", "(Wang", "et", "al.,", "2019)", "14.84", "5.88", "42.5", "-", "REGRETFUL+", "(Ma", "et", "al.,", "2019b)", "-", "5.32", "50.0", "41.0", "FAST+", "(Ke", "et", "al.,", "2019)", "21.17", "4.97", "56.0", "43.0"], "regionBoundary": {"x2": 524.0, "y1": 292.0, "x1": 309.0, "y2": 453.0}, "caption": "Table 6: Sanity check of model trained on R2R and evaluated on its validation unseen split (+: pre-trained with data augmentation; :reimplemented or readapted from the original authors\u2019 released code).", "page": 13}, {"figType": "Figure", "name": "8", "captionBoundary": {"x2": 483.06427001953125, "y1": 693.7376708984375, "x1": 114.39720153808594, "y2": 699.7401123046875}, "imageText": ["HUMAN", "BABYWALK", "RCM", "SF", "SEQ2SEQ"], "regionBoundary": {"x2": 520.0, "y1": 122.0, "x1": 77.0, "y2": 682.0}, "caption": "Figure 8: Additional trajectories by human experts and VLN agents on two navigation tasks.", "page": 17}, {"figType": "Figure", "name": "7", "captionBoundary": {"x2": 527.1568603515625, "y1": 250.23275756835938, "x1": 71.96209716796875, "y2": 316.01129150390625}, "imageText": ["m-1", "m-1", "MLP", "Text", "AttentionLSTM", ",,,\u22ee", "\u22ee", "LSTM", "\u210e", "Concat", "Vision", "Attention", "Dot", "Product", "\u210e", "Softmax", "\u0302", "MLP", "Concat", "(", ")WordEmbedding", "BabyWalk", "Policy", "Bi-LSTM", "Bi-LSTMVisionAttention", "(", ")", "(", ")", "Trajectory", "Encoder", "Instruction", "EncoderMemory", "Buffer"], "regionBoundary": {"x2": 505.222900390625, "y1": 63.30223846435547, "x1": 78.0, "y2": 230.0}, "caption": "Figure 7: Our network architecture at the m-th BABY-STEP sub-task. Red line represents the procedure of encoding context variable zm via summarizing the BABY-STEP trajectory fSUMMARY(v(y\u03021), . . . , v(y\u0302m\u22121)) and the corresponding (micro)instruction fSUMMARY(u(x1), . . . , u(xm\u22121)) in the memory buffer. Blue line represents the procedure of encoding the (micro)instruction u(xm) of the current BABY-STEP. Purple line represents the detailed decision making process of our BABYWALK policy (Ast is denoted as the set of navigable directions at st as defined by Fried et al. (2018))", "page": 12}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 527.1619873046875, "y1": 186.40951538085938, "x1": 306.9295959472656, "y2": 228.27740478515625}, "imageText": ["IL+", "CRL", "w/", "LECTURE", "#", "1st", "24.1", "44.8", "13.5", "24.1", "43.1", "13.6", "2nd", "26.7", "45.9", "15.2", "26.2", "43.7", "14.8", "3rd", "27.9", "47.4", "17.0", "26.7", "45.4", "16.3", "4th", "27.3", "49.4", "17.3", "27.6", "47.9", "17.5", "Setting", "R4R", "\u2192", "R4R", "R4R", "\u2192", "others", "Metrics", "SR\u2191", "CLS\u2191", "SDTW", "\u2191", "SR\u2191", "CLS\u2191", "SDTW", "\u2191", "IL", "24.7", "27.9", "11.1", "24.2", "25.8", "10.2", "IL+RL", "25.0", "45.5", "13.6", "25.0", "43.8", "14.1"], "regionBoundary": {"x2": 524.0, "y1": 68.0, "x1": 309.0, "y2": 175.0}, "caption": "Table 4: BABYWALK\u2019s performances with curriculumbased reinforcement learning (CRL), which improves imitation learning without or with reinforcement learning (IL+RL).", "page": 7}, {"figType": "Figure", "name": "5", "captionBoundary": {"x2": 290.24566650390625, "y1": 203.92562866210938, "x1": 71.96209716796875, "y2": 221.88421630859375}, "imageText": [], "regionBoundary": {"x2": 287.0, "y1": 63.0, "x1": 74.0, "y2": 191.0}, "caption": "Figure 5: Performance by various agents on navigation tasks in different lengths. See texts for details.", "page": 7}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 291.8863525390625, "y1": 376.4480285644531, "x1": 71.6530990600586, "y2": 394.4056091308594}, "imageText": ["\u03b3", "=", "5", "27.5", "46.8", "15.8", "26.7", "44.4", "14.9", "\u03b3", "=", "0.5", "27.3", "49.4", "17.3", "27.6", "47.9", "17.5", "\u03b3", "=", "0.05", "27.5", "47.7", "16.2", "26.0", "45.5", "15.2", "\u03b3", "=", "0", "26.1", "46.6", "15.1", "25.1", "44.3", "14.4", "Setting", "R4R", "\u2192", "R4R", "R4R", "\u2192", "others", "Metrics", "SR\u2191", "CLS\u2191", "SDTW", "\u2191", "SR\u2191", "CLS\u2191", "SDTW", "\u2191", "fSUMMARY", "=", "NULL", "18.9", "43.1", "9.9", "17.1", "42.3", "9.6", "LSTM(\u00b7)", "25.8", "44.0", "14.4", "25.7", "42.1", "14.3", "fSUMMARY", "=", "\u2211m\u22121", "i=1", "\u03b1i", "\u00b7", "(\u00b7),", "i.e.,", "eqs.", "(2,3)"], "regionBoundary": {"x2": 289.0, "y1": 243.0, "x1": 74.0, "y2": 365.0}, "caption": "Table 3: The memory buffer is beneficial to generalizing to different tasks from on which the agent is trained.", "page": 7}, {"figType": "Table", "name": "5", "captionBoundary": {"x2": 527.162353515625, "y1": 368.7549133300781, "x1": 306.9295959472656, "y2": 422.5776062011719}, "imageText": ["Eval", "\u2192", "R2R", "\u2192", "R4R", "Training", "SR\u2191", "CLS\u2191", "SDTW\u2191", "SR\u2191", "CLS\u2191", "SDTW\u2191", "R2R", "43.8", "54.4", "36.9", "21.4", "51.0", "13.8", "R4R", "34.1", "50.4", "27.8", "27.3", "49.4", "17.3", "Eval", "\u2192", "R6R", "\u2192", "R8R", "Training", "SR\u2191", "CLS\u2191", "SDTW\u2191", "SR\u2191", "CLS\u2191", "SDTW\u2191", "R2R", "21.7", "49.0", "11.2", "20.7", "48.7", "9.8", "R4R", "25.5", "47.2", "13.6", "23.1", "46.0", "11.1"], "regionBoundary": {"x2": 524.0, "y1": 249.0, "x1": 309.0, "y2": 357.0}, "caption": "Table 5: (Top) BABYWALK trained on R2R is nearly as effective as the agent trained on R4R when generalizing to longer tasks. (Bottom) BABYWALK trained on R2R adapts to R4R better than the agent trained in the reverse direction.", "page": 7}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 291.88262939453125, "y1": 157.83950805664062, "x1": 71.96209716796875, "y2": 259.48431396484375}, "imageText": [], "regionBoundary": {"x2": 287.0, "y1": 66.0, "x1": 76.0, "y2": 143.0}, "caption": "Figure 2: The BABYWALK agent has a memory buffer storing its past experiences of instructions xm, and its trajectory y\u0302m. When a new BABY-STEP xm is presented, the agent retrieves from the memory a summary of its experiences as the history context. It takes actions conditioning on the context (as well as its state st and the previous action a\u0302t). Upon finishing following the instruction. the trajectory y\u0302m is then sent to the memory to be remembered.", "page": 3}, {"figType": "Table", "name": "10", "captionBoundary": {"x2": 527.1553344726562, "y1": 728.775634765625, "x1": 71.6530990600586, "y2": 758.688232421875}, "imageText": ["(c)", "R6R", "trained", "model", "(d)", "R8R", "trained", "model", "A", "ve", "ra", "ge", "PL", "22.6", "31.1", "10.3", "30.1", "17.7", "19.3", "NE\u2193", "8.7", "8.7", "8.9", "9.2", "8.0", "8.2", "SR\u2191", "19.2", "20.0", "17.6", "20.3", "25.8", "26.5", "SPL\u2191", "12.3", "10.0", "14.2", "7.8", "16.0", "16.2", "CLS\u2191", "26.6", "22.0", "33.1", "30.6", "48.2", "48.1", "NDTW\u2191", "24.4", "21.8", "27.9", "22.2", "40.4", "39.7", "SDTW\u2191", "9.0", "9.0", "8.8", "8.9", "17.2", "17.7", "\u2192", "R", "6R", "PL", "30.9", "42.2", "11.9", "39.9", "26.6", "29.2", "NE\u2193", "9.7", "9.9", "9.9", "10.1", "9.0", "9.3", "SR\u2191", "15.4", "14.7", "14.8", "20.0", "22.9", "22.9", "SPL\u2191", "8.6", "6.7", "11.6", "5.3", "8.4", "7.9", "CLS\u2191", "22.2", "18.5", "29.1", "33.5", "46.9", "46.6", "NDTW\u2191", "18.5", "15.9", "22.5", "20.1", "33.3", "31.8", "SDTW\u2191", "5.5", "4.7", "6.0", "7.8", "12.1", "11.8", "R", "8R", "\u2192", "R", "4R", "PL", "23.1", "31.7", "11.1", "32.5", "17.4", "19.0", "NE\u2193", "8.7", "8.8", "8.7", "9.2", "8.2", "8.5", "SR\u2191", "23.6", "21.8", "23.2", "21.7", "24.4", "24.4", "SPL\u2191", "15.1", "10.5", "18.2", "7.4", "12.6", "12.5", "CLS\u2191", "24.9", "20.8", "32.3", "29.4", "48.1", "48.5", "NDTW\u2191", "22.3", "19.7", "26.4", "20.6", "39.1", "38.5", "SDTW\u2191", "8.8", "7.7", "9.3", "8.4", "14.9", "15.2", "R", "8R", "\u2192", "R", "2R", "PL", "13.7", "19.3", "7.8", "17.8", "9.1", "9.8", "NE\u2193", "7.6", "7.3", "8.0", "8.2", "6.8", "6.7", "SR\u2191", "18.7", "23.4", "14.8", "19.2", "30.0", "32.1", "SPL\u2191", "13.3", "12.9", "12.9", "10.6", "27.0", "28.2", "CLS\u2191", "32.7", "26.6", "37.9", "28.9", "49.5", "49.3", "NDTW\u2191", "32.4", "29.9", "34.9", "25.9", "48.9", "48.9", "SDTW\u2191", "12.7", "14.5", "11.1", "10.5", "24.6", "26.2", "R", "8R", "+", "A", "L", "K", "B", "A", "B", "Y", "W", "A", "L", "K", "B", "A", "B", "Y", "W", "Y", ")+", "E", "L", "IT", "R", "C", "M", "(F", "ID", "O", "A", "L", ")+", "R", "C", "M", "(G", "SF", "+", "E", "Q", "SE", "Q", "2S", "s", "M", "et", "ric", "D", "at", "as", "et", "s", "A", "ve", "ra", "ge", "PL", "27.6", "35.1", "11.3", "23.7", "21.9", "21.2", "NE\u2193", "8.8", "8.5", "8.6", "8.5", "8.5", "8.3", "SR\u2191", "21.2", "21.5", "21.2", "23.5", "25.5", "25.3", "SPL\u2191", "12.7", "10.1", "16.5", "10.9", "15.9", "15.8", "CLS\u2191", "26.2", "22.0", "35.6", "36.0", "48.1", "48.3", "NDTW\u2191", "23.7", "22.2", "30.5", "27.0", "39.0", "39.4", "SDTW\u2191", "9.3", "8.6", "10.8", "10.7", "16.9", "16.8", "\u2192", "R", "8R", "PL", "43.0", "52.8", "14.2", "29.9", "38.3", "36.8", "NE\u2193", "9.9", "9.9", "9.6", "9.7", "10.2", "10.0", "SR\u2191", "20.1", "20.3", "20.3", "22.4", "20.8", "21.0", "SPL\u2191", "11.2", "9.4", "14.9", "8.1", "6.6", "6.8", "CLS\u2191", "20.6", "18.3", "27.7", "38.9", "45.9", "46.3", "NDTW\u2191", "16.3", "15.2", "21.9", "22.2", "28.4", "29.3", "SDTW\u2191", "5.6", "5.0", "6.4", "6.8", "9.6", "9.9", "R", "6R", "\u2192", "R", "4R", "PL", "25.2", "33.0", "11.6", "25.7", "18.1", "17.7", "NE\u2193", "8.7", "8.6", "8.5", "8.4", "8.4", "8.2", "SR\u2191", "24.2", "22.4", "23.6", "25.4", "24.3", "24.3", "SPL\u2191", "13.7", "9.3", "17.5", "10.6", "12.8", "12.9", "CLS\u2191", "25.8", "21.4", "35.8", "34.8", "48.6", "48.6", "NDTW\u2191", "22.9", "20.6", "29.8", "26.5", "39.0", "39.4", "SDTW\u2191", "9.3", "7.5", "10.8", "11.1", "15.1", "15.1", "R", "6R", "\u2192", "R", "2R", "PL", "14.5", "19.4", "8.1", "15.5", "9.4", "9.2", "NE\u2193", "7.7", "7.1", "7.6", "7.5", "6.8", "6.8", "SR\u2191", "19.3", "21.9", "19.6", "22.6", "31.3", "30.6", "SPL\u2191", "13.3", "11.6", "17.2", "14.1", "28.3", "27.8", "CLS\u2191", "32.1", "26.2", "43.2", "34.3", "49.9", "50.0", "NDTW\u2191", "31.9", "30.8", "39.7", "32.4", "49.5", "49.4", "SDTW\u2191", "13.1", "13.3", "15.3", "14.3", "25.9", "25.4", "R", "6R", "+", "A", "L", "K", "B", "A", "B", "Y", "W", "A", "L", "K", "B", "A", "B", "Y", "W", "Y", ")+", "E", "L", "IT", "R", "C", "M", "(F", "ID", "O", "A", "L", ")+", "R", "C", "M", "(G", "SF", "+", "E", "Q", "SE", "Q", "2S", "s", "M", "et", "ric", "D", "at", "as", "et", "s", "(a)", "R2R", "trained", "model", "(b)", "R4R", "trained", "model", "A", "ve", "ra", "ge", "PL", "37.8", "35.6", "12.3", "29.8", "20.2", "27.1", "33.0", "26.1", "NE\u2193", "9.3", "8.8", "8.6", "8.6", "8.8", "8.4", "8.2", "8.6", "SR\u2191", "17.1", "19.9", "22.7", "23.5", "19.8", "25.0", "29.3", "27.6", "SPL\u2191", "9.0", "11.4", "18.2", "11.9", "11.3", "14.6", "14.5", "15.6", "CLS\u2191", "19.9", "22.4", "34.5", "35.7", "31.2", "31.7", "46.0", "47.9", "NDTW\u2191", "19.7", "21.7", "28.0", "26.3", "23.7", "22.9", "34.6", "37.0", "SDTW\u2191", "6.6", "8.3", "10.9", "10.8", "8.8", "9.6", "17.3", "17.5", "\u2192", "R", "8R", "PL", "56.4", "50.8", "13.9", "38.7", "20.7", "28.2", "50.0", "39.9", "NE\u2193", "10.1", "9.5", "9.5", "9.9", "9.5", "9.1", "9.3", "10.1", "SR\u2191", "20.7", "21.6", "22.8", "20.9", "18.7", "27.7", "26.3", "23.1", "SPL\u2191", "10.4", "11.8", "16.9", "9.0", "9.2", "13.7", "7.2", "7.4", "CLS\u2191", "15.0", "17.2", "27.6", "34.6", "29.3", "29.6", "44.7", "46.0", "NDTW\u2191", "13.4", "15.1", "19.5", "21.7", "19.0", "17.7", "27.1", "28.2", "SDTW\u2191", "4.7", "5.0", "5.1", "6.1", "5.6", "6.9", "11.5", "11.1", "R", "4R", "\u2192", "R", "6R", "PL", "40.8", "38.5", "12.8", "33.0", "19.9", "26.6", "37.0", "28.7", "NE\u2193", "9.9", "9.5", "9.2", "9.3", "9.5", "8.9", "8.8", "9.2", "SR\u2191", "14.4", "15.5", "19.3", "20.5", "18.0", "22.1", "26.4", "25.5", "SPL\u2191", "6.8", "8.4", "15.2", "8.5", "10.6", "13.7", "8.1", "9.2", "CLS\u2191", "17.7", "20.4", "31.8", "38.3", "31.7", "31.5", "44.9", "47.2", "NDTW\u2191", "16.4", "18.3", "23.5", "23.7", "23.5", "23.0", "30.1", "32.7", "SDTW\u2191", "4.6", "5.2", "7.3", "7.9", "7.5", "7.7", "13.1", "13.6", "R", "4R", "\u2192", "R", "2R", "PL", "16.2", "17.4", "10.2", "17.7", "20.0", "26.5", "12.1", "9.6", "NE\u2193", "7.8", "7.3", "7.1", "6.7", "7.5", "7.2", "6.6", "6.6", "SR\u2191", "16.3", "22.5", "25.9", "29.1", "22.8", "25.1", "35.2", "34.1", "SPL\u2191", "9.9", "14.1", "22.5", "18.2", "14.0", "16.3", "28.3", "30.2", "CLS\u2191", "27.1", "29.5", "44.2", "34.3", "32.6", "33.9", "48.5", "50.4", "NDTW\u2191", "29.3", "31.8", "41.1", "33.5", "28.5", "27.9", "46.5", "50.0", "SDTW\u2191", "10.6", "14.8", "20.2", "18.3", "13.4", "14.2", "27.2", "27.8", "R", "4R", "+", "A", "L", "K", "B", "A", "B", "Y", "W", "A", "L", "K", "B", "A", "B", "Y", "W", "+", "FA", "ST", "L", "+", "R", "E", "G", "R", "E", "T", "FU", "Y", ")+", "E", "L", "IT", "R", "C", "M", "(F", "ID", "O", "A", "L", ")+", "R", "C", "M", "(G", "SF", "+", "E", "Q", "SE", "Q", "2S", "s", "M", "et", "ric", "D", "at", "as", "et", "s", "A", "ve", "ra", "ge", "PL", "40.1", "40.8", "14.2", "15.6", "16.0", "32.2", "29.0", "25.9", "NE\u2193", "9.7", "9.8", "10.0", "10.1", "9.1", "9.6", "10.0", "9.7", "SR\u2191", "18.6", "16.1", "16.5", "16.8", "19.9", "16.8", "21.2", "21.3", "SPL\u2191", "8.3", "7.4", "11.3", "11.6", "11.5", "10.1", "9.1", "9.5", "CLS\u2191", "26.1", "26.8", "37.3", "36.2", "40.9", "37.7", "48.9", "49.6", "NDTW\u2191", "20.9", "21.4", "27.9", "27.3", "18.6", "23.3", "32.1", "34.0", "SDTW\u2191", "6.3", "5.9", "6.6", "6.6", "6.3", "6.0", "11.7", "11.6", "\u2192", "R", "8R", "PL", "52.3", "52.2", "15.3", "16.9", "16.6", "34.9", "38.3", "34.0", "NE\u2193", "10.5", "10.5", "11.0", "11.1", "10.0", "10.6", "11.1", "10.5", "SR\u2191", "16.9", "13.8", "12.4", "12.6", "16.3", "11.1", "19.6", "20.7", "SPL\u2191", "6.1", "5.6", "7.4", "7.5", "7.7", "6.2", "6.9", "7.8", "CLS\u2191", "22.5", "24.1", "32.4", "30.9", "35.3", "33.7", "48.1", "48.7", "NDTW\u2191", "17.1", "18.2", "23.9", "23.3", "8.1", "14.5", "26.7", "29.1", "SDTW\u2191", "4.1", "3.8", "4.3", "4.3", "2.4", "2.4", "9.4", "9.8", "R", "2R", "\u2192", "R", "6R", "PL", "39.4", "41.4", "14.2", "15.7", "15.9", "32.0", "29.1", "25.9", "NE\u2193", "9.6", "9.8", "9.7", "9.8", "8.8", "9.0", "10.1", "9.8", "SR\u2191", "20.7", "17.9", "22.4", "22.7", "24.2", "26.0", "21.4", "21.7", "SPL\u2191", "11.0", "9.1", "17.7", "18.3", "16.6", "16.5", "7.9", "8.8", "CLS\u2191", "25.9", "26.2", "37.1", "36.4", "40.9", "37.7", "48.4", "49.0", "NDTW\u2191", "20.5", "20.8", "26.6", "26.1", "16.2", "21.9", "30.8", "32.6", "SDTW\u2191", "7.7", "7.2", "8.2", "8.4", "6.8", "8.5", "11.2", "11.2", "R", "2R", "\u2192", "R", "4R", "PL", "28.6", "28.9", "13.2", "14.1", "15.5", "29.7", "19.5", "17.9", "NE\u2193", "9.1", "9.0", "9.2", "9.3", "8.4", "9.1", "8.9", "8.9", "SR\u2191", "18.3", "16.7", "14.7", "15.2", "19.2", "13.3", "22.5", "21.4", "SPL\u2191", "7.9", "7.4", "8.9", "8.9", "10.1", "7.7", "12.6", "11.9", "CLS\u2191", "29.8", "30.0", "42.5", "41.2", "46.4", "41.8", "50.3", "51.0", "NDTW\u2191", "25.1", "25.3", "33.3", "32.4", "31.6", "33.5", "38.9", "40.3", "SDTW\u2191", "7.1", "6.7", "7.3", "7.2", "9.8", "7.2", "14.5", "13.8", "R", "2R", "+", "A", "L", "K", "B", "A", "B", "Y", "W", "A", "L", "K", "B", "A", "B", "Y", "W", "+", "FA", "ST", "L", "+", "R", "E", "G", "R", "E", "T", "FU", "Y", ")+", "E", "L", "IT", "R", "C", "M", "(F", "ID", "O", "A", "L", ")+", "R", "C", "M", "(G", "SF", "+", "E", "Q", "SE", "Q", "2S", "s", "M", "et", "ric", "D", "at", "as", "et", "s"], "regionBoundary": {"x2": 521.0, "y1": 63.0, "x1": 94.0, "y2": 712.3820190429688}, "caption": "Table 10: Transfer results of R2R, R4R, R6R, R8R trained model evaluated on their complementary unseen validation datasets (+: pre-trained with data augmentation; : reimplemented or readapted from the original authors\u2019 released code).", "page": 16}, {"figType": "Figure", "name": "6", "captionBoundary": {"x2": 516.2382202148438, "y1": 273.3127136230469, "x1": 81.21620178222656, "y2": 279.315185546875}, "imageText": ["HUMAN", "BABYWALK", "RCM", "SF", "SEQ2SEQ"], "regionBoundary": {"x2": 519.0, "y1": 68.09707641601562, "x1": 78.0, "y2": 261.0}, "caption": "Figure 6: Trajectories by human experts and VLN agents on two navigation tasks. More are in the Appendix.", "page": 8}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 525.5189819335938, "y1": 398.6330261230469, "x1": 306.9295959472656, "y2": 404.635498046875}, "imageText": ["Avg", "#", "BABY-STEPs", "1.8", "3.6", "5.6", "7.4", "Train", "seen", "instr.", "14,039", "233,532", "89,632", "94,731", "Val", "unseen", "instr.", "2,349", "45,234", "35,777", "43,273", "Avg", "instr.", "length", "29.4", "58.4", "91.2", "121.6", "R2R", "R4R", "R6R", "R8R"], "regionBoundary": {"x2": 524.0, "y1": 318.0, "x1": 309.0, "y2": 387.0}, "caption": "Table 1: Datasets used for VLN learning and evaluation", "page": 4}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 525.5150756835938, "y1": 519.92529296875, "x1": 307.23858642578125, "y2": 535.8908081054688}, "imageText": [], "regionBoundary": {"x2": 522.0, "y1": 425.0, "x1": 308.0, "y2": 507.0}, "caption": "Figure 4: The distribution of lengths of instructions and ground-truth trajectories in our datasets.", "page": 4}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 527.1605834960938, "y1": 229.48660278320312, "x1": 71.96209716796875, "y2": 295.26544189453125}, "imageText": ["Baby", "Walk", "Baby", "Walk", "Baby", "Walk"], "regionBoundary": {"x2": 525.0, "y1": 68.0, "x1": 74.0, "y2": 202.0}, "caption": "Figure 3: Two-phase learning by BABYWALK. (Left) An example instruction-trajectory pair from the R4R dataset is shown. The long instruction is segmented into four BABY-STEP instructions. We use those BABYSTEPs for imitation learning (\u00a7 4.2.1) (Right) Curriculum-based RL. The BABYWALK agent warm-starts from the imitation learning policy, and incrementally learns to handle longer tasks by executing consecutive BABY-STEPs and getting feedback from external rewards (c.f . \u00a7 4.2.2). We illustrate two initial RL lectures using the left example.", "page": 4}, {"figType": "Table", "name": "9", "captionBoundary": {"x2": 291.47576904296875, "y1": 470.6671142578125, "x1": 71.6530990600586, "y2": 512.5347900390625}, "imageText": ["\u2192", "R", "8R", "PL", "40.0", "53.0", "12.4", "42.3", "35.6", "39.1", "NE\u2193", "9.9", "10.1", "10.2", "10.7", "9.6", "9.9", "SR\u2191", "20.2", "18.6", "19.7", "18.2", "22.3", "22.0", "SPL\u2191", "12.4", "9.8", "15.4", "5.3", "7.3", "7.0", "CLS\u2191", "19.8", "16.3", "25.7", "37.2", "46.4", "46.4", "NDTW\u2191", "15.8", "13.5", "19.4", "21.6", "29.6", "28.3", "SDTW\u2191", "5.1", "4.4", "5.8", "7.6", "10.4", "10.1", "R", "8R", "\u2192", "R", "6R", "PL", "34.1", "43.4", "11.8", "28.0", "28.4", "27.2", "NE\u2193", "9.5", "9.6", "9.2", "9.4", "9.4", "9.3", "SR\u2191", "18.1", "17.8", "18.2", "20.5", "21.7", "22.0", "SPL\u2191", "9.6", "7.9", "14.8", "7.4", "7.8", "8.1", "CLS\u2191", "23.4", "20.3", "31.6", "39.0", "47.1", "47.4", "NDTW\u2191", "19.3", "17.8", "25.9", "25.8", "32.6", "33.4", "SDTW\u2191", "6.5", "5.9", "7.6", "9.5", "11.5", "11.8", "R", "6R", "\u2192", "R", "4R", "PL", "28.5", "26.1", "12.3", "26.4", "23.8", "19.0", "NE\u2193", "8.5", "8.3", "7.9", "8.4", "7.9", "8.2", "SR\u2191", "25.7", "24.9", "28.7", "24.7", "29.6", "27.3", "SPL\u2191", "14.1", "16.0", "22.1", "11.6", "14.0", "14.7", "CLS\u2191", "20.7", "23.6", "36.3", "39.2", "47.8", "49.4", "NDTW\u2191", "20.6", "22.7", "31.3", "31.3", "38.1", "39.6", "SDTW\u2191", "9.0", "9.2", "13.2", "13.7", "18.1", "17.3", "R", "4R", "\u2192", "R", "2R", "PL", "15.8", "15.6", "11.1", "10.2", "10.7", "10.2", "NE\u2193", "6.7", "6.5", "6.2", "6.2", "6.2", "5.9", "SR\u2191", "33.6", "35.8", "42.4", "42.1", "42.6", "43.8", "SPL\u2191", "25.5", "27.6", "38.6", "38.6", "38.3", "39.6", "CLS\u2191", "38.5", "39.8", "52.7", "52.6", "52.9", "54.4", "NDTW\u2191", "39.2", "41.0", "51.0", "50.8", "53.4", "55.3", "SDTW\u2191", "24.9", "27.2", "33.5", "34.4", "35.7", "36.9", "R", "2R", "+", "A", "L", "K", "B", "A", "B", "Y", "W", "A", "L", "K", "B", "A", "B", "Y", "W", "Y", ")+", "E", "L", "IT", "R", "C", "M", "(F", "ID", "O", "A", "L", ")+", "R", "C", "M", "(G", "SF", "+", "E", "Q", "SE", "Q", "2S", "s", "M", "et", "ric", "D", "at", "as", "et", "s"], "regionBoundary": {"x2": 283.0, "y1": 62.0, "x1": 79.0, "y2": 459.0}, "caption": "Table 9: Indomain results. Each model is trained on the training set of R2R, R4R, R6R and R8R datasets, and evaluated on the corresponding unseen validation set (+: pre-trained with data augmentation).", "page": 15}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 405.53622775607636, "y1": 291.3841671413845, "x1": 99.51841566297743, "y2": 432.5565761990017}, "name": "1", "caption_text": "Figure 1: Performance of various VLN agents on generalizing from shorter navigation tasks to longer ones. The vertical axis is the newly proposed path-following metric SDTW (Magalhaes et al., 2019), the higher the better. BABYWALK generalizes better than other approaches across different lengths of navigation tasks. Meanwhile, it get very close to the performances of the in-domain agents (the dashed line). Please refer to the texts for details.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 391.0, "y1": 95.0, "x1": 112.0, "y2": 269.0}, "page": 1, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.39254082573785, "y1": 219.22153896755643, "x1": 99.94735717773438, "y2": 360.3948805067274}, "name": "2", "caption_text": "Figure 2: The BABYWALK agent has a memory buffer storing its past experiences of instructions xm, and its trajectory y\u0302m. When a new BABY-STEP xm is presented, the agent retrieves from the memory a summary of its experiences as the history context. It takes actions conditioning on the context (as well as its state st and the previous action a\u0302t). Upon finishing following the instruction. the trajectory y\u0302m is then sent to the memory to be remembered.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 399.0, "y1": 89.0, "x1": 105.0, "y2": 199.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.167477077908, "y1": 318.7313927544488, "x1": 99.94735717773438, "y2": 410.09089152018225}, "name": "3", "caption_text": "Figure 3: Two-phase learning by BABYWALK. (Left) An example instruction-trajectory pair from the R4R dataset is shown. The long instruction is segmented into four BABY-STEP instructions. We use those BABYSTEPs for imitation learning (\u00a7 4.2.1) (Right) Curriculum-based RL. The BABYWALK agent warm-starts from the imitation learning policy, and incrementally learns to handle longer tasks by executing consecutive BABY-STEPs and getting feedback from external rewards (c.f . \u00a7 4.2.2). We illustrate two initial RL lectures using the left example.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 729.0, "y1": 89.0, "x1": 102.0, "y2": 280.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.887474907769, "y1": 553.656980726454, "x1": 426.29110548231336, "y2": 561.9937472873264}, "name": "1", "caption_text": "Table 1: Datasets used for VLN learning and evaluation", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 727.0, "y1": 442.0, "x1": 426.0, "y2": 554.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.8820495605469, "y1": 722.1184624565972, "x1": 426.72025892469617, "y2": 744.2927890353733}, "name": "4", "caption_text": "Figure 4: The distribution of lengths of instructions and ground-truth trajectories in our datasets.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 725.0, "y1": 590.0, "x1": 428.0, "y2": 704.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.866451687283, "y1": 297.7217356363932, "x1": 99.4495603773329, "y2": 355.8726840549045}, "name": "2", "caption_text": "Table 2: VLN agents trained on the R4R dataset and evaluated on the unseen portion of the R4R (in-domain) and the other 3 out-of-the-domain datasets: R2R, R6R and R8R with different distributions in instruction length. The Appendix has more comparisons. (+: pre-trained with data augmentation. : reimplemented or adapted from the original authors\u2019 public codes).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 729.0, "y1": 94.0, "x1": 100.0, "y2": 298.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.11898125542535, "y1": 283.23003980848523, "x1": 99.94735717773438, "y2": 308.17252265082465}, "name": "5", "caption_text": "Figure 5: Performance by various agents on navigation tasks in different lengths. See texts for details.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 398.0, "y1": 88.0, "x1": 103.0, "y2": 265.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.1694268120659, "y1": 258.902104695638, "x1": 426.29110548231336, "y2": 317.05195109049475}, "name": "4", "caption_text": "Table 4: BABYWALK\u2019s performances with curriculumbased reinforcement learning (CRL), which improves imitation learning without or with reinforcement learning (IL+RL).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 727.0, "y1": 94.0, "x1": 426.0, "y2": 259.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.397711859809, "y1": 522.844484117296, "x1": 99.51819313897026, "y2": 547.7855682373047}, "name": "3", "caption_text": "Table 3: The memory buffer is beneficial to generalizing to different tasks from on which the agent is trained.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 401.0, "y1": 337.0, "x1": 100.0, "y2": 523.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.169935438368, "y1": 512.1596018473307, "x1": 426.29110548231336, "y2": 586.913341946072}, "name": "5", "caption_text": "Table 5: (Top) BABYWALK trained on R2R is nearly as effective as the agent trained on R4R when generalizing to longer tasks. (Bottom) BABYWALK trained on R2R adapts to R4R better than the agent trained in the reverse direction.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 727.0, "y1": 346.0, "x1": 429.0, "y2": 495.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 716.9975280761719, "y1": 379.60099114312067, "x1": 112.80028025309244, "y2": 387.93775770399304}, "name": "6", "caption_text": "Figure 6: Trajectories by human experts and VLN agents on two navigation tasks. More are in the Appendix.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 721.0, "y1": 86.0, "x1": 109.0, "y2": 363.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.1623060438368, "y1": 347.5454966227213, "x1": 99.94735717773438, "y2": 438.9045715332031}, "name": "7", "caption_text": "Figure 7: Our network architecture at the m-th BABY-STEP sub-task. Red line represents the procedure of encoding context variable zm via summarizing the BABY-STEP trajectory fSUMMARY(v(y\u03021), . . . , v(y\u0302m\u22121)) and the corresponding (micro)instruction fSUMMARY(u(x1), . . . , u(xm\u22121)) in the memory buffer. Blue line represents the procedure of encoding the (micro)instruction u(xm) of the current BABY-STEP. Purple line represents the detailed decision making process of our BABYWALK policy (Ast is denoted as the set of navigable directions at st as defined by Fried et al. (2018))", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 721.0, "y1": 98.0, "x1": 108.0, "y2": 320.0}, "page": 12, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.8745049370659, "y1": 645.1168484157986, "x1": 426.2227376302083, "y2": 703.2665676540798}, "name": "6", "caption_text": "Table 6: Sanity check of model trained on R2R and evaluated on its validation unseen split (+: pre-trained with data augmentation; :reimplemented or readapted from the original authors\u2019 released code).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 729.0, "y1": 405.0, "x1": 426.0, "y2": 645.0}, "page": 13, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.169681125217, "y1": 710.4198455810547, "x1": 426.29110548231336, "y2": 751.9665188259548}, "name": "8", "caption_text": "Table 8: BABYWALK Agent performances between different segmentation rules (trained on R4R). Refer to text for more details.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 720.0, "y1": 86.0, "x1": 437.0, "y2": 693.0}, "page": 14, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.093507554796, "y1": 738.257344563802, "x1": 99.51819313897026, "y2": 763.1983439127604}, "name": "7", "caption_text": "Table 7: Ablation on BABYWALK after each learning stage (trained on R4R).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 386.0, "y1": 86.0, "x1": 118.0, "y2": 721.0}, "page": 14, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 404.82745700412323, "y1": 653.7043253580729, "x1": 99.51819313897026, "y2": 711.8538750542534}, "name": "9", "caption_text": "Table 9: Indomain results. Each model is trained on the training set of R2R, R4R, R6R and R8R datasets, and evaluated on the corresponding unseen validation set (+: pre-trained with data augmentation).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 393.0, "y1": 86.0, "x1": 110.0, "y2": 637.0}, "page": 15, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.1601867675781, "y1": 1012.1883816189236, "x1": 99.51819313897026, "y2": 1053.733656141493}, "name": "10", "caption_text": "Table 10: Transfer results of R2R, R4R, R6R, R8R trained model evaluated on their complementary unseen validation datasets (+: pre-trained with data augmentation; : reimplemented or readapted from the original authors\u2019 released code).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 723.0, "y1": 87.0, "x1": 131.0, "y2": 992.0}, "page": 16, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 670.9225972493489, "y1": 963.5245429144965, "x1": 158.88500213623047, "y2": 971.8612670898438}, "name": "8", "caption_text": "Figure 8: Additional trajectories by human experts and VLN agents on two navigation tasks.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 722.0, "y1": 169.0, "x1": 107.0, "y2": 946.0}, "page": 17, "dpi": 0}], "error": null, "pdf": "/work/host-output/9c57f2981310db8ac5ed1d2ea9a953ec6106c116/2020.acl-main.229.pdf", "dpi": 100}