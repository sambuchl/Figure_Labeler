{"raw_detected_boxes": [[{"x2": 723.0, "y1": 311.0, "x1": 431.0, "y2": 485.0}], [], [{"x2": 721.0, "y1": 93.0, "x1": 103.0, "y2": 363.0}], [], [{"x2": 701.0, "y1": 93.0, "x1": 127.0, "y2": 272.0}], [{"x2": 395.0, "y1": 87.0, "x1": 104.0, "y2": 355.0}], [{"x2": 728.0, "y1": 90.0, "x1": 429.0, "y2": 476.0}, {"x2": 400.0, "y1": 92.0, "x1": 100.0, "y2": 278.0}], [{"x2": 693.0, "y1": 92.0, "x1": 135.0, "y2": 311.0}], [], [], [], [], [{"x2": 704.0, "y1": 157.0, "x1": 453.0, "y2": 342.0}], [{"x2": 726.0, "y1": 355.0, "x1": 432.0, "y2": 478.0}, {"x2": 715.0, "y1": 574.0, "x1": 442.0, "y2": 709.0}, {"x2": 722.0, "y1": 138.0, "x1": 436.0, "y2": 258.0}], [{"x2": 722.0, "y1": 119.0, "x1": 103.0, "y2": 455.0}, {"x2": 726.0, "y1": 589.0, "x1": 100.0, "y2": 1008.0}], [{"x2": 714.0, "y1": 183.0, "x1": 103.0, "y2": 932.0}]], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Figure", "name": "1", "captionBoundary": {"x2": 527.286865234375, "y1": 363.9495544433594, "x1": 306.9469909667969, "y2": 429.72796630859375}, "imageText": [], "regionBoundary": {"x2": 526.0, "y1": 221.8900146484375, "x1": 306.0, "y2": 351.8900146484375}, "caption": "Figure 1: MORPHEUS looks at each noun, verb, or adjective in the sentence and selects the inflected form (marked in red) that maximizes the target model\u2019s loss. To maximize semantic preservation, MORPHEUS only considers inflections belonging to the same universal part of speech as the original word.", "page": 0}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 285.5528869628906, "y1": 268.0745544433594, "x1": 76.40799713134766, "y2": 274.0770263671875}, "imageText": ["BERTSQuAD", "2", "SpanBERTSQuAD", "2", "87.71", "82.49", "SpanBERTSQuAD", "2", "BERTSQuAD", "2", "81.52", "75.54", "Original", "Transfer", "Clean", "MORPHEUS", "SQuAD", "2.0", "All", "Questions", "(F1)", "SpanBERTSQuAD", "1.1", "GloVe-BiDAF", "78.67", "71.41", "BERTSQuAD", "1.1", "93.14", "87.48", "BERTSQuAD", "2", "81.19", "70.05", "SpanBERTSQuAD", "2", "88.52", "77.89", "GloVe-BiDAF", "78.67", "71.33", "SpanBERTSQuAD", "1.1", "91.88", "88.68", "BERTSQuAD", "2", "81.19", "69.68", "SpanBERTSQuAD", "2", "88.52", "80.11", "BERTSQuAD", "1.1", "BERTSQuAD", "1.1", "93.14", "89.67", "SpanBERTSQuAD", "1.1", "91.88", "90.75", "BERTSQuAD", "2", "81.19", "72.21", "SpanBERTSQuAD", "2", "88.52", "81.95", "GloVe-", "BiDAF", "Original", "Transfer", "Clean", "MORPHEUS", "SQuAD", "2.0", "Answerable", "Questions", "(F1)"], "regionBoundary": {"x2": 288.0, "y1": 62.8900146484375, "x1": 75.0, "y2": 255.8900146484375}, "caption": "Table 3: Transferability of our adversarial examples.", "page": 5}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 527.2009887695312, "y1": 345.30853271484375, "x1": 72.0, "y2": 375.2209777832031}, "imageText": [], "regionBoundary": {"x2": 526.0, "y1": 77.8900146484375, "x1": 72.0, "y2": 333.8900146484375}, "caption": "Figure 4: Effect of shuffling the inflection list on the adversarial distribution. We observe that shuffling the inflection list induces a more uniform inflectional distribution by reducing the higher frequency inflections and boosting the lower frequency ones.", "page": 14}, {"figType": "Table", "name": "9", "captionBoundary": {"x2": 525.5472412109375, "y1": 737.8335571289062, "x1": 71.69100189208984, "y2": 743.8359985351562}, "imageText": ["Original", "Source", "Cue", "stories", "about", "passport", "controls", "at", "Berwick", "and", "a", "barbed", "wire", "border", "along", "Hadrian\u2019s", "Wall.", "Adversarial", "Source", "Cue", "story", "about", "passport", "controls", "at", "Berwick", "and", "a", "barbed", "wires", "borders", "along", "Hadrian\u2019s", "Walls.", "Original", "Translation", "Cue", "histoires", "sur", "le", "contro\u0302le", "des", "passeports", "a\u0300", "Berwick", "et", "une", "frontie\u0300re", "de", "barbele\u0301s", "le", "long", "du", "mur", "d\u2019Hadrien.", "Original", "Source", "The", "Guangzhou-based", "New", "Express", "made", "a", "rare", "public", "plea", "for", "the", "release", "of", "journalist", "Chen", "Yongzhou.", "Adversarial", "Source", "The", "Guangzhou-based", "New", "Expresses", "making", "a", "rare", "public", "plea", "for", "the", "release", "of", "journalist", "Chen", "Yongzhou.", "Original", "Translation", "Le", "New", "Express,", "base\u0301", "a\u0300", "Guangzhou,", "a", "lance\u0301", "un", "rare", "appel", "public", "en", "faveur", "de", "la", "libe\u0301ration", "du", "journaliste", "Chen", "Yongzhou.", "Original", "Source", "Intersex", "children", "pose", "ethical", "dilemma.", "Adversarial", "Source", "Intersex", "child", "posing", "ethical", "dilemma.", "Original", "Translation", "Les", "enfants", "intersexuels", "posent", "un", "dilemme", "e\u0301thique.", "Original", "Source", "According", "to", "Detroit", "News,", "the", "queen", "of", "Soul", "will", "be", "performing", "at", "the", "Sound", "Board", "hall", "of", "MotorCity", "Casino", "Hotel", "on", "21", "December.", "Adversarial", "Source", "Accorded", "to", "Detroit", "News,", "the", "queen", "of", "Soul", "will", "be", "performing", "at", "the", "Sound", "Board", "hall", "of", "MotorCity", "Casino", "Hotel", "on", "21", "December.", "Original", "Translation", "Selon", "Detroit", "News,", "la", "reine", "de", "Soul", "se", "produira", "au", "Sound", "Board", "Hall", "de", "l\u2019ho\u0302tel", "MotorCity", "Casino", "le", "21", "de\u0301cembre."], "regionBoundary": {"x2": 526.0, "y1": 417.8900146484375, "x1": 72.0, "y2": 725.8900146484375}, "caption": "Table 9: Some of the adversaries that caused Transformer-big to output the source sentence instead of a translation.", "page": 14}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 290.2704772949219, "y1": 212.02053833007812, "x1": 71.69100189208984, "y2": 229.97802734375}, "imageText": ["Highly", "Likely", "52.82%", "62.30%", "33.84%", "40.76%", "Likely", "20.51%", "18.71%", "36.15%", "33.84%", "Somewhat", "Likely", "11.02%", "7.94%", "22.82%", "19.48%", "Somewhat", "Unlikely", "6.92%", "6.15%", "5.38%", "4.35%", "Unlikely", "3.58%", "3.07%", "1.53%", "1.28%", "Highly", "Unlikely", "5.12%", "1.79%", "0.25%", "0.25%", "Native", "U.S.", "English", "Speakers", "Unrestricted", "SQuAD", "2.0", "newstest2014", "SQuAD", "2.0", "newstest2014", "Semantic", "Equivalence", "Native", "11.58%", "25.64%", "22.82%", "32.56%", "L2", "Speaker", "42.82%", "42.30%", "53.58%", "52.82%", "Beginner", "31.79%", "23.33%", "17.17%", "10.25%", "Non-human", "13.84%", "8.71%", "6.41%", "4.35%", "Native", "U.S.", "English", "Speakers", "Unrestricted", "SQuAD", "2.0", "newstest2014", "SQuAD", "2.0", "newstest2014", "Plausibility"], "regionBoundary": {"x2": 295.0, "y1": 62.8900146484375, "x1": 72.0, "y2": 199.8900146484375}, "caption": "Table 4: Human judgements for adversarial examples that caused a significant degradation in performance.", "page": 6}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 527.2900390625, "y1": 358.6585388183594, "x1": 306.967041015625, "y2": 424.43695068359375}, "imageText": ["(b)", "SQuAD", "2.0", "training", "set", "(a)", "SQuAD", "2.0", "dev", "set"], "regionBoundary": {"x2": 526.0, "y1": 61.8900146484375, "x1": 307.0, "y2": 343.34100341796875}, "caption": "Figure 2: Comparison of inflectional distributions for SpanBERTSQuAD 2. The adversarial distributions include only examples that degrade model performance. To make the best use of limited space, we omit the RBR, RBS, and NNPS tags since they do not vary much across distributions. Full figures in Appendix D.", "page": 6}, {"figType": "Table", "name": "6", "captionBoundary": {"x2": 527.2001953125, "y1": 198.15457153320312, "x1": 306.9670104980469, "y2": 229.56207275390625}, "imageText": ["newstest2014", "43.16", "20.57", "(-56.25%)", "20.85", "(-51.69%)", "Dataset", "Clean", "Morpheusseq", "Morpheusparallel"], "regionBoundary": {"x2": 524.0, "y1": 157.8900146484375, "x1": 308.0, "y2": 185.8900146484375}, "caption": "Table 6: Results of the parallel and sequential approaches to implementing MORPHEUS on SpanBERTSQuAD 2 and Transformer-big.", "page": 13}, {"figType": "Table", "name": "7", "captionBoundary": {"x2": 525.546630859375, "y1": 356.6375427246094, "x1": 306.9670104980469, "y2": 388.0439758300781}, "imageText": ["No", "SQuAD", "2.0", "Ans", "84.52", "83.15", "(-1.62%)", "SQuAD", "2.0", "All", "87.12", "86.03", "(-1.25%)", "Yes", "SQuAD", "2.0", "Ans", "86.80", "85.17", "(-1.87%)", "SQuAD", "2.0", "All", "86.00", "84.72", "(-1.48%)", "Weighted", "Dataset", "Clean", "Morpheusorig", "SpanBERTSQuAD", "2", "(F1)"], "regionBoundary": {"x2": 524.0, "y1": 255.8900146484375, "x1": 308.0, "y2": 344.8900146484375}, "caption": "Table 7: Comparison of results from using weighted vs. uniform random sampling to the create adversarial training set for fine-tuning SpanBERTSQuAD 2", "page": 13}, {"figType": "Table", "name": "8", "captionBoundary": {"x2": 525.54638671875, "y1": 522.7075805664062, "x1": 306.9670104980469, "y2": 552.6199951171875}, "imageText": ["Full", "43.16", "40.60", "31.99", "Subset", "Original", "Clean", "Morpheusorig", "1", "20", "43.16", "30.90", "24.95", "1", "4", "43.16", "36.59", "29.46"], "regionBoundary": {"x2": 515.0, "y1": 432.8900146484375, "x1": 318.0, "y2": 510.8900146484375}, "caption": "Table 8: Results from adversarially fine-tuning Tranformer-big on different subsets of the original training set.", "page": 13}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 526.79248046875, "y1": 260.1365661621094, "x1": 71.6709976196289, "y2": 325.91510009765625}, "imageText": ["Original", "Israeli", "warplanes", "struck", "a", "target", "inside", "the", "Syrian", "port", "city", "of", "Latakia", "Thursday", "night,", "a", "senior", "administration", "of\ufb01cial", "con\ufb01rms", "to", "Fox", "News.", "Adversary", "Israeli", "warplanes", "strikes", "a", "target", "inside", "the", "Syrian", "port", "city", "of", "Latakia", "Thursday", "night,", "a", "senior", "administration", "of\ufb01cial", "con\ufb01rms", "to", "Foxes", "News.", "Prediction", "Before:", "Un", "haut", "responsable", "de", "l\u2019administration", "con\ufb01rme", "a\u0300", "Fox", "News", "que", "des", "avions", "de", "combat", "israe\u0301liens", "ont", "frappe\u0301", "une", "cible", "a\u0300", "l\u2019inte\u0301rieur", "de", "la", "ville", "portuaire", "syrienne", "de", "Lattaquie\u0301", "dans", "la", "nuit", "de", "jeudi.", "After:", "Le", "pre\u0301sident", "de", "la", "Re\u0301publique,", "Nicolas", "Sarkozy,", "a", "annonce\u0301", "jeudi", "que", "le", "pre\u0301sident", "de", "la", "Re\u0301publique,", "Nicolas", "Sarkozy,", "s\u2019est", "rendu", "en", "Re\u0301publique", "de\u0301mocratique", "du", "Congo.", "Neural", "Machine", "Translation", "Original", "Who", "upon", "arriving", "gave", "the", "original", "viking", "settlers", "a", "common", "identity?", "Adversary", "Who", "upon", "arrive", "give", "the", "original", "viking", "settler", "a", "common", "identities?", "Prediction", "Before:", "Rollo", "After:", "almost", "no", "foreign", "settlers", "Original", "When", "is", "the", "suspended", "team", "scheduled", "to", "return?", "Adversary", "When", "are", "the", "suspended", "team", "schedule", "to", "returned?", "Prediction", "Before:", "2018", "After:", "No", "answer", "Extractive", "Question", "Answering"], "regionBoundary": {"x2": 521.0, "y1": 67.8753662109375, "x1": 74.0, "y2": 251.8900146484375}, "caption": "Table 1: Adversarial examples found for BERT, SpanBERT, and Transformer-big. While not perfectly grammatical, it is plausible for English dialect and second language (L2) speakers to produce such sentences. (Top) Models trained on SQuAD 2.0 are more fragile than those trained on SQuAD 1.1, and have a bias towards predicting \u201cno answer\u201d. Examples are answerable questions and therefore present in both SQuAD 1.1 and 2.0. (Bottom) Perturbing two inflections caused Transformer-big to output a completely irrelevant sentence. In addition, adversarial examples for \u223c1.4% of the test set caused the model to output the source (English) sentences.", "page": 2}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 495.8912658691406, "y1": 262.5945129394531, "x1": 336.9280090332031, "y2": 268.59698486328125}, "imageText": [], "regionBoundary": {"x2": 526.0, "y1": 92.8900146484375, "x1": 307.0, "y2": 250.8900146484375}, "caption": "Figure 3: Amazon Mechanical Turk UI.", "page": 12}, {"figType": "Table", "name": "5", "captionBoundary": {"x2": 525.54443359375, "y1": 236.26657104492188, "x1": 71.69100189208984, "y2": 266.1790771484375}, "imageText": ["newstest2014", "43.16", "20.57", "(\u221256.25%)", "1", "39.84", "31.79", "(\u221220.20%)", "31.43", "(\u221221.10%)4", "40.60", "31.99", "(\u221221.20%)", "30.82", "(\u221224.08%)", "Dataset", "Clean", "MORPHEUS", "Epoch", "Clean", "MORPHEUSorig", "MORPHEUSadv", "Original", "Adversarially", "Fine-tuned", "Transformer-big", "(BLEU)", "SQuAD", "2.0", "All", "87.71", "73.26", "(\u221216.47%)", "1", "86.00", "84.72", "(\u22121.48%)", "82.41", "(\u22124.17%)4", "87.08", "85.93", "(\u22121.32%)", "84.71", "(\u22122.72%)", "SQuAD", "2.0", "Ans", "88.52", "69.47", "(\u221221.52%)", "1", "86.80", "85.17", "(\u22121.87%)", "82.76", "(\u22124.65%)4", "86.15", "84.93", "(\u22121.41%)", "82.92", "(\u22123.74%)", "Dataset", "Clean", "MORPHEUS", "Epoch", "Clean", "MORPHEUSorig", "MORPHEUSadv", "Original", "Adversarially", "Fine-tuned", "SpanBERTSQuAD", "2", "(F1)"], "regionBoundary": {"x2": 501.0, "y1": 62.8900146484375, "x1": 97.0, "y2": 223.8900146484375}, "caption": "Table 5: Results from adversarially fine-tuning SpanBERTSQuAD 2 and Transformer-big. MORPHEUSorig refers to the initial adversarial examples, while MORPHEUSadv refers to the new adversarial examples obtained by running MORPHEUS on the robust model. Relevant results from Table 2 reproduced here for ease of comparison.", "page": 7}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 525.8960571289062, "y1": 207.83554077148438, "x1": 71.69100189208984, "y2": 237.748046875}, "imageText": ["ConvS2S", "40.83", "27.72", "(\u221232.10%)", "17.31", "(\u221257.60%)", "Transformer-big", "43.16", "30.41", "(\u221229.54%)", "20.57", "(\u221256.25%)", "newstest2014", "En-Fr", "(BLEU)", "BERTSQuAD", "2", "81.52", "78.87", "(\u22123.25%)", "67.24", "(\u221217.51%)", "SpanBERTSQuAD", "2", "87.71", "85.46", "(\u22122.56%)", "73.26", "(\u221216.47%)", "SQuAD", "2.0", "All", "Questions", "(F1)", "GloVe-BiDAF", "78.67", "74.00", "(\u22125.93%)", "53.94", "(\u221231.43%)", "ELMo-BiDAF", "80.90", "76.81", "(\u22125.05%)", "62.17", "(\u221223.15%)", "BERTSQuAD", "1.1", "93.14", "90.90", "(\u22122.40%)", "82.79", "(\u221211.11%)", "SpanBERTSQuAD", "1.1", "91.88", "91.61", "(\u22120.29%)", "82.86", "(\u22129.81%)", "BERTSQuAD", "2", "81.19", "74.13", "(\u22128.69%)", "57.47", "(\u221229.21%)", "SpanBERTSQuAD", "2", "88.52", "84.88", "(\u22124.11%)", "69.47", "(\u221221.52%)", "SQuAD", "2.0", "Answerable", "Questions", "(F1)", "Dataset", "Model", "Clean", "Random", "MORPHEUS"], "regionBoundary": {"x2": 506.0, "y1": 62.8900146484375, "x1": 91.0, "y2": 195.8900146484375}, "caption": "Table 2: Results for MORPHEUS on QA and NMT models. The subscript in Modeldataset indicates the dataset used to fine-tune the model. Negated % decrease w.r.t. the scores on clean data are reported in parentheses for easy comparison across models. Bolded values indicate the largest % decrease.", "page": 4}, {"figType": "Figure", "name": "5", "captionBoundary": {"x2": 367.62896728515625, "y1": 689.862548828125, "x1": 229.91600036621094, "y2": 695.864990234375}, "imageText": ["(b)", "SQuAD", "2.0", "training", "set", "(a)", "SQuAD", "2.0", "dev", "set"], "regionBoundary": {"x2": 526.0, "y1": 125.8900146484375, "x1": 72.0, "y2": 674.5450439453125}, "caption": "Figure 5: Full versions of Figure 2", "page": 15}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 732.3428683810764, "y1": 505.48549228244354, "x1": 426.3152652316623, "y2": 596.8443976508246}, "name": "1", "caption_text": "Figure 1: MORPHEUS looks at each noun, verb, or adjective in the sentence and selects the inflected form (marked in red) that maximizes the target model\u2019s loss. To maximize semantic preservation, MORPHEUS only considers inflections belonging to the same universal part of speech as the original word.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 723.0, "y1": 309.0, "x1": 431.0, "y2": 487.0}, "page": 0, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 731.6562228732639, "y1": 361.300786336263, "x1": 99.54305224948459, "y2": 452.65986124674475}, "name": "1", "caption_text": "Table 1: Adversarial examples found for BERT, SpanBERT, and Transformer-big. While not perfectly grammatical, it is plausible for English dialect and second language (L2) speakers to produce such sentences. (Top) Models trained on SQuAD 2.0 are more fragile than those trained on SQuAD 1.1, and have a bias towards predicting \u201cno answer\u201d. Examples are answerable questions and therefore present in both SQuAD 1.1 and 2.0. (Bottom) Perturbing two inflections caused Transformer-big to output a completely irrelevant sentence. In addition, adversarial examples for \u223c1.4% of the test set caused the model to output the source (English) sentences.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 731.0, "y1": 86.0, "x1": 100.0, "y2": 380.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 730.4111904568142, "y1": 288.6604732937283, "x1": 99.57083596123589, "y2": 330.20562065972223}, "name": "2", "caption_text": "Table 2: Results for MORPHEUS on QA and NMT models. The subscript in Modeldataset indicates the dataset used to fine-tune the model. Negated % decrease w.r.t. the scores on clean data are reported in parentheses for easy comparison across models. Bolded values indicate the largest % decrease.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 703.0, "y1": 86.0, "x1": 113.0, "y2": 289.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 396.6012318929036, "y1": 372.3257700602213, "x1": 106.12221823798285, "y2": 380.66253662109375}, "name": "3", "caption_text": "Table 3: Transferability of our adversarial examples.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 399.0, "y1": 86.0, "x1": 104.0, "y2": 372.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.3472764756945, "y1": 498.13685946994354, "x1": 426.34311252170136, "y2": 589.4957648383246}, "name": "2", "caption_text": "Figure 2: Comparison of inflectional distributions for SpanBERTSQuAD 2. The adversarial distributions include only examples that degrade model performance. To make the best use of limited space, we omit the RBR, RBS, and NNPS tags since they do not vary much across distributions. Full figures in Appendix D.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 728.0, "y1": 89.0, "x1": 429.0, "y2": 480.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.1534406873915, "y1": 294.4729699028863, "x1": 99.57083596123589, "y2": 319.41392686631946}, "name": "4", "caption_text": "Table 4: Human judgements for adversarial examples that caused a significant degradation in performance.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 409.0, "y1": 86.0, "x1": 100.0, "y2": 295.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9228244357639, "y1": 328.14801534016925, "x1": 99.57083596123589, "y2": 369.6931627061632}, "name": "5", "caption_text": "Table 5: Results from adversarially fine-tuning SpanBERTSQuAD 2 and Transformer-big. MORPHEUSorig refers to the initial adversarial examples, while MORPHEUSadv refers to the new adversarial examples obtained by running MORPHEUS on the robust model. Relevant results from Table 2 reproduced here for ease of comparison.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 700.0, "y1": 86.0, "x1": 120.0, "y2": 328.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 688.7378692626953, "y1": 364.714601304796, "x1": 467.95556810167096, "y2": 373.0513678656684}, "name": "3", "caption_text": "Figure 3: Amazon Mechanical Turk UI.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 704.0, "y1": 140.0, "x1": 453.0, "y2": 342.0}, "page": 12, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9258761935764, "y1": 495.3299204508463, "x1": 426.3430701361762, "y2": 538.9499664306641}, "name": "7", "caption_text": "Table 7: Comparison of results from using weighted vs. uniform random sampling to the create adversarial training set for fine-tuning SpanBERTSQuAD 2", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 729.0, "y1": 354.0, "x1": 426.0, "y2": 495.0}, "page": 13, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.925537109375, "y1": 725.9827507866753, "x1": 426.3430701361762, "y2": 767.5277709960938}, "name": "8", "caption_text": "Table 8: Results from adversarially fine-tuning Tranformer-big on different subsets of the original training set.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 715.0, "y1": 574.0, "x1": 426.0, "y2": 726.0}, "page": 13, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2224934895833, "y1": 275.21468268500433, "x1": 426.3430701361762, "y2": 318.8362121582031}, "name": "6", "caption_text": "Table 6: Results of the parallel and sequential approaches to implementing MORPHEUS on SpanBERTSQuAD 2 and Transformer-big.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 728.0, "y1": 132.0, "x1": 426.0, "y2": 275.0}, "page": 13, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2235955132378, "y1": 479.5951843261719, "x1": 100.0, "y2": 521.1402469211155}, "name": "4", "caption_text": "Figure 4: Effect of shuffling the inflection list on the adversarial distribution. We observe that shuffling the inflection list induces a more uniform inflectional distribution by reducing the higher frequency inflections and boosting the lower frequency ones.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 726.0, "y1": 113.0, "x1": 103.0, "y2": 458.0}, "page": 14, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9267239040798, "y1": 1024.7688293457031, "x1": 99.57083596123589, "y2": 1033.1055535210503}, "name": "9", "caption_text": "Table 9: Some of the adversaries that caused Transformer-big to output the source sentence instead of a translation.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 730.0, "y1": 580.0, "x1": 100.0, "y2": 1025.0}, "page": 14, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 510.59578789605035, "y1": 958.1424289279513, "x1": 319.32777828640405, "y2": 966.4791531032986}, "name": "5", "caption_text": "Figure 5: Full versions of Figure 2", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 727.0, "y1": 179.0, "x1": 103.0, "y2": 940.0}, "page": 15, "dpi": 0}], "error": null, "pdf": "/work/host-output/e145a54aa12a536a87821c13360c904ab5a60f40/2020.acl-main.263.pdf", "dpi": 100}