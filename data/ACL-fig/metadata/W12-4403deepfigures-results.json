{"raw_detected_boxes": [[], [], [], [{"x2": 739.0, "y1": 100.0, "x1": 442.0, "y2": 327.0}, {"x2": 717.0, "y1": 823.0, "x1": 468.0, "y2": 876.0}], [{"x2": 733.0, "y1": 101.0, "x1": 442.0, "y2": 329.0}, {"x2": 717.0, "y1": 875.0, "x1": 468.0, "y2": 928.0}], [{"x2": 717.0, "y1": 205.0, "x1": 468.0, "y2": 239.0}, {"x2": 382.0, "y1": 290.0, "x1": 133.0, "y2": 342.0}, {"x2": 717.0, "y1": 499.0, "x1": 468.0, "y2": 552.0}], [{"x2": 390.0, "y1": 519.0, "x1": 127.0, "y2": 566.0}], [{"x2": 752.0, "y1": 111.0, "x1": 435.0, "y2": 467.0}], []], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Figure", "name": "2", "captionBoundary": {"x2": 540.3442993164062, "y1": 259.39996337890625, "x1": 313.2008056640625, "y2": 384.4795227050781}, "imageText": [], "regionBoundary": {"x2": 538.0, "y1": 72.0, "x1": 315.0, "y2": 243.0}, "caption": "Figure 2: Comparing the predictions for the String Similarity for the same candidates, to the jointly-learned model. (Coding scheme: tp = true-positive, etc.) The distribution shows that while String Similarity correlates with named-entities, it is not a clean division. Note especially the mass of true-negatives in the bottom-right corner of the graph. These would be a relatively high volume of false-positives for String Similarity alone, but the model that bootstraps knowledge of context, word-shape and alignment has little trouble distinguishing them and correctly assigning them zero-probably of being an entity.", "page": 4}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 540.1988525390625, "y1": 350.9599609375, "x1": 313.2008056640625, "y2": 380.3992004394531}, "imageText": ["Semi-supervised", "Identi\ufb01cation", "Kre\u0300yol:", "0.838", "0.902", "0.869", "English:", "0.846", "0.916", "0.880", "Classi\ufb01cation", "(micro-F)", "English:", "0.512", "0.782", "0.619", "Supervised", "English:", "0.915", "0.206", "0.336", "Unsupervised", "Precision", "Recall", "F-value", "Edit", "likelihood", "deviation", "Kre\u0300yol:", "0.619", "0.619", "0.619", "English:", "0.633", "0.633", "0.633", "Language-speci\ufb01c", "models", "Kre\u0300yol:", "0.907", "0.687", "0.781", "English:", "0.932", "0.766", "0.840", "Jointly-learned", "models", "Kre\u0300yol:", "0.904", "0.794", "0.846", "English:", "0.915", "0.813", "0.861"], "regionBoundary": {"x2": 545.0, "y1": 74.0, "x1": 313.0, "y2": 336.0}, "caption": "Table 1: A summary of the results presented in this paper showing promising new methods for unsupervised and semi-supervised named-entity recognition.", "page": 7}, {"figType": "Figure", "name": "1", "captionBoundary": {"x2": 540.302734375, "y1": 253.9999237060547, "x1": 313.2008056640625, "y2": 343.3193054199219}, "imageText": [], "regionBoundary": {"x2": 538.0, "y1": 72.0, "x1": 315.0, "y2": 238.0}, "caption": "Figure 1: A comparison of the different approaches to generating seeds from edit distance. The comparison shows that local deviation, the novel method introduced in this paper, is the most successful. With about 10% of the most confident entity candidates by Edit Likelihood Deviation or Weighted Deviation Estimate, there is greater than 95% precision, giving a clean enough division of the data to seed a model.", "page": 3}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 750.4204644097222, "y1": 352.77767181396484, "x1": 435.00111897786456, "y2": 476.83236863878034}, "name": "1", "caption_text": "Figure 1: A comparison of the different approaches to generating seeds from edit distance. The comparison shows that local deviation, the novel method introduced in this paper, is the most successful. With about 10% of the most confident entity candidates by Edit Likelihood Deviation or Weighted Deviation Estimate, there is greater than 95% precision, giving a clean enough division of the data to seed a model.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 739.0, "y1": 100.0, "x1": 442.0, "y2": 328.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 750.4781934950087, "y1": 360.2777269151476, "x1": 435.00111897786456, "y2": 533.9993370903862}, "name": "2", "caption_text": "Figure 2: Comparing the predictions for the String Similarity for the same candidates, to the jointly-learned model. (Coding scheme: tp = true-positive, etc.) The distribution shows that while String Similarity correlates with named-entities, it is not a clean division. Note especially the mass of true-negatives in the bottom-right corner of the graph. These would be a relatively high volume of false-positives for String Similarity alone, but the model that bootstraps knowledge of context, word-shape and alignment has little trouble distinguishing them and correctly assigning them zero-probably of being an entity.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 737.0, "y1": 100.0, "x1": 442.0, "y2": 329.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 750.2761840820312, "y1": 487.44439019097223, "x1": 435.00111897786456, "y2": 528.3322228325737}, "name": "1", "caption_text": "Table 1: A summary of the results presented in this paper showing promising new methods for unsupervised and semi-supervised named-entity recognition.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 756.0, "y1": 104.0, "x1": 434.0, "y2": 467.0}, "page": 7, "dpi": 0}], "error": null, "pdf": "/work/host-output/d0ea635002b6d8b29e22c4c4c94f2dda57dcec1f/W12-4403.pdf", "dpi": 100}