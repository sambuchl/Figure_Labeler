{"raw_detected_boxes": [[{"x2": 700.0, "y1": 315.0, "x1": 471.0, "y2": 547.0}], [{"x2": 725.0, "y1": 92.0, "x1": 432.0, "y2": 349.0}], [{"x2": 396.0, "y1": 97.0, "x1": 105.0, "y2": 249.0}], [{"x2": 708.0, "y1": 87.0, "x1": 453.0, "y2": 202.0}], [{"x2": 709.0, "y1": 86.0, "x1": 442.0, "y2": 240.0}, {"x2": 708.0, "y1": 306.0, "x1": 441.0, "y2": 478.0}], [{"x2": 685.0, "y1": 96.0, "x1": 146.0, "y2": 306.0}, {"x2": 722.0, "y1": 373.0, "x1": 431.0, "y2": 662.0}], [{"x2": 631.0, "y1": 95.0, "x1": 197.0, "y2": 311.0}], [], [{"x2": 616.0, "y1": 95.0, "x1": 221.0, "y2": 349.0}], [], [], [], [{"x2": 659.0, "y1": 782.0, "x1": 113.0, "y2": 953.0}]], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Figure", "name": "1", "captionBoundary": {"x2": 527.2003784179688, "y1": 407.7325439453125, "x1": 307.2760009765625, "y2": 473.5109558105469}, "imageText": ["\u2026", "\u2026", "finding", "the", "Location_Name", "of", "Locations", "table", "for", "which", "Location_Name", "contains", "\"film\"", "Film", "Castle", "Film", "Festival", "Location_Name", "Address", "is", "wrong.", "I", "want", "the", "name", "of", "the", "locations", "finding", "the", "Address", "of", "Locations", "table", "for", "which", "Location_Name", "contains", "\"film\"", "14034", "Kohler", "Drive", "770", "Edd", "Lane", "Apt.", "098", "Address", "Find", "all", "the", "locations", "whose", "names", "contain", "the", "word", "\"film\""], "regionBoundary": {"x2": 506.0, "y1": 223.8900146484375, "x1": 339.0, "y2": 393.8900146484375}, "caption": "Figure 1: An example of human interaction with a Textto-SQL system to correct the interpretation of an input utterance. The system generates an initial SQL parse, explains it in natural language, and displays the execution result. Then, the system uses the human-provided natural language feedback to correct the initial parse.", "page": 0}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 507.260498046875, "y1": 232.24551391601562, "x1": 89.97699737548828, "y2": 238.24798583984375}, "imageText": ["Paraphrase", "Feedback:", "[5.0%]", "Question:", "What", "zip", "codes", "have", "a", "station", "with", "a", "max", "temperature", "greater", "than", "or", "equal", "to", "80", "and", "when", "did", "it", "reach", "that", "temperature?", "Pred.", "SQL:", "SELECT", "zip_code", "FROM", "weather", "WHERE", "min_temperature_f", ">", "80", "OR", "min_sea_level_pressure_inches", ">", "80", "Feedback:", "Find", "date", ",", "zip", "code", "whose", "max", "temperature", "f", "greater", "than", "or", "equals", "80.", "Partial", "Feedback:", "[13.5%]", "Question:", "What", "are", "the", "names", "of", "all", "races", "held", "between", "2009", "and", "2011?", "Pred.", "SQL:", "SELECT", "country", "FROM", "circuits", "WHERE", "lat", "BETWEEN", "2009", "AND", "2011", "Feedback:", "You", "should", "use", "races", "table.", "Complete", "Feedback:", "[81.5%]", "Question:", "Show", "the", "types", "of", "schools", "that", "have", "two", "schools.", "Pred.", "SQL:", "SELECT", "TYPE", "FROM", "school", "GROUP", "BY", "TYPE", "HAVING", "count(*)", ">=", "2", "Feedback:", "You", "should", "not", "use", "greater", "than."], "regionBoundary": {"x2": 493.0, "y1": 62.8900146484375, "x1": 104.0, "y2": 219.8900146484375}, "caption": "Table 2: Examples (question, predicted SQL and feedback) of complete, partial and paraphrase feedback", "page": 5}, {"figType": "Figure", "name": "6", "captionBoundary": {"x2": 527.2899780273438, "y1": 492.5245361328125, "x1": 307.2760009765625, "y2": 534.3929443359375}, "imageText": [], "regionBoundary": {"x2": 526.0, "y1": 266.8900146484375, "x1": 307.0, "y2": 480.8900146484375}, "caption": "Figure 6: Patterns of feedback covered in our dataset. Patterns are extracted heuristically using predicates and arguments extracted from the feedback sentence. The figure shows the top 60 frequent patterns.", "page": 5}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 526.932373046875, "y1": 267.2155456542969, "x1": 307.2760009765625, "y2": 332.9930725097656}, "imageText": ["Schema:", "Location_ID", "Location_Name", "Address", "Other_Details", "Gold", "Parse:", "SELECT", "Location_Name", "FROMLOCATIONS", "WHERE", "Location_Name", "LIKE", "'%film%'", "Address", "is", "wrong.", "I", "want", "the", "name", "of", "the", "locations", "Feedback:", "Predicted", "Parse:", "SELECT", "Address", "FROM", "LOCATIONS", "WHERE", "Location_Name", "LIKE", "'%film%'", "Find", "all", "the", "locations", "whose", "names", "contain", "the", "word", "\"film\"", "Question:"], "regionBoundary": {"x2": 522.0, "y1": 69.72633361816406, "x1": 310.9526062011719, "y2": 250.8900146484375}, "caption": "Figure 2: An example from our SQL parse correction task (DB Name: cre_Theme_park and Table Name: Locations). Given a question, initial predicted parse and natural language feedback on the predicted parse, the task is to predict a corrected parse that matches the gold parse.", "page": 1}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 438.4529724121094, "y1": 236.33255004882812, "x1": 158.7830047607422, "y2": 242.33502197265625}, "imageText": ["Information", "-", "Missing", "13%", "I", "also", "need", "the", "number", "of", "different", "services", "-", "Wrong", "36%", "Return", "capacity", "in", "place", "of", "height", "-", "Unnecessary", "4%", "No", "need", "to", "return", "email", "address", "Conditions", "-", "Missing", "10%", "ensure", "they", "are", "FDA", "approved", "-", "Wrong", "19%", "need", "to", "filter", "on", "open", "year", "not", "register", "year", "-", "Unnecessary", "7%", "return", "results", "for", "all", "majors", "Aggregation", "6%", "I", "wanted", "the", "smallest", "ones", "not", "the", "largest", "Order/Uniq", "5%", "only", "return", "unique", "values", "Feedback", "Type", "%", "Example"], "regionBoundary": {"x2": 456.0, "y1": 62.8900146484375, "x1": 142.0, "y2": 223.8900146484375}, "caption": "Table 3: Examples of feedback annotators provided for different types", "page": 6}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 291.92431640625, "y1": 267.2155456542969, "x1": 72.0, "y2": 285.17303466796875}, "imageText": ["Explanation:", "Template:", "BY_col_", "ORDER", "BY", "_aggr_", "_col_", "SELECT", "_cols_", "from", "_table_", "Group", "BY", "id", "ORDER", "BY", "COUNT(*)", "DESC", "SELECT", "id,", "name", "from", "browser", "GROUP", "SQL:", "Step", "2:", "Find", "id,", "name", "of", "browser", "table", "with", "largest", "value", "in", "the", "results", "of", "step", "1.", "Step", "1:", "Find", "the", "number", "of", "rows", "of", "each", "value", "of", "id", "in", "browser", "table."], "regionBoundary": {"x2": 287.0, "y1": 71.20294952392578, "x1": 75.76390838623047, "y2": 249.8900146484375}, "caption": "Figure 3: An example of a SQL query, the corresponding template and the generated explanation.", "page": 2}, {"figType": "Figure", "name": "8", "captionBoundary": {"x2": 525.5431518554688, "y1": 431.310546875, "x1": 72.0, "y2": 461.2239990234375}, "imageText": [], "regionBoundary": {"x2": 524.0, "y1": 102.8900146484375, "x1": 74.0, "y2": 419.8900146484375}, "caption": "Figure 8: An example of the data collection interface. The Predicted SQL is: \u2019SELECT name, salary FROM instructor WHERE dept_name LIKE \"%math%\"\u2019. Note that neither the gold nor the predicted SQL are shown to the annotator.", "page": 12}, {"figType": "Figure", "name": "9", "captionBoundary": {"x2": 481.5010070800781, "y1": 713.2025756835938, "x1": 116.04299926757812, "y2": 719.2050170898438}, "imageText": ["intersect", "show", "the", "rows", "that", "are", "in", "both", "the", "results", "of", "step", "1", "and", "step", "2", "union", "show", "the", "rows", "that", "are", "in", "any", "of", "the", "results", "of", "step", "1", "and", "step", "2", "except", "show", "the", "rows", "that", "are", "in", "the", "results", "of", "step", "1", "but", "not", "in", "the", "results", "of", "step", "2", "limit", "n", "only", "keep", "the", "first", "n", "rows", "of", "the", "results", "in", "step", "1", "join", "for", "each", "row", "in", "Table", "1,", "find", "corresponding", "rows", "in", "Table", "2", "select", "find", "Column", "of", "Table", "aggregation", "find", "each", "value", "of", "Column1", "in", "Table", "along", "with", "the", "OPERATION", "of", "Column2", "of", "the", "corresponding", "rows", "to", "each", "value", "ordering", "order", "Direction", "by", "Column", "condition", "whose", "Column", "Operation", "Value", "distinct", "without", "repetition", "SQL", "Component", "Explanation"], "regionBoundary": {"x2": 526.0, "y1": 552.8900146484375, "x1": 72.0, "y2": 696.8900146484375}, "caption": "Figure 9: Examples of how different SQL components can be explained in natural language", "page": 12}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 469.9588317871094, "y1": 161.91354370117188, "x1": 362.552001953125, "y2": 167.916015625}, "imageText": ["Number", "of", "Train", "Dev", "Test", "Examples", "7,481", "871", "962", "Databases", "111", "9", "20", "Uniq.", "Questions", "2,775", "290", "506", "Uniq.", "Wrong", "Parses", "2,840", "383", "325", "Uniq.", "Gold", "Parses", "1,781", "305", "194", "Uniq.", "Feedbacks", "7,350", "860", "948", "Feedback", "tokens", "(Avg.)", "13.9", "13.8", "13.1"], "regionBoundary": {"x2": 510.0, "y1": 62.8900146484375, "x1": 323.0, "y2": 149.8900146484375}, "caption": "Table 1: SPLASH summary", "page": 3}, {"figType": "Figure", "name": "7", "captionBoundary": {"x2": 375.1851501464844, "y1": 753.3765869140625, "x1": 222.35899353027344, "y2": 759.3790283203125}, "imageText": ["12.", "The", "generated", "steps", "might", "not", "sound", "like", "the", "smartest", "way", "for", "answering", "the", "question.", "But", "it", "is", "the", "most", "systematic", "way", "that", "works", "for", "all", "kinds", "of", "questions", "and", "all", "kinds", "of", "tables.", "Please,", "do", "not", "try", "to", "propose", "smarter", "steps.", "11.", "You", "do", "not", "have", "to", "mention", "which", "steps", "contain", "mistakes.", "If", "in", "doubt,", "do", "not", "refer", "to", "a", "particular", "step.", "10.", "If", "in", "doubt", "about", "how", "to", "correct", "a", "mistake,", "just", "mention", "what", "is", "wrong.", "9.", "Do", "not", "just", "copy", "parts", "of", "the", "questions.", "Be", "precise", "in", "your", "input.", "8.", "Some", "of", "the", "mistakes", "are", "due", "to", "additional", "steps", "or", "parts", "of", "steps.", "Your", "feedback", "can", "suggest", "removing", "those", "parts.", "7.", "If", "the", "steps", "are", "correct,", "just", "check", "the", "\u201cAll", "steps", "are", "correct\u201d", "box", "6.", "There", "could", "be", "more", "than", "one", "wrong", "piece", "in", "the", "steps.", "Please,", "make", "sure", "to", "mention", "all", "of", "them", "not", "just", "one.", "5.", "We", "show", "only", "two", "example", "values", "from", "each", "table", "to", "help", "you", "understand", "the", "contents", "of", "each", "table.", "Tables", "typically", "contain", "several", "rows.", "Never", "use", "the", "shown", "values", "to", "write", "your", "input.", "4.", "Don\u2019t", "rewrite", "the", "steps", "after", "correcting", "them.", "But", "rather,", "just", "describe", "briefly", "the", "change", "that", "needs", "to", "be", "made.", "3.", "Use", "proper", "and", "fluent", "English.", "Don\u2019t", "use", "math", "symbols.", "2.", "For", "each", "question,", "we", "have", "generated", "steps", "to", "answer", "it,", "but", "it", "turned", "out", "that", "something", "is", "wrong", "with", "the", "steps.", "Your", "task", "is", "write", "down", "in", "English", "a", "short", "(one", "sentence", "most", "of", "the", "time)", "description", "of", "the", "mistakes", "and", "how", "it", "can", "be", "correct.", "It", "is", "important", "to", "note", "that", "we", "are", "not", "looking", "for", "rewritings", "of", "steps,", "but", "rather", "we", "want", "to", "get", "short", "natural", "English", "commands", "(15", "words", "at", "most)", "that", "describes", "the", "correction", "to", "be", "made", "to", "get", "the", "correct", "answer.", "1.", "We", "have", "some", "information", "stored", "in", "tables;", "each", "row", "is", "a", "record", "that", "consists", "of", "one", "or", "more", "columns.", "Using", "the", "given", "tables,", "we", "can", "answer", "questions", "by", "doing", "simple", "systematic", "processing", "steps", "over", "the", "tables.", "Notice", "that", "the", "answer", "to", "the", "question", "is", "always", "the", "result", "of", "the", "last", "step.", "Also,", "notice", "that", "the", "steps", "might", "not", "be", "in", "perfect", "English", "as", "they", "were", "generated", "automatically.", "Each", "step,", "generates", "a", "table", "of", "some", "form.", "Correcting", "Steps", "for", "Answering", "Questions."], "regionBoundary": {"x2": 526.0, "y1": 393.8900146484375, "x1": 72.0, "y2": 736.8900146484375}, "caption": "Figure 7: Crowd-sourcing instructions", "page": 11}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 433.1231384277344, "y1": 263.8295593261719, "x1": 164.11300659179688, "y2": 269.83203125}, "imageText": ["Re-ranking", "Upper", "Bound", "36.38", "59.27", "Estimated", "Human", "Accuracy", "81.50", "81.57", "\u21d2", "Seq2Struct", "N/A", "41.30", "\u21d2", "Re-ranking:", "Uniform", "2.39", "42.48", "\u21d2", "Re-ranking:", "Parser", "score", "11.26", "46.86", "\u21d2", "Re-ranking:", "Second", "Best", "11.85", "47.15", "With", "Feedback", "\u21d2", "Re-ranking:", "Handcrafted", "16.63", "49.51", "\u21d2", "Seq2Struct+Feedback", "13.72", "48.08", "\u21d2", "EditSQL+Feedback", "25.16", "53.73", "Exact", "Match", "Accuracy", "(%)", "Baseline", "Correction", "End-to-End", "Without", "Feedback"], "regionBoundary": {"x2": 443.0, "y1": 62.8900146484375, "x1": 154.0, "y2": 251.8900146484375}, "caption": "Table 4: Correction and End-to-End accuracies of Baseline models.", "page": 8}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 525.5465087890625, "y1": 186.62954711914062, "x1": 307.2760009765625, "y2": 204.5870361328125}, "imageText": ["Distance", "between", "Gold", "and", "Predicted", "SQL", "(%", ")", "nc", "y", "qu", "e", "F", "re", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "35%", "30%", "25%", "20%", "15%", "10%", "5%", "0%"], "regionBoundary": {"x2": 511.0, "y1": 62.801246643066406, "x1": 321.9114685058594, "y2": 172.271728515625}, "caption": "Figure 4: A histogram of the distance between the gold and the predicted SQL.", "page": 4}, {"figType": "Figure", "name": "5", "captionBoundary": {"x2": 527.2001953125, "y1": 356.46954345703125, "x1": 307.2760009765625, "y2": 398.33697509765625}, "imageText": ["INSERT", "REPLACE", "DELETE", "SQL", "Keywords", "%", ")", "cy", "(", "e", "n", "q", "u", "F", "re", "0.5", "0.4", "0.45", "0.3", "0.35", "0.2", "0.25", "0.1", "0.15", "0", "0.05"], "regionBoundary": {"x2": 510.0, "y1": 222.02113342285156, "x1": 320.8711853027344, "y2": 343.65936279296875}, "caption": "Figure 5: A histogram of different SQL keywords appearing in edits (between the gold and predicted SQL) and their distribution across edit types (replace, insert or delete).", "page": 4}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 732.2227478027344, "y1": 566.2951999240452, "x1": 426.772223578559, "y2": 657.6541052924262}, "name": "1", "caption_text": "Figure 1: An example of human interaction with a Textto-SQL system to correct the interpretation of an input utterance. The system generates an initial SQL parse, explains it in natural language, and displays the execution result. Then, the system uses the human-provided natural language feedback to correct the initial parse.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 703.0, "y1": 311.0, "x1": 471.0, "y2": 547.0}, "page": 0, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 731.8505181206597, "y1": 371.1327022976345, "x1": 426.772223578559, "y2": 462.4903784857856}, "name": "2", "caption_text": "Figure 2: An example from our SQL parse correction task (DB Name: cre_Theme_park and Table Name: Locations). Given a question, initial predicted parse and natural language feedback on the predicted parse, the task is to predict a corrected parse that matches the gold parse.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 725.0, "y1": 92.0, "x1": 432.0, "y2": 349.0}, "page": 1, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.450439453125, "y1": 371.1327022976345, "x1": 100.0, "y2": 396.0736592610677}, "name": "3", "caption_text": "Figure 3: An example of a SQL query, the corresponding template and the generated explanation.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 398.0, "y1": 95.0, "x1": 105.0, "y2": 266.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 652.7205997043186, "y1": 224.87992180718314, "x1": 503.54444715711804, "y2": 233.21668836805554}, "name": "1", "caption_text": "Table 1: SPLASH summary", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 708.0, "y1": 86.0, "x1": 449.0, "y2": 208.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9257066514757, "y1": 259.20770433213977, "x1": 426.772223578559, "y2": 284.1486612955729}, "name": "4", "caption_text": "Figure 4: A histogram of the distance between the gold and the predicted SQL.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 709.0, "y1": 86.0, "x1": 442.0, "y2": 240.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2224934895833, "y1": 495.0965881347656, "x1": 426.772223578559, "y2": 553.2457987467448}, "name": "5", "caption_text": "Figure 5: A histogram of different SQL keywords appearing in edits (between the gold and predicted SQL) and their distribution across edit types (replace, insert or delete).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 708.0, "y1": 306.0, "x1": 427.0, "y2": 495.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 704.5284695095486, "y1": 322.5632137722439, "x1": 124.96805191040039, "y2": 330.89998033311633}, "name": "2", "caption_text": "Table 2: Examples (question, predicted SQL and feedback) of complete, partial and paraphrase feedback", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 700.0, "y1": 86.0, "x1": 129.0, "y2": 323.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.347191704644, "y1": 684.0618557400173, "x1": 426.772223578559, "y2": 742.212422688802}, "name": "6", "caption_text": "Figure 6: Patterns of feedback covered in our dataset. Patterns are extracted heuristically using predicates and arguments extracted from the feedback sentence. The figure shows the top 60 frequent patterns.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 724.0, "y1": 373.0, "x1": 431.0, "y2": 667.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 608.9624616834852, "y1": 328.2396528455946, "x1": 220.53195105658637, "y2": 336.576419406467}, "name": "3", "caption_text": "Table 3: Examples of feedback annotators provided for different types", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 632.0, "y1": 86.0, "x1": 197.0, "y2": 328.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 601.5599144829644, "y1": 366.429943508572, "x1": 227.93473137749564, "y2": 374.76671006944446}, "name": "4", "caption_text": "Table 4: Correction and End-to-End accuracies of Baseline models.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 616.0, "y1": 86.0, "x1": 214.0, "y2": 366.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 668.7513987223307, "y1": 990.5591328938801, "x1": 161.17083231608072, "y2": 998.8958570692274}, "name": "9", "caption_text": "Figure 9: Examples of how different SQL components can be explained in natural language", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 676.0, "y1": 767.0, "x1": 100.0, "y2": 968.0}, "page": 12, "dpi": 0}], "error": null, "pdf": "/work/host-output/20b7c17d029d2c2166277cd63031cad365bdfe25/2020.acl-main.187.pdf", "dpi": 100}