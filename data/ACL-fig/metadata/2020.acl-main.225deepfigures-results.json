{"raw_detected_boxes": [[{"x2": 719.0, "y1": 314.0, "x1": 437.0, "y2": 530.0}], [], [{"x2": 728.0, "y1": 92.0, "x1": 429.0, "y2": 247.0}], [{"x2": 382.0, "y1": 88.0, "x1": 121.0, "y2": 197.0}], [{"x2": 377.0, "y1": 88.0, "x1": 126.0, "y2": 135.0}, {"x2": 389.0, "y1": 402.0, "x1": 111.0, "y2": 593.0}], [], [], [{"x2": 376.0, "y1": 95.0, "x1": 123.0, "y2": 274.0}, {"x2": 703.0, "y1": 95.0, "x1": 450.0, "y2": 273.0}, {"x2": 378.0, "y1": 429.0, "x1": 123.0, "y2": 607.0}, {"x2": 700.0, "y1": 331.0, "x1": 451.0, "y2": 508.0}], [{"x2": 374.0, "y1": 86.0, "x1": 126.0, "y2": 265.0}, {"x2": 668.0, "y1": 204.0, "x1": 488.0, "y2": 383.0}, {"x2": 373.0, "y1": 332.0, "x1": 120.0, "y2": 508.0}, {"x2": 704.0, "y1": 459.0, "x1": 440.0, "y2": 539.0}], [{"x2": 394.0, "y1": 211.0, "x1": 105.0, "y2": 866.0}]], "raw_pdffigures_output": {"regionless-captions": [{"figType": "Figure", "boundary": {"x2": 291.9244079589844, "y1": 654.819580078125, "x1": 72.0, "y2": 672.7770385742188}, "text": "Figure 5: Examples of sentence-level infills by different models.", "name": "5", "page": 9}], "figures": [{"figType": "Figure", "name": "1", "captionBoundary": {"x2": 527.2003173828125, "y1": 398.5655517578125, "x1": 307.2760009765625, "y2": 500.20892333984375}, "imageText": ["Target", "Output", "Input", "[sep]", "Target", "Data", "Input", "[sep]", "In\ufb01lling", "Language", "Model", "Train", "She", "ate", "leftover", "pasta", "for", "lunch.", "She", "ate", "[blank]", "for", "[blank].", "leftover", "pasta", "[answer]", "lunch", "[answer]", "Data", "Input", "Target"], "regionBoundary": {"x2": 519.0, "y1": 282.8900146484375, "x1": 315.0, "y2": 380.8900146484375}, "caption": "Figure 1: We consider the task of infilling, which takes incomplete text as input and outputs completed text. To tackle this task, our framework constructs training examples by masking random spans to generate pairs of inputs (text with blanks) and targets (answers for each blank). We then train unidirectional language models on the concatenation of each pair. Once trained, a model takes text input with blanks, predicts the answers, and then combines them to produce the output.", "page": 0}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 527.2003784179688, "y1": 195.68954467773438, "x1": 306.9469909667969, "y2": 321.244140625}, "imageText": ["ILM", "LM-All", "LM", "LM-Rev", "Data", "Masked", "She", "ate", "leftover", "pasta", "for", "lunch.", "[end]", ".lunch", "for", "leftover", "pasta", "ate", "She", "[end]", "She", "ate", "[blank]", "for", "[blank].", "She", "ate", "leftover", "pasta", "for", "lunch.", "[end]", "She", "ate", "[blank]", "for", "[blank].", "[sep]", "leftover", "pasta", "[answer]", "lunch", "[answer]", "She", "ate", "leftover", "pasta", "for", "lunch.", "She", "ate", "[blank]", "for", "[blank]."], "regionBoundary": {"x2": 523.0, "y1": 76.8900146484375, "x1": 310.0, "y2": 179.8900146484375}, "caption": "Figure 2: Training examples for three baseline infilling strategies and ILM on a given artificially-masked sentence. For each strategy, we train the same architecture (GPT-2) on such examples. At both training and test time, examples are fed from left to right; anything to the left of a green target is available to the model as context when predicting the target. Precisely, LM only considers past context, and LM-Rev only considers future. LM-All considers all available context but uses long sequence lengths. Our proposed ILM considers all context while using fewer tokens.", "page": 2}, {"figType": "Table", "name": "6", "captionBoundary": {"x2": 507.2041931152344, "y1": 377.7535400390625, "x1": 325.3070068359375, "y2": 383.7560119628906}, "imageText": ["LM", "(scratch)", "36.0", "65.4", "33.5", "LM-Rev", "(scratch)", "35.1", "92.2", "35.8", "LM-All", "(scratch)", "27.1", "53.8", "27.1", "ILM", "(scratch)", "26.7", "51.0", "31.0", "LM", "18.3", "27.9", "27.7", "LM-Rev", "27.1", "46.5", "34.3", "LM-All", "15.6", "22.3", "21.4", "ILM", "15.6", "22.4", "22.6", "STO", "ABS", "LYR"], "regionBoundary": {"x2": 509.0, "y1": 230.8900146484375, "x1": 324.0, "y2": 365.8900146484375}, "caption": "Table 6: Sentence infilling PPL of all models.", "page": 7}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 290.2704162597656, "y1": 455.71954345703125, "x1": 71.69100189208984, "y2": 473.677001953125}, "imageText": ["LM", "(scratch)", "34.0", "52.8", "28.9", "LM-Rev", "(scratch)", "34.9", "59.3", "30.4", "LM-All", "(scratch)", "27.0", "46.2", "24.3", "ILM", "(scratch)", "25.5", "46.0", "27.5", "LM", "17.5", "25.5", "23.9", "LM-Rev", "26.5", "39.0", "29.2", "LM-All", "15.1", "24.4", "19.3", "ILM", "14.9", "23.5", "20.2", "STO", "ABS", "LYR"], "regionBoundary": {"x2": 274.0, "y1": 308.8900146484375, "x1": 88.0, "y2": 443.8900146484375}, "caption": "Table 4: Mixture infilling PPL of all models (a mixture of all granularities).", "page": 7}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 291.92437744140625, "y1": 209.23452758789062, "x1": 71.69100189208984, "y2": 286.96807861328125}, "imageText": ["LM", "(scratch)", "33.4", "52.1", "25.1", "LM-Rev", "(scratch)", "32.9", "53.9", "24.7", "LM-All", "(scratch)", "30.4", "44.6", "26.2", "ILM", "(scratch)", "30.8", "45.3", "30.6", "LM", "17.6", "25.7", "20.8", "LM-Rev", "25.1", "36.7", "23.7", "LM-All", "17.8", "25.2", "21.5", "ILM", "18.1", "23.9", "23.0", "STO", "ABS", "LYR"], "regionBoundary": {"x2": 274.0, "y1": 62.8900146484375, "x1": 88.0, "y2": 196.8900146484375}, "caption": "Table 3: Document infilling PPL (or language modeling) of ILM and baselines initialized either from scratch or from the pre-trained checkpoint across three datasets. Note that PPL of ILM is similar to LM, implying that our infilling strategy can reasonably maintain the ability to perform language modeling while extending the ability to infill.", "page": 7}, {"figType": "Table", "name": "5", "captionBoundary": {"x2": 509.34112548828125, "y1": 209.23452758789062, "x1": 323.1700134277344, "y2": 215.23699951171875}, "imageText": ["LM", "(scratch)", "35.6", "51.5", "25.1", "LM-Rev", "(scratch)", "34.8", "65.1", "24.7", "LM-All", "(scratch)", "33.4", "45.0", "26.2", "ILM", "(scratch)", "34.3", "45.3", "30.6", "LM", "18.3", "24.2", "20.8", "LM-Rev", "26.5", "42.8", "23.7", "LM-All", "20.4", "23.4", "21.5", "ILM", "20.7", "22.5", "23.0", "STO", "ABS", "LYR"], "regionBoundary": {"x2": 509.0, "y1": 62.8900146484375, "x1": 324.0, "y2": 196.8900146484375}, "caption": "Table 5: Paragraph infilling PPL of all models.", "page": 7}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 291.9243469238281, "y1": 155.03756713867188, "x1": 71.64099884033203, "y2": 268.63714599609375}, "imageText": ["LM", "18.3", "27.9", "27.7", "1.00", "LM-Rev", "27.1", "46.5", "34.3", "1.00", "LM-All", "15.6", "22.3", "21.4", "1.81", "ILM", "15.6", "22.4", "22.6", "1.01", "STO", "ABS", "LYR", "Length"], "regionBoundary": {"x2": 275.0, "y1": 62.8900146484375, "x1": 87.0, "y2": 142.8900146484375}, "caption": "Table 1: Quantitative evaluation results. We report test set perplexity (PPL) on the sentence infilling task for different model configurations on all three datasets, as well as average length of all test set examples in tokens relative to that of the original sequence (lower is better for all columns). Our proposed ILM framework achieves better PPL than both LM and LM-Rev, implying that it is able to take advantage of both past and future context. ILM achieves similar PPL to LM-All with shorter sequence lengths (hence less memory).", "page": 3}, {"figType": "Table", "name": "7", "captionBoundary": {"x2": 269.43780517578125, "y1": 209.23452758789062, "x1": 92.52200317382812, "y2": 215.23699951171875}, "imageText": ["LM", "(scratch)", "36.1", "62.5", "34.1", "LM-Rev", "(scratch)", "36.4", "89.1", "36.3", "LM-All", "(scratch)", "26.4", "60.1", "24.3", "ILM", "(scratch)", "23.1", "49.5", "26.3", "LM", "19.2", "25.5", "28.2", "LM-Rev", "26.6", "45.0", "34.8", "LM-All", "14.5", "20.5", "18.6", "ILM", "13.8", "21.5", "18.8", "STO", "ABS", "LYR"], "regionBoundary": {"x2": 274.0, "y1": 62.8900146484375, "x1": 88.0, "y2": 196.8900146484375}, "caption": "Table 7: N-gram infilling PPL of all models.", "page": 8}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 525.5464477539062, "y1": 411.5865478515625, "x1": 307.2760009765625, "y2": 429.54400634765625}, "imageText": ["Identify", "one", "of", "the", "\ufb01ve", "sentences", "generated", "by", "machine.", "\u25cb", "Patty", "was", "excited", "about", "having", "her", "friends", "over.", "\u25cb", "She", "had", "been", "working", "hard", "preparing", "the", "food.", "\u25cb", "Patty", "knew", "her", "friends", "wanted", "pizza.", "\u25cb", "All", "of", "her", "friends", "arrived", "and", "were", "seated", "at", "the", "table.", "\u25cb", "Patty", "had", "a", "great", "time", "with", "her", "friends."], "regionBoundary": {"x2": 515.0, "y1": 322.8900146484375, "x1": 310.0, "y2": 395.8900146484375}, "caption": "Figure 4: Example of a task and instruction for human evaluation on Amazon Mechanical Turk.", "page": 8}, {"figType": "Table", "name": "8", "captionBoundary": {"x2": 264.88995361328125, "y1": 377.6275329589844, "x1": 97.06999969482422, "y2": 383.6300048828125}, "imageText": ["LM", "(scratch)", "32.3", "57.2", "34.8", "LM-Rev", "(scratch)", "31.6", "100.0", "36.7", "LM-All", "(scratch)", "12.6", "51.8", "12.5", "ILM", "(scratch)", "9.2", "37.9", "12.2", "LM", "17.1", "23.0", "28.7", "LM-Rev", "24.1", "45.0", "35.1", "LM-All", "7.5", "15.8", "9.5", "ILM", "5.4", "14.2", "8.5", "STO", "ABS", "LYR"], "regionBoundary": {"x2": 276.0, "y1": 230.8900146484375, "x1": 86.0, "y2": 364.8900146484375}, "caption": "Table 8: Word infilling PPL of all models.", "page": 8}, {"figType": "Table", "name": "9", "captionBoundary": {"x2": 485.7750549316406, "y1": 287.7255554199219, "x1": 346.73699951171875, "y2": 293.72802734375}, "imageText": ["Control", "0", "BERT", "20", "SA", "29", "LM", "(scratch)", "40", "LM", "41", "ILM", "(scratch)", "39", "ILM", "45", "Human", "78", "Score", "(%)"], "regionBoundary": {"x2": 481.0, "y1": 140.8900146484375, "x1": 352.0, "y2": 275.8900146484375}, "caption": "Table 9: Human evaluation results.", "page": 8}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 291.92425537109375, "y1": 114.38956451416016, "x1": 71.6410140991211, "y2": 263.85516357421875}, "imageText": ["Score", "(%)", "20", "29", "41", "45", "BERT", "SA", "LM", "ILM"], "regionBoundary": {"x2": 272.0, "y1": 62.8900146484375, "x1": 90.0, "y2": 101.8900146484375}, "caption": "Table 2: Human evaluation results. We use BERT (Devlin et al., 2019), the best model from Zhu et al. (2019) (SA), and our LM and ILM models to replace random sentences in five-sentence stories from the STORIES test set. Then, we task humans with identifying which sentence of the five was generated by a machine. We report the score of each model: the percentage of infilled stories where the human failed to identify the machine-generated sentence. Our ILM model achieves a higher score than all of the other models. Note that the max score is effectively 80%, as a perfect model would cause annotators to randomly choose one of the five sentences.", "page": 4}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 291.9244384765625, "y1": 440.49053955078125, "x1": 71.6410140991211, "y2": 518.2239379882812}, "imageText": ["and", "were", "seated", "at", "the", "table.", "Patty", "had", "a", "great", "time", "with", "her", "friends.", "[blank]", "All", "of", "her", "friends", "arrived", "Patty", "was", "excited", "about", "having", "her", "friends", "over.", "She", "had", "been", "working", "hard", "preparing", "the", "food.", "ILM", "Human", "favoritea", "\",", "Mary", "brightly", "said.", "She", "wasn't", "sure", "she", "had", "to", "go", "to", "the", "store.", "She", "went", "to", "check", "the", "tv.", "Patty", "knew", "her", "friends", "wanted", "pizza.", "She", "also", "had", "the", "place", "looking", "spotless.", "BERT", "SA", "LM"], "regionBoundary": {"x2": 283.0, "y1": 292.8900146484375, "x1": 80.20858001708984, "y2": 424.78350830078125}, "caption": "Figure 3: Example of a short story in our STORIES dataset with its third sentence masked, and sentences infilled by different models. The sentences generated by BERT and SA models are off-topic, the sentence generated by LM model is irrelevant to the future context, while the ones generated by ILM and Human successfully account for both previous and future context.", "page": 4}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 732.222663031684, "y1": 553.5632663302952, "x1": 426.772223578559, "y2": 694.734615749783}, "name": "1", "caption_text": "Figure 1: We consider the task of infilling, which takes incomplete text as input and outputs completed text. To tackle this task, our framework constructs training examples by masking random spans to generate pairs of inputs (text with blanks) and targets (answers for each blank). We then train unidirectional language models on the concatenation of each pair. Once trained, a model takes text input with blanks, predicts the answers, and then combines them to produce the output.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 722.0, "y1": 310.0, "x1": 437.0, "y2": 530.0}, "page": 0, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2227478027344, "y1": 271.79103427463104, "x1": 426.3152652316623, "y2": 446.17241753472223}, "name": "2", "caption_text": "Figure 2: Training examples for three baseline infilling strategies and ILM on a given artificially-masked sentence. For each strategy, we train the same architecture (GPT-2) on such examples. At both training and test time, examples are fed from left to right; anything to the left of a green target is available to the model as context when predicting the target. Precisely, LM only considers past context, and LM-Rev only considers future. LM-All considers all available context but uses long sequence lengths. Our proposed ILM considers all context while using fewer tokens.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 728.0, "y1": 92.0, "x1": 429.0, "y2": 251.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.45048183865015, "y1": 215.32995435926648, "x1": 99.50138727823892, "y2": 373.1071472167969}, "name": "1", "caption_text": "Table 1: Quantitative evaluation results. We report test set perplexity (PPL) on the sentence infilling task for different model configurations on all three datasets, as well as average length of all test set examples in tokens relative to that of the original sequence (lower is better for all columns). Our proposed ILM framework achieves better PPL than both LM and LM-Rev, implying that it is able to take advantage of both past and future context. ILM achieves similar PPL to LM-All with shorter sequence lengths (hence less memory).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 382.0, "y1": 86.0, "x1": 104.0, "y2": 214.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.45035468207465, "y1": 158.87439515855576, "x1": 99.50140847100151, "y2": 366.4655049641927}, "name": "2", "caption_text": "Table 2: Human evaluation results. We use BERT (Devlin et al., 2019), the best model from Zhu et al. (2019) (SA), and our LM and ILM models to replace random sentences in five-sentence stories from the STORIES test set. Then, we task humans with identifying which sentence of the five was generated by a machine. We report the score of each model: the percentage of infilled stories where the human failed to identify the machine-generated sentence. Our ILM model achieves a higher score than all of the other models. Note that the max score is effectively 80%, as a perfect model would cause annotators to randomly choose one of the five sentences.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 378.0, "y1": 86.0, "x1": 125.0, "y2": 142.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.4506089952257, "y1": 611.7924160427517, "x1": 99.50140847100151, "y2": 719.7554694281683}, "name": "3", "caption_text": "Figure 3: Example of a short story in our STORIES dataset with its third sentence masked, and sentences infilled by different models. The sentences generated by BERT and SA models are off-topic, the sentence generated by LM model is irrelevant to the future context, while the ones generated by ILM and Human successfully account for both previous and future context.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 393.0, "y1": 391.0, "x1": 111.0, "y2": 593.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.45052422417535, "y1": 290.60351053873694, "x1": 99.57083596123589, "y2": 398.5667758517795}, "name": "3", "caption_text": "Table 3: Document infilling PPL (or language modeling) of ILM and baselines initialized either from scratch or from the pre-trained checkpoint across three datasets. Note that PPL of ILM is similar to LM, implying that our infilling strategy can reasonably maintain the ability to perform language modeling while extending the ability to infill.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 380.0, "y1": 86.0, "x1": 106.0, "y2": 291.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 707.4182298448351, "y1": 290.60351053873694, "x1": 448.8472408718533, "y2": 298.9402770996094}, "name": "5", "caption_text": "Table 5: Paragraph infilling PPL of all models.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 707.0, "y1": 86.0, "x1": 449.0, "y2": 290.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.1533559163411, "y1": 632.9438103569878, "x1": 99.57083596123589, "y2": 657.8847249348958}, "name": "4", "caption_text": "Table 4: Mixture infilling PPL of all models (a mixture of all granularities).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 380.0, "y1": 429.0, "x1": 123.0, "y2": 616.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 704.4502682156033, "y1": 524.6576944986979, "x1": 451.8152872721354, "y2": 532.9944610595703}, "name": "6", "caption_text": "Table 6: Sentence infilling PPL of all models.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 707.0, "y1": 320.0, "x1": 450.0, "y2": 525.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 374.2191738552517, "y1": 290.60351053873694, "x1": 128.5027821858724, "y2": 298.9402770996094}, "name": "7", "caption_text": "Table 7: N-gram infilling PPL of all models.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 380.0, "y1": 86.0, "x1": 123.0, "y2": 274.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 674.6875762939453, "y1": 399.6188269721137, "x1": 481.57916598849823, "y2": 407.9555935329861}, "name": "9", "caption_text": "Table 9: Human evaluation results.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 668.0, "y1": 195.0, "x1": 482.0, "y2": 400.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 367.9027133517795, "y1": 524.4826846652561, "x1": 134.81944402058917, "y2": 532.8194512261284}, "name": "8", "caption_text": "Table 8: Word infilling PPL of all models.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 383.0, "y1": 320.0, "x1": 120.0, "y2": 525.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9256218804253, "y1": 571.6479831271702, "x1": 426.772223578559, "y2": 596.5888977050781}, "name": "4", "caption_text": "Figure 4: Example of a task and instruction for human evaluation on Amazon Mechanical Turk.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 717.0, "y1": 448.0, "x1": 430.0, "y2": 550.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.4505666097005, "y1": 909.4716389973958, "x1": 100.0, "y2": 934.4125535753038}, "name": "5", "caption_text": "Figure 5: Examples of sentence-level infills by different models.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 398.0, "y1": 211.0, "x1": 102.0, "y2": 883.0}, "page": 9, "dpi": 0}], "error": null, "pdf": "/work/host-output/b72ac38bf8d180ea1cd520fffd027fe20a5f390e/2020.acl-main.225.pdf", "dpi": 100}