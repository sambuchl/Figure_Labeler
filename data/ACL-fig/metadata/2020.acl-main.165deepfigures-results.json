{"raw_detected_boxes": [[], [], [{"x2": 703.0, "y1": 91.0, "x1": 452.0, "y2": 285.0}], [{"x2": 339.0, "y1": 246.0, "x1": 157.0, "y2": 328.0}, {"x2": 393.0, "y1": 755.0, "x1": 110.0, "y2": 907.0}], [{"x2": 385.0, "y1": 94.0, "x1": 119.0, "y2": 669.0}, {"x2": 715.0, "y1": 655.0, "x1": 442.0, "y2": 787.0}], [{"x2": 667.0, "y1": 515.0, "x1": 484.0, "y2": 670.0}], [{"x2": 718.0, "y1": 86.0, "x1": 436.0, "y2": 328.0}, {"x2": 343.0, "y1": 86.0, "x1": 161.0, "y2": 303.0}, {"x2": 389.0, "y1": 353.0, "x1": 115.0, "y2": 474.0}, {"x2": 346.0, "y1": 757.0, "x1": 153.0, "y2": 973.0}], [{"x2": 400.0, "y1": 86.0, "x1": 100.0, "y2": 261.0}, {"x2": 399.0, "y1": 453.0, "x1": 104.0, "y2": 663.0}, {"x2": 718.0, "y1": 453.0, "x1": 444.0, "y2": 533.0}, {"x2": 743.0, "y1": 671.0, "x1": 432.0, "y2": 812.0}], [{"x2": 407.0, "y1": 89.0, "x1": 110.0, "y2": 198.0}, {"x2": 398.0, "y1": 550.0, "x1": 105.0, "y2": 666.0}], [], [], [{"x2": 415.0, "y1": 191.0, "x1": 102.0, "y2": 440.0}, {"x2": 378.0, "y1": 504.0, "x1": 126.0, "y2": 620.0}]], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Figure", "name": "5", "captionBoundary": {"x2": 525.54638671875, "y1": 501.6745300292969, "x1": 307.2760009765625, "y2": 519.6319580078125}, "imageText": [], "regionBoundary": {"x2": 485.0, "y1": 368.8900146484375, "x1": 348.0, "y2": 489.8900146484375}, "caption": "Figure 5: Computational graph for training the domain proportion layers.", "page": 5}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 473.5755920410156, "y1": 250.08151245117188, "x1": 358.9360046386719, "y2": 256.083984375}, "imageText": ["E/DC", "50.64", "28.48", "17.41", "11.71", "E/DC", "+", "WL", "50.04", "28.17", "17.60", "11.59", "Encoder", "50.21", "27.94", "16.85", "12.03", "Encoder", "+", "WL", "50.11", "27.48", "16.79", "11.93", "Speech", "3.33", "4.90", "18.63", "3.08", "Thesis", "5.90", "5.55", "4.77", "11.06", "Mixed", "48.87", "26.92", "16.38", "12.09", "Embedding", "based", "Methods", "MTL", "49.14", "27.15", "16.34", "11.80", "AdvL", "48.93", "26.51", "16.18", "12.08", "PAdvL", "48.72", "27.07", "15.93", "12.23", "WDC", "+", "WL", "42.16", "25.81", "15.29", "10.14", "Our", "Domain", "Mixing", "Methods", "Laws", "51.98", "3.80", "2.38", "2.64", "News", "6.88", "31.99", "8.12", "4.17", "Method", "Laws", "News", "Speech", "Thesis", "Direct", "Training"], "regionBoundary": {"x2": 519.0, "y1": 62.8900146484375, "x1": 314.0, "y2": 237.8900146484375}, "caption": "Table 4: Chinese-to-English.", "page": 6}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 238.294677734375, "y1": 230.15554809570312, "x1": 123.66500091552734, "y2": 236.15802001953125}, "imageText": ["E/DC", "27.58", "30.33", "E/DC", "+", "WL", "27.55", "30.22", "Encoder", "27.78", "30.30", "Encoder", "+", "WL", "27.67", "30.11", "MTL", "26.90", "29.27", "AdvL", "25.68", "27.46", "PAdvL", "27.06", "29.49", "WDC", "+", "WL", "27.25", "29.43", "Our", "Domain", "Mixing", "Methods", "News", "26.09", "6.15", "TED", "4.90", "29.09", "News", "+", "TED", "26.06", "28.11", "Embedding", "based", "Methods", "Method", "News", "TED", "Direct", "Training"], "regionBoundary": {"x2": 247.0, "y1": 62.8900146484375, "x1": 115.0, "y2": 217.8900146484375}, "caption": "Table 2: English-to-German.", "page": 6}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 236.08326721191406, "y1": 712.9595947265625, "x1": 125.87699890136719, "y2": 718.9620361328125}, "imageText": ["E/DC", "+", "WL", "40.60", "54.39", "WDC", "+", "WL", "39.79", "53.85", "Our", "Domain", "Mixing", "Methods", "Encoder", "40.30", "54.05", "Encoder", "+", "WL", "40.43", "54.14", "E/DC", "40.52", "54.28", "TED", "28.22", "7.32", "Medical", "7.03", "53.73", "Medical", "+", "TED", "39.21", "53.40", "Embedding", "based", "Methods", "MTL", "39.14", "53.37", "AdvL", "39.54", "53.46", "PAdvL", "39.56", "53.23", "Method", "TED", "Medical", "Direct", "Training"], "regionBoundary": {"x2": 253.0, "y1": 544.8900146484375, "x1": 109.0, "y2": 700.8900146484375}, "caption": "Table 3: English-to-French.", "page": 6}, {"figType": "Figure", "name": "6", "captionBoundary": {"x2": 291.92431640625, "y1": 355.4375305175781, "x1": 72.0, "y2": 373.3949890136719}, "imageText": ["News+TED", "MTL", "AdvL", "PAdvL", "WDC", "w/", "WL", "Mixing:", "Encoder", "Mixing:", "E/DC", "ty", "le", "xi", "Pe", "rp", "100", "80", "60", "40", "20", "0", "0", "10", "20", "30", "40", "50", "60", "Epoches"], "regionBoundary": {"x2": 281.6412048339844, "y1": 254.2570343017578, "x1": 83.49732971191406, "y2": 340.237060546875}, "caption": "Figure 6: Perplexity v.s. Number of epochs for Englishto-German.", "page": 6}, {"figType": "Figure", "name": "1", "captionBoundary": {"x2": 519.9266967773438, "y1": 223.28652954101562, "x1": 312.8940124511719, "y2": 229.28900146484375}, "imageText": [], "regionBoundary": {"x2": 508.0, "y1": 61.8900146484375, "x1": 325.0, "y2": 211.8900146484375}, "caption": "Figure 1: Multi-head Scaled Dot-Product Attention.", "page": 2}, {"figType": "Figure", "name": "8", "captionBoundary": {"x2": 527.2003784179688, "y1": 401.90155029296875, "x1": 307.2760009765625, "y2": 455.7239685058594}, "imageText": [], "regionBoundary": {"x2": 519.0, "y1": 321.8900146484375, "x1": 314.0, "y2": 389.8900146484375}, "caption": "Figure 8: Domain proportions of a sentence pair for English-to-German task. White represents the News domain and black represents the TED domain. The domain proportions of both the encoder (bottom) and the decoder (top) are presented.", "page": 7}, {"figType": "Table", "name": "5", "captionBoundary": {"x2": 290.2703857421875, "y1": 200.26754760742188, "x1": 71.67100524902344, "y2": 218.22601318359375}, "imageText": ["Speech", "16.38", "16.15", "16.85", "Thesis", "12.09", "12.03", "12.03", "Laws", "48.87", "48.96", "50.21", "News", "26.92", "27.02", "27.94", "News", "26.06", "26.25", "27.78", "TED", "28.11", "28.27", "30.30", "English-to-French", "TED", "39.21", "39.39", "40.30", "Medical", "53.40", "53.33", "54.05", "Chinese-to-English", "Method", "Direct", "Training", "w/o", "DL", "with", "DL", "(Ours)", "English-to-Germany"], "regionBoundary": {"x2": 290.0, "y1": 62.8900146484375, "x1": 72.0, "y2": 187.8900146484375}, "caption": "Table 5: BLEU Scores with and without domain labels (DL) under equal model capacity.", "page": 7}, {"figType": "Figure", "name": "7", "captionBoundary": {"x2": 290.2705993652344, "y1": 496.7785339355469, "x1": 71.69100189208984, "y2": 526.6909790039062}, "imageText": [], "regionBoundary": {"x2": 299.0, "y1": 321.8900146484375, "x1": 72.0, "y2": 484.8900146484375}, "caption": "Figure 7: Domain proportion of a sentence from the TED domain for English-to-French task. The domain proportion is extracted from all layers of the encoder.", "page": 7}, {"figType": "Figure", "name": "9", "captionBoundary": {"x2": 527.2002563476562, "y1": 601.652587890625, "x1": 307.2760009765625, "y2": 643.52099609375}, "imageText": ["Decoder", "0.0", "1.0", "Encoder", "Layer-1", "2", "3", "4", "5", "6"], "regionBoundary": {"x2": 537.0, "y1": 484.333251953125, "x1": 307.0, "y2": 585.259033203125}, "caption": "Figure 9: Histograms of the domain proportions of each layer in our domain mixing model for English-toGerman Task. Within each histogram, 0 means pure News domain, and 1 means pure TED domain.", "page": 7}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 291.92431640625, "y1": 665.3375854492188, "x1": 72.0, "y2": 683.2960205078125}, "imageText": [], "regionBoundary": {"x2": 284.0, "y1": 540.8900146484375, "x1": 79.0, "y2": 653.8900146484375}, "caption": "Figure 3: Word-level mixing with 3 domains. For simplicity, we omit the subscripts Q, i.", "page": 3}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 290.2705993652344, "y1": 251.43551635742188, "x1": 72.0, "y2": 269.39300537109375}, "imageText": [], "regionBoundary": {"x2": 250.0, "y1": 171.8900146484375, "x1": 113.0, "y2": 239.8900146484375}, "caption": "Figure 2: The Point-wise Linear Transformations are applied at the word-level.", "page": 3}, {"figType": "Figure", "name": "11", "captionBoundary": {"x2": 274.38385009765625, "y1": 464.3935546875, "x1": 87.88400268554688, "y2": 470.3960266113281}, "imageText": [], "regionBoundary": {"x2": 272.0, "y1": 361.8900146484375, "x1": 90.0, "y2": 452.8900146484375}, "caption": "Figure 11: Two variants of layer normalization", "page": 11}, {"figType": "Table", "name": "8", "captionBoundary": {"x2": 236.22718811035156, "y1": 328.9475402832031, "x1": 125.73200988769531, "y2": 334.95001220703125}, "imageText": ["Our", "Proposed", "Mixing", "Method", "Encoder", "50.16", "27.61", "16.92", "11.85", "+", "Decoder", "50.45", "28.15", "17.45", "11.62", "PAdv", "6.58", "3.90", "2.32", "1.80", "WDC.", "w/", "WL", "7.13", "3.87", "2.45", "1.88", "Multitask", "6.16", "3.83", "1.91", "1.53", "Adversarial", "5.93", "3.38", "1.85", "1.37", "Method", "Laws", "News", "Spoken", "Thesis", "Laws", "10.37", "0.45", "0.27", "0.27", "News", "0.39", "5.12", "0.91", "0.57", "Spoken", "0.70", "1.11", "6.19", "0.83", "Thesis", "0.63", "0.25", "0.16", "1.24", "Mixed", "5.45", "4.09", "2.67", "1.85"], "regionBoundary": {"x2": 306.0, "y1": 135.8900146484375, "x1": 72.0, "y2": 316.8900146484375}, "caption": "Table 8: Chinese to English", "page": 11}, {"figType": "Table", "name": "6", "captionBoundary": {"x2": 290.27044677734375, "y1": 492.0045471191406, "x1": 71.64099884033203, "y2": 509.9620056152344}, "imageText": ["+Adv", "+", "WL", "50.24", "28.21", "16.98", "12.00", "+PAdv", "+", "WL", "48.87", "26.86", "16.14", "11.89", "+PAdvL", "49.01", "26.63", "16.06", "12.15", "+Multitask", "+", "WL", "48.75", "26.78", "16.53", "12.11", "Method", "Laws", "News", "Speech", "Thesis", "Encoder", "50.21", "27.94", "16.85", "12.03", "+MTL", "49.15", "26.82", "15.72", "11.93", "+Adv", "50.18", "27.72", "16.99", "12.16"], "regionBoundary": {"x2": 289.0, "y1": 395.8900146484375, "x1": 74.0, "y2": 479.8900146484375}, "caption": "Table 6: BLEU Scores of Domain Mixing + Domain Aware Embedding for Chinese-to-English Task", "page": 8}, {"figType": "Table", "name": "7", "captionBoundary": {"x2": 489.7551574707031, "y1": 117.5285415649414, "x1": 342.7569885253906, "y2": 123.531005859375}, "imageText": ["Laws", "\u201cArticle", "37", "The", "freedom", "of", "marriage", "...\u201d", "\u201c\u7b2c\u4e09\u5341\u4e03\u6761:\u5a5a\u59fb\u7684\u81ea\u7531...\u201d", "Media", "\u201c...", "working", "on", "an", "article", "about", "the", "poems", "...\u201d", "\u201c...", "\u6b63\u5728\u5199\u4e00\u7bc7\u8bd7\u7684\u6587\u7ae0", "...\u201d"], "regionBoundary": {"x2": 523.0, "y1": 62.8900146484375, "x1": 310.0, "y2": 105.8900146484375}, "caption": "Table 7: The ambiguity of \u201carticles\u201d.", "page": 8}, {"figType": "Figure", "name": "10", "captionBoundary": {"x2": 290.27056884765625, "y1": 163.90756225585938, "x1": 72.0, "y2": 181.86602783203125}, "imageText": [], "regionBoundary": {"x2": 301.0, "y1": 61.8900146484375, "x1": 77.0, "y2": 147.8900146484375}, "caption": "Figure 10: Back-propagation for different embedding based methods.", "page": 8}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 516.4547729492188, "y1": 578.6265869140625, "x1": 316.0570068359375, "y2": 584.6290283203125}, "imageText": ["Speech", "219K", "600", "455", "Thesis", "299K", "800", "625", "Laws", "219K", "600", "456", "News", "300K", "800", "650", "ZH-EN", "EN-DE", "News", "184K", "18K", "19KTED", "160K", "7K", "7K", "EN-FR", "TED", "226K", "10K", "10KMEDICAL", "516K", "25K", "25K", "Language", "Domain", "Train", "Valid", "Test"], "regionBoundary": {"x2": 515.0, "y1": 471.8900146484375, "x1": 318.0, "y2": 566.8900146484375}, "caption": "Table 1: The numbers of sentences in the datasets.", "page": 4}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 291.92437744140625, "y1": 500.35455322265625, "x1": 72.0, "y2": 542.2219848632812}, "imageText": [], "regionBoundary": {"x2": 284.0, "y1": 61.8900146484375, "x1": 79.0, "y2": 488.8900146484375}, "caption": "Figure 4: Illustration of Our Multi-domain NMT Model: Normalization and residual connection are omitted for simplicity. For all other detail, please refer to Vaswani et al. (2017).", "page": 4}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 722.1204121907551, "y1": 310.12017991807727, "x1": 434.57501729329425, "y2": 318.45694647894965}, "name": "1", "caption_text": "Figure 1: Multi-head Scaled Dot-Product Attention.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 704.0, "y1": 88.0, "x1": 452.0, "y2": 285.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.1536102294922, "y1": 349.2159949408637, "x1": 100.0, "y2": 374.1569519042969}, "name": "2", "caption_text": "Figure 2: The Point-wise Linear Transformations are applied at the word-level.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 346.0, "y1": 244.0, "x1": 157.0, "y2": 332.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.450439453125, "y1": 924.0799797905815, "x1": 100.0, "y2": 949.022250705295}, "name": "3", "caption_text": "Figure 3: Word-level mixing with 3 domains. For simplicity, we omit the subscripts Q, i.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 393.0, "y1": 751.0, "x1": 100.0, "y2": 924.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.45052422417535, "y1": 694.9368794759114, "x1": 100.0, "y2": 753.0860900878906}, "name": "4", "caption_text": "Figure 4: Illustration of Our Multi-domain NMT Model: Normalization and residual connection are omitted for simplicity. For all other detail, please refer to Vaswani et al. (2017).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 388.0, "y1": 93.0, "x1": 119.0, "y2": 671.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 717.2982957628038, "y1": 803.6480373806423, "x1": 438.9680650499132, "y2": 811.9847615559895}, "name": "1", "caption_text": "Table 1: The numbers of sentences in the datasets.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 715.0, "y1": 638.0, "x1": 429.0, "y2": 804.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.925537109375, "y1": 696.7701805962456, "x1": 426.772223578559, "y2": 721.7110527886284}, "name": "5", "caption_text": "Figure 5: Computational graph for training the domain proportion layers.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 667.0, "y1": 513.0, "x1": 484.0, "y2": 670.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 657.7438778347439, "y1": 347.33543395996094, "x1": 498.522228664822, "y2": 355.6722005208333}, "name": "4", "caption_text": "Table 4: Chinese-to-English.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 721.0, "y1": 86.0, "x1": 436.0, "y2": 330.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 330.96483018663196, "y1": 319.66048346625433, "x1": 171.7569457160102, "y2": 327.9972500271267}, "name": "2", "caption_text": "Table 2: English-to-German.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 343.0, "y1": 86.0, "x1": 161.0, "y2": 320.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.450439453125, "y1": 493.6632368299696, "x1": 100.0, "y2": 518.6041514078776}, "name": "6", "caption_text": "Figure 6: Perplexity v.s. Number of epochs for Englishto-German.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 391.0, "y1": 353.0, "x1": 111.0, "y2": 474.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 327.893426683214, "y1": 990.2216593424479, "x1": 174.82916514078775, "y2": 998.558383517795}, "name": "3", "caption_text": "Table 3: English-to-French.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 351.0, "y1": 757.0, "x1": 152.0, "y2": 990.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.153313530816, "y1": 278.1493716769748, "x1": 99.54306284586588, "y2": 303.0916849772135}, "name": "5", "caption_text": "Table 5: BLEU Scores with and without domain labels (DL) under equal model capacity.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 403.0, "y1": 86.0, "x1": 100.0, "y2": 278.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.1536102294922, "y1": 689.9701860215929, "x1": 99.57083596123589, "y2": 731.5152486165364}, "name": "7", "caption_text": "Figure 7: Domain proportion of a sentence from the TED domain for English-to-French task. The domain proportion is extracted from all layers of the encoder.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 407.0, "y1": 453.0, "x1": 104.0, "y2": 666.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2227478027344, "y1": 558.1965976291233, "x1": 426.772223578559, "y2": 632.949956258138}, "name": "8", "caption_text": "Figure 8: Domain proportions of a sentence pair for English-to-German task. White represents the News domain and black represents the TED domain. The domain proportions of both the encoder (bottom) and the decoder (top) are presented.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 720.0, "y1": 447.0, "x1": 444.0, "y2": 533.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2225782606337, "y1": 835.6285942925347, "x1": 426.772223578559, "y2": 893.7791612413195}, "name": "9", "caption_text": "Figure 9: Histograms of the domain proportions of each layer in our domain mixing model for English-toGerman Task. Within each histogram, 0 means pure News domain, and 1 means pure TED domain.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 746.0, "y1": 671.0, "x1": 426.0, "y2": 814.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.153567843967, "y1": 227.6493920220269, "x1": 100.0, "y2": 252.59170532226562}, "name": "10", "caption_text": "Figure 10: Back-propagation for different embedding based methods.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 414.0, "y1": 87.0, "x1": 108.0, "y2": 198.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.15339830186633, "y1": 683.3396487765842, "x1": 99.50138727823892, "y2": 708.2805633544922}, "name": "6", "caption_text": "Table 6: BLEU Scores of Domain Mixing + Domain Aware Embedding for Chinese-to-English Task", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 401.0, "y1": 550.0, "x1": 100.0, "y2": 683.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 328.09331681993274, "y1": 456.87158372667096, "x1": 174.62779151068793, "y2": 465.2083502875434}, "name": "8", "caption_text": "Table 8: Chinese to English", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 424.0, "y1": 189.0, "x1": 100.0, "y2": 457.0}, "page": 11, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 381.0886806911892, "y1": 644.9910481770833, "x1": 122.06111484103732, "y2": 653.3278147379557}, "name": "11", "caption_text": "Figure 11: Two variants of layer normalization", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 378.0, "y1": 504.0, "x1": 126.0, "y2": 620.0}, "page": 11, "dpi": 0}], "error": null, "pdf": "/work/host-output/6be81fe90e17578f8206b1606c109640d2a2cf82/2020.acl-main.165.pdf", "dpi": 100}