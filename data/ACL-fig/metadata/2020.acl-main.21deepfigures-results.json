{"raw_detected_boxes": [[], [{"x2": 682.0, "y1": 87.0, "x1": 147.0, "y2": 283.0}], [], [{"x2": 720.0, "y1": 89.0, "x1": 107.0, "y2": 370.0}], [{"x2": 378.0, "y1": 94.0, "x1": 124.0, "y2": 292.0}], [], [{"x2": 716.0, "y1": 86.0, "x1": 111.0, "y2": 258.0}, {"x2": 698.0, "y1": 354.0, "x1": 458.0, "y2": 428.0}], [{"x2": 399.0, "y1": 86.0, "x1": 104.0, "y2": 202.0}, {"x2": 726.0, "y1": 86.0, "x1": 431.0, "y2": 221.0}], [{"x2": 710.0, "y1": 161.0, "x1": 117.0, "y2": 325.0}], [], [], [], [{"x2": 701.0, "y1": 401.0, "x1": 122.0, "y2": 884.0}]], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Table", "name": "1", "captionBoundary": {"x2": 525.547119140625, "y1": 216.20852661132812, "x1": 71.69100189208984, "y2": 234.166015625}, "imageText": ["1", "Who", "was", "at", "the", "park?", "Who", "was", "at", "the", "park?", "John", "2", "What", "was", "John", "doing", "at", "the", "park?", "What", "was", "he", "doing", "there?", "swinging", "3", "Where", "was", "John", "swinging?", "On", "what?", "on", "the", "wings", "4", "Who", "was", "with", "John", "at", "the", "park?", "Who", "was", "he", "with?", "his", "friend", "5", "What", "is", "the", "name", "of", "John\u2019s", "friend?", "Named?", "Tim", "6", "What", "was", "Tim", "doing?", "What", "was", "he", "doing?", "played", "on", "the", "side", "7", "What", "did", "John", "asked", "Tim?", "What", "did", "John", "asked", "him?", "if", "he", "could", "play", "on", "the", "slide", "8", "What", "did", "Tim", "say", "to", "John?", "What", "did", "he", "say?", "no", "(1)", "A", "small", "boy", "named", "[John]1", "was", "at", "the", "park", "one", "day.", "(2)", "He", "was", "[swinging]2", "[on", "the", "swings]3", "and", "[his", "friend]4", "named", "[Tim]5", "[played", "on", "the", "slide]6.", "(3)", "John", "wanted", "to", "play", "on", "the", "slide", "now.", "(4)", "He", "asked", "Tim", "[if", "he", "could", "play", "on", "the", "slide]7.", "(5)", "Tim", "said", "[no]8,", "and", "he", "cried.", "Turn", "TQG", "SQG", "Answer"], "regionBoundary": {"x2": 492.0, "y1": 62.8900146484375, "x1": 106.0, "y2": 203.8900146484375}, "caption": "Table 1: Comparison of Traditional Question Generation (TQG) and Sequential Question Generation (SQG). The given passage contains five sentences, and we mark the given answers in the passage as blue.", "page": 1}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 526.7924194335938, "y1": 199.47055053710938, "x1": 71.69100189208984, "y2": 229.384033203125}, "imageText": ["HRAN", "(Xing", "et", "al.,", "2018)", "30.18", "12.53", "7.65", "35.06", "12.95", "5.02", "ReDR", "(Pan", "et", "al.,", "2019)", "30.84", "15.17", "9.81", "35.58", "15.41", "5.58", "CorefNet", "(Gao", "et", "al.,", "2019)", "32.72", "16.01", "10.97", "37.48", "16.09", "5.96", "Ours", "35.70", "19.64", "12.06", "38.15", "17.26", "6.03", "CoreNQG", "(Du", "and", "Cardie,", "2018)", "33.84", "14.69", "8.72", "34.38", "14.05", "6.08", "VHRED", "(Serban", "et", "al.,", "2017)", "30.51", "11.95", "6.94", "31.93", "12.42", "4.83", "Model", "BLEU1", "BLEU2", "BLEU3", "ROUGE", "METEOR", "Length", "Seq2seq", "(Du", "et", "al.,", "2017)", "28.72", "10.16", "6.30", "31.75", "13.10", "5.78", "CopyNet", "(See", "et", "al.,", "2017)", "29.40", "12.14", "6.53", "33.71", "14.20", "5.77"], "regionBoundary": {"x2": 520.0, "y1": 63.8900146484375, "x1": 77.0, "y2": 187.8900146484375}, "caption": "Table 2: Experimental results. In each column, we bold / underline the best performance over all / baseline methods, respectively. Under the evaluation of BLEU, ROUGE-L and METEOR, our model differs from others (except the METEOR score of CorefNet) significantly based on the one-side paired t-test with p < 0.05.", "page": 6}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 525.5464477539062, "y1": 320.6165466308594, "x1": 306.9670104980469, "y2": 338.5740051269531}, "imageText": ["SQuAD", "CoQA", "Ours", "Passage", "117", "271", "271", "Question", "10.1", "5.5", "6.6", "Answer", "3.2", "2.7", "3.2"], "regionBoundary": {"x2": 503.0, "y1": 252.8900146484375, "x1": 330.0, "y2": 308.8900146484375}, "caption": "Table 3: Average number of words in passage, question and answer in different datasets.", "page": 6}, {"figType": "Table", "name": "7", "captionBoundary": {"x2": 369.072998046875, "y1": 648.9765625, "x1": 228.16200256347656, "y2": 654.97900390625}, "imageText": ["(The", "white", "cat)", "15", "Is", "this", "his", "original", "one?", "(yes)", "Deleted", "16", "What", "is", "its", "gender?", "(female)", "Deleted", "17", "What", "does", "he", "call", "it?", "(Snowball)", "What", "does", "he", "call", "it?", "(Snowball)", "18", "Is", "there", "one", "called", "Binky?", "(No)", "Deleted", "19", "How", "about", "Scruff?", "(No)", "Deleted", "14", "Which", "is", "the", "most", "liked?", "(The", "white", "cat)", "Which", "is", "the", "most", "liked?", "(orange,", "black,", "spotted,", "and", "white)", "Deleted", "aren\u2019t", "good", "for", "cats)", "11", "What", "toys", "do", "they", "like?", "(balls", "of", "paper)", "What", "toys", "do", "they", "like?", "(balls", "of", "paper)", "12", "Who", "creates", "them?", "(Brendan)", "Who", "creates", "them?", "(Brendan)", "13", "What", "colors", "are", "the", "felines?", "10", "Why?", "(because", "those", "foods", "aren\u2019t", "good", "for", "cats)", "Why?", "(because", "those", "foods", "(chips", "and", "cake", "and", "candy)", "What", "foods", "are", "avoided?", "(chips", "and", "cake", "and", "candy)", "1", "What", "does", "he", "care", "for?", "(cats)", "What", "does", "he", "care", "for?", "(cats)", "2", "How", "many", "does", "he", "have?", "(Eight)", "How", "many", "does", "he", "have?", "(8)", "3", "Are", "there", "more", "males", "or", "females?", "(females)", "Deleted", "4", "How", "many?", "(7", "girl", "cats", "and", "only", "1", "boy", "cat)", "How", "many", "males", "and", "females?", "(7", "girl", "cats", "and", "only", "1", "boy", "cat)", "5", "What", "is", "groomed?", "(cat\u2019s", "hair)", "What", "is", "groomed?", "(cat\u2019s", "hair)", "6", "What", "do", "they", "get", "fed?", "(treats)", "What", "do", "they", "get", "fed?", "(treats)", "7", "How", "many?", "(Three)", "How", "many?", "(3)", "8", "Why", "(because", "he", "loves", "them)", "Why", "(because", "he", "loves", "them)", "9", "What", "foods", "are", "avoided?", "Brendan", "loves", "cats.", "He", "owns", "8", "cats.", "He", "has", "7", "girl", "cats", "and", "only", "1", "boy", "cat.", "Brendan", "brushes", "the", "cats\u2019", "hair", "every", "day.", "He", "makes", "sure", "to", "feed", "them", "every", "morning", "and", "evening", "and", "always", "checks", "to", "see", "if", "the", "cats", "have", "water.", "Sometimes", "he", "feeds", "them", "special", "treats", "because", "he", "loves", "them.", "Each", "cat", "gets", "3", "treats.", "He", "doesn\u2019t", "give", "them", "food", "like", "chips", "and", "cake", "and", "candy,", "because", "those", "foods", "aren\u2019t", "good", "for", "cats.", "He", "likes", "to", "play", "with", "the", "cats.", "The", "cats", "like", "to", "chase", "balls", "of", "paper", "that", "Brendan", "makes", "for", "them.", "Some", "of", "his", "cats", "have", "orange", "fur,", "some", "have", "black", "fur,", "some", "are", "spotted", "and", "one", "is", "white.", "The", "white", "cat", "is", "Brendan\u2019s", "favorite.", "She", "is", "the", "\ufb01rst", "cat", "he", "owned.", "Her", "name", "is", "Snowball.", "When", "he", "\ufb01rst", "got", "Snowball", "she", "was", "a", "kitten.", "His", "other", "cats", "are", "named", "Fluffy,", "Salem,", "Jackie,", "Cola,", "Snickers,", "Pumpkin", "and", "Whiskers.", "turn", "Original", "QA-Pairs", "New", "QA-Pairs"], "regionBoundary": {"x2": 512.0, "y1": 166.8900146484375, "x1": 85.0, "y2": 636.8900146484375}, "caption": "Table 7: Example for data labeling.", "page": 12}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 291.92425537109375, "y1": 157.62753295898438, "x1": 71.69100189208984, "y2": 175.58502197265625}, "imageText": ["CoreNQG", "CorefNet", "Ours", "Fluency", "2.36", "2.51", "2.44", "Coherence", "1.53", "2.04", "2.17", "Coreference", "1.15", "1.56", "1.54", "Answerability", "1.12", "1.18", "1.45", "Relevance", "1.47", "1.24", "1.62"], "regionBoundary": {"x2": 290.0, "y1": 62.8900146484375, "x1": 73.0, "y2": 145.8900146484375}, "caption": "Table 4: Human evaluation results. Scores of each metric ranges between 1 to 3 and larger scores are better.", "page": 7}, {"figType": "Table", "name": "5", "captionBoundary": {"x2": 484.2306213378906, "y1": 171.57553100585938, "x1": 348.281005859375, "y2": 177.5780029296875}, "imageText": ["Ours", "12.06", "38.15", "17.26", "BLEU3", "ROUGE", "METEOR", "No", "interact", "11.35", "37.31", "17.05", "Uni-graph", "9.86", "36.44", "15.87", "Uni-heads", "10.33", "37.48", "16.24", "No", "co2\ufb01ne", "11.75", "37.92", "17.17", "Non-auto", "7.79", "33.62", "14.83"], "regionBoundary": {"x2": 523.0, "y1": 62.8900146484375, "x1": 310.0, "y2": 159.8900146484375}, "caption": "Table 5: Results for ablation tests.", "page": 7}, {"figType": "Figure", "name": "1", "captionBoundary": {"x2": 459.2402038574219, "y1": 280.0675354003906, "x1": 138.30499267578125, "y2": 286.07000732421875}, "imageText": [], "regionBoundary": {"x2": 525.0, "y1": 61.8900146484375, "x1": 73.0, "y2": 267.8900146484375}, "caption": "Figure 1: Architecture of our model. The example is corresponding with Table 1", "page": 3}, {"figType": "Table", "name": "6", "captionBoundary": {"x2": 497.8651428222656, "y1": 246.09652709960938, "x1": 99.37100219726562, "y2": 252.0989990234375}, "imageText": ["1", "How", "long", "was", "Peter", "at", "pet", "store?", "How", "long", "he", "had", "been", "there?", "How", "long", "was", "Peter", "there?", "2", "Why", "couldn\u2019t", "he", "get", "someone?", "What", "his", "fur", "was?", "What", "did", "he", "thought?", "3", "Who", "came", "into", "the", "store?", "Who", "came", "into", "the", "store?", "Who", "came", "into", "the", "store?", "4", "What", "for?", "What", "was", "Sammie", "looking?", "Who", "was", "she", "looking", "for?", "5", "What", "did", "peter", "wanted", "to", "show", "off?", "What", "Peter", "wanted", "show", "off?", "What", "he", "show", "off?", "6", "Why", "not?", "Why", "he", "wanted?", "What", "was", "he?", "7", "What", "did", "he", "do", "with", "her?", "And", "else?", "What", "did", "he", "do?", "8", "Who", "did", "she", "take?", "Who", "was", "Sammie", "took?", "What", "Sammie", "took", "that", "day?", "Peter", "was", "a", "very", "sad", "puppy.", "He", "had", "been", "inside", "of", "the", "pet", "store", "for", "a", "very", "long", "time.", "In", "fact,", "he", "had", "been", "there", "for", "[three", "months]1!", "Peter", "had", "seen", "many", "other", "puppies", "\ufb01nd", "a", "person;", "he", "began", "to", "wonder", "why", "he", "could", "not", "get", "one.", "He", "thought", "that", "[maybe", "his", "fur", "was", "not", "pretty", "enough", "or", "maybe", "his", "bark", "was", "not", "loud", "enough]2.", "He", "tried", "and", "tried", "to", "please", "every", "person", "who", "came", "to", "the", "store,", "but", "they", "all", "picked", "smaller", "puppies.", "However,", "one", "day", "all", "of", "this", "changed.", "[Sammie]3", "came", "into", "the", "store", "looking", "for", "[a", "golden", "puppy]4.", "She", "wanted", "a", "puppy", "she", "could", "snuggle", "with.", "It", "so", "happened", "that", "Peter", "was", "very", "sad", "and", "tired", "that", "day.", "Sammie", "came", "to", "hold", "him.", "Peter", "wanted", "to", "show", "off", "[his", "bark]5,", "but", "he", "was", "[too", "tired]6.", "He", "[fell", "right", "to", "sleep]7.", "Sammie", "loved", "him", "at", "once", "and", "loved", "holding", "him", "in", "her", "arms.", "Sammie", "took", "[Peter]8", "home", "that", "day,", "and", "they", "made", "lots", "of", "fun", "memories.", "Turn", "Gold", "Standard", "CorefNet", "Ours"], "regionBoundary": {"x2": 515.0, "y1": 64.77877044677734, "x1": 83.0, "y2": 233.8900146484375}, "caption": "Table 6: Example outputs from different models. We mark the given answers in the passage as blue.", "page": 8}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 290.2706604003906, "y1": 224.23855590820312, "x1": 72.0, "y2": 242.196044921875}, "imageText": [], "regionBoundary": {"x2": 276.0, "y1": 61.8900146484375, "x1": 87.0, "y2": 212.8900146484375}, "caption": "Figure 2: Illustration of answer embeddings and an answer-attention head for the forth sentence in Table 1.", "page": 4}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 729.9265543619791, "y1": 300.2896202935113, "x1": 99.57083596123589, "y2": 325.23057725694446}, "name": "1", "caption_text": "Table 1: Comparison of Traditional Question Generation (TQG) and Sequential Question Generation (SQG). The given passage contains five sentences, and we mark the given answers in the passage as blue.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 699.0, "y1": 86.0, "x1": 135.0, "y2": 300.0}, "page": 1, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 637.8336164686415, "y1": 388.9826880560981, "x1": 192.09026760525174, "y2": 397.31945461697046}, "name": "1", "caption_text": "Figure 1: Architecture of our model. The example is corresponding with Table 1", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 724.0, "y1": 89.0, "x1": 104.0, "y2": 371.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.15369500054254, "y1": 311.4424387613932, "x1": 100.0, "y2": 336.38339572482636}, "name": "2", "caption_text": "Figure 2: Illustration of answer embeddings and an answer-attention head for the forth sentence in Table 1.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 379.0, "y1": 92.0, "x1": 124.0, "y2": 292.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 731.6561381022135, "y1": 277.0424313015408, "x1": 99.57083596123589, "y2": 318.58893500434027}, "name": "2", "caption_text": "Table 2: Experimental results. In each column, we bold / underline the best performance over all / baseline methods, respectively. Under the evaluation of BLEU, ROUGE-L and METEOR, our model differs from others (except the METEOR score of CorefNet) significantly based on the one-side paired t-test with p < 0.05.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 722.0, "y1": 86.0, "x1": 108.0, "y2": 260.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9256218804253, "y1": 445.3007592095269, "x1": 426.3430701361762, "y2": 470.2416737874349}, "name": "3", "caption_text": "Table 3: Average number of words in passage, question and answer in different datasets.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 715.0, "y1": 350.0, "x1": 441.0, "y2": 445.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.45035468207465, "y1": 218.9271291097005, "x1": 99.57083596123589, "y2": 243.86808607313367}, "name": "4", "caption_text": "Table 4: Human evaluation results. Scores of each metric ranges between 1 to 3 and larger scores are better.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 402.0, "y1": 86.0, "x1": 100.0, "y2": 219.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 672.5425296359592, "y1": 238.29934861924912, "x1": 483.7236192491319, "y2": 246.63611518012152}, "name": "5", "caption_text": "Table 5: Results for ablation tests.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 726.0, "y1": 86.0, "x1": 430.0, "y2": 238.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 691.4793650309244, "y1": 341.8007320827908, "x1": 138.01528082953558, "y2": 350.1374986436632}, "name": "6", "caption_text": "Table 6: Example outputs from different models. We mark the given answers in the passage as blue.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 715.0, "y1": 144.0, "x1": 115.0, "y2": 342.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 512.6013861762152, "y1": 901.3563368055555, "x1": 316.8916702270508, "y2": 909.6930609809027}, "name": "7", "caption_text": "Table 7: Example for data labeling.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 712.0, "y1": 384.0, "x1": 118.0, "y2": 901.0}, "page": 12, "dpi": 0}], "error": null, "pdf": "/work/host-output/22a9216195111f423d598ea5e842a012b79a2333/2020.acl-main.21.pdf", "dpi": 100}