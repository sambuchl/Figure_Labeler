{"raw_detected_boxes": [[{"x2": 715.0, "y1": 309.0, "x1": 442.0, "y2": 489.0}], [], [{"x2": 723.0, "y1": 88.0, "x1": 108.0, "y2": 244.0}], [{"x2": 718.0, "y1": 88.0, "x1": 437.0, "y2": 258.0}], [{"x2": 682.0, "y1": 95.0, "x1": 145.0, "y2": 174.0}, {"x2": 394.0, "y1": 249.0, "x1": 116.0, "y2": 310.0}, {"x2": 718.0, "y1": 334.0, "x1": 433.0, "y2": 548.0}], [{"x2": 651.0, "y1": 92.0, "x1": 177.0, "y2": 217.0}], [{"x2": 710.0, "y1": 92.0, "x1": 120.0, "y2": 199.0}, {"x2": 396.0, "y1": 647.0, "x1": 107.0, "y2": 892.0}], [{"x2": 697.0, "y1": 98.0, "x1": 133.0, "y2": 308.0}, {"x2": 718.0, "y1": 371.0, "x1": 100.0, "y2": 522.0}], [], [], []], "raw_pdffigures_output": {"regionless-captions": [{"figType": "Figure", "boundary": {"x2": 526.2833251953125, "y1": 406.46954345703125, "x1": 71.72200012207031, "y2": 422.43402099609375}, "text": "Figure 6: Captions generated by the coherence-aware and coherence-agnostic models. (Photo credits: YesVideo; TinnaPong; Sok Chien Lim; GoPro)", "name": "6", "page": 7}], "figures": [{"figType": "Figure", "name": "1", "captionBoundary": {"x2": 527.2003173828125, "y1": 365.841552734375, "x1": 306.9169921875, "y2": 443.57501220703125}, "imageText": [], "regionBoundary": {"x2": 515.0, "y1": 221.8900146484375, "x1": 318.0, "y2": 353.8900146484375}, "caption": "Figure 1: Output of a coherence-aware model for various coherence relations. Content that establishes the intended relation is underlined. (Photo credit: Blue Destiny / Alamy Stock Photo) Visible: horse and rider jumping a fence. Meta: horse and rider jumping a fence during a race. Subjective: the most beautiful horse in the world.", "page": 0}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 526.3726196289062, "y1": 168.58651733398438, "x1": 71.69100189208984, "y2": 186.54400634765625}, "imageText": ["SVM", "(text-only)", "0.83", "0.12", "0.32", "0.21", "0.19", "0.00", "0.48", "GloVe", "(text-only)", "0.80", "0.44", "0.58", "0.57", "0.44", "0.08", "0.63", "BERT", "(text-only)", "0.82", "0.35", "0.62", "0.62", "0.44", "0.06", "0.65", "GloVe", "+", "ResNet", "0.81", "0.36", "0.58", "0.60", "0.45", "0.07", "0.64", "BERT", "+", "ResNet", "0.83", "0.36", "0.69", "0.62", "0.44", "0.06", "0.67", "Visible", "Subjective", "Action", "Story", "Meta", "Irrelevant", "Weighted"], "regionBoundary": {"x2": 470.0, "y1": 62.8900146484375, "x1": 128.0, "y2": 155.8900146484375}, "caption": "Table 3: The F1 scores of the multi-class classification methods described in Section 4.1; 80-20 train-test split; 5-fold cross validation.", "page": 5}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 525.5426025390625, "y1": 155.43551635742188, "x1": 71.69100189208984, "y2": 173.39398193359375}, "imageText": ["Ground-truth", "Distribution", "46.65%", "7.07%", "1.31%", "19.09%", "23.42%", "2.46%", "BERT", "+", "ResNet", "0.64", "0.26", "0.02", "0.52", "0.46", "0.07", "0.52", "BERT", "+", "GraphRise", "0.59", "0.15", "0.00", "0.42", "0.34", "0.00", "0.45", "USE", "+", "GraphRise", "0.69", "0.45", "0.00", "0.57", "0.48", "0.00", "0.57", "Visible", "Subjective", "Action", "Story", "Meta", "Irrelevant", "Weighted"], "regionBoundary": {"x2": 511.0, "y1": 65.8900146484375, "x1": 87.0, "y2": 142.8900146484375}, "caption": "Table 4: The F1 scores of coherence relation classifiers with label mapping. The aggregated Weighted scores use the numbers in the first row as weights.", "page": 6}, {"figType": "Figure", "name": "5", "captionBoundary": {"x2": 284.9247741699219, "y1": 654.6765747070312, "x1": 77.34400177001953, "y2": 660.6790161132812}, "imageText": ["Trainable", "Components", "Pre-trained", "Components", "Model", "Outputs", "Model", "Inputs", "Coherence-Aware", "Caption", "Image", "Start", "Token", "Coherence", "Label", "Object", "Classifier", "Image", "Features", "Extractor", "Transformer", "Decoder", "Transformer", "Encoder"], "regionBoundary": {"x2": 285.0, "y1": 464.8900146484375, "x1": 77.0, "y2": 642.8900146484375}, "caption": "Figure 5: Coherence-aware image captioning model", "page": 6}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 526.931884765625, "y1": 198.80459594726562, "x1": 72.0, "y2": 240.673095703125}, "imageText": ["(d)", "CAPTION:", "young", "girl", "walking", "on", "the", "dry", "grass", "\ufb01eld", "under", "daylight.", "Irrelevant", "(c)", "CAPTION:", "approaching", "our", "campsite,", "at", "1550m", "of", "el-", "evation", "on", "the", "slopes.", "Meta,", "Action,", "Story", "(b)", "CAPTION:", "young", "happy", "boy", "swimming", "in", "the", "lake.", "Visible,", "Action,", "Subjective", "(a)", "CAPTION:", "forest", "on", "a", "sunny", "day", "Visible,", "Meta"], "regionBoundary": {"x2": 524.0, "y1": 64.37972259521484, "x1": 74.19599914550781, "y2": 177.510009765625}, "caption": "Figure 2: We use a constrained set of coherence relations to summarize the structural, logical and purposeful relationships between the contributions of text and the contributions of images. Multiple coherence relations can be found simultaneously. (Image\u2013caption pairs are chosen from the Conceptual Caption dataset; photo credits: Dmytro Zinkevych; Shutterstock user yauhenka; Danilo Hegg; Andre Seale)", "page": 2}, {"figType": "Table", "name": "5", "captionBoundary": {"x2": 525.547119140625, "y1": 233.64456176757812, "x1": 71.69100189208984, "y2": 251.60302734375}, "imageText": ["Visible", "52.1%", "79.9%", "31.7%", "25.0%", "42.80%", "Subjective", "11.4%", "2.6%", "24.4%", "2.6%", "1.9%", "Action", "10.7%", "10.8%", "6.3%", "8.8%", "11.4%", "Story", "51.3%", "16.0%", "45.0%", "58.8%", "17.34%", "Meta", "31.2%", "32.8%", "15.1%", "17.7%", "46.5%", "Irrelevant", "12.2%", "12.3%", "10.7%", "9.9%", "21.40%", "When", "9.5%", "5.6%", "4.1%", "17.7%", "9.6%", "How", "21.3%", "21.3%", "9.6%", "25.0%", "30.26%", "Where", "5.3%", "8.6%", "4.1%", "8.8%", "16.6%", "Coherence", "agnostic", "Visible", "coherence-aware", "Subjective", "coherence-aware", "Story", "coherence-aware", "Meta", "coherence-aware"], "regionBoundary": {"x2": 502.0, "y1": 62.8900146484375, "x1": 95.0, "y2": 220.8900146484375}, "caption": "Table 5: The distribution of coherence relations in image\u2013caption pairs when captions are generated with the discourse\u2013aware model vs the discourse agnostic model (the mode of the distribution in bold).", "page": 7}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 525.894775390625, "y1": 206.44552612304688, "x1": 306.9169921875, "y2": 236.3590087890625}, "imageText": ["(b)", "CAPTION:", "actor", "in", "retail", "at", "the", "mother.", "Other\u2013Gibberish", "(a)", "CAPTION:", "a", "gardener", "may", "water", "the", "plant", "daily", "but", "fruits", "grow", "only", "in", "the", "season.", "Other\u2013Text"], "regionBoundary": {"x2": 517.8547973632812, "y1": 64.3797836303711, "x1": 314.0, "y2": 185.1510009765625}, "caption": "Figure 3: Examples of image\u2013caption pairs in the Other category. (Photo credit: santabanta.com; Mary Sollosi)", "page": 3}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 475.77276611328125, "y1": 144.23953247070312, "x1": 121.46299743652344, "y2": 150.24200439453125}, "imageText": ["Ground-truth", "+", "Model", "66.91%", "6.58%", "15.68%", "24.67%", "38.65%", "8.77%", "Ground-truth", "64.97%", "9.77%", "18.77%", "29.84%", "24.59%", "3.09%", "Model", "output", "69.72%", "1.99%", "11.22%", "17.19%", "58.94%", "16.97%", "Visible", "Subjective", "Action", "Story", "Meta", "Irrelevant"], "regionBoundary": {"x2": 495.0, "y1": 62.8900146484375, "x1": 103.0, "y2": 131.8900146484375}, "caption": "Table 1: Distribution of coherence relations over the ground-truth and the model outputs.", "page": 4}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 290.27044677734375, "y1": 235.55654907226562, "x1": 71.69100189208984, "y2": 253.5140380859375}, "imageText": ["When", "How", "Where", "Ground-truth", "33.74%", "64.40%", "28.60%", "Model", "output", "21.75", "%", "72.84%", "41.03%"], "regionBoundary": {"x2": 285.0, "y1": 173.8900146484375, "x1": 77.0, "y2": 222.8900146484375}, "caption": "Table 2: Distribution of fine-grain relations in the Meta category over the ground-truth and the model outputs.", "page": 4}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 527.2003784179688, "y1": 409.5465393066406, "x1": 307.2760009765625, "y2": 451.41497802734375}, "imageText": [], "regionBoundary": {"x2": 525.0, "y1": 237.8900146484375, "x1": 308.0, "y2": 397.8900146484375}, "caption": "Figure 4: Different resources have different kinds image\u2013caption pairs. The graph shows the distribution of labels in the top four domains present in the Conceptual Captions dataset.", "page": 4}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 732.222663031684, "y1": 508.1132676866319, "x1": 426.27360026041663, "y2": 616.0764058430989}, "name": "1", "caption_text": "Figure 1: Output of a coherence-aware model for various coherence relations. Content that establishes the intended relation is underlined. (Photo credit: Blue Destiny / Alamy Stock Photo) Visible: horse and rider jumping a fence. Meta: horse and rider jumping a fence during a race. Subjective: the most beautiful horse in the world.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 715.0, "y1": 309.0, "x1": 442.0, "y2": 491.0}, "page": 0, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 731.849839952257, "y1": 276.11749437120227, "x1": 100.0, "y2": 334.2681884765625}, "name": "2", "caption_text": "Figure 2: We use a constrained set of coherence relations to summarize the structural, logical and purposeful relationships between the contributions of text and the contributions of images. Multiple coherence relations can be found simultaneously. (Image\u2013caption pairs are chosen from the Conceptual Caption dataset; photo credits: Dmytro Zinkevych; Shutterstock user yauhenka; Danilo Hegg; Andre Seale)", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 727.0, "y1": 88.0, "x1": 103.0, "y2": 249.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 730.409410264757, "y1": 286.72989739312067, "x1": 426.27360026041663, "y2": 328.2764010959201}, "name": "3", "caption_text": "Figure 3: Examples of image\u2013caption pairs in the Other category. (Photo credit: santabanta.com; Mary Sollosi)", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 720.0, "y1": 88.0, "x1": 437.0, "y2": 258.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 660.7955084906683, "y1": 200.33268398708768, "x1": 168.698607550727, "y2": 208.66945054796005}, "name": "1", "caption_text": "Table 1: Distribution of coherence relations over the ground-truth and the model outputs.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 687.0, "y1": 86.0, "x1": 143.0, "y2": 183.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.15339830186633, "y1": 327.16187371148004, "x1": 99.57083596123589, "y2": 352.1028306749132}, "name": "2", "caption_text": "Table 2: Distribution of fine-grain relations in the Meta category over the ground-truth and the model outputs.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 396.0, "y1": 241.0, "x1": 100.0, "y2": 327.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2227478027344, "y1": 568.8146379258898, "x1": 426.772223578559, "y2": 626.9652472601996}, "name": "4", "caption_text": "Figure 4: Different resources have different kinds image\u2013caption pairs. The graph shows the distribution of labels in the top four domains present in the Conceptual Captions dataset.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 722.0, "y1": 334.0, "x1": 433.0, "y2": 548.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 731.0730828179253, "y1": 234.14794074164496, "x1": 99.57083596123589, "y2": 259.0888977050781}, "name": "3", "caption_text": "Table 3: The F1 scores of the multi-class classification methods described in Section 4.1; 80-20 train-test split; 5-fold cross validation.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 667.0, "y1": 86.0, "x1": 160.0, "y2": 234.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9202813042534, "y1": 215.88266160753037, "x1": 99.57083596123589, "y2": 240.82497490776908}, "name": "4", "caption_text": "Table 4: The F1 scores of coherence relation classifiers with label mapping. The aggregated Weighted scores use the numbers in the first row as weights.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 710.0, "y1": 86.0, "x1": 103.0, "y2": 216.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 395.7288530137804, "y1": 909.2730204264323, "x1": 107.42222468058267, "y2": 917.6097446017794}, "name": "5", "caption_text": "Figure 5: Coherence-aware image captioning model", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 396.0, "y1": 645.0, "x1": 107.0, "y2": 909.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9265543619791, "y1": 324.5063357883029, "x1": 99.57083596123589, "y2": 349.44864908854163}, "name": "5", "caption_text": "Table 5: The distribution of coherence relations in image\u2013caption pairs when captions are generated with the discourse\u2013aware model vs the discourse agnostic model (the mode of the distribution in bold).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 704.0, "y1": 86.0, "x1": 120.0, "y2": 325.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 730.9490627712673, "y1": 564.5410325792101, "x1": 99.61388905843098, "y2": 586.7139180501301}, "name": "6", "caption_text": "Figure 6: Captions generated by the coherence-aware and coherence-agnostic models. (Photo credits: YesVideo; TinnaPong; Sok Chien Lim; GoPro)", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 718.0, "y1": 371.0, "x1": 100.0, "y2": 539.0}, "page": 7, "dpi": 0}], "error": null, "pdf": "/work/host-output/3aa6243d31b562024d4aeac0ef2db9a6cc9b784b/2020.acl-main.583.pdf", "dpi": 100}