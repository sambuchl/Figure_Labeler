{"raw_detected_boxes": [[], [], [], [{"x2": 660.0, "y1": 105.0, "x1": 161.0, "y2": 460.0}], [{"x2": 397.0, "y1": 90.0, "x1": 100.0, "y2": 240.0}], [{"x2": 658.0, "y1": 86.0, "x1": 170.0, "y2": 163.0}], [{"x2": 714.0, "y1": 86.0, "x1": 119.0, "y2": 241.0}, {"x2": 399.0, "y1": 320.0, "x1": 104.0, "y2": 697.0}, {"x2": 727.0, "y1": 869.0, "x1": 434.0, "y2": 943.0}], [{"x2": 388.0, "y1": 89.0, "x1": 113.0, "y2": 360.0}, {"x2": 370.0, "y1": 497.0, "x1": 125.0, "y2": 697.0}], []], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Table", "name": "1", "captionBoundary": {"x2": 527.2001953125, "y1": 487.3855285644531, "x1": 306.9670104980469, "y2": 505.3429870605469}, "imageText": ["Prompt:", "What", "kind", "of", "\ufb02owers", "do", "you", "like?", "On-topic:", "I", "like", "iris", "and", "it", "has", "different", "mean-", "ing", "of", "it", "a", "wide", "is", "the", "white", "and", "um", "and", "the", "size", "of", "a", "as", "a", "ride", "is", "means", "the", "ride", "means", "love", "but", "I", "can", "not", "speak.", "Off-topic:", "Sometimes", "I", "would", "like", "to", "invite", "my", "friends", "to", "my", "home", "and", "we", "can", "play", "the", "Chinese", "chess", "dishes", "this", "is", "my", "favorite", "games", "at", "what", "I", "was", "child."], "regionBoundary": {"x2": 522.0, "y1": 351.8900146484375, "x1": 311.0, "y2": 474.8900146484375}, "caption": "Table 1: An example of on-topic and off-topic responses for a prompt.", "page": 0}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 422.3489685058594, "y1": 130.52957153320312, "x1": 174.88800048828125, "y2": 136.53204345703125}, "imageText": ["Data", "#Prompt", "#Resp.", "#Resp./Prompt", "On-topic", "Off-topic", "Train", "1356", "1,12M", "822", "564.3K", "551.3K", "Test", "Seen", "156", "33.6K", "216", "17.7K", "15.9K", "Unseen", "50", "10.1K", "202", "5.0K", "5.1K"], "regionBoundary": {"x2": 475.0, "y1": 62.8900146484375, "x1": 123.0, "y2": 118.8900146484375}, "caption": "Table 3: The train and test datasets for off-topic detection task", "page": 5}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 527.29052734375, "y1": 185.52352905273438, "x1": 71.64099884033203, "y2": 203.48101806640625}, "imageText": ["+", "Bi-Attention", "90.4", "78.3", "56.0", "39.7", "+", "RNN\u2192CNN", "89.7", "76.6", "66.0", "43.7", "+", "maxpooling", "92.3", "79.1", "68.0", "42.2", "+", "Res-conn", "in", "gated", "unit", "(GCBiA)", "93.6", "79.2", "68.0", "45.0", "This", "work", "Systems", "Model", "Seen", "Unseen", "PPR3", "AOR", "PPR3", "AOR", "Malinin", "et", "al.,", "2017", "Att-RNN", "84.6", "72.2", "32.0", "21.0", "Our", "baseline", "model", "G-Att-RNN", "87.8", "76.8", "54.0", "38.1"], "regionBoundary": {"x2": 514.0, "y1": 63.8900146484375, "x1": 84.0, "y2": 173.8900146484375}, "caption": "Table 4: The comparison of different models based on over 0.999 on-topic recall on seen and unseen benchmarks. AOR means Average Off-topic Recall (%) and PRR3 means Prompt Ratio over off-topic Recall 0.3 (%).", "page": 6}, {"figType": "Table", "name": "5", "captionBoundary": {"x2": 525.7159423828125, "y1": 691.3685913085938, "x1": 306.9670104980469, "y2": 721.281982421875}, "imageText": ["Model", "Seen", "Unseen", "PPR3", "AOR", "PPR3", "AOR", "GCBiA", "93.6", "79.2", "68.0", "45.0", "+", "neg", "sampling", "94.2", "88.2", "79.4", "69.1"], "regionBoundary": {"x2": 527.0, "y1": 622.8900146484375, "x1": 307.0, "y2": 678.8900146484375}, "caption": "Table 5: The performance of GCBiA with negative sampling augmentation method conditioned on over 0.999 on-topic recall.", "page": 6}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 290.2705993652344, "y1": 517.599609375, "x1": 72.0, "y2": 535.5570068359375}, "imageText": ["(c)", "Attention", "on", "the", "off-topic", "response.", "(b)", "Attention", "on", "the", "on-topic", "response.", "(a)", "Attention", "on", "the", "prompt."], "regionBoundary": {"x2": 291.0, "y1": 226.8900146484375, "x1": 72.0, "y2": 502.2820129394531}, "caption": "Figure 2: The heatmap of attention on the prompt and response.", "page": 6}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 290.9380798339844, "y1": 517.7495727539062, "x1": 72.0, "y2": 535.70703125}, "imageText": [], "regionBoundary": {"x2": 299.0, "y1": 334.8900146484375, "x1": 72.0, "y2": 505.8900146484375}, "caption": "Figure 4: Trends of AOR (Average Off-topic Recall) on seen and unseen prompts with datasize variation.", "page": 7}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 291.92437744140625, "y1": 274.8375549316406, "x1": 71.99999237060547, "y2": 316.705078125}, "imageText": ["(c)", "True", "response", "distribu-", "tion", "on", "divergent", "prompt.", "(d)", "Model\u2019s", "resp", "distribu-", "tion", "on", "divergent", "prompt.", "(a)", "True", "resp", "distribution", "on", "clear-semantic", "topic", "prompt.", "(b)", "Model\u2019s", "resp", "distribu-", "tion", "on", "clear-semantic", "topic", "prompt."], "regionBoundary": {"x2": 282.201416015625, "y1": 61.8900146484375, "x1": 81.0, "y2": 259.52001953125}, "caption": "Figure 3: The analysis of response distribution on different types of prompts. The yellow and black colours represent the on-topic and off-topic response results respectively.", "page": 7}, {"figType": "Figure", "name": "1", "captionBoundary": {"x2": 525.5472412109375, "y1": 357.4425354003906, "x1": 71.7509994506836, "y2": 411.2659606933594}, "imageText": [], "regionBoundary": {"x2": 499.0, "y1": 61.8900146484375, "x1": 99.0, "y2": 345.8900146484375}, "caption": "Figure 1: An overview of GCBiA. Residual connections were widely used to connect each two-layer. The first two layers are applied to both prompt and response. Convolutions are used in contextual encoder layer and bi-attention mechanism is applied in attention layer. After calculating by the relevance layer with the gated unit, the relevance vector is then fed into the output layer which consists of the normalization layer, dropout, two fully connection layers and softmax.", "page": 3}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 291.9241943359375, "y1": 184.72653198242188, "x1": 71.69100189208984, "y2": 202.68402099609375}, "imageText": ["Part3", "Do", "you", "trust", "advertisements?", "ing", "to", "an", "advertisement", "you", "saw.", "what", "it", "was", "where", "you", "saw", "or", "heard", "about", "it", "what", "it", "was", "about.", "Part", "Prompt", "Part1", "How", "long", "have", "you", "lived", "in", "your", "home-", "town?", "Part2", "Describe", "something", "you", "bought", "accord-"], "regionBoundary": {"x2": 290.0, "y1": 62.8900146484375, "x1": 72.0, "y2": 172.8900146484375}, "caption": "Table 2: An example from our IELTS speaking test mobile app.", "page": 4}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 729.9267239040798, "y1": 496.44796583387586, "x1": 99.6541659037272, "y2": 571.2027231852213}, "name": "1", "caption_text": "Figure 1: An overview of GCBiA. Residual connections were widely used to connect each two-layer. The first two layers are applied to both prompt and response. Convolutions are used in contextual encoder layer and bi-attention mechanism is applied in attention layer. After calculating by the relevance layer with the gated unit, the relevance vector is then fed into the output layer which consists of the normalization layer, dropout, two fully connection layers and softmax.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 660.0, "y1": 105.0, "x1": 159.0, "y2": 460.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.4502699110243, "y1": 256.5646277533637, "x1": 99.57083596123589, "y2": 281.5055847167969}, "name": "2", "caption_text": "Table 2: An example from our IELTS speaking test mobile app.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 403.0, "y1": 86.0, "x1": 100.0, "y2": 257.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 586.5957895914713, "y1": 181.29107157389322, "x1": 242.9000006781684, "y2": 189.62783813476562}, "name": "3", "caption_text": "Table 3: The train and test datasets for off-topic detection task", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 660.0, "y1": 86.0, "x1": 170.0, "y2": 180.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.3479546440972, "y1": 257.67156812879773, "x1": 99.50138727823892, "y2": 282.6125250922309}, "name": "4", "caption_text": "Table 4: The comparison of different models based on over 0.999 on-topic recall on seen and unseen benchmarks. AOR means Average Off-topic Recall (%) and PRR3 means Prompt Ratio over off-topic Recall 0.3 (%).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 719.0, "y1": 86.0, "x1": 102.0, "y2": 258.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.1536102294922, "y1": 718.8883463541666, "x1": 100.0, "y2": 743.8291761610243}, "name": "2", "caption_text": "Figure 2: The heatmap of attention on the prompt and response.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 399.0, "y1": 320.0, "x1": 104.0, "y2": 700.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 730.1610310872395, "y1": 960.234154595269, "x1": 426.3430701361762, "y2": 1001.780531141493}, "name": "5", "caption_text": "Table 5: The performance of GCBiA with negative sampling augmentation method conditioned on over 0.999 on-topic recall.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 732.0, "y1": 865.0, "x1": 426.0, "y2": 960.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.45052422417535, "y1": 381.7188262939453, "x1": 99.9999894036187, "y2": 439.8681640625}, "name": "3", "caption_text": "Figure 3: The analysis of response distribution on different types of prompts. The yellow and black colours represent the on-topic and off-topic response results respectively.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 392.0, "y1": 89.0, "x1": 113.0, "y2": 363.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 404.0806664360894, "y1": 719.0966288248698, "x1": 100.0, "y2": 744.0375434027777}, "name": "4", "caption_text": "Figure 4: Trends of AOR (Average Off-topic Recall) on seen and unseen prompts with datasize variation.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 375.0, "y1": 497.0, "x1": 125.0, "y2": 697.0}, "page": 7, "dpi": 0}], "error": null, "pdf": "/work/host-output/722e296918ab0664ea299a2c4fc2411f2a8b13db/2020.acl-main.56.pdf", "dpi": 100}