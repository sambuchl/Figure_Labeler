{"raw_detected_boxes": [[], [], [{"x2": 722.0, "y1": 84.0, "x1": 434.0, "y2": 412.0}], [{"x2": 727.0, "y1": 80.0, "x1": 102.0, "y2": 337.0}], [], [{"x2": 719.0, "y1": 87.0, "x1": 117.0, "y2": 300.0}], [{"x2": 688.0, "y1": 78.0, "x1": 148.0, "y2": 234.0}, {"x2": 398.0, "y1": 364.0, "x1": 102.0, "y2": 581.0}, {"x2": 715.0, "y1": 370.0, "x1": 447.0, "y2": 500.0}], [{"x2": 393.0, "y1": 78.0, "x1": 102.0, "y2": 211.0}], [{"x2": 663.0, "y1": 80.0, "x1": 171.0, "y2": 173.0}, {"x2": 397.0, "y1": 284.0, "x1": 106.0, "y2": 497.0}, {"x2": 710.0, "y1": 291.0, "x1": 447.0, "y2": 403.0}], [], [], [{"x2": 714.0, "y1": 548.0, "x1": 112.0, "y2": 1020.0}, {"x2": 706.0, "y1": 94.0, "x1": 101.0, "y2": 498.0}], [], [], []], "raw_pdffigures_output": {"regionless-captions": [{"figType": "Table", "boundary": {"x2": 260.6202697753906, "y1": 169.32998657226562, "x1": 99.36019897460938, "y2": 173.8599853515625}, "text": "Table 4: Effect of different components.", "name": "4", "page": 7}], "figures": [{"figType": "Table", "name": "1", "captionBoundary": {"x2": 520.2630615234375, "y1": 229.23696899414062, "x1": 77.5198974609375, "y2": 315.4599609375}, "imageText": ["Doc-reranker", "q\u2032", "53.63", "54.51", "54.23", "54.86", "45.17", "This", "work", "Doc-reranker", "q", "51.99", "52.77", "52.84", "51.84", "44.17", "Sent-reranker", "q", "51.33", "52.23", "52.36", "51.63", "43.63", "Sent-transformer", "\u2013", "47.72", "47.21", "49.08", "46.86", "40.18", "Doc-transformer", "(q)", "\u2013", "49.79", "49.29", "50.17", "48.99", "41.70", "Backtranslation", "(q\u2032)", "\u2013", "50.77", "51.80", "51.61", "51.81", "42.47", "Baseline", "(Wang", "et", "al.,", "2017)", "RNNsearch", "\u2013", "37.76", "\u2013", "\u2013", "36.89", "27.57", "(Kuang", "et", "al.,", "2017)", "Transformer", "+", "cache", "\u2013", "48.14", "48.05", "47.91", "48.53", "38.38", "(Zhang", "et", "al.,", "2018)", "Doc-transformer", "\u2013", "49.69", "50.21", "49.73", "49.46", "39.69", "Method", "Model", "Proposal", "MT06", "MT03", "MT04", "MT05", "MT08"], "regionBoundary": {"x2": 521.0, "y1": 65.55697631835938, "x1": 80.0, "y2": 216.0}, "caption": "Table 1: Comparison with prior work on NIST Chinese\u2013English translation task. The evaluation metric is tokenized case-insensitive BLEU. The first three rows are numbers reported in the papers of prior work. The first two baselines are the results that we obtained by running the transformer (Vaswani et al., 2017) and the document transformer (Zhang et al., 2018) on the NIST dataset. The sent-reranker is a variation of our model in which sentences in documents are assumed to be independent. The backtranslation baseline is obtained by training the document transformer using additional synthetic parallel documents generated by backtranslation.", "page": 5}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 495.4390563964844, "y1": 183.03701782226562, "x1": 102.1199951171875, "y2": 228.7001953125}, "imageText": ["transformer-XL:", "NIST", "+", "Gigaword", "51.33", "51.99", "transformer-XL:", "NIST", "51.27", "51.68", "LSTM:", "NIST", "50.75", "51.20", "Doc-transformer", "transformer-XL:", "NIST", "+", "Gigaword", "50.19", "50.93", "transformer-XL:", "NIST", "50.29", "50.56", "LSTM:", "NIST", "49.92", "50.24", "Sent-transformer", "Proposal", "model", "Language", "model", "Sent-reranker", "Doc-reranker"], "regionBoundary": {"x2": 496.0, "y1": 57.0, "x1": 104.0, "y2": 170.0}, "caption": "Table 2: BLEU scores on NIST dev set MT06 from rerankers which are incorporated with various language models. In the language model column X: Y means the language model X is trained on dataset Y. A bigger language model improves the doc-reranker but does not help the sent-reranker.", "page": 6}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 240.2200927734375, "y1": 438.7300109863281, "x1": 121.91999816894531, "y2": 443.260009765625}, "imageText": [], "regionBoundary": {"x2": 289.0, "y1": 262.0, "x1": 73.0, "y2": 419.0}, "caption": "Figure 3: Effect of n-best list.", "page": 6}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 514.763916015625, "y1": 373.35699462890625, "x1": 318.1199951171875, "y2": 405.4604187011719}, "imageText": ["LSTM", "NIST", "doc", "71.6", "transformer-XL", "NIST", "doc", "43.8", "transformer-XL", "NIST", "+", "GW", "doc", "43.4", "transformer-XL", "NIST", "sent", "83.3", "transformer-XL", "NIST", "+", "GW", "sent", "96.5", "Architecture", "Data", "PPL"], "regionBoundary": {"x2": 515.0, "y1": 260.0, "x1": 320.0, "y2": 360.0}, "caption": "Table 3: Perplexity per word of language models on NIST dev set. GW refers to Gigaword.", "page": 6}, {"figType": "Figure", "name": "1", "captionBoundary": {"x2": 526.1307983398438, "y1": 315.2500305175781, "x1": 307.32000732421875, "y2": 403.53985595703125}, "imageText": [], "regionBoundary": {"x2": 521.0, "y1": 58.0, "x1": 311.0, "y2": 299.0}, "caption": "Figure 1: Graphical model showing the factorization of our noisy channel model where yi indicates the ith target language sentence andxi indicates the ith source language sentence. In the prior (top) the target sentences (the yi\u2019s) only influence the corresponding source sentence and therefore can be learned and modeled independently, but at test time (bottom), when the target is not observed, each yi depends on every xi.", "page": 2}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 525.5598754882812, "y1": 261.1299743652344, "x1": 72.0, "y2": 289.54010009765625}, "imageText": [], "regionBoundary": {"x2": 525.0, "y1": 58.0, "x1": 72.0, "y2": 243.0}, "caption": "Figure 2: The decoding process. In Phase 1, the auxiliary proposal model generates candidate translations (3 candidates in the diagram) for each sentence in the document (containing 4 sentences). In Phase 2, beam search is employed to search for the best path from the candidate translations.", "page": 3}, {"figType": "Table", "name": "7", "captionBoundary": {"x2": 485.73980712890625, "y1": 374.6499938964844, "x1": 111.95999908447266, "y2": 379.17999267578125}, "imageText": ["ref:", "author:", "sword", "of", "justice", "out1:", "author:", "the", "sword", "of", "righteousness", "out2:", "author:", "the", "sword", "of", "justice", "4", "cxt:", "sword", "of", "justice:", "prospects", "for", "2006", "ref:", "what", "are", "the", "basic", "problems?", "out1:", "what", "is", "the", "basic", "problem?", "out2:", "what", "are", "the", "basic", "questions?", "smoothly", "after", "the", "political", "review", "starts.", "therefore,", "we", "have", "to", "solve", "some", "basic", "problems", "first", "and", "this", "is", "a", "different", "thing", "all", "together.", "cxt:", ".", ".", ".", "however,", "legislator", "yeung,", "i", "wish", "to", "tell", "you", "what", "i", "am", "doing", "today", "is", "to", "ensure", "every", "matter", "can", "proceed", "3", "src:", "\u57fa\u672c\u7684\u95ee\u9898\u662f\u4ec0\u4e48\u5462?", "withdrawn", "to", "karachi.", "at", "present,", "their", "sentiments", "are", "stable.", "out2:", "at", "the", "same", "time,", "more", "than", "ten", "chinese", "personnel", "working", "with", "these", "people", "on", "the", "same", "site", "have", "also", "sentiments", "are", "now", "stable.", "out1:", "at", "the", "same", "time,", "more", "than", "ten", "chinese", "personnel", "working", "at", "the", "same", "site", "have", "also", "withdrawn", "to", "karachi.", "their", "back", "to", "karachi.", "at", "present", "they", "are", "emotionally", "stabilized.", "2", "ref:", "in", "the", "meantime,", "more", "than", "10", "chinese", "personnel", "working", "in", "the", "same", "place", "with", "these", "people", "have", "been", "called", "drugs", "and", "alcohol,", "he", "was", "really", "unscrupulous.", "out2:", "in", "an", "interview", "with", "the", "cbs", "news", "magazine", "\u2018\u201860", "minutes\",", "hoffmann", "sighed", "that", "at", "that", "time", "in", "order", "to", "obtain", "unscrupulous", "in", "getting", "drugs", "and", "alcohol.", "out1:", "in", "an", "interview", "with", "the", "cbs", "news", "magazine", "\u2018\u201860", "minutes\",", "hoffmann", "sighed", "that", "those", "days", "were", "really", "do", "anything", "to", "get", "drugs", "and", "alcohol.", "ref:", "in", "an", "interview", "on", "us", "cbs", "news", "magazine", "60", "minutes,", "hoffman", "softly", "sighed", "that", "in", "such", "period", "he", "would", "truly", "1", "src:", "\u970d\u592b\u66fc\u5728\u63a5\u53d7\u7f8e\u56fd\u54e5\u4f26\u6bd4\u4e9a\u5e7f\u64ad\u516c\u53f8\u65b0\u95fb\u6742\u5fd7\u300c\u516d\u5341\u5206\u949f\u300d\u8bbf\u95ee\u65f6\u8f7b\u53f9,\u90a3\u6bb5\u65f6\u671f\u4e3a\u4e86\u5f97\u5230\u6bd2\u54c1\u548c\u9152,\u771f", "\u662f\u4e0d\u62e9\u624b\u6bb5\u3002"], "regionBoundary": {"x2": 509.0, "y1": 67.59998321533203, "x1": 71.0, "y2": 362.0}, "caption": "Table 7: Example outputs from the document transformer (out1) and our doc-reranker (out2).", "page": 11}, {"figType": "Table", "name": "8", "captionBoundary": {"x2": 509.5002136230469, "y1": 747.6098022460938, "x1": 88.0801010131836, "y2": 752.1397705078125}, "imageText": ["ref:", "second,", "\u2018\u2018quota\"", "limits", "the", "nice", "growth", "trend", "in", "sino-america", "trade", "relation.", "out1:", "second,", "the", "\u2018\u2018restriction\"", "restricts", "the", "good", "development", "momentum", "of", "sino-us", "economic", "and", "trade", "relations.", "out2:", "second,", "the", "\u2018\u2018quota\"", "restricts", "the", "good", "development", "momentum", "of", "sino-us", "economic", "and", "trade", "relations.", "principle.", "on", "one", "hand,", "u.s.", "is", "talking", "in", "high-sounding", "tone", "about", "\u2018\u2018free", "trade\".", "on", "the", "other", "hand,", "it", "re-establishes", "trade", "barriers", "and", "stabs", "your", "back", "at", "will", "with", "\u2018\u2018quotas\".", "does", "it", "appear", "too", "arbitrary", "and", "unfair?", "9", "cxt:", ".", ".", ".", "first,", "such", "abuse", "of", "\u2018\u2018quota\"", "restricts", "the", "thorough", "implementation", "of", "world", "trade", "organization\u2019s", "free", "trade", "ref:", "\u2013", "continue", "to", "effectively", "tackle", "the", "tough", "issue", "of", "restructuring", "and", "shutting", "down.", "out1:", "\u2013", "we", "should", "continue", "to", "make", "a", "success", "of", "the", "rectification", "and", "closure", "battle.", "out2:", "\u2013", "continue", "to", "fight", "the", "battle", "of", "rectification", "and", "closure.", "to", "effectively", "tackle", "the", "tough", "issue", "of", "controlling", "methane.", ".", ".", ".", "8", "cxt:", ".", ".", ".", "with", "regard", "to", "coalmine", "safety", "this", "year,", "saws", "will", "effectively", "carry", "out", "the", "following", "three", "tasks:", "\u2013continue", "ref:", "the", "section", "of", "the", "highway", "from", "harbin", "to", "shuangcheng", "was", "closed,", "with", "many", "vehicles", "detoured.", "out1:", "part", "of", "the", "roads", "heading", "towards", "shuangcheng", "in", "harbin", "are", "closed,", "and", "many", "vehicles", "are", "bypassing.", "out2:", "part", "of", "the", "road", "from", "harbin", "to", "shuangcheng", "was", "closed", ",", "and", "many", "vehicles", "were", "bypassing.", "tanker", "leading", "to", "the", "closure", "of", "a", "section", "of", "the", "highway.", ".", ".", ".", "it", "was", "learned", "that", "the", "oil", "tanker", "contained", "waste", "oil", "from", "charcoal", "production.", ".", ".", ".", "7", "cxt:", ".", ".", ".", "a", "traffic", "accident", "occurred", "at", "the", "58", "kilometer", "point", "of", "the", "beijing-harbin", "highway,", "with", "a", "spill", "from", "an", "oil", "ref:", "now", "they", "will", "also", "be", "escorted", "home", "safely.", "out1:", "now", "they", "have", "to", "send", "it", "home", "safely.", "out2:", "now", "they", "want", "to", "send", "them", "safely", "to", "their", "homes.", "airport", "heaved", "a", "long", "sigh", "of", "relief.", ".", ".", ".", "after", "the", "incident", "occurred,", "it", "made", "proper", "arrangements", "for", "them.", "6", "src:", "\u73b0\u5728\u53c8\u8981\u5e73\u5b89\u7684\u9001\u5230\u5bb6\u91cc\u3002", "cxt:", ".", ".", ".", "when", "the", "plane", "carrying", "the", "three", "survivors", "and", "11", "other", "personnel", "arrived", "in", "Hefei,", "people", "waiting", "at", "the", "ref:", "at", "the", "same", "time,", "we", "in", "china", "verified", "the", "identities", "of", "the", "dead", "within", "the", "shortest", "possible", "time.", "out1:", "at", "the", "same", "time,", "we", "spent", "the", "shortest", "time", "in", "china", "to", "verify", "the", "identity", "of", "the", "deceased.", "out2:", "at", "the", "same", "time,", "we", "spent", "the", "shortest", "time", "in", "china", "to", "verify", "the", "identities", "of", "the", "deceased.", "5", "src:", "\u540c\u65f6\u6211\u4eec\u5728\u56fd\u5185\u7528\u6700\u77ed\u7684\u65f6\u95f4\uff0c\u6838\u5b9e\u6e05\u695a\u4e86\u6b7b\u4ea1\u4eba\u5458\u7684\u8eab\u4efd\u3002", "cxt:", ".", ".", ".", "the", "criminal", "used", "a", "submachine", "gun", "to", "fire", "a", "barrage", "of", "shots,", "and", "three", "engineers", "died", "unfortunately.", ".", ".", "."], "regionBoundary": {"x2": 517.0, "y1": 395.0, "x1": 80.0, "y2": 735.0}, "caption": "Table 8: Example outputs from the sent-reranker (out1) and the doc-reranker (out2). cxt refers to context.", "page": 11}, {"figType": "Table", "name": "6", "captionBoundary": {"x2": 513.6842041015625, "y1": 303.2770080566406, "x1": 319.44000244140625, "y2": 457.3018798828125}, "imageText": ["1", "70.41", "53.63", "2", "59.09", "54.70", "4", "53.54", "55.21", "Doc-transformer", "human", "4", "21.40", "\u2013", "Proposal", "#Experts", "pBLEU", "BLEU"], "regionBoundary": {"x2": 514.0, "y1": 204.0, "x1": 322.0, "y2": 290.0}, "caption": "Table 6: Pairwise-BLEU (pBLEU) (Shen et al., 2019) for candidate translations generated from different number of experts. BLEU from the doc-reranker taking different sets of candidate translations. We obtain different experts by training the document transformer (Zhang et al., 2018) with backtranslation with different random initialization. The size of the candidate pool is 50. The experts for the human proposal baseline are the reference translations.", "page": 8}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 290.25982666015625, "y1": 376.80999755859375, "x1": 72.0, "y2": 381.3399963378906}, "imageText": [], "regionBoundary": {"x2": 289.0, "y1": 205.0, "x1": 73.0, "y2": 358.0}, "caption": "Figure 4: Ratio of different models picking true targets.", "page": 8}, {"figType": "Table", "name": "5", "captionBoundary": {"x2": 476.74017333984375, "y1": 142.47702026367188, "x1": 120.83999633789062, "y2": 174.46002197265625}, "imageText": ["Gigaword", "+", "WMT", "63.8", "25.5", "26.3", "27.1", "This", "work", "Doc-reranker", "WMT", "106.3", "24.9", "26.0", "27.1", "Baseline", "transformer", "big", "\u2013", "\u2013", "23.9", "23.9", "24.5", "Method", "Model", "Unpaired", "Data", "LM", "PPL", "Test17", "Test18", "Test19"], "regionBoundary": {"x2": 477.0, "y1": 57.0, "x1": 123.0, "y2": 129.0}, "caption": "Table 5: SacreBLEU of different models on WMT19 validation and test sets and perplexity per word of the language models on the English side of WMT19 validation set.", "page": 8}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 730.7372199164496, "y1": 437.8472646077474, "x1": 426.8333435058594, "y2": 560.4720221625433}, "name": "1", "caption_text": "Figure 1: Graphical model showing the factorization of our noisy channel model where yi indicates the ith target language sentence andxi indicates the ith source language sentence. In the prior (top) the target sentences (the yi\u2019s) only influence the corresponding source sentence and therefore can be learned and modeled independently, but at test time (bottom), when the target is not observed, each yi depends on every xi.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 722.0, "y1": 80.0, "x1": 434.0, "y2": 412.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9442715115017, "y1": 362.6805199517144, "x1": 100.0, "y2": 402.13902791341144}, "name": "2", "caption_text": "Figure 2: The decoding process. In Phase 1, the auxiliary proposal model generates candidate translations (3 candidates in the diagram) for each sentence in the document (containing 4 sentences). In Phase 2, beam search is employed to search for the best path from the candidate translations.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 729.0, "y1": 80.0, "x1": 100.0, "y2": 337.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 722.5875854492188, "y1": 318.3846791585286, "x1": 107.66652425130208, "y2": 438.13883463541663}, "name": "1", "caption_text": "Table 1: Comparison with prior work on NIST Chinese\u2013English translation task. The evaluation metric is tokenized case-insensitive BLEU. The first three rows are numbers reported in the papers of prior work. The first two baselines are the results that we obtained by running the transformer (Vaswani et al., 2017) and the document transformer (Zhang et al., 2018) on the NIST dataset. The sent-reranker is a variation of our model in which sentences in documents are assumed to be independent. The backtranslation baseline is obtained by training the document transformer using additional synthetic parallel documents generated by backtranslation.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 723.0, "y1": 78.0, "x1": 108.0, "y2": 317.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 688.1098005506727, "y1": 254.21808030870224, "x1": 141.8333265516493, "y2": 317.63916015625}, "name": "2", "caption_text": "Table 2: BLEU scores on NIST dev set MT06 from rerankers which are incorporated with various language models. In the language model column X: Y means the language model X is trained on dataset Y. A bigger language model improves the doc-reranker but does not help the sent-reranker.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 688.0, "y1": 78.0, "x1": 145.0, "y2": 251.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 333.6390177408854, "y1": 609.3472374810112, "x1": 169.3333307902018, "y2": 615.638902452257}, "name": "3", "caption_text": "Figure 3: Effect of n-best list.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 401.0, "y1": 363.0, "x1": 101.0, "y2": 581.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 714.9498833550347, "y1": 518.5513814290365, "x1": 441.8333265516493, "y2": 563.1394704182942}, "name": "3", "caption_text": "Table 3: Perplexity per word of language models on NIST dev set. GW refers to Gigaword.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 715.0, "y1": 361.0, "x1": 442.0, "y2": 517.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 361.97259691026477, "y1": 235.18053690592447, "x1": 138.00027635362412, "y2": 241.47220187717014}, "name": "4", "caption_text": "Table 4: Effect of different components.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 393.0, "y1": 78.0, "x1": 99.0, "y2": 217.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 662.1391296386719, "y1": 197.88475036621094, "x1": 167.8333282470703, "y2": 242.30558607313367}, "name": "5", "caption_text": "Table 5: SacreBLEU of different models on WMT19 validation and test sets and perplexity per word of the language models on the English side of WMT19 validation set.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 663.0, "y1": 78.0, "x1": 171.0, "y2": 179.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.1386481391059, "y1": 523.3472188313802, "x1": 100.0, "y2": 529.6388838026259}, "name": "4", "caption_text": "Figure 4: Ratio of different models picking true targets.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 401.0, "y1": 284.0, "x1": 102.0, "y2": 497.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 713.4502834743923, "y1": 421.21806674533417, "x1": 443.66667005750867, "y2": 635.1414998372395}, "name": "6", "caption_text": "Table 6: Pairwise-BLEU (pBLEU) (Shen et al., 2019) for candidate translations generated from different number of experts. BLEU from the doc-reranker taking different sets of candidate translations. We obtain different experts by training the document transformer (Zhang et al., 2018) with backtranslation with different random initialization. The size of the candidate pool is 50. The experts for the human proposal baseline are the reference translations.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 714.0, "y1": 283.0, "x1": 444.0, "y2": 420.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 707.6391855875651, "y1": 1038.346947564019, "x1": 122.33347362942165, "y2": 1044.6385701497395}, "name": "8", "caption_text": "Table 8: Example outputs from the sent-reranker (out1) and the doc-reranker (out2). cxt refers to context.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 718.0, "y1": 548.0, "x1": 112.0, "y2": 1037.0}, "page": 11, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 674.6386210123698, "y1": 520.3472137451172, "x1": 155.49999872843424, "y2": 526.6388787163628}, "name": "7", "caption_text": "Table 7: Example outputs from the document transformer (out1) and our doc-reranker (out2).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 706.0, "y1": 83.0, "x1": 99.0, "y2": 502.0}, "page": 11, "dpi": 0}], "error": null, "pdf": "/work/host-output/d87f36502bb5c2ea08786eb545bdb66c272809ef/2020.tacl-1.23.pdf", "dpi": 100}