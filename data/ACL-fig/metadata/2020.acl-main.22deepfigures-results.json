{"raw_detected_boxes": [[{"x2": 722.0, "y1": 310.0, "x1": 434.0, "y2": 387.0}], [{"x2": 715.0, "y1": 90.0, "x1": 439.0, "y2": 271.0}], [{"x2": 723.0, "y1": 95.0, "x1": 104.0, "y2": 296.0}, {"x2": 729.0, "y1": 453.0, "x1": 432.0, "y2": 731.0}], [], [], [{"x2": 720.0, "y1": 92.0, "x1": 107.0, "y2": 226.0}], [{"x2": 729.0, "y1": 93.0, "x1": 105.0, "y2": 278.0}, {"x2": 727.0, "y1": 405.0, "x1": 431.0, "y2": 552.0}], [{"x2": 382.0, "y1": 86.0, "x1": 121.0, "y2": 160.0}, {"x2": 380.0, "y1": 258.0, "x1": 123.0, "y2": 349.0}, {"x2": 697.0, "y1": 94.0, "x1": 449.0, "y2": 276.0}], [], [], [], [], [{"x2": 398.0, "y1": 92.0, "x1": 102.0, "y2": 312.0}, {"x2": 717.0, "y1": 90.0, "x1": 439.0, "y2": 282.0}, {"x2": 725.0, "y1": 735.0, "x1": 429.0, "y2": 781.0}], [{"x2": 722.0, "y1": 92.0, "x1": 102.0, "y2": 1020.0}], [{"x2": 409.0, "y1": 196.0, "x1": 101.0, "y2": 446.0}, {"x2": 396.0, "y1": 527.0, "x1": 103.0, "y2": 752.0}]], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Figure", "name": "1", "captionBoundary": {"x2": 527.2898559570312, "y1": 292.8935241699219, "x1": 307.2760009765625, "y2": 370.6269836425781}, "imageText": ["Transformer", "seq2seq", "Source", "order", "encoding", "The", "game", "was", "won", "by", "the", "Clippers.", "YNP", "won", "by", "XNP", "4", "3", "1", "2", "Clippers", "won", "the", "game", "XNP", "won", "YNP", "NPVBD", "NP", "VP", "won", "the", "game", "Clippers", "Rearrangement", "Aware", "ParaphrasingSource", "Order", "Rewriting", "S"], "regionBoundary": {"x2": 520.0, "y1": 223.8900146484375, "x1": 312.0, "y2": 278.8900146484375}, "caption": "Figure 1: Overview of our paraphrase model. First, we choose various pairs of constituents to abstract away in the source sentence, then use a neural transducer to generate possible reorderings of the abstracted sentences. From these, we construct a guide reordering of the input sentence which then informs the generation of output paraphrases.", "page": 0}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 525.5473022460938, "y1": 177.94754028320312, "x1": 71.64099884033203, "y2": 207.86004638671875}, "imageText": ["SOW-REAP", "(LSTM)", "27.0", "57.9", "34.8", "57.5", "31.7", "46.2", "53.9", "SOW-REAP", "30.9", "62.3", "40.2", "61.7", "15.9", "38.0", "57.9", "Transformer", "seq2seq", "32.8", "63.1", "41.4", "63.3", "12.7", "50.7", "35.4", "+", "diverse-decoding", "24.8", "56.8", "33.2", "56.4", "21.3", "34.2", "58.1", "copy-input", "18.4", "54.4", "27.2", "49.2", "0", "\u2212", "\u2212", "SCPN", "21.3", "53.2", "30.3", "51.0", "40.6", "35.9", "63.4", "BLEU", "ROUGE-1", "ROUGE-2", "ROUGE-L", "%", "rejected", "self-BLEU", "\u2193", "self-WER", "\u2191", "Model", "oracle", "quality", "(over", "10", "sentences,", "no", "rejection)", "\u2191", "pairwise", "diversity", "(post-rejection)"], "regionBoundary": {"x2": 523.0, "y1": 65.8900146484375, "x1": 74.0, "y2": 165.8900146484375}, "caption": "Table 1: Quality and diversity metrics for the different models. Our proposed approach outperforms other diverse models (SCPN and diverse decoding) in terms of all the quality metrics. These models exhibit higher diversity, but with many more rejected paraphrases, indicating that these models more freely generate bad paraphrases.", "page": 5}, {"figType": "Table", "name": "10", "captionBoundary": {"x2": 290.2704772949219, "y1": 556.6555786132812, "x1": 71.69100189208984, "y2": 574.613037109375}, "imageText": ["Ignored", "tags", "DT,", "IN,", "CD,", "MD,", "TO,", "PRP", "Max.", "no.", "of", "rules", "3", "Recombination", "of", "rules/transductions", "Optimizer", "Adam,", "\u03b2", "=", "(0.9,", "0.999),", "=", "10\u22128", "Learning", "rate", "0.0001", "Batch", "size", "32", "Epochs", "50", "(maximum)", "Training", "Hidden", "size", "256", "Num", "layers", "2", "Num", "heads", "8", "Dropout", "0.1", "Seq2seq", "transformer", "architecture"], "regionBoundary": {"x2": 285.0, "y1": 379.8900146484375, "x1": 72.0, "y2": 541.8900146484375}, "caption": "Table 10: Hyperparameters used in the implementation of the SOW model.", "page": 14}, {"figType": "Table", "name": "9", "captionBoundary": {"x2": 290.2704772949219, "y1": 333.61053466796875, "x1": 71.69100189208984, "y2": 351.5679931640625}, "imageText": ["k", "in", "top-k", "20", "Beam", "Size", "10", "Inference", "epochs),", "0", "(rest)", "Optimizer", "Adam,", "\u03b2", "=", "(0.9,", "0.999),", "=", "10\u22128", "Learning", "rate", "0.0001", "Batch", "size", "32", "Epochs", "50", "(maximum)", "Coverage", "loss", "coeff.", "1", "(\ufb01rst", "10", "epochs),", "0.5", "(10", "-", "20", "Training", "Hidden", "size", "256", "Num", "layers", "2", "Num", "heads", "8", "Dropout", "0.1", "Seq2seq", "transformer", "architecture"], "regionBoundary": {"x2": 295.0, "y1": 136.8900146484375, "x1": 72.0, "y2": 320.8900146484375}, "caption": "Table 9: Hyperparameters used in the implementation of the REAP model.", "page": 14}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 527.2003784179688, "y1": 208.48153686523438, "x1": 307.2760009765625, "y2": 274.26007080078125}, "imageText": ["The", "game", "Clippers", "won", "the", "game", "BOS", "The", "Output", "tokens", "y", "1", "2", "3", "4", "4", "3", "1", "2", "New", "Encoder", "Output", "E", "Target", "Order", "r", "Encoder", "Output", "EM", "Original", "Order", "Token", "embeddings", "Input", "tokens", "x", "+", "+", "+", "+", "Encoder", "Decoder"], "regionBoundary": {"x2": 516.0, "y1": 64.8900146484375, "x1": 316.1635437011719, "y2": 195.8900146484375}, "caption": "Figure 2: Rearrangement aware paraphrasing (REAP) model. The gray area corresponds to the standard transformer encoder-decoder system. Our model adds position embeddings corresponding to the target reordering to encoder outputs. The decoder attends over these augmented encodings during both training and inference.", "page": 1}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 525.8960571289062, "y1": 212.32052612304688, "x1": 71.69100189208984, "y2": 242.2330322265625}, "imageText": ["i", "used", "hydroponics", "for", "the", "\ufb01rst", "time", ".", "i", "used", "to", "use", "hydroponics", ".", "to", "use", "hydroponics", "the", "\ufb01rst", "time", "i", "was", ".", "\ufb01rst", "i", "was", "the", "\ufb01rst", "grower", "to", "use", "hydroponics", "i", "was", "the", "\ufb01rst", "grower", "to", "use", "hydroponics", ".", "to", "use", "hydroponics", ",", "i", "was", "the", "\ufb01rst", "one", ".", "where", "did", "i", "have", "the", "\ufb01rst", "tendency", "to", "use", "hydropon-", "ics", "?", "if", "the", "product", "integrity", "of", "this", "container", "is", "compromised", "it", "should", "not", "be", "used", ".", "i", "should", "not", "use", "if", "at", "any", "time", "in", "the", "preparation", "of", "this", "product", ",", "it", "should", "not", "be", "used", ".", "if", "at", "any", "time", "in", "the", "preparation", "of", "this", "product", "the", "integrity", "of", "this", "container", "is", "compromised", "it", "should", "not", "be", "used", ".", "this", "container", "should", "not", "be", "used", "if", "any", "time", "in", "the", "preparation", "of", "this", "product", "is", "compromised", "in", "the", "preparation", "of", "this", "product", ",", "the", "integrity", "of", "this", "container", "is", "compromised", ",", "but", "it", "should", "not", "be", "used", ".", "if", "the", "integrity", "of", "the", "packaging", "is", "impaired", "at", "any", "time", ",", "the", "product", "should", "not", "be", "used", ".", "where", "is", "the", "integrity", "of", "this", "product", "of", "this", "container", "the", "integrity", "of", "this", "container", "should", "not", "be", "used", ".", "Input", "SOW-REAP", "SCPN"], "regionBoundary": {"x2": 528.0, "y1": 68.30574798583984, "x1": 72.0, "y2": 199.8900146484375}, "caption": "Table 2: Examples of paraphrases generated by our system and the baseline SCPN model. Our model successfully rearranges the different structural components of the input sentence to obtain meaningful rearrangements. SCPN conforms to pre-enumerated templates that may not align with a given input.", "page": 6}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 527.2003784179688, "y1": 409.2855529785156, "x1": 306.9670104980469, "y2": 475.06396484375}, "imageText": ["Generated", "Sentence:", "this", "container", "should", "not", "be", "used", "if", "the", "product", "is", "compromised", "at", "any", "time", "in", "preparation", ".", "the", "NN", "of", "NP\u2192", "NP", "NN", "(parse", "tree", "level:", "2)", "at", "NP", "the", "integrity", "of", "this", "container", "VBZ", "compromised\u2192", "this", "container", "VBZ", "weakened", "at", "NP", "(parse", "tree", "level:", "1)", "Rule", "Sequence:", "if", "S", "it", "should", "not", "VB", "used", ".\u2192", "should", "not", "VB", "used", "if", "S", "(parse", "tree", "level:", "0)", "Input", "Sentence:", "if", "at", "any", "time", "in", "the", "preparation", "of", "this", "product", "the", "integrity", "of", "this", "container", "is", "compromised", "it", "should", "not", "be", "used", "."], "regionBoundary": {"x2": 527.0, "y1": 265.8900146484375, "x1": 307.0, "y2": 396.8900146484375}, "caption": "Table 3: Examples of our model\u2019s rearrangements applied to a given input sentence. Parse tree level indicates the rule subtree\u2019s depth from the root node of the sentence. The REAP model\u2019s final generation considers the rule reordering at the higher levels of the tree but ignores the rearrangement within the lower sub-tree.", "page": 6}, {"figType": "Table", "name": "8", "captionBoundary": {"x2": 525.5473022460938, "y1": 746.2915649414062, "x1": 71.64099884033203, "y2": 788.1589965820312}, "imageText": ["that", "S", "i", "VBP", ".\u2192", "i", "VBP", "S", ".", "NN", "gets", "me", "PP\u2192", "PP", ",", "NN", "gets", "me", ".", "i", "want", "a", "dress", "in", "front", "of", "me", ".", "S", "(", "VP", ".", ")", "i", "want", "everywhere", ".", "that", "S", "i", "VBP", ".\u2192", "i", "VBP", "S", ".", "i", "want", "that", "dress", "gets", "me", "into", "the", "place", ".", "NP", "(", "NP", ".", ")", "that", "dress", "gets", "me", "in", "there", ",", "i", "wish", ".", "Input:", "that", "dress", "gets", "me", "into", "anywhere", "i", "want", ".", "NP1", "considering", "NP2", ".", "\u2192", "consid-", "ering", "NP2", "for", "NP1", "NN", "of", "NP\u2192", "NP", "NN", "NP", "in", "JJ", "york\u2192", "JJ", "york", "NP", "in", "new", "york", ",", "the", "number", "of", "phones", "is", "a", "minor", "risk", ".", "FRAG", "(", "SBAR", ")", ".", "that", "minor", "risk", "is", "the", "num-", "ber", "of", "telephones", "in", "new", "york", ".", "a", "JJ", "risk", "considering", "NP", ".\u2192", "NP", "is", "a", "JJ", "risk", ".", "the", "NN", "of", "NP\u2192", "NP", "NN", "phones", "in", "new", "york", "are", "a", "mi-", "nor", "risk", "considering", ".", "SBARQ", "(", "WHADVP", "SQ", ".)", "when", "do", "you", "consider", "the", "number", "of", "telephones", "in", "new", "york", "?", "Input:", "a", "minor", "risk", "considering", "the", "number", "of", "telephones", "in", "new", "york", ".", "ADJP", ",", "S", ".\u2192", "S", ",", "ADJP", ".", "well", ",", "NP", "VBZ", "calling\u2192", "VBZ", "call-", "ing", "NP", "we", "\u2019ll", "call", "it", "tonight", ",", "okay", "?", "S", "(", "ADVP", "NP", "VP", ".", ")", "of", "course", ",", "the", "occasion", "is", "calling", ".", "ADJP", ",", "S", ".\u2192", "S", ",", "ADJP", ".", "well", ",", "NN", "the", "occasion", "VP\u2192", "the", "occasion", "VP", ",", "NN", "the", "occasion", "is", "calling", "today", ",", "okay", "?", "S", "(", "NP", "VP", ".", ")", "the", "opportunity", "is", "calling", ".", "Input:", "okay", ",", "well", ",", "tonight", "the", "occasion", "is", "calling", ".", "ADVP", "VBN", "in", "future", "reviews", "\u2192", "VBN", "in", "future", "reviews", "ADVP", "priority", "measures", "should", "be", "speci\ufb01ed", "in", "future", "reviews", "clearly", ".", "SBARQ", "(", "WHADVP", "SQ", ".", ")", "where", "should", "priority", "ac-", "tions", "are", "more", "clearly", "spec-", "i\ufb01ed", "in", "future", "reviews", "?", "NP", "should", "be", "more", "clearly", "speci\ufb01ed", "PP", ".\u2192", "PP", ",", "NP", "should", "be", "clearly", "speci\ufb01ed", ".", "in", "future", "reviews", ",", "priority", "measures", "should", "be", "more", "clearly", "speci\ufb01ed", ".", "S", "(", "S", ",", "CC", "S", ".", ")", "priority", "actions", "should", "be", "more", "clearly", "speci\ufb01ed", "in", "future", "reviews", ",", "and", "they", "should", "be", "informed", ".", "Input:", "priority", "actions", "should", "be", "more", "clearly", "speci\ufb01ed", "in", "future", "reviews", ".", "NP", "knew", "SBAR", ".", "\u2192", "SBAR", ",", "S", "knew", ".", "where", "the", "money", "came", "from", ",", "i", "lent", "it", "to", "me", "before", "i", "knew", ".", "S", "(", "NP", "VP", ".", ")", "i", "borrowed", "money", "before", "i", "knew", "where", "the", "money", "came", "from", ".", "i", "VBN", "it", "before", "i", "VP", ".\u2192", "before", "i", "VP", ",", "i", "VBN", "it", ".", "before", "i", "knew", "where", "the", "money", "came", "from", ",", "i", "rented", "it", ".", "SBARQ", "(", "WHADVP", "SQ", ".", ")", "where", "did", "you", "learn", "that", "it", "was", "the", "money", "?", "Input:", "i", "leased", "it", "before", "i", "knew", "where", "the", "money", "came", "from", ".", "the", "story", "PP", "NNS", "here", ".", "\u2192", "there", "NNS", "a", "story", "PP", ".", "here", "ends", "the", "story", "of", "obi-", "wan", "kenobi", ".", "S", "(", "S", ",", "CC", "S", ".", ")", "the", "story", "of", "obi-wan", "kenobi", "is", "here", ",", "and", "it", "ends", "here", ".", "end", "of", "the", "obi-wan", "kenobi", "story", ".", "S", "(", "VP", ".", ")", "tell", "the", "story", "of", "obi-wan", "kenobi", ".", "NP", "VP", ".\u2192", "VP", "is", "NP", "the", "NN", "of", "NP\u2192", "NP", "NN", ".", "Input:", "the", "story", "of", "obi-wan", "kenobi", "ends", "here", ".", "NP", "normally", "VP", ":\u2192", "usually", "VP", ",", "NP", "VBZ", "the", "following", "NN\u2192", "the", "NN", "VBZ", "normally", "the", "following", "in-", "formation", "shall", "be", "included", "in", "the", "public", "procurement", "re-", "sult", "report", ":", "S", "(", "PP", ",", "NP", "VP", ".", ")", "in", "the", "public", "competition", ",", "the", "report", "on", "competition", "contains", "the", "following", "in-", "formation", ".", "NP", "normally", "contains", "the", "following", "NN:", "\u2192", "the", "following", "NN", "usually", "contains", "in", "NP", ":", "the", "following", "information", "shall", "normally", "be", "included", "in", "the", "public", "procurement", "re-", "port", ":", "SBARQ", "(", "WHADVP", "SQ", ".", ")", "where", "is", "the", "public", "pro-", "curement", "report", "report", "usu-", "ally", "contains", "the", "following", "information", ".", "Input:", "the", "public", "tender", "result", "message", "normally", "contains", "the", "following", "information", ":", "Rules", "(SOW)", "Output", "(REAP)", "Template", "(SCPN)", "Output", "(SCPN)"], "regionBoundary": {"x2": 521.0, "y1": 65.8900146484375, "x1": 72.0, "y2": 733.8900146484375}, "caption": "Table 8: Examples of paraphrases generated by our system and the baseline SCPN model. The outputs from our model successfully rearranges the different structural components of the input sentence to obtain meaningful rearrangements. SCPN on the other hand tends to conform to pre-specified templates that are often not aligned with a given input.", "page": 13}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 526.7926025390625, "y1": 228.89157104492188, "x1": 72.0, "y2": 282.715087890625}, "imageText": ["Derived", "reorderings", "REORDER(B)", "r", "r", "If", "it", "continues", "to", "rain,", "an", "umbrella", "is", "what", "i", "will", "carry.", "REORDER(A)", "If", "it", "continues", "to", "rain", "I", "will", "carry", "an", "umbrella", "6", "7", "10", "8", "9", "1", "2", "3", "4", "5", "If", "S", "I", "will", "carry", "an", "umbrella", "6", "7", "1", "2", "3", "4", "5", "SBAR", "I", "will", "carry", "NP", "If", "S", "I", "will", "VP", "4", "5", "1", "2", "3", "SOW", "REAP", "FOR", "A,", "B", "SELECTSEGMENTPAIRS(S0)\u2208", "REORDERPHRASE", "(S0,", "A,", "B)", "I", "will", "carry", "an", "umbrella", "if", "rain", "continues.", "Final", "paraphrases", "Source", "reordering", "SBAR", "NP", "I", "carry", "If", "S", "I", "will", "VP", "I", "will", "VP", "if", "S", "SBAR", "I", "will", "carry", "NP", "REORDERPHRASE:", "Use", "seq2seq", "model", "to", "reorder", "phrase", "SELECTSEGMENTPAIRS:", "Choose", "constituents", "to", "abstract", "SBAR", "I", "will", "carry", "NP", "1", "3", "4", "5", "2", "If", "S", "I", "will", "VP", "4", "5", "1", "2", "3", "SOW", "Input", "SOW", "Output", "VB", "NP", "MD", "VP2", "If", "it", "continues", "to", "rain", "I", "will", "carry", "an", "umbrella", "PRP", "VP1", "IN", "S", "SBAR", "PRP", "VP", "S0", "REORDER(S0):", "Recursively", "reorder", "constituents", "to", "get", "\ufb01nal", "ordering"], "regionBoundary": {"x2": 523.0, "y1": 67.8900146484375, "x1": 72.0, "y2": 212.8900146484375}, "caption": "Figure 3: Overview of the source sentence rearrangement workflow for one level of recursion at the root node. First, candidate tree segment pairs contained within the input node are selected. A transduction operation is applied over the abstracted phrase, giving the reordering 4 5 1 2 3 for the case shown in red, then the process recursively continues for each abstracted node. This results in a reordering for the full source sentence; the reordering indices serve as additional input to the REAP model.", "page": 2}, {"figType": "Table", "name": "6", "captionBoundary": {"x2": 291.9244079589844, "y1": 236.73153686523438, "x1": 71.69100189208984, "y2": 278.59906005859375}, "imageText": ["there", "is", "already", "a", "ring", "NN", "PP", ".", "PP", "circular", "NN", "exist", ".", "was", "a", "black", "NN", "passage", "PP", ".", "PP", "was", "a", "black", "NN", "archway", ".", "in", "the", "abandoned", "NNS", ",", "there", "was", "NP", ".", "NP", "lingered", "in", "the", "deserted", "NNS", ".", "in", "the", "case", "of", "imposition", "of", "NP", ",", "they", "would", "consider", "VP", "they", "might", "consider", "VP", "if", "NP", "were", "imposed", "removing", "the", "NN", "from", "NP", "excluding", "this", "NN", "from", "NP", "SOW", "Input", "SOW", "Output"], "regionBoundary": {"x2": 292.0, "y1": 62.8900146484375, "x1": 72.0, "y2": 224.8900146484375}, "caption": "Table 6: Examples of aligned phrase pairs with exactly two sub-phrases abstracted out and replaced with constituent labels. These phrase pairs are used to train the SOW MODEL.", "page": 12}, {"figType": "Table", "name": "7", "captionBoundary": {"x2": 526.7916870117188, "y1": 574.215576171875, "x1": 306.9670104980469, "y2": 604.1280517578125}, "imageText": ["I:", "his", "teammates", "eyes", "got", "an", "ugly,", "hostile", "expression.", "E:", "the", "smell", "of", "\ufb02owers", "was", "thick", "and", "sweet.", "O:", "the", "eyes", "of", "his", "teammates", "had", "turned", "ugly", "and", "hostile."], "regionBoundary": {"x2": 525.0, "y1": 524.8900146484375, "x1": 307.0, "y2": 561.8900146484375}, "caption": "Table 7: Example of input (I), syntactic exemplar (E), and the reference output (O) from the evaluation test set of (Chen et al., 2019b).", "page": 12}, {"figType": "Figure", "name": "6", "captionBoundary": {"x2": 527.2001953125, "y1": 215.28451538085938, "x1": 307.2759704589844, "y2": 257.15301513671875}, "imageText": ["Token", "Embeddings", "Y", "if", "If", "X", "then", "Y", "BOS", "Y", "Output", "tokens", "y", "1", "2", "3", "4", "0", "2", "0", "1", "New", "Encoder", "Output", "E", "Order", "o", "=", "FLIP", "Encoder", "Output", "EM", "Original", "Order", "POS", "Embeddings", "Input", "tokens", "x", "+", "+", "+", "+", "Encoder", "Decoder"], "regionBoundary": {"x2": 516.0, "y1": 64.8900146484375, "x1": 316.1635437011719, "y2": 203.8900146484375}, "caption": "Figure 6: Source Order reWriting (SOW) model. Our model encodes order preference MONOTONE or FLIP through position embeddings added to the encoder output.", "page": 12}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 291.92437744140625, "y1": 127.1425552368164, "x1": 71.69100189208984, "y2": 169.01007080078125}, "imageText": ["SOW-REAP", "44.5", "22.6", "32.9", "SCPN", "(Iyyer", "et", "al.,", "2018)", "35.9", "24.8", "39.3", "Transformer", "seq2seq", "45.1", "20.6", "34.3", "Model", "2", "1", "0"], "regionBoundary": {"x2": 275.0, "y1": 62.8900146484375, "x1": 87.0, "y2": 114.8900146484375}, "caption": "Table 4: Human annotated quality across different models. The evaluation was done on a 3 point quality scale, 2 = grammatical paraphrase, 1 = ungrammatical paraphrase, 0 = not a paraphrase.", "page": 7}, {"figType": "Figure", "name": "5", "captionBoundary": {"x2": 527.2003784179688, "y1": 216.16757202148438, "x1": 307.2759704589844, "y2": 293.901123046875}, "imageText": ["Monotone", "r", "Ground", "Truth", "r\u2217", "t", "tp", "u", "ed", "ou", "er", "at", "ge", "n", "an", "d", "u", "t", "in", "p", "b", "/w", "e", "n", "t", "g", "e", "m", "a", "n", "R", "e", "a", "rr", "e", "o", "f", "D", "e", "g", "re", "ie", "v", "e", "d", "A", "ch", "1.0", "0.5", "0.0", "\u22120.5", "\u22120.5", "\u22120.2", "0.1", "0.4", "0.7", "1.0", "Target", "Degree", "of", "Rearrangement", "b/w", "input", "and", "ground", "truth", "output"], "regionBoundary": {"x2": 504.0, "y1": 67.8900146484375, "x1": 326.1356506347656, "y2": 196.4840087890625}, "caption": "Figure 5: The degree of rearrangement (Kendall\u2019s Tau) achieved by conditioning on monotone and pseudoground truth reorderings (r\u2217). The dotted line denotes the ideal performance (in terms of reorderingcompliance) of the REAP model, when supplied with perfect reordering r\u2217. The actual performance of the REAP model mirrors the ideal performance.", "page": 7}, {"figType": "Table", "name": "5", "captionBoundary": {"x2": 291.92437744140625, "y1": 266.0375671386719, "x1": 71.69100189208984, "y2": 295.9510498046875}, "imageText": ["Ground", "Truth", "7.79", "36.40", "SOW", "8.14", "30.02", "Monotone", "10.59", "27.98", "Random", "9.32", "27.10", "Ordering", "oracle-ppl", "\u2193", "oracle-BLEU", "\u2191"], "regionBoundary": {"x2": 274.0, "y1": 185.8900146484375, "x1": 89.0, "y2": 253.8900146484375}, "caption": "Table 5: Comparison of different source reordering strategies. Our proposed approach outperforms baseline monotone and random rearrangement strategies.", "page": 7}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 291.9244079589844, "y1": 121.42557525634766, "x1": 72.0, "y2": 151.33905029296875}, "imageText": ["I", "will", "carry", "an", "umbrella", "if", "rain", "continues", "If", "it", "continues", "to", "rain", "I", "will", "carry", "an", "umbrella"], "regionBoundary": {"x2": 262.21051025390625, "y1": 66.8900146484375, "x1": 95.0, "y2": 108.8900146484375}, "caption": "Figure 4: Paraphrase sentence pair and its aligned tuples A \u2192 B,C and A\u2032 \u2192 B\u2032, C \u2032. These produce the training data for the SOW MODEL.", "page": 4}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 732.3470221625433, "y1": 406.7965613471137, "x1": 426.772223578559, "y2": 514.7596995035807}, "name": "1", "caption_text": "Figure 1: Overview of our paraphrase model. First, we choose various pairs of constituents to abstract away in the source sentence, then use a neural transducer to generate possible reorderings of the abstracted sentences. From these, we construct a guide reordering of the input sentence which then informs the generation of output paraphrases.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 723.0, "y1": 310.0, "x1": 434.0, "y2": 387.0}, "page": 0, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2227478027344, "y1": 289.5576900906033, "x1": 426.772223578559, "y2": 380.9167650010851}, "name": "2", "caption_text": "Figure 2: Rearrangement aware paraphrasing (REAP) model. The gray area corresponds to the standard transformer encoder-decoder system. Our model adds position embeddings corresponding to the target reordering to encoder outputs. The decoder attends over these augmented encodings during both training and inference.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 717.0, "y1": 90.0, "x1": 439.0, "y2": 271.0}, "page": 1, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 731.6563924153645, "y1": 317.9049597846137, "x1": 100.0, "y2": 392.65984429253473}, "name": "3", "caption_text": "Figure 3: Overview of the source sentence rearrangement workflow for one level of recursion at the root node. First, candidate tree segment pairs contained within the input node are selected. A transduction operation is applied over the abstracted phrase, giving the reordering 4 5 1 2 3 for the case shown in red, then the process recursively continues for each abstracted node. This results in a reordering for the full source sentence; the reordering indices serve as additional input to the REAP model.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 727.0, "y1": 93.0, "x1": 100.0, "y2": 296.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9268086751301, "y1": 247.14936150444876, "x1": 99.50138727823892, "y2": 288.6945088704427}, "name": "1", "caption_text": "Table 1: Quality and diversity metrics for the different models. Our proposed approach outperforms other diverse models (SCPN and diverse decoding) in terms of all the quality metrics. These models exhibit higher diversity, but with many more rejected paraphrases, indicating that these models more freely generate bad paraphrases.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 727.0, "y1": 86.0, "x1": 103.0, "y2": 230.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 730.4111904568142, "y1": 294.8896196153429, "x1": 99.57083596123589, "y2": 336.4347669813368}, "name": "2", "caption_text": "Table 2: Examples of paraphrases generated by our system and the baseline SCPN model. Our model successfully rearranges the different structural components of the input sentence to obtain meaningful rearrangements. SCPN conforms to pre-enumerated templates that may not align with a given input.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 733.0, "y1": 86.0, "x1": 100.0, "y2": 295.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2227478027344, "y1": 568.452156914605, "x1": 426.3430701361762, "y2": 659.8110622829861}, "name": "3", "caption_text": "Table 3: Examples of our model\u2019s rearrangements applied to a given input sentence. Parse tree level indicates the rule subtree\u2019s depth from the root node of the sentence. The REAP model\u2019s final generation considers the rule reordering at the higher levels of the tree but ignores the rearrangement within the lower sub-tree.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 732.0, "y1": 389.0, "x1": 426.0, "y2": 569.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.45052422417535, "y1": 176.5868822733561, "x1": 99.57083596123589, "y2": 234.7362094455295}, "name": "4", "caption_text": "Table 4: Human annotated quality across different models. The evaluation was done on a 3 point quality scale, 2 = grammatical paraphrase, 1 = ungrammatical paraphrase, 0 = not a paraphrase.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 399.0, "y1": 86.0, "x1": 104.0, "y2": 177.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.45052422417535, "y1": 369.49662102593317, "x1": 99.57083596123589, "y2": 411.0431247287326}, "name": "5", "caption_text": "Table 5: Comparison of different source reordering strategies. Our proposed approach outperforms baseline monotone and random rearrangement strategies.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 380.0, "y1": 258.0, "x1": 123.0, "y2": 353.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2227478027344, "y1": 300.2327389187283, "x1": 426.7721811930338, "y2": 408.1960042317708}, "name": "5", "caption_text": "Figure 5: The degree of rearrangement (Kendall\u2019s Tau) achieved by conditioning on monotone and pseudoground truth reorderings (r\u2217). The dotted line denotes the ideal performance (in terms of reorderingcompliance) of the REAP model, when supplied with perfect reordering r\u2217. The actual performance of the REAP model mirrors the ideal performance.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 700.0, "y1": 94.0, "x1": 449.0, "y2": 276.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.4505666097005, "y1": 328.7938012017144, "x1": 99.57083596123589, "y2": 386.9431389702691}, "name": "6", "caption_text": "Table 6: Examples of aligned phrase pairs with exactly two sub-phrases abstracted out and replaced with constituent labels. These phrase pairs are used to train the SOW MODEL.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 405.0, "y1": 86.0, "x1": 100.0, "y2": 329.0}, "page": 12, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2224934895833, "y1": 299.0062713623047, "x1": 426.7721811930338, "y2": 357.1569654676649}, "name": "6", "caption_text": "Figure 6: Source Order reWriting (SOW) model. Our model encodes order preference MONOTONE or FLIP through position embeddings added to the encoder output.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 717.0, "y1": 90.0, "x1": 427.0, "y2": 299.0}, "page": 12, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 731.6551208496094, "y1": 797.5216335720486, "x1": 426.3430701361762, "y2": 839.0667385525173}, "name": "7", "caption_text": "Table 7: Example of input (I), syntactic exemplar (E), and the reference output (O) from the evaluation test set of (Chen et al., 2019b).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 729.0, "y1": 729.0, "x1": 426.0, "y2": 798.0}, "page": 12, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9268086751301, "y1": 1036.5160624186199, "x1": 99.50138727823892, "y2": 1094.6652730305989}, "name": "8", "caption_text": "Table 8: Examples of paraphrases generated by our system and the baseline SCPN model. The outputs from our model successfully rearranges the different structural components of the input sentence to obtain meaningful rearrangements. SCPN on the other hand tends to conform to pre-specified templates that are often not aligned with a given input.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 724.0, "y1": 86.0, "x1": 100.0, "y2": 1037.0}, "page": 13, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.1534406873915, "y1": 463.34796481662323, "x1": 99.57083596123589, "y2": 488.28887939453125}, "name": "9", "caption_text": "Table 9: Hyperparameters used in the implementation of the REAP model.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 410.0, "y1": 190.0, "x1": 100.0, "y2": 463.0}, "page": 14, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.1534406873915, "y1": 773.1327480740017, "x1": 99.57083596123589, "y2": 798.0736626519097}, "name": "10", "caption_text": "Table 10: Hyperparameters used in the implementation of the SOW model.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 396.0, "y1": 527.0, "x1": 100.0, "y2": 756.0}, "page": 14, "dpi": 0}], "error": null, "pdf": "/work/host-output/c10708e00751ab5400128b7819c76e9cb1d03f7f/2020.acl-main.22.pdf", "dpi": 100}