{"raw_detected_boxes": [[], [{"x2": 699.0, "y1": 90.0, "x1": 463.0, "y2": 333.0}], [], [{"x2": 353.0, "y1": 94.0, "x1": 147.0, "y2": 221.0}, {"x2": 341.0, "y1": 316.0, "x1": 162.0, "y2": 490.0}], [{"x2": 360.0, "y1": 90.0, "x1": 136.0, "y2": 468.0}], [{"x2": 725.0, "y1": 367.0, "x1": 431.0, "y2": 597.0}], [{"x2": 718.0, "y1": 93.0, "x1": 106.0, "y2": 561.0}], []], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Figure", "name": "5", "captionBoundary": {"x2": 527.200439453125, "y1": 445.38153076171875, "x1": 307.0270080566406, "y2": 487.2499694824219}, "imageText": [], "regionBoundary": {"x2": 526.0, "y1": 261.8900146484375, "x1": 307.0, "y2": 433.8900146484375}, "caption": "Figure 5: Scatterplot of sentence-level NDCG@10 vs sentence-level BLEU on zh-en and en-gu. For better visualization, only 300 random samples from each language direction are shown.", "page": 5}, {"figType": "Figure", "name": "1", "captionBoundary": {"x2": 527.200439453125, "y1": 254.32052612304688, "x1": 307.2760009765625, "y2": 332.05401611328125}, "imageText": [], "regionBoundary": {"x2": 508.0, "y1": 61.8900146484375, "x1": 325.0, "y2": 242.8900146484375}, "caption": "Figure 1: The system architecture of CLIReval. Documents from input files are separately indexed into two instances of IR systems. Generated search queries are used to query both IR instances. Search scores from REF-IR are converted to discrete relevance judgment labels as required by trec eval. Finally, CLIReval uses trec eval to calculate IR metrics.", "page": 1}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 525.5472412109375, "y1": 418.59454345703125, "x1": 71.69100189208984, "y2": 448.5069885253906}, "imageText": ["zh\u2192en", "0.899", "0.921", "0.840", "0.942", "0.930", "0.922", "0.622", "0.957", "ru\u2192en", "0.879", "0.925", "0.917", "0.915", "0.922", "0.920", "0.866", "0.961", "lt\u2192en", "0.961", "0.944", "0.960", "0.947", "0.636", "0.612", "0.929", "0.865", "kk\u2192en", "0.946", "0.942", "0.799", "0.986", "0.970", "0.968", "0.986", "0.983", "gu\u2192en", "0.834", "0.930", "0.890", "0.952", "0.814", "0.809", "0.782", "0.824", "\ufb01\u2192en", "0.982", "0.986", "0.984", "0.993", "0.956", "0.955", "0.944", "0.960", "en\u2192zh", "0.901", "0.884", "0.856", "0.803", "0.928", "0.930", "0.772", "0.902", "en\u2192ru", "0.986", "0.988", "0.995", "0.977", "0.865", "0.862", "0.980", "0.953", "en\u2192lt", "0.989", "0.993", "0.994", "0.982", "0.776", "0.791", "0.903", "0.916", "en\u2192kk", "0.852", "0.930", "0.940", "0.971", "0.982", "0.982", "0.963", "0.968", "en\u2192gu", "0.737", "0.786", "0.865", "0.829", "0.912", "0.909", "0.833", "0.847", "en\u2192\ufb01", "0.969", "0.971", "0.981", "0.989", "0.915", "0.906", "0.927", "0.944", "en\u2192de", "0.921", "0.321", "0.969", "0.983", "0.953", "0.953", "0.977", "0.982", "en\u2192cs", "0.897", "0.896", "0.980", "0.990", "0.882", "0.889", "0.909", "0.983", "de\u2192en", "0.849", "0.813", "0.874", "0.906", "0.865", "0.869", "0.654", "0.858", "de\u2192cs", "0.941", "0.954", "0.890", "0.978", "0.971", "0.968", "0.965", "0.991", "LD", "BLEU", "NIST", "TER", "BEER", "MAP@10", "NDCG@10", "MAP@10", "NDCG@10", "query", "in", "document", "Jenks"], "regionBoundary": {"x2": 523.0, "y1": 65.8900146484375, "x1": 75.0, "y2": 405.8900146484375}, "caption": "Table 1: Pearson correlations (r) of various metrics against human judgments. Best scores for every language direction are highlighted in bold. Note that BEER is trained on in-domain resources from the WMT2019 metrics task. We show MAP@10 and NDCG@10 scores for CLIReval with two relevance label conversion settings.", "page": 6}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 291.5125732421875, "y1": 369.14654541015625, "x1": 72.0, "y2": 411.0140075683594}, "imageText": [], "regionBoundary": {"x2": 250.0, "y1": 222.8900146484375, "x1": 113.0, "y2": 356.8900146484375}, "caption": "Figure 3: Sample outputs from the query generator. In sentences mode, all sentences from R (Figure 2) are used as search queries while in the unique terms mode, the unique terms in R are the search queries.", "page": 3}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 290.2706604003906, "y1": 174.64755249023438, "x1": 72.0, "y2": 204.56005859375}, "imageText": [], "regionBoundary": {"x2": 261.0, "y1": 61.8900146484375, "x1": 101.0, "y2": 162.8900146484375}, "caption": "Figure 2: R is a set of sample reference documents and each document contains two sentences, while T is a set of sample translated documents.", "page": 3}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 290.2707214355469, "y1": 352.40655517578125, "x1": 72.0, "y2": 406.22998046875}, "imageText": [], "regionBoundary": {"x2": 267.0, "y1": 61.8900146484375, "x1": 96.0, "y2": 340.8900146484375}, "caption": "Figure 4: Given queries from the query generator and documents from R, we can obtain relevance scores from an IR system. The relevance label converter then converts those relevance scores into discrete relevance labels via different conversion modes.", "page": 4}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 732.2228325737847, "y1": 353.2229529486762, "x1": 426.772223578559, "y2": 461.1861334906684}, "name": "1", "caption_text": "Figure 1: The system architecture of CLIReval. Documents from input files are separately indexed into two instances of IR systems. Generated search queries are used to query both IR instances. Search scores from REF-IR are converted to discrete relevance judgment labels as required by trec eval. Finally, CLIReval uses trec eval to calculate IR metrics.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 701.0, "y1": 90.0, "x1": 463.0, "y2": 334.0}, "page": 1, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.15369500054254, "y1": 242.5660451253255, "x1": 100.0, "y2": 284.11119249131946}, "name": "2", "caption_text": "Figure 2: R is a set of sample reference documents and each document contains two sentences, while T is a set of sample translated documents.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 356.0, "y1": 93.0, "x1": 147.0, "y2": 221.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 404.8785739474826, "y1": 512.7035352918837, "x1": 100.0, "y2": 570.852788289388}, "name": "3", "caption_text": "Figure 3: Sample outputs from the query generator. In sentences mode, all sentences from R (Figure 2) are used as search queries while in the unique terms mode, the unique terms in R are the search queries.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 342.0, "y1": 316.0, "x1": 161.0, "y2": 490.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.1537797715929, "y1": 489.4535488552517, "x1": 100.0, "y2": 564.2083062065972}, "name": "4", "caption_text": "Figure 4: Given queries from the query generator and documents from R, we can obtain relevance scores from an IR system. The relevance label converter then converts those relevance scores into discrete relevance labels via different conversion modes.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 365.0, "y1": 90.0, "x1": 136.0, "y2": 468.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2228325737847, "y1": 618.585459391276, "x1": 426.42640007866754, "y2": 676.7360687255859}, "name": "5", "caption_text": "Figure 5: Scatterplot of sentence-level NDCG@10 vs sentence-level BLEU on zh-en and en-gu. For better visualization, only 300 random samples from each language direction are shown.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 726.0, "y1": 367.0, "x1": 431.0, "y2": 597.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9267239040798, "y1": 581.3813103569878, "x1": 99.57083596123589, "y2": 622.9263729519314}, "name": "1", "caption_text": "Table 1: Pearson correlations (r) of various metrics against human judgments. Best scores for every language direction are highlighted in bold. Note that BEER is trained on in-domain resources from the WMT2019 metrics task. We show MAP@10 and NDCG@10 scores for CLIReval with two relevance label conversion settings.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 726.0, "y1": 86.0, "x1": 104.0, "y2": 564.0}, "page": 6, "dpi": 0}], "error": null, "pdf": "/work/host-output/9b90e4c83af9cdda1a71d0808da9afdb3d1adb68/2020.acl-demos.18.pdf", "dpi": 100}