{"raw_detected_boxes": [[{"x2": 725.0, "y1": 309.0, "x1": 427.0, "y2": 446.0}], [], [{"x2": 402.0, "y1": 91.0, "x1": 102.0, "y2": 282.0}], [{"x2": 723.0, "y1": 93.0, "x1": 108.0, "y2": 187.0}], [], [{"x2": 691.0, "y1": 95.0, "x1": 137.0, "y2": 321.0}], [{"x2": 730.0, "y1": 89.0, "x1": 431.0, "y2": 232.0}], [{"x2": 690.0, "y1": 94.0, "x1": 134.0, "y2": 284.0}, {"x2": 372.0, "y1": 411.0, "x1": 131.0, "y2": 605.0}], [], [], []], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Figure", "name": "1", "captionBoundary": {"x2": 527.2003173828125, "y1": 334.8955383300781, "x1": 307.2760009765625, "y2": 364.8089904785156}, "imageText": [], "regionBoundary": {"x2": 526.0, "y1": 221.8900146484375, "x1": 307.0, "y2": 322.8900146484375}, "caption": "Figure 1: A speech sequence from our phone call dataset. Two audio segments are highlighted: a question (in blue) and a reported symptom (in yellow).", "page": 0}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 526.7886352539062, "y1": 247.72555541992188, "x1": 71.69100189208984, "y2": 325.4591064453125}, "imageText": ["MultiQT-TF", "A+T", "85.0\u00b11.8", "83.3\u00b12.6", "83.9\u00b11.7", "78.9\u00b12.1", "75.2\u00b12.3", "76.7\u00b11.2", "MultiQT-TF-MT", "A+T", "85.1\u00b13.2", "83.1\u00b11.6", "83.8\u00b11.7", "78.7\u00b13.7", "75.0\u00b11.6", "76.5\u00b11.4", "MultiQT-MT", "A", "84.6\u00b15.1", "57.4\u00b13.9", "66.2\u00b12.9", "77.7\u00b15.6", "56.0\u00b12.8", "62.8\u00b12.0", "MultiQT-MT", "T", "81.9\u00b11.1", "80.6\u00b12.8", "81.0\u00b11.8", "75.9\u00b11.5", "71.2\u00b12.4", "73.3\u00b11.7", "MultiQT-MT", "A+T", "85.2\u00b12.7", "83.2\u00b11.2", "84.1\u00b12.0", "78.5\u00b12.5", "74.0\u00b10.7", "76.0\u00b11.1", "MultiQT", "A", "87.4\u00b11.9", "60.6\u00b14.0", "70.3\u00b13.1", "79.2\u00b11.3", "57.8\u00b13.3", "65.0\u00b12.4", "MultiQT", "T", "84.2\u00b11.6", "78.5\u00b12.8", "81.1\u00b12.0", "78.8\u00b11.2", "69.4\u00b12.0", "73.5\u00b11.3", "MultiQT", "A+T", "83.6\u00b12.2", "83.3\u00b12.5", "83.3\u00b11.6", "75.7\u00b12.2", "73.8\u00b12.3", "74.5\u00b11.3", "RF-BOW", "T", "61.8\u00b13.5", "88.5\u00b10.9", "72.2\u00b12.2", "39.3\u00b11.1", "70.4\u00b11.0", "48.1\u00b11.0", "FNN-BOW", "T", "42.2\u00b11.4", "92.8\u00b10.6", "57.5\u00b11.3", "38.1\u00b10.7", "71.0\u00b11.7", "46.9\u00b10.8", "Model", "Modality", "P", "R", "F1", "P", "R", "F1", "INSTANCE", "TIMESTEP"], "regionBoundary": {"x2": 501.0, "y1": 62.8900146484375, "x1": 97.0, "y2": 235.8900146484375}, "caption": "Table 2: Question tracking results on audio (A) and text (T) modalities with variations of MultiQT using modality concatenation (MultiQT) or tensor fusion (MultiQT-TF) and the auxiliary task (MultiQT-MT). The evaluation metrics are precision (P), recall (R), and (F1) at the macro level per TIMESTEP or INSTANCE. We report means and standard deviations for five-fold cross-validation runs. All F1 differences are statistically significant at p < 0.001, save for between MulitQT [T] & MulitQT-MT [T], and MulitQT [A+T] & MulitQT-TF-MT [A+T] (p \u2248 0.64). We employ the approximate randomization test with R = 1000 and Bonferonni correction (Dror et al., 2018). Bold face indicates the highest F1 score within each metric and MultiQT model group.", "page": 5}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 527.2003784179688, "y1": 180.22854614257812, "x1": 307.2760009765625, "y2": 210.14202880859375}, "imageText": ["error", "margins", "[s]", "stop", "0.6", "0.4", "0.2", "0.0", "0.2", "0.4", "0.6", "start", "0.6", "0.4", "0.2", "0.0", "0.2", "0.4", "0.6"], "regionBoundary": {"x2": 525.5391235351562, "y1": 63.8900146484375, "x1": 307.0, "y2": 166.617431640625}, "caption": "Figure 3: Error margin distributions for start and stop timestamps of question sequences. The dotted lines depict the ground truth start and stop timestamps.", "page": 6}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 291.91998291015625, "y1": 217.53652954101562, "x1": 72.0, "y2": 310.43218994140625}, "imageText": [], "regionBoundary": {"x2": 291.0, "y1": 61.8900146484375, "x1": 72.0, "y2": 205.8900146484375}, "caption": "Figure 2: MultiQT model illustration for two timesteps i and j. We depict the convolutional transformations fa and fs of the audio and character temporal softmax inputs into the respective modality encodings z(i)a and z (i) s , along with the corresponding receptive fields and strides: ra, sa and rs, ss. The convolutions are followed by multimodal fusion and finally dense layers g and h to predict the question labels y\u0302(i) and y\u0302(j).", "page": 2}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 290.2679138183594, "y1": 448.0515441894531, "x1": 72.0, "y2": 466.010009765625}, "imageText": ["[A+T]", "permuted", "[A+T]", "[T]", "[A]", "or", "e", "F1", "sc", "0.72", "0.70", "0.68", "0.66", "0.64", "0.62", "0-20", "0-25", "0-30", "0-35", "0-40", "0-45", "0-50", "0-100", "ASR", "WER", "[%]"], "regionBoundary": {"x2": 268.4466552734375, "y1": 295.8900146484375, "x1": 96.80445861816406, "y2": 434.360595703125}, "caption": "Figure 4: Relation between TIMESTEP F1 and WER on call-taker utterances without the \u201cNone\u201d label.", "page": 7}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 525.5477905273438, "y1": 216.67251586914062, "x1": 71.69100189208984, "y2": 270.49603271484375}, "imageText": ["A+T", "No", "T", "0.0\u00b10.0", "0.0\u00b10.0", "0.0\u00b10.0", "16.2\u00b10.0", "16.7\u00b10.0", "16.4\u00b10.0", "A+T", "No", "A", "89.5\u00b13.1", "69.2\u00b14.4", "77.0\u00b12.5", "84.3\u00b12.6", "63.7\u00b13.5", "71.0\u00b12.0", "A+T", "No", "-", "83.6\u00b12.2", "83.3\u00b12.5", "83.3\u00b11.6", "75.7\u00b12.2", "73.8\u00b12.3", "74.5\u00b11.3", "A", "No", "-", "87.4\u00b11.9", "60.6\u00b14.0", "70.3\u00b13.1", "79.2\u00b11.3", "57.8\u00b13.3", "65.0\u00b12.4", "T", "No", "-", "84.2\u00b11.6", "78.5\u00b12.8", "81.1\u00b12.0", "78.8\u00b11.2", "69.4\u00b12.0", "73.5\u00b11.3", "A+T", "Yes", "T", "82.2\u00b14.9", "60.1\u00b15.6", "68.6\u00b15.7", "79.0\u00b14.7", "58.4\u00b13.7", "64.7\u00b13.5", "A+T", "Yes", "A", "82.6\u00b13.2", "75.9\u00b12.9", "78.7\u00b11.6", "78.3\u00b12.4", "68.3\u00b12.7", "72.3\u00b11.1", "A+T", "Yes", "-", "86.3\u00b11.6", "83.8\u00b12.8", "84.8\u00b12.0", "80.4\u00b11.0", "74.1\u00b12.2", "76.9\u00b11.3", "Modality", "Training", "Test", "P", "R", "F1", "P", "R", "F1", "Permuted", "INSTANCE", "TIMESTEP"], "regionBoundary": {"x2": 501.0, "y1": 62.8900146484375, "x1": 97.0, "y2": 204.8900146484375}, "caption": "Table 3: Results from the modality ablation on the MultiQT model. We compare multimodal MultiQT trained with the audio (A) and text (T) modalities temporally permuted in turn during training with probability pa = 0.1 and ps = 0.5 to MultiQT trained without modality permutation, unimodally and multimodally (some results copied from Table 2). We can obtain robustness to loosing a modality while maintaining (or even slightly improving) the multimodal performance. All results are based on five-fold cross-validation as in Table 2.", "page": 7}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 455.62811279296875, "y1": 146.46353149414062, "x1": 141.60699462890625, "y2": 152.46600341796875}, "imageText": ["Q1", "Question", "about", "the", "address", "of", "the", "incident.", "What\u2019s", "the", "address?", "663", "26.3%", "Q2", "Initial", "question", "of", "the", "call-taker", "to", "begin", "assessing", "the", "situation.", "What\u2019s", "the", "problem?", "546", "21.6%", "Q3", "Question", "about", "the", "age", "of", "the", "patient.", "How", "old", "is", "she?", "537", "21.3%", "Q4", "All", "questions", "related", "to", "patient\u2019s", "quality", "of", "breathing.", "Is", "she", "breathing", "in", "a", "normal", "pattern?", "293", "11.6%", "Q5", "All", "question", "about", "patient\u2019s", "consciousness", "or", "responsiveness.", "Is", "he", "conscious", "and", "awake?", "484", "19.2%", "Label", "Description", "Example", "Count", "Fraction"], "regionBoundary": {"x2": 524.0, "y1": 68.0268783569336, "x1": 74.0, "y2": 133.8900146484375}, "caption": "Table 1: Explanation and prevalence of the questions used for the experiments.", "page": 3}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 732.222663031684, "y1": 465.13269212510846, "x1": 426.772223578559, "y2": 506.6791534423828}, "name": "1", "caption_text": "Figure 1: A speech sequence from our phone call dataset. Two audio segments are highlighted: a question (in blue) and a reported symptom (in yellow).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 730.0, "y1": 309.0, "x1": 427.0, "y2": 448.0}, "page": 0, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.44442070855035, "y1": 302.1340688069661, "x1": 100.0, "y2": 431.1558193630642}, "name": "2", "caption_text": "Figure 2: MultiQT model illustration for two timesteps i and j. We depict the convolutional transformations fa and fs of the audio and character temporal softmax inputs into the respective modality encodings z(i)a and z (i) s , along with the corresponding receptive fields and strides: ra, sa and rs, ss. The convolutions are followed by multimodal fusion and finally dense layers g and h to predict the question labels y\u0302(i) and y\u0302(j).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 402.0, "y1": 89.0, "x1": 102.0, "y2": 282.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 632.8168233235676, "y1": 203.42157151963974, "x1": 196.67638142903644, "y2": 211.75833808051215}, "name": "1", "caption_text": "Table 1: Explanation and prevalence of the questions used for the experiments.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 728.0, "y1": 86.0, "x1": 102.0, "y2": 204.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 731.650882297092, "y1": 344.06327141655817, "x1": 99.57083596123589, "y2": 452.02653672960065}, "name": "2", "caption_text": "Table 2: Question tracking results on audio (A) and text (T) modalities with variations of MultiQT using modality concatenation (MultiQT) or tensor fusion (MultiQT-TF) and the auxiliary task (MultiQT-MT). The evaluation metrics are precision (P), recall (R), and (F1) at the macro level per TIMESTEP or INSTANCE. We report means and standard deviations for five-fold cross-validation runs. All F1 differences are statistically significant at p < 0.001, save for between MulitQT [T] & MulitQT-MT [T], and MulitQT [A+T] & MulitQT-TF-MT [A+T] (p \u2248 0.64). We employ the approximate randomization test with R = 1000 and Bonferonni correction (Dror et al., 2018). Bold face indicates the highest F1 score within each metric and MultiQT model group.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 696.0, "y1": 86.0, "x1": 134.0, "y2": 327.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2227478027344, "y1": 250.31742519802518, "x1": 426.772223578559, "y2": 291.86392890082465}, "name": "3", "caption_text": "Figure 3: Error margin distributions for start and stop timestamps of question sequences. The dotted lines depict the ground truth start and stop timestamps.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 730.0, "y1": 89.0, "x1": 427.0, "y2": 249.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.927486843533, "y1": 300.93404981825086, "x1": 99.57083596123589, "y2": 375.6889343261719}, "name": "3", "caption_text": "Table 3: Results from the modality ablation on the MultiQT model. We compare multimodal MultiQT trained with the audio (A) and text (T) modalities temporally permuted in turn during training with probability pa = 0.1 and ps = 0.5 to MultiQT trained without modality permutation, unimodally and multimodally (some results copied from Table 2). We can obtain robustness to loosing a modality while maintaining (or even slightly improving) the multimodal performance. All results are based on five-fold cross-validation as in Table 2.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 701.0, "y1": 86.0, "x1": 120.0, "y2": 301.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.1498803032769, "y1": 622.2938113742405, "x1": 100.0, "y2": 647.2361246744791}, "name": "4", "caption_text": "Figure 4: Relation between TIMESTEP F1 and WER on call-taker utterances without the \u201cNone\u201d label.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 384.0, "y1": 410.0, "x1": 130.0, "y2": 622.0}, "page": 7, "dpi": 0}], "error": null, "pdf": "/work/host-output/409cd653d2de4159bcfc6d5f5b80c940f95d940f/2020.acl-main.215.pdf", "dpi": 100}