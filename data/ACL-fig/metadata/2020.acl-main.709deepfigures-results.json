{"raw_detected_boxes": [[], [{"x2": 727.0, "y1": 90.0, "x1": 103.0, "y2": 285.0}], [], [{"x2": 404.0, "y1": 99.0, "x1": 101.0, "y2": 234.0}, {"x2": 721.0, "y1": 88.0, "x1": 436.0, "y2": 413.0}, {"x2": 387.0, "y1": 276.0, "x1": 100.0, "y2": 592.0}], [{"x2": 404.0, "y1": 89.0, "x1": 103.0, "y2": 343.0}], [{"x2": 706.0, "y1": 88.0, "x1": 121.0, "y2": 248.0}, {"x2": 396.0, "y1": 334.0, "x1": 103.0, "y2": 499.0}], [{"x2": 389.0, "y1": 86.0, "x1": 111.0, "y2": 200.0}], [{"x2": 668.0, "y1": 86.0, "x1": 163.0, "y2": 286.0}, {"x2": 393.0, "y1": 414.0, "x1": 107.0, "y2": 501.0}, {"x2": 695.0, "y1": 416.0, "x1": 452.0, "y2": 535.0}, {"x2": 390.0, "y1": 578.0, "x1": 110.0, "y2": 664.0}], [{"x2": 407.0, "y1": 86.0, "x1": 100.0, "y2": 269.0}], [], [], [], [{"x2": 378.0, "y1": 382.0, "x1": 125.0, "y2": 440.0}, {"x2": 340.0, "y1": 521.0, "x1": 163.0, "y2": 607.0}, {"x2": 333.0, "y1": 767.0, "x1": 163.0, "y2": 849.0}, {"x2": 726.0, "y1": 95.0, "x1": 428.0, "y2": 216.0}, {"x2": 723.0, "y1": 273.0, "x1": 438.0, "y2": 669.0}], [{"x2": 699.0, "y1": 86.0, "x1": 460.0, "y2": 172.0}, {"x2": 699.0, "y1": 218.0, "x1": 458.0, "y2": 318.0}, {"x2": 403.0, "y1": 517.0, "x1": 103.0, "y2": 631.0}, {"x2": 404.0, "y1": 711.0, "x1": 103.0, "y2": 823.0}], [{"x2": 707.0, "y1": 100.0, "x1": 113.0, "y2": 451.0}], [{"x2": 716.0, "y1": 115.0, "x1": 110.0, "y2": 590.0}, {"x2": 718.0, "y1": 688.0, "x1": 108.0, "y2": 972.0}], [{"x2": 700.0, "y1": 102.0, "x1": 127.0, "y2": 956.0}], [{"x2": 695.0, "y1": 98.0, "x1": 137.0, "y2": 461.0}]], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Table", "name": "2", "captionBoundary": {"x2": 525.5468139648438, "y1": 191.67953491210938, "x1": 71.6409912109375, "y2": 221.593017578125}, "imageText": ["Similarity-based", "models", "Jaccard", "(Xu", "et", "al.,", "2015)", "94.93", "76.69", "84.84", "73.43", "75.61", "74.51", "TF-IDF", "(Paetzold", "et", "al.,", "2017)", "96.24", "83.05", "89.16", "66.78", "69.69", "68.20", "LR", "(S\u030ctajner", "et", "al.,", "2018)", "93.11", "84.96", "88.85", "73.21", "74.74", "73.97", "Similarity-based", "models", "w/", "alignment", "strategy", "(previous", "SOTA)", "JaccardAlign", "(Xu", "et", "al.,", "2015)", "98.66", "67.58", "80.22\u2020", "51.34", "86.76", "64.51\u2020", "MASSAlign", "(Paetzold", "et", "al.,", "2017)", "95.49", "82.27", "88.39\u2020", "40.98", "87.11", "55.74\u2020", "CATS", "(S\u030ctajner", "et", "al.,", "2018)", "88.56", "91.31", "89.92\u2020", "38.29", "97.39", "54.97\u2020", "Our", "CRF", "Aligner", "97.86", "93.43", "95.59", "87.56", "89.55", "88.54", "Task", "1", "(aligned&partial", "vs.", "others)", "Task", "2", "(aligned", "vs.", "others)", "Precision", "Recall", "F1", "Precision", "Recall", "F1"], "regionBoundary": {"x2": 508.0, "y1": 63.8900146484375, "x1": 87.0, "y2": 179.8900146484375}, "caption": "Table 2: Performance of different sentence alignment methods on the NEWSELA-MANUAL test set. \u2020 Previous work was designed only for Task 1 and used alignment strategy (greedy algorithm or dynamic programming) to improve either precision or recall.", "page": 5}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 278.98712158203125, "y1": 368.571533203125, "x1": 82.9729995727539, "y2": 374.5740051269531}, "imageText": ["+", "ParaAlign", "98.4", "84.2", "90.7", "91.9", "79.0", "85.0", "Neural", "CRF", "aligner", "Our", "CRF", "Aligner", "96.5", "90.1", "93.2", "88.6", "87.7", "88.1", "+", "gold", "ParaAlign", "97.3", "91.1", "94.1", "88.9", "88.0", "88.4", "Neural", "sentence", "pair", "models", "Infersent", "92.8", "69.7", "79.6", "87.8", "74.0", "80.3", "ESIM", "91.5", "71.2", "80.0", "82.5", "73.7", "77.8", "BERTScore", "90.6", "76.5", "83.0", "83.2", "74.3", "78.5", "BERTembedding", "84.7", "53.0", "65.2", "77.0", "74.7", "75.8", "BERTfinetune", "93.3", "84.3", "88.6", "90.2", "80.0", "84.8", "Task", "1", "Task", "2", "P", "R", "F1", "P", "R", "F1"], "regionBoundary": {"x2": 288.0, "y1": 236.8900146484375, "x1": 72.0, "y2": 358.8900146484375}, "caption": "Table 3: Ablation study of our aligner on dev set.", "page": 5}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 527.2907104492188, "y1": 340.13055419921875, "x1": 71.53199768066406, "y2": 358.0880126953125}, "imageText": [], "regionBoundary": {"x2": 515.0, "y1": 77.8900146484375, "x1": 83.0, "y2": 327.8900146484375}, "caption": "Figure 4: Instructions provided to Amazon Mechanical Turk workers to evaluate generated simplified sentences. We used the same instructions as described in Kriz et al. (2019).", "page": 14}, {"figType": "Figure", "name": "1", "captionBoundary": {"x2": 525.5471801757812, "y1": 218.28652954101562, "x1": 71.6709976196289, "y2": 238.7349853515625}, "imageText": [], "regionBoundary": {"x2": 526.0, "y1": 61.8900146484375, "x1": 72.0, "y2": 206.8900146484375}, "caption": "Figure 1: An example of sentence alignment between an original news article (right) and its simplified version (left) in Newsela. The label ai for each simple sentence si is the index of complex sentence cai it aligns to.", "page": 1}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 291.9241943359375, "y1": 156.03353881835938, "x1": 71.69100189208984, "y2": 185.947021484375}, "imageText": ["#", "of", "article", "pairs", "13k", "7.9k", "138k", "65k", "#", "of", "sent.", "pairs", "(train)", "394k", "94k", "488k", "298k", "#", "of", "sent.", "pairs", "(dev)", "43k", "1.1k", "2k", "2k", "#", "of", "sent.", "pairs", "(test)", "44k", "1k", "359", "359", "avg.", "sent.", "len", "(complex)", "25.4", "25.8", "26.6", "25.2", "avg.", "sent.", "len", "(simple)", "13.8", "15.7", "18.7", "18.5", "Newsela", "Wikipedia", "Auto", "Old", "Auto", "Old"], "regionBoundary": {"x2": 280.0, "y1": 62.8900146484375, "x1": 80.0, "y2": 143.8900146484375}, "caption": "Table 4: Statistics of our newly constructed parallel corpora for sentence simplification compared to the old datasets (Xu et al., 2015; Zhang and Lapata, 2017).", "page": 6}, {"figType": "Table", "name": "13", "captionBoundary": {"x2": 290.2704772949219, "y1": 604.560546875, "x1": 71.69100189208984, "y2": 622.5180053710938}, "imageText": ["+", "ParaAlign", "86.6", "82.4", "84.5", "Our", "CRF", "Aligner", "(WIKI-MANUAL)", "89.3", "81.6", "85.3", "MASSAlign", "(Paetzold", "et", "al.,", "2017)", "68.6", "72.5", "70.5", "CATS", "(S\u030ctajner", "et", "al.,", "2018)", "68.4", "74.4", "71.3", "BERTfinetune", "(NEWSELA-MANUAL)", "80.6", "78.8", "79.6", "BERTfinetune", "(WIKI-MANUAL)", "86.3", "82.4", "84.3", "Test", "set", "P", "R", "F"], "regionBoundary": {"x2": 291.0, "y1": 509.8900146484375, "x1": 72.0, "y2": 592.8900146484375}, "caption": "Table 13: Performance of different sentence alignment methods on the WIKI-MANUAL test set for Task 1.", "page": 13}, {"figType": "Table", "name": "12", "captionBoundary": {"x2": 290.2704772949219, "y1": 467.8355407714844, "x1": 71.69100189208984, "y2": 485.7929992675781}, "imageText": ["+", "ParaAlign", "88.6", "85.4", "87.0", "Our", "CRF", "Aligner", "(WIKI-MANUAL)", "92.4", "85.8", "89.0", "MASSAlign", "(Paetzold", "et", "al.,", "2017)", "72.9", "79.5", "76.1", "CATS", "(S\u030ctajner", "et", "al.,", "2018)", "65.6", "82.7", "73.2", "BERTfinetune", "(NEWSELA-MANUAL)", "82.6", "83.9", "83.2", "BERTfinetune", "(WIKI-MANUAL)", "87.9", "85.4", "86.6", "Dev", "set", "P", "R", "F"], "regionBoundary": {"x2": 291.0, "y1": 372.8900146484375, "x1": 72.0, "y2": 455.8900146484375}, "caption": "Table 12: Performance of different sentence alignment methods on the WIKI-MANUAL dev set for Task 1.", "page": 13}, {"figType": "Table", "name": "15", "captionBoundary": {"x2": 501.1665344238281, "y1": 240.91555786132812, "x1": 331.343994140625, "y2": 246.91802978515625}, "imageText": ["min", "vocab", "freq", "3", "seed", "13", "lr", "0.001", "optimizer", "Adam", "clipping", "5", "epochs", "30", "embedding", "dim", "300", "max", "len", "100", "#", "of", "layers", "2", "dropout", "0.2", "Parameter", "Value", "Parameter", "Value", "hidden", "units", "256", "batch", "size", "64"], "regionBoundary": {"x2": 503.0, "y1": 156.8900146484375, "x1": 330.0, "y2": 228.8900146484375}, "caption": "Table 15: Parameters of our LSTM model.", "page": 13}, {"figType": "Table", "name": "14", "captionBoundary": {"x2": 512.6036376953125, "y1": 136.10855102539062, "x1": 319.9070129394531, "y2": 142.11102294921875}, "imageText": ["attention", "heads", "12", "dropout", "0.1", "loss", "CE", "seed", "13", "\ufb01lter", "size", "3072", "max", "len", "100", "#", "of", "layers", "12", "activation", "GELU", "Parameter", "Value", "Parameter", "Value", "hidden", "units", "768", "batch", "size", "32"], "regionBoundary": {"x2": 503.0, "y1": 62.8900146484375, "x1": 330.0, "y2": 123.8900146484375}, "caption": "Table 14: Parameters of our Transformer model.", "page": 13}, {"figType": "Figure", "name": "6", "captionBoundary": {"x2": 459.6490783691406, "y1": 353.3015441894531, "x1": 137.89700317382812, "y2": 359.30401611328125}, "imageText": [], "regionBoundary": {"x2": 526.0, "y1": 77.8900146484375, "x1": 72.0, "y2": 341.8900146484375}, "caption": "Figure 6: Annotation interface for correcting the crowdsourced alignment labels.", "page": 17}, {"figType": "Table", "name": "10", "captionBoundary": {"x2": 291.9241943359375, "y1": 449.0675354003906, "x1": 71.69100189208984, "y2": 467.0249938964844}, "imageText": ["Threshold", "Value", "\u03c41", "0.1", "\u03c42", "0.34", "\u03c43", "0.9998861788416304", "\u03c44", "0.998915818299745", "\u03c45", "0.5"], "regionBoundary": {"x2": 249.0, "y1": 375.8900146484375, "x1": 114.0, "y2": 436.8900146484375}, "caption": "Table 10: The thresholds in paragraph alignment Algorithm 2 for Newsela data.", "page": 12}, {"figType": "Table", "name": "11", "captionBoundary": {"x2": 291.9241943359375, "y1": 623.5545654296875, "x1": 71.69100189208984, "y2": 641.5120239257812}, "imageText": ["Threshold", "Value", "\u03c41", "0.991775706637882", "\u03c42", "0.8", "\u03c43", "0.5", "\u03c44", "5", "\u03c45", "0.9958"], "regionBoundary": {"x2": 246.0, "y1": 549.8900146484375, "x1": 116.0, "y2": 611.8900146484375}, "caption": "Table 11: The thresholds in paragraph alignment Algorithm 4 for Wikipedia data.", "page": 12}, {"figType": "Table", "name": "9", "captionBoundary": {"x2": 291.92425537109375, "y1": 329.1615295410156, "x1": 71.69100189208984, "y2": 347.1189880371094}, "imageText": ["max", "sequence", "length", "128", "batch", "size", "8", "Parameter", "Value", "Parameter", "Value", "hidden", "units", "768", "#", "of", "layers", "12", "learning", "rate", "0.00002", "#", "of", "heads", "12"], "regionBoundary": {"x2": 281.0, "y1": 275.8900146484375, "x1": 81.0, "y2": 316.8900146484375}, "caption": "Table 9: Parameters of our neural CRF sentence alignment model.", "page": 12}, {"figType": "Table", "name": "6", "captionBoundary": {"x2": 292.01263427734375, "y1": 372.800537109375, "x1": 71.6710205078125, "y2": 402.7139892578125}, "imageText": ["Model", "F", "A", "S", "Avg.", "LSTM", "3.44", "2.86", "3.31", "3.20", "EditNTS", "(Dong", "et", "al.,", "2019)\u2020", "3.32", "2.79", "3.48", "3.20", "Rerank", "(Kriz", "et", "al.,", "2019)\u2020", "3.50", "2.80", "3.46", "3.25", "Transformerbert", "(this", "work)", "3.64", "3.12", "3.45", "3.40", "Simple", "(reference)", "3.98", "3.23", "3.70", "3.64"], "regionBoundary": {"x2": 286.0, "y1": 296.8900146484375, "x1": 75.0, "y2": 360.8900146484375}, "caption": "Table 6: Human evaluation of fluency (F), adequacy (A) and simplicity (S) on the old NEWSELA test set. \u2020We used the system outputs shared by the authors.", "page": 7}, {"figType": "Table", "name": "7", "captionBoundary": {"x2": 290.61614990234375, "y1": 490.28253173828125, "x1": 71.6710205078125, "y2": 508.2409973144531}, "imageText": ["Model", "Train", "F", "A", "S", "Avg.", "LSTM", "old", "3.57", "3.27", "3.11", "3.31", "LSTM", "new", "3.55", "2.98", "3.12", "3.22", "Transformerbert", "old", "2.91", "2.56", "2.67", "2.70", "Transformerbert", "new", "3.76", "3.21", "3.18", "3.39", "Simple", "(reference)", "\u2014", "4.34", "3.34", "3.37", "3.69"], "regionBoundary": {"x2": 281.0, "y1": 415.8900146484375, "x1": 79.0, "y2": 477.8900146484375}, "caption": "Table 7: Human evaluation of fluency (F), adequacy (A) and simplicity (S) on NEWSELA-AUTO test set.", "page": 7}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 527.1975708007812, "y1": 399.4815368652344, "x1": 307.135986328125, "y2": 429.39398193359375}, "imageText": [], "regionBoundary": {"x2": 506.0, "y1": 296.8900146484375, "x1": 324.0, "y2": 387.8900146484375}, "caption": "Figure 3: Manual inspection of 100 random sentences generated by Transformerbert trained on NEWSELAAUTO and existing NEWSELA datasets, respectively.", "page": 7}, {"figType": "Table", "name": "5", "captionBoundary": {"x2": 527.2009887695312, "y1": 217.80252075195312, "x1": 71.64099884033203, "y2": 283.580078125}, "imageText": ["Complex", "(input)", "11.9", "0.0", "35.5", "0.0", "12", "24.3", "12.5", "0.0", "37.7", "0.0", "11", "22.9", "Models", "trained", "on", "old", "dataset", "(original", "NEWSELA", "corpus", "released", "in", "(Xu", "et", "al.,", "2015))", "Transformerrand", "33.1", "1.8", "22.1", "75.4", "6.8", "14.2", "34.1", "2.0", "25.5", "74.8", "6.7", "14.2", "LSTM", "35.6", "2.8", "32.1", "72.0", "8.2", "16.9", "36.2", "2.5", "34.9", "71.3", "7.7", "16.3", "EditNTS", "35.5", "1.8", "30.0", "75.4", "7.1", "14.1", "36.1", "1.7", "32.8", "73.8", "7.0", "14.1", "Transformerbert", "34.4", "2.4", "25.2", "75.8", "7.0", "14.5", "35.1", "2.7", "27.8", "74.8", "6.8", "14.3", "Models", "trained", "on", "our", "new", "dataset", "(NEWSELA-AUTO)", "Transformerrand", "35.6", "3.2", "28.4", "75.0", "7.1", "14.4", "35.2", "2.5", "29.7", "73.5", "7.0", "14.2", "LSTM", "35.8", "3.9", "30.5", "73.1", "7.0", "14.3", "36.4", "3.3", "33.0", "72.9", "6.6", "14.0", "EditNTS", "35.8", "2.4", "29.4", "75.6", "6.3", "11.6", "35.7", "1.8", "31.1", "74.2", "6.1", "11.5", "Transformerbert", "36.6", "4.5", "31.0", "74.3", "6.8", "13.3", "36.8", "3.8", "33.1", "73.4", "6.8", "13.5", "Simple", "(reference)", "\u2013", "\u2013", "\u2013", "\u2013", "6.6", "13.2", "\u2013", "\u2013", "\u2013", "\u2013", "6.2", "12.6", "Evaluation", "on", "our", "new", "test", "set", "Evaluation", "on", "old", "test", "set", "SARI", "add", "keep", "del", "FK", "Len", "SARI", "add", "keep", "del", "FK", "Len"], "regionBoundary": {"x2": 481.0, "y1": 62.8900146484375, "x1": 114.0, "y2": 205.8900146484375}, "caption": "Table 5: Automatic evaluation results on NEWSELA test sets comparing models trained on our dataset NEWSELAAUTO against the existing dataset (Xu et al., 2015). We report SARI, the main automatic metric for simplification, precision for deletion and F1 scores for adding and keeping operations. Add scores are low partially because we are using one reference. Bold typeface and underline denote the best and the second best performances respectively. For Flesch-Kincaid (FK) grade level and average sentence length (Len), we consider the values closest to reference as the best.", "page": 7}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 527.2899780273438, "y1": 309.2495422363281, "x1": 306.9670104980469, "y2": 398.9379577636719}, "imageText": ["Article", "level", "#", "of", "original", "articles", "50", "1,882", "#", "of", "article", "pairs", "500", "18,820", "Sentence", "level", "#", "of", "original", "sent.", "(level", "0)", "2,190", "59,752", "#", "of", "sentence", "pairs", "1.01M\u2020", "666,645", "#", "of", "unique", "complex", "sent.", "7,001", "195,566", "#", "of", "unique", "simple", "sent.", "8,008", "246,420", "avg.", "length", "of", "simple", "sent.", "13.9", "14.8", "avg.", "length", "of", "complex", "sent.", "21.3", "24.9", "Labels", "of", "sentence", "pairs", "#", "of", "aligned", "(not", "identical)", "5,182", "666,645#", "of", "partially-aligned", "14,023", "#", "of", "not-aligned", "0.99M", "\u2013", "Text", "simpli\ufb01cation", "phenomenon", "#", "of", "sent.", "rephrasing", "(1-to-1)", "8,216", "307,450", "#", "of", "sent.", "copying", "(1-to-1)", "3,842", "147,327", "#", "of", "sent.", "splitting", "(1-to-n)", "4,237", "160,300", "#", "of", "sent.", "merging", "(n-to-1)", "232", "\u2013", "#", "of", "sent.", "fusion", "(m-to-n)", "252", "\u2013", "#", "of", "sent.", "deletion", "(1-to-0)", "6,247", "\u2013", "Newsela", "Newsela", "-Manual", "-Auto"], "regionBoundary": {"x2": 519.0, "y1": 62.8900146484375, "x1": 314.0, "y2": 296.8900146484375}, "caption": "Table 1: Statistics of our manually and automatically created sentence alignment annotations on Newsela. \u2020 This number includes all complex-simple sentence pairs (including aligned, partially-aligned, or notaligned) across all 10 combinations of 5 readability levels (level 0-4), of which 20,343 sentence pairs between adjacent readability levels were manually annotated and the rest of labels were derived.", "page": 3}, {"figType": "Figure", "name": "5", "captionBoundary": {"x2": 519.9620971679688, "y1": 715.4165649414062, "x1": 77.58300018310547, "y2": 721.4190063476562}, "imageText": [], "regionBoundary": {"x2": 515.0, "y1": 100.8900146484375, "x1": 83.0, "y2": 703.8900146484375}, "caption": "Figure 5: Instructions and an example question for our crowdsourcing annotation on the Figure Eight platform.", "page": 16}, {"figType": "Table", "name": "8", "captionBoundary": {"x2": 291.9215393066406, "y1": 208.83554077148438, "x1": 71.69100189208984, "y2": 238.7490234375}, "imageText": ["SARI", "add", "keep", "del", "FK", "Len", "Complex", "(input)", "25.9", "0.0", "77.8", "0.0", "13.6", "22.4", "Models", "trained", "on", "old", "dataset", "(WIKILARGE)", "LSTM", "33.8", "2.5", "65.6", "33.4", "11.6", "20.6", "Transformerrand", "33.5", "3.2", "64.1", "33.2", "11.1", "17.7", "EditNTS", "35.3", "3.0", "63.9", "38.9", "11.1", "18.5", "Transformerbert", "35.3", "4.4", "66.0", "35.6", "10.9", "17.9", "Models", "trained", "on", "our", "new", "dataset", "(WIKI-AUTO)", "LSTM", "34.0", "2.8", "64.0", "35.2", "11.0", "19.3", "Transformerrand", "34.7", "3.3", "68.8", "31.9", "11.7", "18.7", "EditNTS", "36.4", "3.6", "66.1", "39.5", "11.6", "20.2", "Transformerbert", "36.6", "5.0", "67.6", "37.2", "11.4", "18.7", "Simple", "(reference)", "\u2013", "\u2013", "\u2013", "\u2013", "11.7", "20.2"], "regionBoundary": {"x2": 296.0, "y1": 62.8900146484375, "x1": 72.0, "y2": 195.8900146484375}, "caption": "Table 8: Automatic evaluation results on Wikipedia TURK corpus comparing models trained on WIKIAUTO and WIKILARGE (Zhang and Lapata, 2017).", "page": 8}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 291.9245300292969, "y1": 275.6645202636719, "x1": 71.53201293945312, "y2": 353.3980407714844}, "imageText": [], "regionBoundary": {"x2": 299.0, "y1": 61.8900146484375, "x1": 72.0, "y2": 249.8900146484375}, "caption": "Figure 2: Manual inspection of 100 random sentence pairs from our corpora (NEWSELA-AUTO and WIKIAUTO) and the existing Newsela (Xu et al., 2015) and Wikipedia (Zhang and Lapata, 2017) corpora. Our corpora contain at least 44% more complex rewrites (Deletion + Paraphrase or Splitting + Paraphrase) and 27% less defective pairs (Not Aligned or Not Simpler).", "page": 4}, {"figType": "Table", "name": "16", "captionBoundary": {"x2": 525.9021606445312, "y1": 436.9505310058594, "x1": 71.69100189208984, "y2": 478.8189697265625}, "imageText": ["Generated", "by", "LSTM", "baseline", "Complex", "(input)", "In", "Seattle", ",", "eight", "activists", "between", "ages", "10", "and", "15", "petitioned", "Washington", "state", "last", "year", "to", "adopt", "stricter", "science-based", "regulations", "to", "protect", "them", "against", "climate", "change.", "Simple", "(reference)", "In", "Seattle,", "eight", "youths", "between", "10", "to", "15", "years", "old", "petitioned", "the", "state", "of", "Washington", "to", "change", "the", "law.", "New", "(this", "work)", "in", "seattle", ",", "eight", "activists", "between", "ages", "10", "and", "15", "asked", "washington", "state", "last", "year", "to", "keep", "the", "environment", "safe.", "(Phrasal", "Praphrase", "+", "Deletion)", "Old", "(Xu", "et", "al.,", "2015)", "in", "seattle", ",", "eight", "activists", "between", "ages", "10", "and", "15", "asked", "washington", "state", "last", "year", "to", "adopt", "stricter", "science", "-", "based", "rules", "to", "protect", "them", "against", "climate", "change.", "(Lexical", "Paraphrase)", "Complex", "(input)", "He", "recognized", "that", "another", "recommendation", "would", "be", "controversial", "with", "police", "groups:", "inde-", "pendent", "investigations", "after", "police", "shootings.", "Simple", "(reference)", "He", "admitted", "that", "police", "would", "not", "like", "one", "of", "the", "recommendations.", "New", "(this", "work)", "he", "thought", "another", "suggestion", "would", "be", "against", "the", "police.", "(Phrasal", "Paraphrase", "+", "Deletion)", "Old", "(Xu", "et", "al.,", "2015)", "he", "recognized", "that", "another", "suggestion", "would", "be", "controversial", "with", "police", "groups.", "(Lexical", "Paraphrase", "+", "Deletion)", "Complex", "(input)", "The", "Philadelphia", "Museum", "of", "Art", "has", "two", "famous", "sel\ufb01e", "spots", ",", "both", "from", "the", "movie", "\u201d", "Rocky.", "\u201d", "Simple", "(reference)", "The", "Philadelphia", "Museum", "of", "Art", "has", "two", "big", "sel\ufb01e", "spots.", "New", "(this", "work)", "the", "philadelphia", "museum", "of", "art", "has", "two", "picture", "spots.", "(Lexical", "Paraphrase", "+", "Deletion)", "Old", "(Xu", "et", "al.,", "2015)", "the", "philadelphia", "museum", "of", "art", "has", "two", "famous", "spots.", "(Deletion)", "Generated", "by", "Transformerbert", "Complex", "(input)", "Some", "Chicago", "residents", "got", "angry", "about", "it.", "Simple", "(reference)", "The", "plan", "made", "some", "people", "angry.", "New", "(this", "work)", "some", "people", "in", "chicago", "were", "angry.", "(Phrasal", "Paraphrase)", "Old", "(Xu", "et", "al.,", "2015)", "some", "chicago", "residents", "got", "angry.", "(Deletion)", "Complex", "(input)", "Emissions", "standards", "have", "been", "tightened", ",", "and", "the", "government", "is", "investing", "money", "in", "solar", ",", "wind", "and", "other", "renewable", "energy.", "Simple", "(reference)", "China", "has", "also", "put", "a", "great", "deal", "of", "money", "into", "solar,", "wind", "and", "other", "renewable", "energy.", "New", "(this", "work)", "the", "government", "is", "putting", "aside", "money", "for", "new", "types", "of", "energy.", "(Phrasal", "Paraphrase", "+", "Deletion)", "Old", "(Xu", "et", "al.,", "2015)", "the", "government", "is", "investing", "in", "money", ",", "wind", "and", "other", "equipment.", "(Lexical", "Paraphrase", "+", "Deletion)", "Complex", "(input)", "On", "Feb.", "9", ",", "1864", ",", "he", "was", "sitting", "for", "several", "portraits", ",", "including", "the", "one", "used", "for", "the", "$5", "bill.", "Simple", "(reference)", "On", "Feb.", "9,", "1864,", "several", "artists", "painted", "pictures", "of", "him.", "New", "(this", "work)", "on", "feb.", "9,", "1864", ",", "he", "was", "sitting", "for", "several", "portraits.", "(Deletion)", "Old", "(Xu", "et", "al.,", "2015)", "on", "feb", "9,", "1864", ",", "he", "was", "sitting", "for", "several", ",", "including", "the", "$", "5", "bill", "for", "the", "bill.", "(Deletion)"], "regionBoundary": {"x2": 517.0, "y1": 91.8900146484375, "x1": 79.0, "y2": 424.8900146484375}, "caption": "Table 16: Examples of simplified sentences generated by LSTM and Transformerbert models trained on our new NEWSELA-AUTO (this work) and old existing NEWSELA (Xu et al., 2015) datasets. The source sentences are from our new NEWSELA-AUTO test set. Models trained on our new data rephrase the input sentence more often than the models trained on old data. Bold indicates deletions or paraphrases.", "page": 15}, {"figType": "Table", "name": "17", "captionBoundary": {"x2": 526.7881469726562, "y1": 711.9195556640625, "x1": 71.69097900390625, "y2": 753.7869873046875}, "imageText": ["Examples", "Complex", "(input)", "Now", "at", "age", "9,", "his", "teachers", "say", "Richie", "reads", "at", "the", "level", "of", "a", "student", "in", "high", "school,", "and", "his", "vocabulary", "is", "well", "above", "those", "of", "his", "classmates.", "Simple", "(reference)", "He", "reads", "like", "a", "high", "school", "student.", "LSTM", "now", "he", "is", "age", "9.", "EditNTS", "(Dong", "et", "al.,", "2019)", "he", "say", "his", "classmates", "are", "using", "a", "special", "job.", "Rerank", "(Kriz", "et", "al.,", "2019)", "but", "it", "is", "well", "above", "those", "of", "his", "classmates.", "Transfomerbert", "(this", "work)", "now", "at", "age", "9", ",", "his", "teachers", "say", "that", "richie", "reads", "high", "schoolwork.", "Complex", "(input)", "He", "can", "recall", "the", "special", "feeling", "when,", "at", "age", "7,", "he", "built", "his", "\ufb01rst", "kite", "and", "saw", "it", "waft", "into", "the", "air.", "Simple", "(reference)", "He", "can", "remember", "the", "special", "feeling", "when", "he", "built", "his", "\ufb01rst", "kite", ".", "LSTM", "he", "can", "remember", "the", "people", "when", "he", "was", "age", "7.", "EditNTS", "(Dong", "et", "al.,", "2019)", ",", "at", "age", "7,", "he", "built", "his", "\ufb01rst", "kite.", "Rerank", "(Kriz", "et", "al.,", "2019)", "he", "could", "remember", "the", "special", "feeling", "when.", "Transfomerbert", "(this", "work)", "he", "can", "remember", "the", "special", "feeling", "when", "he", "was", "7", "years", "old.", "Complex", "(input)", "Following", "the", "action", ",", "two", "middle-aged", "brothers", "slid", "down", "a", "hill", "holding", "signs.", "Simple", "(reference)", "For", "example", ",", "two", "grownup", "brothers", "slid", "down", "a", "hill", "holding", "signs", ".", "LSTM", "<unk>", "middle", "-", "aged", "brothers", "slid", "down", "a", "hill", "holding", "signs.", "EditNTS", "(Dong", "et", "al.,", "2019)", "two", "middle-aged", "brothers", ",", "14", ",", "heard", "down", "a", "hill", "signs.", "Rerank", "(Kriz", "et", "al.,", "2019)", "he", "made", "a", "hill", "holding", "signs.", "Transfomerbert", "(this", "work)", "two", "middle-aged", "brothers", "slid", "down", "a", "hill", "holding", "signs."], "regionBoundary": {"x2": 518.0, "y1": 487.8900146484375, "x1": 78.0, "y2": 699.8900146484375}, "caption": "Table 17: Examples of simplifications generated by our best model, Transformerbert, and other baselines, namely, EditNTS (Dong et al., 2019), Rerank (Kriz et al., 2019) and LSTM on the old NEWSELA test set. Both LSTM and Transformerbert are trained on NEWSELA-AUTO. For EditNTS and Rerank, we use the system outputs shared by their original authors. Bold indicates new phrases introduced by the model.", "page": 15}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 729.9266391330295, "y1": 303.1757354736328, "x1": 99.54305224948459, "y2": 331.5763685438368}, "name": "1", "caption_text": "Figure 1: An example of sentence alignment between an original news article (right) and its simplified version (left) in Newsela. The label ai for each simple sentence si is the index of complex sentence cai it aligns to.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 728.0, "y1": 88.0, "x1": 100.0, "y2": 302.0}, "page": 1, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.347191704644, "y1": 429.5132531060113, "x1": 426.3430701361762, "y2": 554.0804968939887}, "name": "1", "caption_text": "Table 1: Statistics of our manually and automatically created sentence alignment annotations on Newsela. \u2020 This number includes all complex-simple sentence pairs (including aligned, partially-aligned, or notaligned) across all 10 combinations of 5 readability levels (level 0-4), of which 20,343 sentence pairs between adjacent readability levels were manually annotated and the rest of labels were derived.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 723.0, "y1": 86.0, "x1": 426.0, "y2": 430.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.4507361518012, "y1": 382.8673892550998, "x1": 99.35001797146268, "y2": 490.8306121826172}, "name": "2", "caption_text": "Figure 2: Manual inspection of 100 random sentence pairs from our corpora (NEWSELA-AUTO and WIKIAUTO) and the existing Newsela (Xu et al., 2015) and Wikipedia (Zhang and Lapata, 2017) corpora. Our corpora contain at least 44% more complex rewrites (Deletion + Paraphrase or Splitting + Paraphrase) and 27% less defective pairs (Not Aligned or Not Simpler).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 412.0, "y1": 89.0, "x1": 103.0, "y2": 345.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9261305067274, "y1": 266.22157626681854, "x1": 99.50137668185764, "y2": 307.76807996961804}, "name": "2", "caption_text": "Table 2: Performance of different sentence alignment methods on the NEWSELA-MANUAL test set. \u2020 Previous work was designed only for Task 1 and used alignment strategy (greedy algorithm or dynamic programming) to improve either precision or recall.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 710.0, "y1": 86.0, "x1": 104.0, "y2": 265.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 387.4821133083767, "y1": 511.9049072265625, "x1": 115.24027718438042, "y2": 520.2416737874349}, "name": "3", "caption_text": "Table 3: Ablation study of our aligner on dev set.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 399.0, "y1": 329.0, "x1": 101.0, "y2": 516.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.4502699110243, "y1": 216.71324835883246, "x1": 99.57083596123589, "y2": 258.25975206163196}, "name": "4", "caption_text": "Table 4: Statistics of our newly constructed parallel corpora for sentence simplification compared to the old datasets (Xu et al., 2015; Zhang and Lapata, 2017).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 389.0, "y1": 86.0, "x1": 100.0, "y2": 217.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2235955132378, "y1": 302.50350104437933, "x1": 99.50138727823892, "y2": 393.86121961805554}, "name": "5", "caption_text": "Table 5: Automatic evaluation results on NEWSELA test sets comparing models trained on our dataset NEWSELAAUTO against the existing dataset (Xu et al., 2015). We report SARI, the main automatic metric for simplification, precision for deletion and F1 scores for adding and keeping operations. Add scores are low partially because we are using one reference. Bold typeface and underline denote the best and the second best performances respectively. For Flesch-Kincaid (FK) grade level and average sentence length (Len), we consider the values closest to reference as the best.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 678.0, "y1": 86.0, "x1": 153.0, "y2": 303.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.5731031629774, "y1": 517.7785237630209, "x1": 99.54308403862846, "y2": 559.3249850802952}, "name": "6", "caption_text": "Table 6: Human evaluation of fluency (F), adequacy (A) and simplicity (S) on the old NEWSELA test set. \u2020We used the system outputs shared by the authors.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 396.0, "y1": 412.0, "x1": 100.0, "y2": 518.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2188483344183, "y1": 554.8354678683811, "x1": 426.5777587890625, "y2": 596.3805304633246}, "name": "3", "caption_text": "Figure 3: Manual inspection of 100 random sentences generated by Transformerbert trained on NEWSELAAUTO and existing NEWSELA datasets, respectively.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 701.0, "y1": 414.0, "x1": 452.0, "y2": 535.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.63354153103296, "y1": 680.9479607476128, "x1": 99.54308403862846, "y2": 705.8902740478516}, "name": "7", "caption_text": "Table 7: Human evaluation of fluency (F), adequacy (A) and simplicity (S) on NEWSELA-AUTO test set.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 390.0, "y1": 561.0, "x1": 100.0, "y2": 681.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.44658237033417, "y1": 290.0493621826172, "x1": 99.57083596123589, "y2": 331.59586588541663}, "name": "8", "caption_text": "Table 8: Automatic evaluation results on Wikipedia TURK corpus comparing models trained on WIKIAUTO and WIKILARGE (Zhang and Lapata, 2017).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 412.0, "y1": 86.0, "x1": 100.0, "y2": 272.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.45035468207465, "y1": 457.16879102918836, "x1": 99.57083596123589, "y2": 482.1097056070963}, "name": "9", "caption_text": "Table 9: Parameters of our neural CRF sentence alignment model.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 390.0, "y1": 365.0, "x1": 108.0, "y2": 457.0}, "page": 12, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.4502699110243, "y1": 623.7049102783203, "x1": 99.57083596123589, "y2": 648.6458248562283}, "name": "10", "caption_text": "Table 10: The thresholds in paragraph alignment Algorithm 2 for Newsela data.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 345.0, "y1": 521.0, "x1": 156.0, "y2": 624.0}, "page": 12, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.4502699110243, "y1": 866.0480075412327, "x1": 99.57083596123589, "y2": 890.9889221191406}, "name": "11", "caption_text": "Table 11: The thresholds in paragraph alignment Algorithm 4 for Wikipedia data.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 342.0, "y1": 763.0, "x1": 156.0, "y2": 866.0}, "page": 12, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 711.9494967990452, "y1": 189.03965420193143, "x1": 444.3152957492404, "y2": 197.3764207628038}, "name": "14", "caption_text": "Table 14: Parameters of our Transformer model.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 708.0, "y1": 86.0, "x1": 444.0, "y2": 189.0}, "page": 13, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 696.0646311442057, "y1": 334.60494147406683, "x1": 460.19999186197913, "y2": 342.9417080349392}, "name": "15", "caption_text": "Table 15: Parameters of our LSTM model.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 699.0, "y1": 218.0, "x1": 458.0, "y2": 335.0}, "page": 13, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.1534406873915, "y1": 649.7715844048394, "x1": 99.57083596123589, "y2": 674.7124989827473}, "name": "12", "caption_text": "Table 12: Performance of different sentence alignment methods on the WIKI-MANUAL dev set for Task 1.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 404.0, "y1": 500.0, "x1": 100.0, "y2": 633.0}, "page": 13, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.1534406873915, "y1": 839.6674262152777, "x1": 99.57083596123589, "y2": 864.6083407931858}, "name": "13", "caption_text": "Table 13: Performance of different sentence alignment methods on the WIKI-MANUAL test set for Task 1.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 404.0, "y1": 707.0, "x1": 100.0, "y2": 840.0}, "page": 13, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.3482089572483, "y1": 472.4035474989149, "x1": 99.34999677870009, "y2": 497.3444620768229}, "name": "4", "caption_text": "Figure 4: Instructions provided to Amazon Mechanical Turk workers to evaluate generated simplified sentences. We used the same instructions as described in Kriz et al. (2019).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 710.0, "y1": 91.0, "x1": 100.0, "y2": 451.0}, "page": 14, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 730.4196675618489, "y1": 606.875737508138, "x1": 99.57083596123589, "y2": 665.0263468424479}, "name": "16", "caption_text": "Table 16: Examples of simplified sentences generated by LSTM and Transformerbert models trained on our new NEWSELA-AUTO (this work) and old existing NEWSELA (Xu et al., 2015) datasets. The source sentences are from our new NEWSELA-AUTO test set. Models trained on our new data rephrase the input sentence more often than the models trained on old data. Bold indicates deletions or paraphrases.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 717.0, "y1": 98.0, "x1": 100.0, "y2": 607.0}, "page": 15, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 731.6502041286892, "y1": 988.7771606445312, "x1": 99.57080417209201, "y2": 1046.9263712565105}, "name": "17", "caption_text": "Table 17: Examples of simplifications generated by our best model, Transformerbert, and other baselines, namely, EditNTS (Dong et al., 2019), Rerank (Kriz et al., 2019) and LSTM on the old NEWSELA test set. Both LSTM and Transformerbert are trained on NEWSELA-AUTO. For EditNTS and Rerank, we use the system outputs shared by their original authors. Bold indicates new phrases introduced by the model.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 721.0, "y1": 678.0, "x1": 100.0, "y2": 989.0}, "page": 15, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 722.1695793999565, "y1": 993.6341179741753, "x1": 107.75416692097981, "y2": 1001.9708421495226}, "name": "5", "caption_text": "Figure 5: Instructions and an example question for our crowdsourcing annotation on the Figure Eight platform.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 712.0, "y1": 90.0, "x1": 110.0, "y2": 958.0}, "page": 16, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 638.4014977349175, "y1": 490.6965891520182, "x1": 191.52361551920572, "y2": 499.0333557128906}, "name": "6", "caption_text": "Figure 6: Annotation interface for correcting the crowdsourced alignment labels.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 712.0, "y1": 91.0, "x1": 120.0, "y2": 466.0}, "page": 17, "dpi": 0}], "error": null, "pdf": "/work/host-output/376efc9a321c990bee4c2b788c845db1643c0f2e/2020.acl-main.709.pdf", "dpi": 100}