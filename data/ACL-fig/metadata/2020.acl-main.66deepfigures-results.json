{"raw_detected_boxes": [[], [{"x2": 726.0, "y1": 91.0, "x1": 427.0, "y2": 226.0}], [{"x2": 702.0, "y1": 86.0, "x1": 451.0, "y2": 130.0}, {"x2": 726.0, "y1": 260.0, "x1": 426.0, "y2": 403.0}], [], [{"x2": 399.0, "y1": 91.0, "x1": 100.0, "y2": 232.0}], [], [{"x2": 726.0, "y1": 86.0, "x1": 111.0, "y2": 143.0}, {"x2": 724.0, "y1": 213.0, "x1": 426.0, "y2": 381.0}], [{"x2": 381.0, "y1": 86.0, "x1": 119.0, "y2": 185.0}], [{"x2": 724.0, "y1": 94.0, "x1": 103.0, "y2": 275.0}], [], [], [{"x2": 391.0, "y1": 90.0, "x1": 99.0, "y2": 363.0}, {"x2": 726.0, "y1": 100.0, "x1": 432.0, "y2": 518.0}], [{"x2": 395.0, "y1": 92.0, "x1": 102.0, "y2": 430.0}, {"x2": 684.0, "y1": 89.0, "x1": 470.0, "y2": 221.0}, {"x2": 671.0, "y1": 322.0, "x1": 482.0, "y2": 470.0}], [{"x2": 376.0, "y1": 89.0, "x1": 128.0, "y2": 217.0}]], "raw_pdffigures_output": {"regionless-captions": [{"figType": "Figure", "boundary": {"x2": 527.2003173828125, "y1": 176.51351928710938, "x1": 306.9469909667969, "y2": 218.38104248046875}, "text": "Figure 1: Fitting a mixture of Gaussians with a single Gaussian using distinguishability (TV) and log loss (KL). As shown, log loss is extremely sensitive to outliers, resulting in poor estimation.", "name": "1", "page": 1}, {"figType": "Figure", "boundary": {"x2": 291.92449951171875, "y1": 285.2765197753906, "x1": 72.0, "y2": 315.19000244140625}, "text": "Figure 6: Examples of titles that require hallucinating new facts and titles that are directly entailed from context.", "name": "6", "page": 11}], "figures": [{"figType": "Table", "name": "2", "captionBoundary": {"x2": 525.5470581054688, "y1": 110.9035415649414, "x1": 71.69097900390625, "y2": 128.86102294921875}, "imageText": ["HUSE-D", "0.88", "0.12", "0.98", "0.18", "0.59", "0.65", "0.25", "HUSE-Q", "0.70", "0.92", "0.58", "0.86", "0.73", "0.67", "0.75", "Loss", "trunc.", "Trunc+reject", "(\u03b1", "=", "0.1)", "Full", "samp.", "Beam", "top-k", "(k", "=", "100)", "top-p", "(p", "=", "0.9)", "GAN", "HUSE", "0.58", "0.04", "0.55", "0.04", "0.32", "0.32", "0.003"], "regionBoundary": {"x2": 523.0, "y1": 61.8900146484375, "x1": 73.0, "y2": 103.8900146484375}, "caption": "Table 2: HUSE, HUSE-D, and HUSE-Q scores for loss truncation and baselines. As shown, loss truncation outperforms all baselines on HUSE score.", "page": 6}, {"figType": "Figure", "name": "5", "captionBoundary": {"x2": 527.2901000976562, "y1": 287.3635559082031, "x1": 306.9670104980469, "y2": 329.2320556640625}, "imageText": ["Method", "Trunc.", "Trunc+reject", "Samp.", "Beam", "top-k", "top-p", "-Q", "HU", "SE", "1.0", "0.8", "0.6", "0.4", "0.2", "0.0", "0.0", "0.2", "0.4", "0.6", "0.8", "1.0", "HUSE-D"], "regionBoundary": {"x2": 522.8763427734375, "y1": 151.78033447265625, "x1": 310.34307861328125, "y2": 273.64837646484375}, "caption": "Figure 5: HUSE-D vs HUSE-Q for loss truncation, truncation + rejection sampling, and baselines. The red line shows the best achievable frontier via ensembling. Truncation and rejection outperform all baselines.", "page": 6}, {"figType": "Table", "name": "7", "captionBoundary": {"x2": 291.924560546875, "y1": 173.16952514648438, "x1": 71.69099426269531, "y2": 191.12701416015625}, "imageText": ["Condition", "ROUGE-L", "Log-loss,", "beam", "41.4", "Log-loss,", "full", "sampling", "27.9", "Truncation,", "top-k", "=", "100", "33.4", "Truncation,", "top-k", "=", "2", "38.9", "Truncation,", "top-p", "=", "0.9", "35.1", "Truncation,", "top-p", "=", "0.1", "40.9"], "regionBoundary": {"x2": 271.0, "y1": 65.72029876708984, "x1": 91.0, "y2": 156.7760009765625}, "caption": "Table 7: Loss truncation combined with top-k and topp decoding.", "page": 13}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 525.54638671875, "y1": 100.9405288696289, "x1": 306.96697998046875, "y2": 166.71905517578125}, "imageText": ["New", "facts", "Directly", "entailed", "Percent", "35%", "65%", "Avg.", "log", "loss", "34.3", "20.5"], "regionBoundary": {"x2": 506.0, "y1": 61.8900146484375, "x1": 325.0, "y2": 93.8900146484375}, "caption": "Table 1: Fraction of the data and log loss of titles that require hallucinating new facts (left column) and titles that are entailed from the context (right column). As shown, 35% of titles require hallucinating new facts and the average log loss of titles requiring new facts is over 1.7\u00d7 the loss of the directly entailed sequences.", "page": 2}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 525.5465698242188, "y1": 310.9405212402344, "x1": 307.2760009765625, "y2": 376.71893310546875}, "imageText": ["Directly", "entailed", "New", "facts", "ity", "De", "ns", "0.04", "0.02", "0.00", "0", "20", "40", "60", "80", "Log-loss"], "regionBoundary": {"x2": 523.0, "y1": 187.8900146484375, "x1": 312.4643859863281, "y2": 288.55078125}, "caption": "Figure 3: Normalized histogram of log losses for titles that require hallucinating new facts compared to those that can be directly entailed. As shown, titles requiring new facts incur significantly higher loss and more than 80% of examples with greater than 40 log loss require hallucinating new facts.", "page": 2}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 291.9244384765625, "y1": 164.12350463867188, "x1": 71.64099884033203, "y2": 194.0360107421875}, "imageText": ["Context:", "For", "the", "\ufb01rst", "time", "in", "\ufb01ve", "years,", "Mi-", "crosoft", "corp.", "is", "\ufb01nally", "unveiling", "a", "new", "system", "for", "operating", "personal", "computers.", "Title:", "Microsoft", "Makes", "Long-Awaited", "Soft-", "ware", "Upgrade", "Available", "to", "Businesses", "Thurs-", "day."], "regionBoundary": {"x2": 291.0, "y1": 61.8900146484375, "x1": 72.0, "y2": 152.8900146484375}, "caption": "Figure 2: Example of an article title from the Gigaword dataset that requires hallucinating new facts such as \u2018Thursday\u2019 (colored red).", "page": 2}, {"figType": "Table", "name": "6", "captionBoundary": {"x2": 525.546630859375, "y1": 354.4415588378906, "x1": 306.96697998046875, "y2": 396.3089904785156}, "imageText": ["Condition", "BLEU", "Truncation,", "c", "=", "0.9", "0.72", "Truncation,", "c", "=", "0.8", "0.71", "Truncation,", "c", "=", "0.7", "0.70", "Truncation,", "c", "=", "0.6", "0.69", "Truncation,", "c", "=", "0.5", "0.69", "Baseline", "0.64", "0.72", "0.64"], "regionBoundary": {"x2": 486.0, "y1": 233.4423370361328, "x1": 347.0, "y2": 338.04803466796875}, "caption": "Table 6: BLEU scores for loss truncation at various c and the baseline model on the E2E task. As shown, loss truncation outperforms the baseline on BLEU score at a range of hyperparameters.", "page": 12}, {"figType": "Figure", "name": "8", "captionBoundary": {"x2": 290.08489990234375, "y1": 325.4715270996094, "x1": 72.18299865722656, "y2": 331.4739990234375}, "imageText": ["(b)", "Prompt", "for", "measuring", "factuality.", "(a)", "Prompt", "for", "measuring", "HUSE."], "regionBoundary": {"x2": 291.0, "y1": 61.8900146484375, "x1": 72.0, "y2": 310.15399169921875}, "caption": "Figure 8: Prompts for measuring HUSE and factuality.", "page": 12}, {"figType": "Table", "name": "5", "captionBoundary": {"x2": 525.7128295898438, "y1": 173.16952514648438, "x1": 306.967041015625, "y2": 215.03704833984375}, "imageText": ["Condition", "ROUGE-L", "Truncation,", "c", "=", "0.9", "24.3", "Truncation,", "c", "=", "0.8", "24.9", "Truncation,", "c", "=", "0.7", "24.0", "Truncation,", "c", "=", "0.6", "23.2", "top-k", "=", "100", "22.8", "top-p", "=", "0.9", "22.8"], "regionBoundary": {"x2": 496.0, "y1": 65.72029876708984, "x1": 337.0, "y2": 156.7760009765625}, "caption": "Table 5: ROUGE-L scores for loss truncation at various c and entropy-matched top-k and top-p decoding for summarization. As shown, loss truncation outperforms on ROUGE-L for a range of c.", "page": 12}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 291.9244079589844, "y1": 141.18954467773438, "x1": 71.69100952148438, "y2": 183.05706787109375}, "imageText": ["Condition", "Mean", "score", "Human", "3.63", "\u00b1", "0.05", "Truncation", "+", "Rejection", "(\u03b1", "=", "0.1)", "3.79", "\u00b1", "0.06", "Beam", "3.51", "\u00b1", "0.05", "top-p", "(p", "=", "0.4)", "3.42", "\u00b1", "0.05", "top-k", "(k", "=", "2)", "3.29", "\u00b1", "0.05", "Sampling", "2.96", "\u00b1", "0.05"], "regionBoundary": {"x2": 275.0, "y1": 61.8900146484375, "x1": 85.0, "y2": 133.8900146484375}, "caption": "Table 3: Mean scores and standard errors of factuality in generated news titles given articles. As shown, rejection sampling outperforms all baselines and matches the human reference score.", "page": 7}, {"figType": "Figure", "name": "7", "captionBoundary": {"x2": 525.5465087890625, "y1": 405.2885437011719, "x1": 307.2759704589844, "y2": 423.24700927734375}, "imageText": ["(b)", "Example", "2.", "Context:", "British", "investment", "fund", "Fidelity", "has", "increased", "its", "stake", "in", "Puma,", "the", "German", "maker", "of", "sportswear", "and", "equipment,", "to", "just", "over", "\ufb01ve", "percent,", "Puma", "said", "on", "Thursday.", "Ground", "truth:", "Private", "equity", "\ufb01rm", "Fidelity", "raises", "stake", "in", "Puma", "to", "over", "\ufb01ve", "pct", "Selected", "sample:", "Fidelity", "increases", "stake", "in", "Puma", "Rejected", "sample:", "Boost", "higher", "\ufb01rst-half", "stake", "in", "Puma", "says", "Puma", "(a)", "Example", "1.", "Context:", "At", "least", "two", "people", "have", "tested", "pos-", "itive", "for", "the", "bird", "\ufb02u", "virus", "in", "Eastern", "Turkey,", "health", "minister", "Recep", "Akdag", "told", "a", "news", "con-", "ference", "Wednesday.", "Ground", "truth:", "Two", "test", "positive", "for", "bird", "\ufb02u", "virus", "in", "Turkey", "Selected", "sample:", "Two", "reported", "positive", "for", "bird", "\ufb02u", "in", "Eastern", "Turkey", "Rejected", "sample:", "Two", "of\ufb01cials", "fail", "to", "get", "good", "for", "bird", "\ufb02u", "in", "Eastern", "Turkey"], "regionBoundary": {"x2": 524.0, "y1": 61.8900146484375, "x1": 307.0, "y2": 389.97100830078125}, "caption": "Figure 7: Examples of sampled titles that were selected and rejected in rejection sampling at \u03b1 = 0.1.", "page": 11}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 525.5471801757812, "y1": 208.73654174804688, "x1": 71.69100189208984, "y2": 238.6490478515625}, "imageText": ["at", "least", "##", "dead", "##,###", "homeless", "as", "\ufb02oods", "hit", "southern", "africa", "Method", "Example", "Context", "at", "least", "##", "people", "have", "been", "killed", "and", "more", "than", "##,###", "made", "homeless", "by", "\ufb02oods", "that", "swept", "across", "southern", "africa", "in", "the", "past", "week", ",", "striking", "a", "region", "already", "grappling", "with", "severe", "food", "shortages", ".", "Gold", "\ufb02oods", "kill", "##", "in", "famine-hit", "southern", "africa", "Loss", "truncation", "at", "least", "##", "people", "killed", "##,###", "evacuated", "in", "\ufb02oods", "in", "southern", "african", "region", "\ufb02oods", "that", "sweep", "parts", "of", "africa", "kill", "at", "least", "##", "Beam", "\ufb02ooding", "hits", "southern", "africa", "as", "deaths", "rise", "Full", "sampling", "child", "farming", "stalls", "in", "southern", "africa", "earthquake", "kills", "##", "in", "southern", "africa", "top-p", "(p", "=", "0.9)", "torrential", "rains", "prompt", "warnings", "in", "southern", "africa", "toll", "nears", "##", "in", "southern", "africa", "top-k", "(k", "=", "2)", "at", "least", "##", "killed", "##,###", "homeless", "in", "southern", "africa", "\ufb02oods"], "regionBoundary": {"x2": 526.0, "y1": 61.8900146484375, "x1": 72.0, "y2": 200.8900146484375}, "caption": "Table 4: Examples of generations for various baselines and loss truncation (two replicates shown for sampled outputs). As shown, loss truncation can achieve diverse and high quality outputs. In contrast, baselines either are not diverse (beam, top-k) or poor quality (full sampling, top-p). We color incorrect facts in red.", "page": 8}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 291.9248046875, "y1": 181.32656860351562, "x1": 71.7509994506836, "y2": 247.1051025390625}, "imageText": ["2", "4", "Pinsker's", "Loss-truncated", "(ours)", "TV^2", "0", "1", "2", "3", "4", "5", "6", "0"], "regionBoundary": {"x2": 289.9220886230469, "y1": 65.8900146484375, "x1": 72.0, "y2": 166.7413330078125}, "caption": "Figure 4: Pinsker\u2019s inequality, our bound, and the total variation squared of parameter estimates for different parameter estimates (c = 0.2). As shown, loss truncation can significantly improve bounds over Pinsker\u2019s inequality and, in this case, has a nearly identical minimizer to directly minimizing total variation.", "page": 4}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 732.222663031684, "y1": 245.1576656765408, "x1": 426.3152652316623, "y2": 303.30700344509546}, "name": "1", "caption_text": "Figure 1: Fitting a mixture of Gaussians with a single Gaussian using distinguishability (TV) and log loss (KL). As shown, log loss is extremely sensitive to outliers, resulting in poor estimation.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 726.0, "y1": 91.0, "x1": 427.0, "y2": 226.0}, "page": 1, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.925537109375, "y1": 140.1951789855957, "x1": 426.343027750651, "y2": 231.55424329969617}, "name": "1", "caption_text": "Table 1: Fraction of the data and log loss of titles that require hallucinating new facts (left column) and titles that are entailed from the context (right column). As shown, 35% of titles require hallucinating new facts and the average log loss of titles requiring new facts is over 1.7\u00d7 the loss of the directly entailed sequences.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 719.0, "y1": 86.0, "x1": 434.0, "y2": 147.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.4506089952257, "y1": 227.94931199815537, "x1": 99.50138727823892, "y2": 269.4944593641493}, "name": "2", "caption_text": "Figure 2: Example of an article title from the Gigaword dataset that requires hallucinating new facts such as \u2018Thursday\u2019 (colored red).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 726.0, "y1": 260.0, "x1": 426.0, "y2": 403.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.45111762152777, "y1": 251.8424563937717, "x1": 99.6541659037272, "y2": 343.2015313042535}, "name": "4", "caption_text": "Figure 4: Pinsker\u2019s inequality, our bound, and the total variation squared of parameter estimates for different parameter estimates (c = 0.2). As shown, loss truncation can significantly improve bounds over Pinsker\u2019s inequality and, in this case, has a nearly identical minimizer to directly minimizing total variation.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 402.0, "y1": 91.0, "x1": 100.0, "y2": 232.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9264695909288, "y1": 154.03269661797418, "x1": 99.57080417209201, "y2": 178.97364298502603}, "name": "2", "caption_text": "Table 2: HUSE, HUSE-D, and HUSE-Q scores for loss truncation and baselines. As shown, loss truncation outperforms all baselines on HUSE score.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 729.0, "y1": 86.0, "x1": 100.0, "y2": 160.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.3473612467448, "y1": 399.11604987250433, "x1": 426.3430701361762, "y2": 457.26674397786456}, "name": "5", "caption_text": "Figure 5: HUSE-D vs HUSE-Q for loss truncation, truncation + rejection sampling, and baselines. The red line shows the best achievable frontier via ensembling. Truncation and rejection outperform all baselines.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 726.0, "y1": 211.0, "x1": 426.0, "y2": 398.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.4505666097005, "y1": 196.09658983018662, "x1": 99.57084655761719, "y2": 254.2459275987413}, "name": "3", "caption_text": "Table 3: Mean scores and standard errors of factuality in generated news titles given articles. As shown, rejection sampling outperforms all baselines and matches the human reference score.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 398.0, "y1": 86.0, "x1": 102.0, "y2": 202.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9266391330295, "y1": 289.911863538954, "x1": 99.57083596123589, "y2": 331.4570109049479}, "name": "4", "caption_text": "Table 4: Examples of generations for various baselines and loss truncation (two replicates shown for sampled outputs). As shown, loss truncation can achieve diverse and high quality outputs. In contrast, baselines either are not diverse (beam, top-k) or poor quality (full sampling, top-p). We color incorrect facts in red.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 730.0, "y1": 86.0, "x1": 100.0, "y2": 292.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.450693766276, "y1": 396.2173885769314, "x1": 100.0, "y2": 437.7638922797309}, "name": "6", "caption_text": "Figure 6: Examples of titles that require hallucinating new facts and titles that are directly entailed from context.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 401.0, "y1": 86.0, "x1": 99.0, "y2": 376.0}, "page": 11, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9257066514757, "y1": 562.9007551405165, "x1": 426.7721811930338, "y2": 587.8430684407551}, "name": "7", "caption_text": "Figure 7: Examples of sampled titles that were selected and rejected in rejection sampling at \u03b1 = 0.1.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 727.0, "y1": 86.0, "x1": 426.0, "y2": 535.0}, "page": 11, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 402.89569430881073, "y1": 452.0437876383463, "x1": 100.25416480170355, "y2": 460.38055419921875}, "name": "8", "caption_text": "Figure 8: Prompts for measuring HUSE and factuality.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 403.0, "y1": 88.0, "x1": 100.0, "y2": 433.0}, "page": 12, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 730.1567077636719, "y1": 240.5132293701172, "x1": 426.34311252170136, "y2": 298.6625671386719}, "name": "5", "caption_text": "Table 5: ROUGE-L scores for loss truncation at various c and entropy-matched top-k and top-p decoding for summarization. As shown, loss truncation outperforms on ROUGE-L for a range of c.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 688.0, "y1": 89.0, "x1": 468.0, "y2": 221.0}, "page": 12, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9258761935764, "y1": 492.2799428304036, "x1": 426.343027750651, "y2": 550.4291534423828}, "name": "6", "caption_text": "Table 6: BLEU scores for loss truncation at various c and the baseline model on the E2E task. As shown, loss truncation outperforms the baseline on BLEU score at a range of hyperparameters.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 675.0, "y1": 322.0, "x1": 482.0, "y2": 470.0}, "page": 12, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.45077853732636, "y1": 240.5132293701172, "x1": 99.5708253648546, "y2": 265.45418633355035}, "name": "7", "caption_text": "Table 7: Loss truncation combined with top-k and topp decoding.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 376.0, "y1": 89.0, "x1": 127.0, "y2": 221.0}, "page": 13, "dpi": 0}], "error": null, "pdf": "/work/host-output/38e3ff390142ae206843f3e9888e8f2d0bf41e26/2020.acl-main.66.pdf", "dpi": 100}