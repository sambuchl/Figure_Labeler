{"raw_detected_boxes": [[], [], [{"x2": 680.0, "y1": 95.0, "x1": 147.0, "y2": 249.0}], [{"x2": 329.0, "y1": 95.0, "x1": 153.0, "y2": 525.0}, {"x2": 677.0, "y1": 95.0, "x1": 480.0, "y2": 288.0}], [], [{"x2": 723.0, "y1": 97.0, "x1": 434.0, "y2": 278.0}], [{"x2": 714.0, "y1": 95.0, "x1": 443.0, "y2": 451.0}], [{"x2": 363.0, "y1": 86.0, "x1": 139.0, "y2": 240.0}, {"x2": 719.0, "y1": 95.0, "x1": 435.0, "y2": 330.0}], [{"x2": 342.0, "y1": 86.0, "x1": 156.0, "y2": 251.0}], [], [], [{"x2": 724.0, "y1": 95.0, "x1": 433.0, "y2": 220.0}, {"x2": 688.0, "y1": 781.0, "x1": 476.0, "y2": 870.0}], [{"x2": 395.0, "y1": 99.0, "x1": 107.0, "y2": 278.0}, {"x2": 396.0, "y1": 362.0, "x1": 107.0, "y2": 543.0}, {"x2": 704.0, "y1": 342.0, "x1": 453.0, "y2": 484.0}]], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Figure", "name": "4", "captionBoundary": {"x2": 527.2003784179688, "y1": 220.64852905273438, "x1": 307.2760009765625, "y2": 250.56103515625}, "imageText": [], "regionBoundary": {"x2": 526.0, "y1": 61.8900146484375, "x1": 307.0, "y2": 208.8900146484375}, "caption": "Figure 4: Average runtimes of each model according to the number of words on STS and reranking tasks, subscripted as sts and rrk, respectively.", "page": 5}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 527.2900390625, "y1": 343.32354736328125, "x1": 306.9469909667969, "y2": 409.1019592285156}, "imageText": ["w/", "uniLM", "3.82", "11.73", "4.05", "12.63", "w/", "biLM", "3.73", "11.53", "3.97", "12.41", "w/", "T-TA", "3.67", "11.56", "3.97", "12.38", "Seq2SeqASR", "4.11", "12.31", "4.31", "13.14", "w/", "n-gram", "3.94", "11.93", "4.15", "12.89", "w/", "BERT", "3.72", "11.59", "3.97", "12.46", "w/", "BERT\\M", "4.09", "12.26", "4.28", "13.15", "w/", "uniLM", "5.07", "16.20", "5.14", "17.00", "w/", "biLM", "4.94", "16.09", "5.14", "16.81", "w/", "T-TA", "4.98", "16.09", "5.11", "16.91", "Shin", "et", "al.", "7.17", "19.79", "7.25", "20.37", "w/", "n-gram", "5.62", "16.85", "5.75", "17.72", "w/", "\u2217uniSANLMw", "6.05", "17.32", "6.11", "18.13", "w/", "\u2217biSANLMw", "5.52", "16.61", "5.65", "17.37", "w/", "BERT", "5.24", "16.56", "5.38", "17.46", "w/", "BERT\\M", "7.08", "19.61", "7.14", "20.18", "clean", "other", "clean", "other", "Method", "dev", "test"], "regionBoundary": {"x2": 514.0, "y1": 62.8900146484375, "x1": 319.0, "y2": 330.8900146484375}, "caption": "Table 1: WERs after reranking with each language model on LibriSpeech. The \u2018other\u2019 sets are recorded in noisier environments than the \u2018clean\u2019 sets. Bold font denotes the best performance on each subtask, and \u2217 signifies a word-level language model from Shin et al. (2019).", "page": 6}, {"figType": "Figure", "name": "1", "captionBoundary": {"x2": 503.185302734375, "y1": 195.63552856445312, "x1": 94.3599853515625, "y2": 201.63800048828125}, "imageText": ["(a)", "Causal", "language", "modeling", "(b)", "Masked", "language", "modeling", "(c)", "Language", "autoencoding"], "regionBoundary": {"x2": 492.0, "y1": 61.8900146484375, "x1": 105.0, "y2": 180.3179931640625}, "caption": "Figure 1: Schematic diagrams of language models for the (a) CLM, (b) MLM, and (c) LAE objectives.", "page": 2}, {"figType": "Table", "name": "7", "captionBoundary": {"x2": 525.5463256835938, "y1": 360.7785339355469, "x1": 306.9670104980469, "y2": 378.7359924316406}, "imageText": ["test", "clean", "uniLM", "[4.05]", "495.5", "73.18", "biLM", "[3.97]", "(75.43)", "(12.72)", "T-TA", "[3.97]", "(590.0)", "(12.43)", "dev", "clean", "uniLM", "[3.82]", "341.5", "70.80", "biLM", "[3.73]", "(76.49)", "(11.93)", "T-TA", "[3.67]", "(293.4)", "(11.69)", "Method", "[WER]", "(p)PPLa", "(p)PPLm"], "regionBoundary": {"x2": 507.0, "y1": 240.8900146484375, "x1": 326.0, "y2": 348.8900146484375}, "caption": "Table 7: (pseudo)Perplexities and corresponding WERs of the language models on LibriSpeech.", "page": 12}, {"figType": "Figure", "name": "5", "captionBoundary": {"x2": 290.2705383300781, "y1": 220.64852905273438, "x1": 72.0, "y2": 238.60601806640625}, "imageText": [], "regionBoundary": {"x2": 291.0, "y1": 61.8900146484375, "x1": 72.0, "y2": 208.8900146484375}, "caption": "Figure 5: Runtimes according to the number of words for the uniLM and T-TA.", "page": 12}, {"figType": "Figure", "name": "6", "captionBoundary": {"x2": 291.9242858886719, "y1": 412.5905456542969, "x1": 72.0, "y2": 442.50299072265625}, "imageText": [], "regionBoundary": {"x2": 291.0, "y1": 253.8900146484375, "x1": 72.0, "y2": 400.8900146484375}, "caption": "Figure 6: Runtimes according to the number of words for the biLM and T-TA in the GPU-augmented environment.", "page": 12}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 291.92437744140625, "y1": 185.32254028320312, "x1": 71.69100189208984, "y2": 239.14605712890625}, "imageText": ["Seq2SeqNMT", "27.83", "29.63", "w/", "n-gram", "28.41", "30.04", "w/", "BERT", "29.31", "30.52", "w/", "uniLM", "28.80", "30.21", "w/", "biLM", "28.76", "30.32", "w/", "T-TA", "28.83", "30.20", "Method", "De\u2192En", "Fr\u2192En"], "regionBoundary": {"x2": 262.0, "y1": 62.8900146484375, "x1": 100.0, "y2": 172.8900146484375}, "caption": "Table 2: BLEU scores after reranking with each language model on WMT13. Bold font denotes the best performance on each subtask, and the underlined values signify the best performances in our implementations.", "page": 7}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 527.2874755859375, "y1": 256.5505676269531, "x1": 305.9510498046875, "y2": 286.46405029296875}, "imageText": ["uniLM", "56.25", "63.87", "39.57", "55.00", "uniLM[EOS]", "40.75", "38.30", "biLM", "59.99", "-", "50.76", "-", "biLM\\M", "53.20", "58.80", "36.51", "49.08", "T-TA", "71.88", "54.75", "62.27", "44.74", "GloVe", "-", "52.4", "-", "40.6", "Word2Vec", "-", "70.0", "-", "56.5", "BERT", "64.78", "-", "54.22", "-", "BERT\\M", "59.17", "60.07", "47.91", "48.19", "BERT[CLS]", "29.16", "17.18", "context", "embed", "context", "embed", "Method", "STS-B-dev", "STS-B-test"], "regionBoundary": {"x2": 519.0, "y1": 62.8900146484375, "x1": 313.0, "y2": 243.8900146484375}, "caption": "Table 3: Pearson\u2019s r\u00d7100 results on the STS-B dataset. \u201c-\u201d denotes an infeasible value, and bold font denotes the top 2-performing models on each subtask.", "page": 7}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 525.5465698242188, "y1": 226.07351684570312, "x1": 307.2760009765625, "y2": 255.98699951171875}, "imageText": [], "regionBoundary": {"x2": 488.0, "y1": 61.8900146484375, "x1": 345.0, "y2": 213.8900146484375}, "caption": "Figure 3: Diagonal masking of the scaled dot-product attention mechanism. The highlighted box and dashed arrow represent the innovations reported in this paper.", "page": 3}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 290.27056884765625, "y1": 395.44354248046875, "x1": 72.0, "y2": 425.3559875488281}, "imageText": [], "regionBoundary": {"x2": 253.0, "y1": 61.8900146484375, "x1": 110.0, "y2": 383.8900146484375}, "caption": "Figure 2: Architecture of our T-TA. The highlighted box and dashed arrows are the innovations presented in this paper.", "page": 3}, {"figType": "Table", "name": "6", "captionBoundary": {"x2": 525.5465087890625, "y1": 638.7625732421875, "x1": 306.8070068359375, "y2": 656.7200317382812}, "imageText": ["Seq2SeqNMT", "27.83", "29.63", "oracle", "38.18", "39.58", "De\u2192En", "Fr\u2192En", "Method", "WMT13"], "regionBoundary": {"x2": 497.0, "y1": 556.8900146484375, "x1": 336.0, "y2": 626.8900146484375}, "caption": "Table 6: Oracle BLEU scores of the 50 best lists on WMT13", "page": 11}, {"figType": "Table", "name": "5", "captionBoundary": {"x2": 527.2001342773438, "y1": 176.85256958007812, "x1": 306.9670104980469, "y2": 194.81005859375}, "imageText": ["Seq2SeqASR", "4.11", "12.31", "4.31", "13.14", "oracle", "1.80", "7.90", "1.96", "8.39", "Shin", "et", "al.", "7.17", "19.79", "7.26", "20.37", "oracle", "3.18", "12.98", "3.19", "13.61", "clean", "other", "clean", "other", "Method", "dev", "test"], "regionBoundary": {"x2": 521.0, "y1": 62.8900146484375, "x1": 312.0, "y2": 164.8900146484375}, "caption": "Table 5: Oracle WERs of the 50 best lists on LibriSpeech from each ASR system.", "page": 11}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 292.0157470703125, "y1": 199.16757202148438, "x1": 70.67500305175781, "y2": 229.080078125}, "imageText": ["uniLM", "54.20", "65.69", "biLM", "58.98", "-", "biLM\\M", "53.79", "62.67", "T-TA", "69.49", "60.77", "BERT", "64.31", "-", "BERT\\M", "61.18", "64.63", "context", "embed", "Method", "SICK-test"], "regionBoundary": {"x2": 250.0, "y1": 62.8900146484375, "x1": 112.0, "y2": 186.8900146484375}, "caption": "Table 4: Pearson\u2019s r \u00d7 100 results on the SICK dataset. \u201c-\u201d denotes an infeasible value, and bold font denotes the best performance on each subtask.", "page": 8}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 698.8684760199652, "y1": 271.7160118950738, "x1": 131.05553521050348, "y2": 280.05277845594617}, "name": "1", "caption_text": "Figure 1: Schematic diagrams of language models for the (a) CLM, (b) MLM, and (c) LAE objectives.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 680.0, "y1": 95.0, "x1": 147.0, "y2": 253.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.153567843967, "y1": 549.2271423339844, "x1": 100.0, "y2": 590.772204928928}, "name": "2", "caption_text": "Figure 2: Architecture of our T-TA. The highlighted box and dashed arrows are the innovations presented in this paper.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 342.0, "y1": 95.0, "x1": 153.0, "y2": 526.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.925791422526, "y1": 313.9909956190321, "x1": 426.772223578559, "y2": 355.5374993218316}, "name": "3", "caption_text": "Figure 3: Diagonal masking of the scaled dot-product attention mechanism. The highlighted box and dashed arrow represent the innovations reported in this paper.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 677.0, "y1": 95.0, "x1": 480.0, "y2": 288.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2227478027344, "y1": 306.45629035101996, "x1": 426.772223578559, "y2": 348.00143771701386}, "name": "4", "caption_text": "Figure 4: Average runtimes of each model according to the number of words on STS and reranking tasks, subscripted as sts and rrk, respectively.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 723.0, "y1": 95.0, "x1": 434.0, "y2": 278.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.3472764756945, "y1": 476.8382602267795, "x1": 426.3152652316623, "y2": 568.1971655951605}, "name": "1", "caption_text": "Table 1: WERs after reranking with each language model on LibriSpeech. The \u2018other\u2019 sets are recorded in noisier environments than the \u2018clean\u2019 sets. Bold font denotes the best performance on each subtask, and \u2217 signifies a word-level language model from Shin et al. (2019).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 714.0, "y1": 86.0, "x1": 443.0, "y2": 460.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.45052422417535, "y1": 257.39241706000433, "x1": 99.57083596123589, "y2": 332.14730156792535}, "name": "2", "caption_text": "Table 2: BLEU scores after reranking with each language model on WMT13. Bold font denotes the best performance on each subtask, and the underlined values signify the best performances in our implementations.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 374.0, "y1": 86.0, "x1": 122.0, "y2": 257.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.3437160915798, "y1": 356.32023281521265, "x1": 424.9320136176215, "y2": 397.86673651801215}, "name": "3", "caption_text": "Table 3: Pearson\u2019s r\u00d7100 results on the STS-B dataset. \u201c-\u201d denotes an infeasible value, and bold font denotes the top 2-performing models on each subtask.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 721.0, "y1": 86.0, "x1": 435.0, "y2": 339.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.5774264865451, "y1": 276.6216278076172, "x1": 98.15972646077473, "y2": 318.1667751736111}, "name": "4", "caption_text": "Table 4: Pearson\u2019s r \u00d7 100 results on the SICK dataset. \u201c-\u201d denotes an infeasible value, and bold font denotes the best performance on each subtask.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 347.0, "y1": 86.0, "x1": 156.0, "y2": 260.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.222408718533, "y1": 245.6285688612196, "x1": 426.3430701361762, "y2": 270.56952582465277}, "name": "5", "caption_text": "Table 5: Oracle WERs of the 50 best lists on LibriSpeech from each ASR system.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 724.0, "y1": 86.0, "x1": 433.0, "y2": 229.0}, "page": 11, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9257066514757, "y1": 887.1702406141493, "x1": 426.120842827691, "y2": 912.1111551920573}, "name": "6", "caption_text": "Table 6: Oracle BLEU scores of the 50 best lists on WMT13", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 695.0, "y1": 773.0, "x1": 464.0, "y2": 887.0}, "page": 11, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.15352545844183, "y1": 306.45629035101996, "x1": 100.0, "y2": 331.3972473144531}, "name": "5", "caption_text": "Figure 5: Runtimes according to the number of words for the uniLM and T-TA.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 396.0, "y1": 95.0, "x1": 107.0, "y2": 278.0}, "page": 12, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.4503970675998, "y1": 573.0424245198567, "x1": 100.0, "y2": 614.5874871148003}, "name": "6", "caption_text": "Figure 6: Runtimes according to the number of words for the biLM and T-TA in the GPU-augmented environment.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 396.0, "y1": 362.0, "x1": 107.0, "y2": 545.0}, "page": 12, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9254523383246, "y1": 501.081297132704, "x1": 426.3430701361762, "y2": 526.0222117106119}, "name": "7", "caption_text": "Table 7: (pseudo)Perplexities and corresponding WERs of the language models on LibriSpeech.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 721.0, "y1": 334.0, "x1": 440.0, "y2": 501.0}, "page": 12, "dpi": 0}], "error": null, "pdf": "/work/host-output/b4f45840134cf1799afe52e904f0afb7faafcc41/2020.acl-main.76.pdf", "dpi": 100}