{"raw_detected_boxes": [[{"x2": 728.0, "y1": 315.0, "x1": 442.0, "y2": 719.0}], [], [{"x2": 727.0, "y1": 90.0, "x1": 101.0, "y2": 283.0}], [], [], [], [{"x2": 719.0, "y1": 93.0, "x1": 108.0, "y2": 237.0}, {"x2": 711.0, "y1": 328.0, "x1": 432.0, "y2": 581.0}, {"x2": 390.0, "y1": 335.0, "x1": 109.0, "y2": 437.0}, {"x2": 389.0, "y1": 541.0, "x1": 108.0, "y2": 676.0}], [{"x2": 383.0, "y1": 84.0, "x1": 105.0, "y2": 308.0}, {"x2": 730.0, "y1": 97.0, "x1": 432.0, "y2": 374.0}], [], [], []], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Figure", "name": "2", "captionBoundary": {"x2": 525.5472412109375, "y1": 218.03854370117188, "x1": 72.0, "y2": 259.90704345703125}, "imageText": ["[Jingle", "Bells,", "the", "song]", "[my", "cat,", "the", "cat]", "[I,", "I,", "my,", "me,", "I,", "me]", "Mention", "Linking", "Module", "Coreference", "Clusters", "I", "was", "hired", "to", "do", "some", "Christmas", "music,", "and", "it", "was", "just", "\u201cJingle", "Bells\u201d", "and", "I", "brought", "my", "cat", "with", "me", "to", "the", "studio,", "and", "I", "was", "working", "on", "the", "song", "and", "the", "cat", "jumped", "up", "into", "the", "record", "booth", "and", "started", "meowing", "along,", "meowing", "to", "me.", "I", "was", "hired", "to", "do", "some", "Christmas", "music,", "and", "it", "was", "just", "\u201cJingle", "Bells\u201d", "and", "I", "brought", "my", "cat", "with", "me", "to", "the", "studio,", "and", "I", "was", "working", "on", "the", "song", "and", "the", "cat", "jumped", "up", "into", "the", "record", "booth", "and", "started", "meowing", "along,", "meowing", "to", "me.", "I", "was", "hired", "to", "do", "some", "Christmas", "music,", "and", "it", "was", "just", "\u201cJingle", "Bells\u201d", "and", "I", "brought", "my", "cat", "with", "me", "to", "the", "studio,", "and", "I", "was", "working", "on", "the", "song", "and", "the", "cat", "jumped", "up", "into", "the", "record", "booth", "and", "started", "meowing", "along,", "meowing", "to", "me.", "And", "I", "was", "working", "on", "<mention>", "the", "song", "<\\mention>", "And", "I", "brought", "<mention>", "my", "cat", "<\\mention>with", "me", "to", "the", "studio", "<mention>", "I", "<\\mention>", "was", "hired", "to", "do", "some", "Christmas", "music", "Passage:", "Passage:", "Passage:", "Question:", "Question:", "Question:", "Mentions", "me", "\u2026", "the", "song", "\u2026", "my", "cat", "\u2026", "I", "Mention", "Proposal", "Module", "I", "was", "hired", "to", "do", "some", "Christmas", "music,", "and", "it", "was", "just", "\u201cJingle", "Bells\u201d", "and", "I", "brought", "my", "cat", "with", "me", "to", "the", "studio,", "and", "I", "was", "working", "on", "the", "song", "and", "the", "cat", "jumped", "up", "into", "the", "record", "booth", "and", "started", "meowing", "along,", "meowing", "to", "me.", "I", "was", "hired", "to", "do", "some", "Christmas", "music,", "and", "it", "was", "just", "\u201cJingle", "Bells\u201d", "and", "I", "brought", "my", "cat", "with", "me", "to", "the", "studio,", "and", "I", "was", "working", "on", "the", "song", "and", "the", "cat", "jumped", "up", "into", "the", "record", "booth", "and", "started", "meowing", "along,", "meowing", "to", "me."], "regionBoundary": {"x2": 523.0, "y1": 63.8900146484375, "x1": 73.0, "y2": 203.8900146484375}, "caption": "Figure 2: The overall architecture of our CorefQA model. The input passage is first fed into the Mention Proposal Module 3.3 to obtain candidate mentions. Then the Mention Linking Module 3.4 is used to extract coreferent mentions from the passage for each proposed mention. The coreference clusters are obtained using the scores produced in the above two stages.", "page": 2}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 525.5465698242188, "y1": 296.0065612792969, "x1": 307.2760009765625, "y2": 349.83001708984375}, "imageText": ["3", "Paula", "Zahn:", "[Thelma", "Gutierrez]", "went", "in-", "side", "the", "forensic", "laboratory", "where", "scientists", "are", "trying", "to", "solve", "this", "mystery.", "Thelma", "Gutierrez:", "In", "this", "laboratory", "alone", "[I]", "\u2019m", "surrounded", "by", "the", "remains", "of", "at", "least", "twenty", "different", "service", "members", "who", "are", "in", "the", "process", "of", "being", "identi\ufb01ed", "so", "that", "they", "too", "can", "go", "home.", "2", "[A", "traveling", "reporter]", "now", "on", "leave", "and", "joins", "us", "to", "tell", "[her]", "story.", "Thank", "[you]", "for", "coming", "in", "to", "share", "this", "with", "us.", "1", "[Freddie", "Mac]", "is", "giving", "golden", "parachutes", "to", "two", "of", "its", "ousted", "executives.", ".", ".", ".", "Yesterday", "Federal", "Prosecutions", "announced", "a", "criminal", "probe", "into", "[the", "company]."], "regionBoundary": {"x2": 526.0, "y1": 62.8900146484375, "x1": 307.0, "y2": 283.8900146484375}, "caption": "Table 4: Example mention clusters that were correctly predicted by our model, but wrongly predicted by c2fcoref + SpanBERT-large. Bold spans in brackets represent coreferent mentions. Italic spans represent the speaker\u2019s name of the utterance.", "page": 7}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 290.2706298828125, "y1": 237.60952758789062, "x1": 72.0, "y2": 255.5670166015625}, "imageText": ["Joshi", "et", "al.", "(2019a)", "(various", "\u03bb)", "Joshi", "et", "al.", "(2019a)", "(actual", "\u03bb)", "Our", "model", "(various", "\u03bb)", "Our", "model", "(actual", "\u03bb)", "%", ")", "al", "l(", "R", "ec", "tio", "n", "M", "en", "the", "number", "of", "spans", "\u03bb", "kept", "per", "word", "100", "90", "80", "70", "60", "50", "40", "10", "20", "30", "40", "50", "30"], "regionBoundary": {"x2": 277.0, "y1": 60.76582336425781, "x1": 79.7509765625, "y2": 219.7030029296875}, "caption": "Figure 4: Change of mention recalls as we increase the number of spans \u03bb kept per word.", "page": 7}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 525.5430297851562, "y1": 183.43753051757812, "x1": 72.0, "y2": 213.3509521484375}, "imageText": ["CorefQA", "+", "SpanBERT-base", "85.2", "87.4", "86.3", "78.7", "76.5", "77.6", "76.0", "75.6", "75.8", "79.9", "(+0.3)", "CorefQA", "+", "SpanBERT-large", "88.6", "87.4", "88.0", "82.4", "82.0", "82.2", "79.9", "78.3", "79.1", "83.1", "(+3.5)", "e2e-coref(Lee", "et", "al.,", "2017)", "78.4", "73.4", "75.8", "68.6", "61.8", "65.0", "62.7", "59.0", "60.8", "67.2", "c2f-coref", "+", "ELMo", "(Lee", "et", "al.,", "2018)", "81.4", "79.5", "80.4", "72.2", "69.5", "70.8", "68.2", "67.1", "67.6", "73.0", "EE", "+", "BERT-large", "(Kantor", "and", "Globerson,", "2019)", "82.6", "84.1", "83.4", "73.3", "76.2", "74.7", "72.4", "71.1", "71.8", "76.6", "c2f-coref", "+", "BERT-large", "(Joshi", "et", "al.,", "2019b)", "84.7", "82.4", "83.5", "76.5", "74.0", "75.3", "74.1", "69.8", "71.9", "76.9", "c2f-coref", "+", "SpanBERT-large", "(Joshi", "et", "al.,", "2019a)", "85.8", "84.8", "85.3", "78.3", "77.9", "78.1", "76.4", "74.2", "75.3", "79.6", "MUC", "B3", "CEAF\u03c64", "P", "R", "F1", "P", "R", "F1", "P", "R", "F1", "Avg.", "F1"], "regionBoundary": {"x2": 523.0, "y1": 67.59719848632812, "x1": 75.0, "y2": 170.8900146484375}, "caption": "Table 1: Evaluation results on the English CoNLL-2012 shared task. The average F1 of MUC, B3, and CEAF\u03c64 is the main evaluation metric. Ensemble models are not included in the table for a fair comparison. P , R and F1 in the first row represent precision, recall and F1 score respectively.", "page": 6}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 290.2705993652344, "y1": 326.5985412597656, "x1": 71.99999237060547, "y2": 368.46697998046875}, "imageText": ["c2f-coref", "+", "SpanBERT-large", "88.8", "84.9", "0.96", "86.8", "CorefQA", "+", "SpanBERT-large", "88.9", "86.1", "0.97", "87.5", "e2e-coref", "67.2", "62.2", "0.92", "64.7", "c2f-coref", "+", "ELMo", "75.8", "71.1", "0.94", "73.5", "c2f-coref", "+", "BERT-large", "86.9", "83.0", "0.95", "85.0", "Model", "M", "F", "B", "O"], "regionBoundary": {"x2": 285.0, "y1": 236.8900146484375, "x1": 78.0, "y2": 313.8900146484375}, "caption": "Table 2: CorefQA achieves the state-of-the-art performance on all metrics including F1 scores on Masculine and Feminine examples, a Bias factor (F / M) and the Overall F1 score.", "page": 6}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 525.5465698242188, "y1": 434.487548828125, "x1": 307.2760009765625, "y2": 524.1759033203125}, "imageText": ["F1(Speaker", "as", "feature)", "F1(Speaker", "as", "input)", "Frequency", "%", "Number", "of", "speakers", "per", "document", "100", "90", "80", "70", "60", "50", "40", "30", "20", "10", "1", "2", "3", "4", "5", "6", "7+"], "regionBoundary": {"x2": 512.0, "y1": 236.8900146484375, "x1": 315.76995849609375, "y2": 416.58203125}, "caption": "Figure 3: Performance on the development set of the CoNLL-2012 dataset with various number of speakers. F1(Speaker as feature): F1 score for the strategy that treats speaker information as a mention-pair feature. F1(Speaker as input): F1 score for our strategy that treats speaker names as token input. Frequency: percentage of documents with specific number of speakers.", "page": 6}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 290.2705993652344, "y1": 503.70654296875, "x1": 72.0, "y2": 557.5289306640625}, "imageText": ["CorefQA", "83.4", "\u2212\u2013", "SpanBERT", "79.6", "-3.8", "\u2212\u2013", "Mention", "Proposal", "Pre-train", "75.9", "-7.5", "\u2212\u2013", "Question", "Answering", "75.0", "-8.4", "\u2212\u2013", "Quoref", "Pre-train", "82.7", "-0.7", "\u2212\u2013", "SQuAD", "Pre-train", "83.1", "-0.3", "Avg.", "F1", "\u2206"], "regionBoundary": {"x2": 289.0, "y1": 383.8900146484375, "x1": 74.0, "y2": 491.8900146484375}, "caption": "Table 3: Ablation studies on the CoNLL-2012 development set. SpanBERT token representations, the mention-proposal pre-training, and the question answering pre-training all contribute significantly to the good performance of the full model.", "page": 6}, {"figType": "Figure", "name": "1", "captionBoundary": {"x2": 525.5465698242188, "y1": 533.841552734375, "x1": 307.2760009765625, "y2": 575.708984375}, "imageText": ["Original", "Passage", "In", "addition", ",", "many", "people", "were", "poisoned", "when", "toxic", "gas", "was", "released.", "They", "were", "poi-", "soned", "and", "did", "not", "know", "how", "to", "protect", "them-", "selves", "against", "the", "poison.", "Our", "formulation", "Q1:", "Who", "were", "poisoned", "when", "toxic", "gas", "was", "released?", "A1:", "[They,", "themselves]", "Q2:", "What", "was", "released", "when", "many", "people", "were", "poisoned?", "A2:", "[the", "poison]", "Q3:", "Who", "were", "poisoned", "and", "did", "not", "know", "how", "to", "protect", "themselves", "against", "the", "poison?", "A3:", "[many", "people,", "themselves]", "Q4:", "Whom", "did", "they", "not", "know", "how", "to", "protect", "against", "the", "poison?", "A4:", "[many", "people,", "They]", "Q5:", "They", "were", "poisoned", "and", "did", "not", "know", "how", "to", "protect", "themselves", "against", "what?", "A5:", "[toxic", "gas]"], "regionBoundary": {"x2": 524.0, "y1": 221.8900146484375, "x1": 309.0, "y2": 521.8900146484375}, "caption": "Figure 1: An illustration of the paradigm shift from coreference resolution to query-based span prediction. Spans with the same format represent coreferent mentions.", "page": 0}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 729.925791422526, "y1": 741.4466010199652, "x1": 426.772223578559, "y2": 799.5958116319445}, "name": "1", "caption_text": "Figure 1: An illustration of the paradigm shift from coreference resolution to query-based span prediction. Spans with the same format represent coreferent mentions.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 728.0, "y1": 308.0, "x1": 429.0, "y2": 725.0}, "page": 0, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9267239040798, "y1": 302.831310696072, "x1": 100.0, "y2": 360.98200480143225}, "name": "2", "caption_text": "Figure 2: The overall architecture of our CorefQA model. The input passage is first fed into the Mention Proposal Module 3.3 to obtain candidate mentions. Then the Mention Linking Module 3.4 is used to extract coreferent mentions from the passage for each proposed mention. The coreference clusters are obtained using the scores produced in the above two stages.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 727.0, "y1": 89.0, "x1": 101.0, "y2": 283.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9208747016058, "y1": 254.77434794108072, "x1": 100.0, "y2": 296.32076687282984}, "name": "1", "caption_text": "Table 1: Evaluation results on the English CoNLL-2012 shared task. The average F1 of MUC, B3, and CEAF\u03c64 is the main evaluation metric. Ensemble models are not included in the table for a fair comparison. P , R and F1 in the first row represent precision, recall and F1 score respectively.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 726.0, "y1": 86.0, "x1": 100.0, "y2": 254.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.925791422526, "y1": 603.4549289279514, "x1": 426.772223578559, "y2": 728.0220879448784}, "name": "3", "caption_text": "Figure 3: Performance on the development set of the CoNLL-2012 dataset with various number of speakers. F1(Speaker as feature): F1 score for the strategy that treats speaker information as a mention-pair feature. F1(Speaker as input): F1 score for our strategy that treats speaker names as token input. Frequency: percentage of documents with specific number of speakers.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 711.0, "y1": 328.0, "x1": 432.0, "y2": 581.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.1536102294922, "y1": 453.6090850830078, "x1": 99.9999894036187, "y2": 511.7596944173177}, "name": "2", "caption_text": "Table 2: CorefQA achieves the state-of-the-art performance on all metrics including F1 scores on Masculine and Feminine examples, a Bias factor (F / M) and the Overall F1 score.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 395.0, "y1": 328.0, "x1": 100.0, "y2": 454.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.1536102294922, "y1": 699.5924207899305, "x1": 100.0, "y2": 774.3457370334202}, "name": "3", "caption_text": "Table 3: Ablation studies on the CoNLL-2012 development set. SpanBERT token representations, the mention-proposal pre-training, and the question answering pre-training all contribute significantly to the good performance of the full model.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 401.0, "y1": 533.0, "x1": 103.0, "y2": 683.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.15365261501734, "y1": 330.01323276095917, "x1": 100.0, "y2": 354.95418972439234}, "name": "4", "caption_text": "Figure 4: Change of mention recalls as we increase the number of spans \u03bb kept per word.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 385.0, "y1": 81.0, "x1": 105.0, "y2": 308.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.925791422526, "y1": 411.12022399902344, "x1": 426.772223578559, "y2": 485.8750237358941}, "name": "4", "caption_text": "Table 4: Example mention clusters that were correctly predicted by our model, but wrongly predicted by c2fcoref + SpanBERT-large. Bold spans in brackets represent coreferent mentions. Italic spans represent the speaker\u2019s name of the utterance.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 730.0, "y1": 86.0, "x1": 426.0, "y2": 388.0}, "page": 7, "dpi": 0}], "error": null, "pdf": "/work/host-output/e57c2d444171480ebe4b9f37cfe6b8efcc1dfde1/2020.acl-main.622.pdf", "dpi": 100}