{"raw_detected_boxes": [[{"x2": 725.0, "y1": 570.0, "x1": 429.0, "y2": 671.0}], [], [{"x2": 708.0, "y1": 86.0, "x1": 449.0, "y2": 182.0}, {"x2": 708.0, "y1": 247.0, "x1": 443.0, "y2": 375.0}, {"x2": 363.0, "y1": 445.0, "x1": 127.0, "y2": 559.0}, {"x2": 364.0, "y1": 336.0, "x1": 138.0, "y2": 398.0}], [{"x2": 697.0, "y1": 88.0, "x1": 134.0, "y2": 320.0}], [{"x2": 710.0, "y1": 94.0, "x1": 447.0, "y2": 165.0}, {"x2": 731.0, "y1": 260.0, "x1": 431.0, "y2": 646.0}, {"x2": 710.0, "y1": 909.0, "x1": 448.0, "y2": 1004.0}], [{"x2": 724.0, "y1": 456.0, "x1": 427.0, "y2": 614.0}], [{"x2": 712.0, "y1": 87.0, "x1": 118.0, "y2": 217.0}, {"x2": 682.0, "y1": 426.0, "x1": 476.0, "y2": 523.0}, {"x2": 690.0, "y1": 605.0, "x1": 468.0, "y2": 768.0}, {"x2": 690.0, "y1": 851.0, "x1": 467.0, "y2": 1013.0}, {"x2": 360.0, "y1": 805.0, "x1": 143.0, "y2": 900.0}], [{"x2": 395.0, "y1": 345.0, "x1": 104.0, "y2": 426.0}, {"x2": 389.0, "y1": 575.0, "x1": 112.0, "y2": 627.0}, {"x2": 724.0, "y1": 496.0, "x1": 432.0, "y2": 727.0}], [{"x2": 712.0, "y1": 86.0, "x1": 118.0, "y2": 208.0}, {"x2": 717.0, "y1": 276.0, "x1": 107.0, "y2": 405.0}], []], "raw_pdffigures_output": {"regionless-captions": [{"figType": "Figure", "boundary": {"x2": 290.27056884765625, "y1": 415.4975891113281, "x1": 71.99998474121094, "y2": 433.4560546875}, "text": "Figure 2: Distribution of implicit sentiment (IS) and explicit sentiment (ES).", "name": "2", "page": 2}], "figures": [{"figType": "Figure", "name": "1", "captionBoundary": {"x2": 525.546630859375, "y1": 496.0515441894531, "x1": 307.2760009765625, "y2": 514.009033203125}, "imageText": ["\"This", "is", "so", "good,", "that", "I", "am", "gonna", "enjoy", "it", "in", "the", "balcony.", "I", "can", "enjoy", "my", "view,", "whilst", "I", "enjoy", "my", "desert.\""], "regionBoundary": {"x2": 525.0, "y1": 405.8900146484375, "x1": 308.0, "y2": 482.8900146484375}, "caption": "Figure 1: Example to show that sentiment and emotion of the speaker can influence sarcasm detection", "page": 0}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 476.6187438964844, "y1": 453.9395446777344, "x1": 355.8919982910156, "y2": 459.9420166015625}, "imageText": ["Parameters", "Speaker", "Dependent", "Speaker", "Independent", "Bi-GRU", "2\u00d7200", "neurons,", "dropout=0.3", "Dense", "layer", "200", "neurons,", "dropout=0.3", "Activations", "ReLu", "Optimizer", "Adam", "(lr=0.001)", "Output", "Softmax", "(Sent)", "&", "Sigmoid", "(Emo)", "Loss", "Categorical", "cross-entropy", "(Sent)", "Binary", "cross-entropy", "(Emo)", "Batch", "32", "Epochs", "200", "#Segments", "(k)", "50", "25"], "regionBoundary": {"x2": 526.0, "y1": 323.8900146484375, "x1": 307.0, "y2": 441.8900146484375}, "caption": "Table 3: Model configurations", "page": 5}, {"figType": "Table", "name": "6", "captionBoundary": {"x2": 527.2001953125, "y1": 388.9135437011719, "x1": 306.9670104980469, "y2": 406.8710021972656}, "imageText": ["P", "R", "F1", "P", "R", "F1", "81.77", "88.29", "83.88", "83.64", "88.35", "84.37", "80.66", "88.51", "83.57", "85.01", "88.90", "85.12", "Speaker", "Independent", "Speaker", "Dependent", "Implicit", "Sentiment", "Explicit", "Sentiment", "P", "R", "F1", "P", "R", "F1"], "regionBoundary": {"x2": 494.0, "y1": 306.8900146484375, "x1": 338.0, "y2": 376.8900146484375}, "caption": "Table 6: Results for Single-task experiments for Emotion analysis (T+A+V).", "page": 6}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 471.916015625, "y1": 169.71456909179688, "x1": 125.31900024414062, "y2": 175.717041015625}, "imageText": ["Sar", "+", "Sent", "+", "Emo", "65.48", "65.48", "65.67", "59.13", "59.98", "50.27", "65.59", "63.76", "63.90", "69.53", "66.01", "65.90", "STL", "Sar", "60.11", "60.18", "60.16", "58.23", "57.69", "57.91", "60.44", "60.96", "60.52", "65.98", "65.45", "65.60", "MTL", "Sar", "+", "Sent", "62.74", "62.92", "62.81", "59.25", "59.55", "52.89", "61.60", "60.95", "61.14", "66.97", "63.76", "63.68", "Sar", "+", "Emo", "65.11", "65.16", "65.13", "59.59", "59.55", "59.58", "63.19", "63.76", "62.91", "66.35", "65.44", "65.63", "Speaker", "Independent", "Sar", "+", "Sent", "+", "Emo", "72.76", "71.88", "71.61", "62.23", "61.15", "59.61", "72.73", "71.88", "71.81", "73.40", "72.75", "72.57", "MTL", "Sar", "+", "Sent", "69.65", "69.42", "69.33", "64.09", "60.72", "58.21", "72.20", "71.45", "71.18", "72.52", "71.73", "72.07", "Sar", "+", "Emo", "71.76", "70.86", "70.54", "65.76", "65.65", "65.60", "72.60", "71.59", "71.25", "72.76", "71.88", "72.11", "STL", "Sar", "71.52", "70.61", "69.32", "64.20", "64.20", "63.88", "71.90", "71.01", "70.64", "72.08", "71.62", "72.01", "Speaker", "Dependent", "T", "+", "V", "T", "+", "A", "A", "+", "V", "T", "+", "A", "+", "V", "Labels", "P", "R", "F1", "P", "R", "F1", "P", "R", "F1", "P", "R", "F1"], "regionBoundary": {"x2": 513.0, "y1": 63.8900146484375, "x1": 85.0, "y2": 157.8900146484375}, "caption": "Table 4: Single Task vs Multi Task: Without Context and Without Speaker information.", "page": 6}, {"figType": "Table", "name": "7", "captionBoundary": {"x2": 527.2001953125, "y1": 565.1676025390625, "x1": 306.9670104980469, "y2": 583.125}, "imageText": ["An", "74.0", "85.9", "79.5", "85.0", "92.2", "88.4", "Ex", "94.9", "97.3", "96.1", "91.5", "95.6", "93.5", "Fr", "95.9", "97.8", "96.9", "98.3", "99.1", "98.7", "Sd", "68.0", "82.3", "74.5", "72.1", "83.0", "75.5", "Sp", "91.8", "95.8", "93.7", "90.1", "94.9", "92.5", "Fs", "84.2", "91.7", "87.8", "93.4", "96.7", "95.0", "Hp", "67.1", "79.5", "71.4", "66.6", "71.7", "66.5", "Neu", "60.9", "71.6", "60.5", "70.9", "68.3", "58.1", "Dg", "89.0", "94.3", "91.6", "97.1", "98.5", "97.8", "Setup", "Implicit", "Emotion", "Explicit", "Emotion", "P", "R", "F1", "P", "R", "F1", "Speaker", "Dependent"], "regionBoundary": {"x2": 497.0, "y1": 435.8900146484375, "x1": 336.0, "y2": 552.8900146484375}, "caption": "Table 7: Emotion-wise results for Single-Task experiments - Speaker Dependent setup.", "page": 6}, {"figType": "Table", "name": "8", "captionBoundary": {"x2": 527.2001953125, "y1": 741.4215698242188, "x1": 306.9670104980469, "y2": 759.3790283203125}, "imageText": ["An", "72.0", "84.8", "77.9", "81.3", "90.1", "85.5", "Ex", "95.6", "97.7", "96.6", "94.5", "97.2", "95.8", "Fr", "95.0", "97.5", "96.2", "97.8", "98.9", "98.3", "Sd", "74.8", "86.5", "80.3", "72.9", "85.4", "78.7", "Sp", "92.8", "96.3", "94.5", "93.4", "96.6", "94.9", "Fs", "88.5", "94.1", "91.2", "91.7", "95.8", "93.7", "Hp", "65.6", "71.6", "60.8", "67.4", "62.9", "49.6", "Neu", "50.9", "71.3", "59.4", "52.9", "72.7", "61.3", "Dg", "94.5", "97.2", "95.8", "96.1", "98.0", "97.0", "Setup", "Implicit", "Emotion", "Explicit", "Emotion", "P", "R", "F1", "P", "R", "F1", "Speaker", "Independent"], "regionBoundary": {"x2": 497.0, "y1": 611.8900146484375, "x1": 336.0, "y2": 729.8900146484375}, "caption": "Table 8: Emotion-wise results for Single-Task experiments - Speaker Independent setup.", "page": 6}, {"figType": "Table", "name": "5", "captionBoundary": {"x2": 291.9242248535156, "y1": 660.28857421875, "x1": 71.69100189208984, "y2": 678.2460327148438}, "imageText": ["P", "R", "F1", "P", "R", "F1", "47.05", "49.15", "40.99", "47.73", "50.0", "45.24", "49.27", "57.39", "49.12", "48.32", "52.46", "48.11", "Speaker", "Independent", "Speaker", "Dependent", "Implicit", "Sentiment", "Explicit", "Sentiment", "P", "R", "F1", "P", "R", "F1"], "regionBoundary": {"x2": 259.0, "y1": 577.8900146484375, "x1": 103.0, "y2": 647.8900146484375}, "caption": "Table 5: Results for Single-task experiments for Sentiment analysis (T+A+V).", "page": 6}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 244.1576690673828, "y1": 284.8895568847656, "x1": 117.802001953125, "y2": 290.89202880859375}, "imageText": ["Implicit", "Sentiment", "Explicit", "Sentiment", "Neg", "Neu", "Pos", "Neg", "Neu", "Pos", "391", "89", "210", "246", "119", "325"], "regionBoundary": {"x2": 263.0, "y1": 235.8900146484375, "x1": 99.0, "y2": 272.8900146484375}, "caption": "Table 1: Sentiment distribution.", "page": 2}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 476.3894348144531, "y1": 143.21859741210938, "x1": 356.1210021972656, "y2": 149.2210693359375}, "imageText": ["Explicit", "Emotion", "An", "Ex", "Fr", "Sd", "Sp", "Fs", "Hp", "Neu", "Dg", "54", "30", "6", "118", "35", "23", "206", "228", "10", "Implicit", "Emotion", "An", "Ex", "Fr", "Sd", "Sp", "Fs", "Hp", "Neu", "Dg", "97", "18", "14", "121", "29", "57", "143", "198", "39"], "regionBoundary": {"x2": 510.0, "y1": 62.8900146484375, "x1": 323.0, "y2": 130.8900146484375}, "caption": "Table 2: Emotion distribution.", "page": 2}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 527.2001953125, "y1": 287.2826232910156, "x1": 307.2760009765625, "y2": 305.2401123046875}, "imageText": ["(b)", "Show-wise-EE.", "An", "ge", "r", "Ex", "cit", "ed", "Fe", "ar", "Sa", "d", "Su", "rpr", "ise", "Fru", "str", "ati", "on", "Ha", "pp", "y", "Ne", "utr", "al", "Dis", "gu", "st", "250", "200", "150", "100", "50", "0", "(a)", "Show-wise-IE.", "An", "ge", "r", "Ex", "cit", "ed", "Fe", "ar", "Sa", "d", "Su", "rpr", "ise", "Fru", "str", "ati", "on", "Ha", "pp", "y", "Ne", "utr", "al", "Dis", "gu", "st", "250", "200", "150", "100", "50", "P", "E", "N", "N", "Y", "A", "M", "Y", "LE", "O", "N", "A", "R", "D", "S", "H", "E", "LD", "O", "N", "B", "E", "R", "N", "A", "D", "E", "T", "H", "O", "W", "A", "R", "D", "R", "A", "J", "O", "TH", "E", "R", "S0", "100", "75", "50", "25", "0", "U", "tte", "ra", "nc", "es", "Negative", "Neutral", "Positive", "200", "150", "100", "50", "0"], "regionBoundary": {"x2": 510.0, "y1": 191.0771026611328, "x1": 321.1832275390625, "y2": 281.352783203125}, "caption": "Figure 3: Distribution of implicit emotion (IE) and explicit emotion (EE).", "page": 2}, {"figType": "Table", "name": "10", "captionBoundary": {"x2": 291.9241638183594, "y1": 463.9725341796875, "x1": 71.69100189208984, "y2": 481.92999267578125}, "imageText": ["Setup", "Speaker", "Dependent", "Speaker", "Independent", "P", "R", "F1", "P", "R", "F1", "W/o", "Attention", "71.53", "69.71", "69.02", "60.53", "61.23", "60.44", "Proposed", "73.40", "72.75", "72.57", "69.53", "66.01", "65.90"], "regionBoundary": {"x2": 282.0, "y1": 413.8900146484375, "x1": 80.0, "y2": 451.8900146484375}, "caption": "Table 10: Ablation study: Proposed Attention v/s Without Attention.", "page": 7}, {"figType": "Figure", "name": "7", "captionBoundary": {"x2": 527.2003173828125, "y1": 542.185546875, "x1": 307.2759704589844, "y2": 561.636962890625}, "imageText": ["(f)", "AT.", "s", "1", "s", "2", "s", "3", "s", "4", "s", "5", "s", "6", "s", "7", "s", "8", "s", "9", "s", "1", "0", "s", "1", "1", "s", "1", "2", "(e)", "VA.", "s", "1", "s", "2", "s", "3", "s", "4", "s", "5", "s", "6", "s", "7", "s", "8", "s", "9", "s", "1", "0", "s", "1", "1", "s", "1", "2", "(d)", "VT.", "s", "1", "s", "2", "s", "3", "s", "4", "s", "5", "s", "6", "s", "7", "s", "8", "s", "9", "s", "1", "0", "s", "1", "1", "s", "1", "2", "(c)", "TA.", "s", "1", "s", "2", "s", "3", "s", "4", "s", "5", "s", "6", "s", "7", "s", "8", "s", "9", "s", "1", "0", "s", "1", "1", "s", "1", "2", "(b)", "AV.", "s", "1", "s", "2", "s", "3", "s", "4", "s", "5", "s", "6", "s", "7", "s", "8", "s", "9", "s", "1", "0", "s", "1", "1", "s", "1", "2", "(a)", "TV.", "s1", "s2", "s3", "s4", "s5", "s6", "s7", "s8", "s9", "s10", "s11", "s12", "s", "1", "0", "s", "1", "1", "s", "1", "2", "s", "1", "s", "2", "s", "3", "s", "4", "s", "5", "s", "6", "s", "7", "s", "8", "s", "9"], "regionBoundary": {"x2": 524.0, "y1": 356.8900146484375, "x1": 311.0, "y2": 521.885986328125}, "caption": "Figure 7: Heatmaps for the all combinations of modalities for Ie-Attention.", "page": 7}, {"figType": "Table", "name": "9", "captionBoundary": {"x2": 291.9241943359375, "y1": 318.7595520019531, "x1": 71.69100189208984, "y2": 336.718017578125}, "imageText": ["7", "7", "73.40", "72.75", "72.57", "69.53", "66.01", "65.90", "7", "3", "77.09", "76.67", "76.57", "74.69", "74.43", "74.51", "3", "7", "72.34", "71.88", "71.74", "71.51", "71.35", "70.46", "3", "3", "76.07", "75.79", "75.72", "74.88", "75.01", "74.72", "Setups", "Speaker", "Dependent", "Speaker", "Independent", "Context", "Speaker", "P", "R", "F1", "P", "R", "F1"], "regionBoundary": {"x2": 289.0, "y1": 248.8900146484375, "x1": 74.0, "y2": 306.8900146484375}, "caption": "Table 9: Results for different combination of ContextSpeaker Experiments", "page": 7}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 477.143798828125, "y1": 244.06552124023438, "x1": 120.40299987792969, "y2": 250.0679931640625}, "imageText": [], "regionBoundary": {"x2": 502.0, "y1": 62.8900146484375, "x1": 96.0, "y2": 230.8900146484375}, "caption": "Figure 4: Overall architecture of the proposed multi-modal sarcasm detection framework.", "page": 3}, {"figType": "Table", "name": "11", "captionBoundary": {"x2": 525.5472412109375, "y1": 161.89755249023438, "x1": 71.69100189208984, "y2": 179.85504150390625}, "imageText": ["T-test", "-", "-", "-", "-", "-", "-", "-", "-", "-", "0.0002", "0.0006", "0.0012", "Baseline", "62.2", "61.5", "61.7", "64.7", "62.9", "63.1", "64.1", "61.8", "61.9", "64.3", "62.6", "62.8", "Proposed", "Model", "65.5", "65.5", "65.7", "59.1", "60.0", "50.3", "65.6", "63.8", "63.9", "69.5", "66.0", "65.9", "Speaker", "Independent", "T-test", "-", "-", "-", "-", "-", "-", "-", "-", "-", "0.0023", "0.0098", "0.0056", "Baseline", "72.0", "71.6", "71.6", "66.6", "66.2", "66.2", "66.2", "65.7", "65.7", "71.9", "71.4", "71.5", "Proposed", "Model", "72.8", "71.9", "71.6", "62.2", "61.2", "59.6", "72.7", "71.9", "71.8", "73.4", "72.8", "72.6", "Speaker", "Dependent", "Setup", "Model", "T", "+", "V", "T", "+", "A", "A", "+", "V", "T", "+", "A", "+", "V", "P", "R", "F1", "P", "R", "F1", "P", "R", "F1", "P", "R", "F1"], "regionBoundary": {"x2": 513.0, "y1": 63.8900146484375, "x1": 85.0, "y2": 149.8900146484375}, "caption": "Table 11: Comparative Analysis of the proposed approach with recent state-of-the-art systems. We evaluated on extended, publicly available MUSTARD dataset (Castro et al., 2019). Significance test p-values< 0.05", "page": 8}, {"figType": "Figure", "name": "8", "captionBoundary": {"x2": 283.5516662597656, "y1": 497.0135498046875, "x1": 192.6699981689453, "y2": 504.510009765625}, "imageText": ["s10", "s11", "s12", "s1", "s2", "s3", "s4", "s5", "s6", "s7", "s8", "s9", "s", "1", "s", "2", "s", "3", "s", "4", "s", "5", "s", "6", "s", "7", "s", "8", "s", "9", "s", "1", "0", "s", "1", "1", "s", "1", "2"], "regionBoundary": {"x2": 289.0, "y1": 390.8900146484375, "x1": 187.90574645996094, "y2": 482.0932312011719}, "caption": "Figure 8: Ia-Attention.", "page": 8}, {"figType": "Table", "name": "12", "captionBoundary": {"x2": 525.547119140625, "y1": 303.6975402832031, "x1": 71.69100189208984, "y2": 321.655029296875}, "imageText": ["3", "Now?!", "-", "No,", "after", "my", "tongue", "has", "swollen", "to", "the", "size", "of", "a", "brisket!", "S", "S", "S", "4", "There\u2019s", "no", "hurry.", "Tell", "them", "more", "about", "their", "secret", "love", "for", "each", "other.", "NS", "S", "NS", "5", "I\u2019m", "not", "saying", "that", "you\u2019re", "not", "fun.", "You\u2019re", "the", "most", "fun", "person", "I", "know.", "NS", "S", "NS", "6", "Well,", "I\u2019m", "sorry,", "too,", "but", "there\u2019s", "just", "no", "room", "for", "you", "in", "my", "wallet.", "S", "S", "S", "you\u2019re", "in", "there", "spending", "an", "hour", "on", "your", "hair.", "S", "NS", "S", "1", "Oh", "yeah", "ok,", "including", "the", "waf\ufb02es", "last", "week,", "you", "now", "owe", "me,", "seventeen", "zillion", "dollars.", "S", "NS", "S", "2", "I", "love", "that", "you", "take", "pride", "in", "your", "looks,", "even", "when", "I", "have", "to", "pee", "in", "the", "morning,", "and", "Sarcasm", "(T+A+V)", "Utterances", "Actual", "STL", "MTL"], "regionBoundary": {"x2": 524.0, "y1": 194.8900146484375, "x1": 74.0, "y2": 291.8900146484375}, "caption": "Table 12: Comparison between multi-task learning (Sar + Sent + Emo) and single-task learning (Sar) frameworks for tri-modal (T+A+V) inputs. Few error cases where MTL framework performs better than the STL framework.", "page": 8}, {"figType": "Figure", "name": "5", "captionBoundary": {"x2": 525.543212890625, "y1": 138.80154418945312, "x1": 307.2759704589844, "y2": 156.759033203125}, "imageText": [], "regionBoundary": {"x2": 511.0, "y1": 63.8900146484375, "x1": 322.0, "y2": 125.8900146484375}, "caption": "Figure 5: Procedure of the proposed Ie-Attention Mechanism.", "page": 4}, {"figType": "Figure", "name": "6", "captionBoundary": {"x2": 525.5432739257812, "y1": 736.4405517578125, "x1": 307.2760009765625, "y2": 754.3980102539062}, "imageText": [], "regionBoundary": {"x2": 511.0, "y1": 650.8900146484375, "x1": 322.0, "y2": 722.8900146484375}, "caption": "Figure 6: Procedure of the proposed Ia-Attention mechanism", "page": 4}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 729.9258761935764, "y1": 688.9604780409071, "x1": 426.772223578559, "y2": 713.9014350043402}, "name": "1", "caption_text": "Figure 1: Example to show that sentiment and emotion of the speaker can influence sarcasm detection", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 729.0, "y1": 564.0, "x1": 427.0, "y2": 688.0}, "page": 0, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 661.6519927978516, "y1": 198.9147186279297, "x1": 494.6125030517578, "y2": 207.25148518880206}, "name": "2", "caption_text": "Table 2: Emotion distribution.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 708.0, "y1": 86.0, "x1": 449.0, "y2": 199.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2224934895833, "y1": 399.0036434597439, "x1": 426.772223578559, "y2": 423.94460042317706}, "name": "3", "caption_text": "Figure 3: Distribution of implicit emotion (IE) and explicit emotion (EE).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 708.0, "y1": 247.0, "x1": 440.0, "y2": 380.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.153567843967, "y1": 577.0799848768446, "x1": 99.99997880723741, "y2": 602.0222981770833}, "name": "2", "caption_text": "Figure 2: Distribution of implicit sentiment (IS) and explicit sentiment (ES).", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 374.0, "y1": 441.0, "x1": 110.0, "y2": 576.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 339.10787370469836, "y1": 395.67994011773004, "x1": 163.6138916015625, "y2": 404.0167066786024}, "name": "1", "caption_text": "Table 1: Sentiment distribution.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 366.0, "y1": 327.0, "x1": 138.0, "y2": 405.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 662.699720594618, "y1": 338.97989061143664, "x1": 167.22638871934677, "y2": 347.316657172309}, "name": "4", "caption_text": "Figure 4: Overall architecture of the proposed multi-modal sarcasm detection framework.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 697.0, "y1": 87.0, "x1": 133.0, "y2": 320.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.921129014757, "y1": 192.77992248535156, "x1": 426.7721811930338, "y2": 217.72087944878473}, "name": "5", "caption_text": "Figure 5: Procedure of the proposed Ie-Attention Mechanism.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 710.0, "y1": 88.0, "x1": 447.0, "y2": 174.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9212137858073, "y1": 1022.8340996636284, "x1": 426.772223578559, "y2": 1047.7750142415364}, "name": "6", "caption_text": "Figure 6: Procedure of the proposed Ia-Attention mechanism", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 710.0, "y1": 904.0, "x1": 448.0, "y2": 1004.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 661.9704776340061, "y1": 630.4715898301866, "x1": 494.29444207085504, "y2": 638.808356391059}, "name": "3", "caption_text": "Table 3: Model configurations", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 730.0, "y1": 449.0, "x1": 427.0, "y2": 631.0}, "page": 5, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 655.4389105902777, "y1": 235.71467929416232, "x1": 174.05416700575086, "y2": 244.0514458550347}, "name": "4", "caption_text": "Table 4: Single Task vs Multi Task: Without Context and Without Speaker information.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 712.0, "y1": 87.0, "x1": 118.0, "y2": 219.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2224934895833, "y1": 540.1576995849609, "x1": 426.3430701361762, "y2": 565.0986141628689}, "name": "6", "caption_text": "Table 6: Results for Single-task experiments for Emotion analysis (T+A+V).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 687.0, "y1": 426.0, "x1": 463.0, "y2": 540.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2224934895833, "y1": 784.9550035264757, "x1": 426.3430701361762, "y2": 809.8958333333333}, "name": "7", "caption_text": "Table 7: Emotion-wise results for Single-Task experiments - Speaker Dependent setup.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 690.0, "y1": 605.0, "x1": 461.0, "y2": 785.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2224934895833, "y1": 1029.7521803114148, "x1": 426.3430701361762, "y2": 1054.693094889323}, "name": "8", "caption_text": "Table 8: Emotion-wise results for Single-Task experiments - Speaker Independent setup.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 690.0, "y1": 850.0, "x1": 462.0, "y2": 1030.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.45031229654944, "y1": 917.0674641927083, "x1": 99.57083596123589, "y2": 942.0083787706163}, "name": "5", "caption_text": "Table 5: Results for Single-task experiments for Sentiment analysis (T+A+V).", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 377.0, "y1": 803.0, "x1": 136.0, "y2": 917.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.4502699110243, "y1": 442.72160000271265, "x1": 99.57083596123589, "y2": 467.66391330295136}, "name": "9", "caption_text": "Table 9: Results for different combination of ContextSpeaker Experiments", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 401.0, "y1": 328.0, "x1": 100.0, "y2": 443.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.45022752549914, "y1": 644.4062974717882, "x1": 99.57083596123589, "y2": 669.3472120496962}, "name": "10", "caption_text": "Table 10: Ablation study: Proposed Attention v/s Without Attention.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 397.0, "y1": 558.0, "x1": 100.0, "y2": 644.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.222663031684, "y1": 753.0354817708333, "x1": 426.7721811930338, "y2": 780.0513373480902}, "name": "7", "caption_text": "Figure 7: Heatmaps for the all combinations of modalities for Ie-Attention.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 728.0, "y1": 496.0, "x1": 432.0, "y2": 728.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9267239040798, "y1": 224.8577117919922, "x1": 99.57083596123589, "y2": 249.79866875542533}, "name": "11", "caption_text": "Table 11: Comparative Analysis of the proposed approach with recent state-of-the-art systems. We evaluated on extended, publicly available MUSTARD dataset (Castro et al., 2019). Significance test p-values< 0.05", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 712.0, "y1": 86.0, "x1": 101.0, "y2": 225.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9265543619791, "y1": 421.80213928222656, "x1": 99.57083596123589, "y2": 446.74309624565973}, "name": "12", "caption_text": "Table 12: Comparison between multi-task learning (Sar + Sent + Emo) and single-task learning (Sar) frameworks for tri-modal (T+A+V) inputs. Few error cases where MTL framework performs better than the STL framework.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 727.0, "y1": 271.0, "x1": 100.0, "y2": 422.0}, "page": 8, "dpi": 0}], "error": null, "pdf": "/work/host-output/958d541ddccce914fa1436f0a9d107e39eed03ac/2020.acl-main.401.pdf", "dpi": 100}