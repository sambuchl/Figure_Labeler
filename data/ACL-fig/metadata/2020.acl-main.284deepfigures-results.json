{"raw_detected_boxes": [[{"x2": 719.0, "y1": 309.0, "x1": 437.0, "y2": 654.0}], [], [{"x2": 716.0, "y1": 572.0, "x1": 441.0, "y2": 697.0}], [{"x2": 695.0, "y1": 90.0, "x1": 135.0, "y2": 202.0}, {"x2": 401.0, "y1": 296.0, "x1": 103.0, "y2": 446.0}], [{"x2": 389.0, "y1": 92.0, "x1": 114.0, "y2": 509.0}], [], [], [{"x2": 658.0, "y1": 86.0, "x1": 172.0, "y2": 239.0}, {"x2": 401.0, "y1": 333.0, "x1": 105.0, "y2": 427.0}], [{"x2": 401.0, "y1": 88.0, "x1": 103.0, "y2": 297.0}, {"x2": 393.0, "y1": 448.0, "x1": 107.0, "y2": 522.0}], []], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Figure", "name": "1", "captionBoundary": {"x2": 527.2002563476562, "y1": 483.65655517578125, "x1": 307.2760009765625, "y2": 501.614013671875}, "imageText": [], "regionBoundary": {"x2": 519.0, "y1": 221.8900146484375, "x1": 314.0, "y2": 471.8900146484375}, "caption": "Figure 1: Various non-natural language segments labelled from a problem on AskUbuntu", "page": 0}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 525.5465698242188, "y1": 518.8765869140625, "x1": 307.2760009765625, "y2": 536.833984375}, "imageText": [], "regionBoundary": {"x2": 519.0, "y1": 407.8900146484375, "x1": 314.0, "y2": 506.8900146484375}, "caption": "Figure 3: Relative frequencies of each tag in the dataset.", "page": 2}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 525.5469970703125, "y1": 367.0135498046875, "x1": 72.0, "y2": 384.97100830078125}, "imageText": ["(b)", "(a)"], "regionBoundary": {"x2": 473.0, "y1": 61.8900146484375, "x1": 124.0, "y2": 351.6960144042969}, "caption": "Figure 2: Sample problems from Ask Ubuntu with \u3008code\u3009 tag used to present (a) an error message, and (b) contents of a configuration file", "page": 2}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 527.2010498046875, "y1": 184.72653198242188, "x1": 71.69100189208984, "y2": 214.6390380859375}, "imageText": ["SL", "w/o", "LM", "embeddings", "74.57", "75.51", "75.04", "SL", "+", "pre-trained", "ELMo", "76.88", "74.49", "75.67", "SL", "+", "CDME", "combined", "pre-trained", "Embeddings", "78.30", "79.29", "78.80", "Model", "P", "R", "F1", "Sent.", "Only", "Baseline", "47.77", "31.75", "38.15", "Sent.", "Context", "Baseline", "52.52", "34.03", "41.3", "Supervised", "Text", "Segmentation", "Baseline", "44.13", "40.43", "42.20"], "regionBoundary": {"x2": 473.0, "y1": 62.8900146484375, "x1": 124.0, "y2": 172.8900146484375}, "caption": "Table 2: Results comparing the three baselines against variants of our sequence labelling model. The best performing variant uses CDME to combine pre-trained embeddings from multiple language models trained on different datasources.", "page": 7}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 291.5162048339844, "y1": 319.8195495605469, "x1": 71.5320053100586, "y2": 373.6429748535156}, "imageText": ["+", "weighted", "Attn.", "62.34", "57.0", "59.55", "+", "un-weighted", "Attn.", "69.21", "56.15", "62.0", "fastText", "74.57", "75.51", "75.04", "Model", "P", "R", "F1", "Word2Vec", "(w/o", "Attn)", "65.20", "58.59", "61.72"], "regionBoundary": {"x2": 290.0, "y1": 237.8900146484375, "x1": 72.0, "y2": 307.8900146484375}, "caption": "Table 3: Results for experiments between using Word2Vec and fastText embeddings. Also includes results of using attention on top of the model with Word2Vec. Since attention results were not promising, we did not repeat them with fastText.", "page": 7}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 527.29052734375, "y1": 157.62753295898438, "x1": 71.69100189208984, "y2": 187.541015625}, "imageText": ["Train", "1053", "899.33", "4.91", "2.14", "1.20", "0.63", "0.30", "0.14", "0.49", "Val", "131", "783.43", "4.67", "2.17", "1.04", "0.66", "0.26", "0.19", "0.36", "Test", "133", "994.10", "4.64", "2.08", "1.36", "0.47", "0.35", "0.09", "0.28", "#Questions", "Avg.", "#Words", "Avg.", "#SpansTotal", "CC", "CO", "ES", "FC", "SS", "PU", "Dataset", "1317", "897.37", "4.86", "2.13", "1.20", "0.62", "0.30", "0.14", "0.46"], "regionBoundary": {"x2": 501.0, "y1": 62.8900146484375, "x1": 97.0, "y2": 145.8900146484375}, "caption": "Table 1: Statistics of the tagged dataset for segmentation with average number of words and spans per question. The last 6 columns contain average number of spans for each tag type - CC: Command/Code, CO: Command Output, ES: Error Message/Stack Trace, FC: File Content, SS: Semi-structured Information, PU: Path/URL", "page": 3}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 291.92437744140625, "y1": 333.6865539550781, "x1": 72.0, "y2": 411.4199523925781}, "imageText": [], "regionBoundary": {"x2": 290.0, "y1": 210.8900146484375, "x1": 72.0, "y2": 321.8900146484375}, "caption": "Figure 4: Confusion Matrix to show the word-level agreement between annotations of 2 annotators on 50 questions. The relatively large off-diagonal values represent the inherent difficulty in the task. Abbreviations for tags - CC: Command/Code, CO: Command Output, ES: Error Message/Stack Trace, FC: File Content, SS: Semi-structured Information, PU: Path/URL", "page": 3}, {"figType": "Table", "name": "4", "captionBoundary": {"x2": 291.9243469238281, "y1": 226.17056274414062, "x1": 71.64098358154297, "y2": 303.90411376953125}, "imageText": ["Model", "P", "R", "F1", "No", "Pretraining", "74.57", "75.51", "75.04", "Simple", "Concat", "-", "1", "(en)", "76.88", "74.49", "75.67", "Simple", "Concat", "-", "2", "(en", "+", "con\ufb01g)", "77.67", "76.12", "76.89", "Simple", "Concat", "-", "3", "(en", "+", "code", "+", "con\ufb01g)", "79.64", "77.72", "78.67", "Simple", "Concat", "-", "4", "(ALL)", "76.05", "76.65", "76.35", "DME", "77.42", "75.82", "76.61", "CDME", "78.30", "79.29", "78.80"], "regionBoundary": {"x2": 290.0, "y1": 62.8900146484375, "x1": 72.0, "y2": 213.8900146484375}, "caption": "Table 4: Results comparing the models using various pre-trained embeddings. The en data source is the downloaded pre-trained ELMo model. For simple concatenation, we present the results for the best model at each n combinations of data sources. For example, when concatenating any 2 datasources, the en + config combination gives the best performance.", "page": 8}, {"figType": "Table", "name": "5", "captionBoundary": {"x2": 290.2706298828125, "y1": 388.06854248046875, "x1": 71.69100189208984, "y2": 417.9809875488281}, "imageText": ["Method", "MRR", "Full", "Question", "0.292", "Segmented", "Question", "-", "Gold", "0.300", "Segmented", "Question", "-", "Predicted", "0.298"], "regionBoundary": {"x2": 286.0, "y1": 319.8900146484375, "x1": 76.0, "y2": 375.8900146484375}, "caption": "Table 5: Retrieval results, comparing the performance of querying with the full question against segmented question (gold segments and predicted segments)", "page": 8}, {"figType": "Figure", "name": "5", "captionBoundary": {"x2": 290.2706298828125, "y1": 381.24053955078125, "x1": 72.0, "y2": 399.197998046875}, "imageText": [], "regionBoundary": {"x2": 284.0, "y1": 61.8900146484375, "x1": 79.0, "y2": 369.8900146484375}, "caption": "Figure 5: Model architecture for segmenting technical support problems.", "page": 4}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 732.2225782606337, "y1": 671.7452155219183, "x1": 426.772223578559, "y2": 696.6861300998264}, "name": "1", "caption_text": "Figure 1: Various non-natural language segments labelled from a problem on AskUbuntu", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 721.0, "y1": 309.0, "x1": 427.0, "y2": 671.0}, "page": 0, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.925791422526, "y1": 720.6619262695312, "x1": 426.772223578559, "y2": 745.6027560763889}, "name": "3", "caption_text": "Figure 3: Relative frequencies of each tag in the dataset.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 716.0, "y1": 572.0, "x1": 441.0, "y2": 697.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.3479546440972, "y1": 218.9271291097005, "x1": 99.57083596123589, "y2": 260.4736328125}, "name": "1", "caption_text": "Table 1: Statistics of the tagged dataset for segmentation with average number of words and spans per question. The last 6 columns contain average number of spans for each tag type - CC: Command/Code, CO: Command Output, ES: Error Message/Stack Trace, FC: File Content, SS: Semi-structured Information, PU: Path/URL", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 695.0, "y1": 86.0, "x1": 120.0, "y2": 219.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.45052422417535, "y1": 463.4535471598307, "x1": 100.0, "y2": 571.4166005452473}, "name": "4", "caption_text": "Figure 4: Confusion Matrix to show the word-level agreement between annotations of 2 annotators on 50 questions. The relatively large off-diagonal values represent the inherent difficulty in the task. Abbreviations for tags - CC: Command/Code, CO: Command Output, ES: Error Message/Stack Trace, FC: File Content, SS: Semi-structured Information, PU: Path/URL", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 403.0, "y1": 293.0, "x1": 100.0, "y2": 463.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.15365261501734, "y1": 529.5007493760851, "x1": 100.0, "y2": 554.441663953993}, "name": "5", "caption_text": "Figure 5: Model architecture for segmenting technical support problems.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 390.0, "y1": 92.0, "x1": 114.0, "y2": 509.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2236802842882, "y1": 256.5646277533637, "x1": 99.57083596123589, "y2": 298.1097751193576}, "name": "2", "caption_text": "Table 2: Results comparing the three baselines against variants of our sequence labelling model. The best performing variant uses CDME to combine pre-trained embeddings from multiple language models trained on different datasources.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 659.0, "y1": 86.0, "x1": 172.0, "y2": 256.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 404.8836178249783, "y1": 444.1938188340929, "x1": 99.35000737508138, "y2": 518.9485761854384}, "name": "3", "caption_text": "Table 3: Results for experiments between using Word2Vec and fastText embeddings. Also includes results of using attention on top of the model with Word2Vec. Since attention results were not promising, we did not repeat them with fastText.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 403.0, "y1": 330.0, "x1": 100.0, "y2": 444.0}, "page": 7, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 405.45048183865015, "y1": 314.12578158908417, "x1": 99.50136608547635, "y2": 422.0890469021267}, "name": "4", "caption_text": "Table 4: Results comparing the models using various pre-trained embeddings. The en data source is the downloaded pre-trained ELMo model. For simple concatenation, we present the results for the best model at each n combinations of data sources. For example, when concatenating any 2 datasources, the en + config combination gives the best performance.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 403.0, "y1": 86.0, "x1": 100.0, "y2": 314.0}, "page": 8, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 403.15365261501734, "y1": 538.9840867784288, "x1": 99.57083596123589, "y2": 580.5291493733723}, "name": "5", "caption_text": "Table 5: Retrieval results, comparing the performance of querying with the full question against segmented question (gold segments and predicted segments)", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 397.0, "y1": 444.0, "x1": 100.0, "y2": 539.0}, "page": 8, "dpi": 0}], "error": null, "pdf": "/work/host-output/cfa5c4c33209a63aef4243212787c8b4b3c5508a/2020.acl-main.284.pdf", "dpi": 100}