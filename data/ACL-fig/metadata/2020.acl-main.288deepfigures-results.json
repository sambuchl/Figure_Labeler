{"raw_detected_boxes": [[], [{"x2": 708.0, "y1": 91.0, "x1": 121.0, "y2": 294.0}], [{"x2": 645.0, "y1": 87.0, "x1": 182.0, "y2": 362.0}], [{"x2": 631.0, "y1": 87.0, "x1": 202.0, "y2": 340.0}], [{"x2": 676.0, "y1": 86.0, "x1": 147.0, "y2": 144.0}, {"x2": 398.0, "y1": 240.0, "x1": 107.0, "y2": 339.0}], [], [{"x2": 708.0, "y1": 86.0, "x1": 126.0, "y2": 324.0}], [{"x2": 703.0, "y1": 86.0, "x1": 454.0, "y2": 242.0}], [], []], "raw_pdffigures_output": {"regionless-captions": [], "figures": [{"figType": "Figure", "name": "1", "captionBoundary": {"x2": 512.0465698242188, "y1": 224.41653442382812, "x1": 85.49800109863281, "y2": 230.41900634765625}, "imageText": ["(b)", "ECPE-2D", "(Our", "approach)", "Em", "o", "tio", "n", "clau", "se", "Cause", "clause", "c5-c3", "c5-c4", "c5-c5c5-c2c5-c1", "c5-c6", "c6-c3", "c6-c4", "c6-c5c6-c2c6-c1", "c6-c6", "c4-c3", "c4-c4", "c4-c5c4-c2c4-c1", "c4-c6", "c3-c3", "c3-c4", "c3-c5c3-c2c3-c1", "c3-c6", "c2-c3", "c2-c4", "c2-c5c2-c2c2-c1", "c2-c6", "c1-c3", "c1-c4", "c1-c5c1-c2c1-c1", "c1-c6", "(a)", "ECPE-2Step", "(Xia", "and", "Ding,", "2019)", "Step", "1", "Step", "2", "-", "Pairing", "Cause", "set:", "{c2,", "c3,", "c6}", "Emotion", "set:", "{c4,", "c5}", "c1:", "Yesterday", "morning,", "c2:", "a", "policeman", "visited", "the", "old", "man", "with", "the", "lost", "money,", "c3:", "and", "told", "him", "that", "the", "thief", "was", "caught.", "c4:", "The", "old", "man", "was", "very", "happy.", "c5:", "But", "he", "still", "feels", "worried,", "c6:", "as", "he", "doesn\u2019t", "know", "how", "to", "keep", "so", "much", "money.", "Step", "2", "-", "Filtering", "Valid", "Emotion-Cause", "Pairs:", "{c4-c2,", "c4-c3,", "c4-c6,", "c5-c2,", "c5-c3,", "c5-c6}", "All", "possible", "Emotion-Cause", "Pairs:", "{c4-c2,", "c4-c3,", "c4-c6,", "c5-c2,", "c5-c3,", "c5-c6}"], "regionBoundary": {"x2": 510.0, "y1": 65.8900146484375, "x1": 88.0, "y2": 209.44549560546875}, "caption": "Figure 1: An example showing two frameworks for solving the emotion-cause pair extraction (ECPE) task.", "page": 1}, {"figType": "Table", "name": "2", "captionBoundary": {"x2": 527.2008666992188, "y1": 246.09652709960938, "x1": 71.69100189208984, "y2": 264.05401611328125}, "imageText": ["Inter-EC", "-", "70.73", "64.86", "67.47", "86.22", "91.82", "88.88", "73.46", "68.79", "70.96", "(BERT)", "+WC", "72.92", "65.44", "68.89", "86.27", "92.21", "89.10", "73.36", "69.34", "71.23+CR", "69.35", "67.85", "68.37", "85.48", "92.44", "88.78", "72.72", "69.27", "70.87", "+WC", "71.18", "59.84", "64.94", "85.11", "82.37", "83.65", "71.33", "62.85", "66.72", "+CR", "69.60", "61.18", "64.96", "85.12", "82.20", "83.58", "72.72", "62.98", "67.38", "Inter-EC", "-", "71.73", "57.54", "63.66", "85.37", "81.97", "83.54", "71.51", "62.74", "66.76", "+WC", "68.62", "58.70", "63.18", "84.97", "82.58", "83.70", "69.24", "59.15", "63.65", "+CR", "69.22", "59.04", "63.56", "84.82", "82.88", "83.76", "69.80", "58.78", "63.68", "Inter-CE", "-", "69.35", "57.24", "62.61", "86.12", "82.40", "84.16", "69.77", "59.42", "63.98", "+CR", "69.12", "58.78", "63.38", "85.27", "81.82", "83.44", "69.73", "59.37", "63.99", "(Ours)", "Indep", "-", "71.60", "55.95", "62.63", "86.32", "81.52", "83.80", "69.15", "59.72", "63.97", "+WC", "69.01", "59.58", "63.80", "85.08", "81.82", "83.35", "71.57", "59.08", "64.64", "ECPE-2D", "ECPE-", "Indep", "68.32", "50.82", "58.18", "83.75", "80.71", "82.10", "69.02", "56.73", "62.05", "2Steps", "Inter-CE", "69.02", "51.35", "59.01", "84.94", "81.22", "83.00", "68.09", "56.34", "61.51Inter-EC", "67.21", "57.05", "61.28", "83.64", "81.07", "82.30", "70.41", "60.83", "65.07", "Framework", "Approach", "Emotion-Cause", "Pair", "Ext.", "Emotion", "Ext.", "Cause", "Ext.", "P", "R", "F1", "P", "R", "F1", "P", "R", "F1"], "regionBoundary": {"x2": 510.0, "y1": 63.8900146484375, "x1": 87.0, "y2": 233.8900146484375}, "caption": "Table 2: Performance of our models and baseline models (Xia and Ding 2019) using precision, recall, and F1measure as metrics on the ECPE task as well as the two sub-tasks.", "page": 6}, {"figType": "Figure", "name": "2", "captionBoundary": {"x2": 472.1014709472656, "y1": 275.0635681152344, "x1": 125.44300079345703, "y2": 281.0660400390625}, "imageText": ["\ud835\udc93|\ud835\udc51|", "cau", "\u0ddd\ud835\udc9a|\ud835\udc51|", "cau", "\ud835\udc931", "emo", "\u0ddd\ud835\udc9a1", "emo", "\u2026", "\u2026", "\u2026", "\u2026", "\u0ddd\ud835\udc9a\ud835\udc56", "emo", "\u2026", "\u0ddd\ud835\udc9a\ud835\udc56,\ud835\udc57", "pair", "so", "ftm", "ax", "\ud835\udc93\ud835\udc57", "cau", "\u0ddd\ud835\udc9a\ud835\udc57", "cau", "\ud835\udc93\ud835\udc56", "emo", "B", "i-LSTM", "&", "A", "tte", "n", "tio", "n", "\ud835\udc64|\ud835\udc51|,|\ud835\udc50|\ud835\udc51||", "\ud835\udc50|\ud835\udc51|", "\u2026", "\ud835\udc64|\ud835\udc51|,1", "B", "i-LSTM", "&", "atte", "n", "tio", "n", "\ud835\udc641,|\ud835\udc501|", "\ud835\udc501", "\u2026", "\ud835\udc641,1", "\u2026", "\u2026", "C", "o", "p", "y", "\u2026", "\ud835\udc94|\ud835\udc51|", "\ud835\udc94|\ud835\udc51|", "\ud835\udc941", "\ud835\udc93\ud835\udc91\ud835\udc86\ud835\udc56,\ud835\udc57", "so", "ftm", "ax", "so", "ftm", "ax", "B", "i-LSTM", "\u2026", "B", "i-LSTM", "\ud835\udc941", "\ud835\udc931", "cau", "\u0ddd\ud835\udc9a1", "cau", "so", "ftm", "ax", "so", "ftm", "ax", "B", "i-LSTM", "\u2026", "B", "i-LSTM", "\ud835\udc941", "\ud835\udc94|\ud835\udc51|", "\u2026", "\u0ddd\ud835\udc9a|\ud835\udc51|", "emo", "\u2026", "\u2026\u2026", "\u2026", "\u2295", "\ud835\udc93|\ud835\udc51|", "emo", "\u2026", "\u2295", "\u2295", "\u2026", "\u2026", "\u2295"], "regionBoundary": {"x2": 467.0, "y1": 62.849727630615234, "x1": 129.48895263671875, "y2": 260.8900146484375}, "caption": "Figure 2: Overview of the proposed joint framework for emotion-cause pair extraction.", "page": 2}, {"figType": "Table", "name": "3", "captionBoundary": {"x2": 527.200439453125, "y1": 186.32052612304688, "x1": 306.9170227050781, "y2": 228.18804931640625}, "imageText": ["Inter-EC-AS", "66.46", "56.69", "61.08", "Inter-EC+WC-AS", "67.79", "60.47", "63.81", "Inter-EC+CR-AS", "69.26", "60.06", "64.17", "Indep-AS", "67.26", "56.46", "61.24", "Indep+WC-AS", "68.87", "59.78", "63.86", "Indep+CR-AS", "67.48", "60.66", "63.76", "Inter-CE-AS", "68.36", "54.40", "60.42", "Inter-CE+WC-AS", "67.12", "60.79", "63.44", "Inter-CE+CR-AS", "67.28", "61.08", "63.85", "Emotion-Cause", "Pair", "Ext.", "P", "R", "F1"], "regionBoundary": {"x2": 506.0, "y1": 62.8900146484375, "x1": 327.0, "y2": 173.8900146484375}, "caption": "Table 3: Performance of our models on the ECPE task when the auxiliary supervisions of emotion extraction and cause extraction are removed. For brevity, the prefix \u201dECPE-2D\u201d of all methods in this table are omitted.", "page": 7}, {"figType": "Figure", "name": "3", "captionBoundary": {"x2": 473.4216003417969, "y1": 259.3105163574219, "x1": 124.12300109863281, "y2": 265.31298828125}, "imageText": ["2D", "Self", "attention", "Nx", "(b)", "Cross-road", "2D", "transformer.", "Nx", "Add", "&", "Normalize", "Add", "&", "Normalize", "\ud835\udc90|\ud835\udc51|,1", "\u2026", "\ud835\udc901,1", "\ud835\udc901,|\ud835\udc51|", "\u0ddd\ud835\udc90|\ud835\udc51|,1", "\u2026", "\u0ddd\ud835\udc901,1", "\u0ddd\ud835\udc901,|\ud835\udc51|", "\ud835\udc9b|\ud835\udc51|,1", "\u2026", "\ud835\udc9b1,1", "\ud835\udc9b1,|\ud835\udc51|", "\u0ddc\ud835\udc9b|\ud835\udc51|,1", "\u2026", "\u0ddc\ud835\udc9b1,1", "\u0ddc\ud835\udc9b1,|\ud835\udc51|", "\ud835\udc74|\ud835\udc51|,1", "\u2026", "\ud835\udc741,1", "\ud835\udc741,|\ud835\udc51|", "FFN2D", "Self", "attention", "Add", "&", "Normalize", "Add", "&", "Normalize", "\ud835\udc90|\ud835\udc51|,1", "\u2026", "\ud835\udc901,1", "\ud835\udc901,|\ud835\udc51|", "\u0ddd\ud835\udc90|\ud835\udc51|,1", "\u2026", "\u0ddd\ud835\udc901,1", "\u0ddd\ud835\udc901,|\ud835\udc51|", "\ud835\udc9b|\ud835\udc51|,1", "\u2026", "\ud835\udc9b1,1", "\ud835\udc9b1,|\ud835\udc51|", "\u0ddc\ud835\udc9b|\ud835\udc51|,1", "\u2026", "\u0ddc\ud835\udc9b1,1", "\u0ddc\ud835\udc9b1,|\ud835\udc51|", "\u2026", "\ud835\udc74|\ud835\udc51|,1", "\ud835\udc741,1", "\ud835\udc741,|\ud835\udc51|", "FFN", "(a)", "Window-constrained", "2D", "transformer."], "regionBoundary": {"x2": 455.0, "y1": 62.8900146484375, "x1": 143.6466827392578, "y2": 245.219482421875}, "caption": "Figure 3: Two simplified versions of 2D transformer for emotion-cause pair interaction.", "page": 3}, {"figType": "Table", "name": "1", "captionBoundary": {"x2": 525.5425415039062, "y1": 116.1835708618164, "x1": 71.69100189208984, "y2": 146.0960693359375}, "imageText": ["Window-constrained", "O(batch", "\u2217", "|d|", "\u2217", "w", "\u2217", "n", "\u2217", "(|d|", "\u2217", "w", "+", "n))", "O(batch", "\u2217", "|d|", "\u2217", "w", "\u2217", "(|d|", "\u2217", "w", "+", "n))", "Cross-road", "O(batch", "\u2217", "|d|", "\u2217", "|d|", "\u2217", "n", "\u2217", "(|d|+", "n))", "O(batch", "\u2217", "|d|", "\u2217", "|d|", "\u2217", "(|d|+", "n))", "2D", "transformer", "Time", "complexity", "Space", "complexity", "Standard", "O(batch", "\u2217", "|d|", "\u2217", "|d|", "\u2217", "n", "\u2217", "(|d|", "\u2217", "|d|+", "n))", "O(batch", "\u2217", "|d|", "\u2217", "|d|", "\u2217", "(|d|", "\u2217", "|d|+", "n))"], "regionBoundary": {"x2": 496.0, "y1": 62.8900146484375, "x1": 102.0, "y2": 103.8900146484375}, "caption": "Table 1: Comparison of three kinds of 2D transformer in resource consumption. batch indicates the batch size during training, |d| indicates the number of clauses in the document, n refers to the hidden state size, w is equal to 2 \u2217 window + 1, and window is the window size used in window-constrained 2D transformer.", "page": 4}, {"figType": "Figure", "name": "4", "captionBoundary": {"x2": 291.5159606933594, "y1": 258.4075622558594, "x1": 71.6709976196289, "y2": 300.27606201171875}, "imageText": ["(a)", "(b)", "(c)"], "regionBoundary": {"x2": 286.0, "y1": 172.8900146484375, "x1": 76.0, "y2": 242.27880859375}, "caption": "Figure 4: Examples of attentions to be calculated in three 2D Transformers: (a) Standard 2D-Transformer, (b) Window-constrained 2D Transformer, and (c) Cross-road 2D Transformer.", "page": 4}]}, "figures": [{"page_width": 0, "caption_boundary": {"x2": 711.175791422526, "y1": 311.6896311442057, "x1": 118.74722374810112, "y2": 320.0263977050781}, "name": "1", "caption_text": "Figure 1: An example showing two frameworks for solving the emotion-cause pair extraction (ECPE) task.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 708.0, "y1": 91.0, "x1": 119.0, "y2": 311.0}, "page": 1, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 655.6964874267578, "y1": 382.03273349338104, "x1": 174.22638999091254, "y2": 390.3695000542535}, "name": "2", "caption_text": "Figure 2: Overview of the proposed joint framework for emotion-cause pair extraction.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 649.0, "y1": 87.0, "x1": 180.0, "y2": 362.0}, "page": 2, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 657.5300004747179, "y1": 360.1534949408637, "x1": 172.39305708143445, "y2": 368.4902615017361}, "name": "3", "caption_text": "Figure 3: Two simplified versions of 2D transformer for emotion-cause pair interaction.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 631.0, "y1": 87.0, "x1": 200.0, "y2": 342.0}, "page": 3, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 729.9201965332031, "y1": 161.36607064141168, "x1": 99.57083596123589, "y2": 202.9112074110243}, "name": "1", "caption_text": "Table 1: Comparison of three kinds of 2D transformer in resource consumption. batch indicates the batch size during training, |d| indicates the number of clauses in the document, n refers to the hidden state size, w is equal to 2 \u2217 window + 1, and window is the window size used in window-constrained 2D transformer.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 689.0, "y1": 86.0, "x1": 135.0, "y2": 161.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 404.8832787407769, "y1": 358.8993920220269, "x1": 99.54305224948459, "y2": 417.05008612738715}, "name": "4", "caption_text": "Figure 4: Examples of attentions to be calculated in three 2D Transformers: (a) Standard 2D-Transformer, (b) Window-constrained 2D Transformer, and (c) Cross-road 2D Transformer.", "figure_type": "Figure", "uri": null, "page_height": 0, "figure_boundary": {"x2": 398.0, "y1": 240.0, "x1": 106.0, "y2": 339.0}, "page": 4, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2234259711371, "y1": 341.8007320827908, "x1": 99.57083596123589, "y2": 366.74168904622394}, "name": "2", "caption_text": "Table 2: Performance of our models and baseline models (Xia and Ding 2019) using precision, recall, and F1measure as metrics on the ECPE task as well as the two sub-tasks.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 725.0, "y1": 86.0, "x1": 114.0, "y2": 341.0}, "page": 6, "dpi": 0}, {"page_width": 0, "caption_boundary": {"x2": 732.2228325737847, "y1": 258.77850850423175, "x1": 426.27364264594183, "y2": 316.92784627278644}, "name": "3", "caption_text": "Table 3: Performance of our models on the ECPE task when the auxiliary supervisions of emotion extraction and cause extraction are removed. For brevity, the prefix \u201dECPE-2D\u201d of all methods in this table are omitted.", "figure_type": "Table", "uri": null, "page_height": 0, "figure_boundary": {"x2": 704.0, "y1": 86.0, "x1": 440.0, "y2": 259.0}, "page": 7, "dpi": 0}], "error": null, "pdf": "/work/host-output/edd52abfb1790b0f02a47b3b8f4adb730ee4ae0c/2020.acl-main.288.pdf", "dpi": 100}