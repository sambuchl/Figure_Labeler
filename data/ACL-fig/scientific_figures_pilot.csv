sci_fig,caption,inline_reference,metadata,label,acl_paper_id
2007.sigdial-1.44.pdf-Figure2.png,Figure 2: Dialogue SDRS updating algorithm.,[u'A rather informal statement of this improved algorithm is presented in Figure 2 ; steps added in the present version of the algorithm are shown in bold-face. A'],2007.sigdial-1.44deepfigures-results.json,algorithms,2007.sigdial-1.44
C00-2139.pdf-Figure2.png,Figure 2: Alignment learning algorithm,[u'2 Assign non-terminals to the constituents i.e. distinct parts of s 1 and s 2 Figure 2 : Alignment learning algorithm tences as shown in gure 1.'],C00-2139deepfigures-results.json,algorithms,C00-2139
C02-1014.pdf-Figure1.png,Figure 1. Implementation of the automatic process using genus and relater information,[u'. figure 1 shows the algorithm used.'],C02-1014deepfigures-results.json,algorithms,C02-1014
C02-1144.pdf-Figure1.png,Figure 1. Phase II of CBC.,[u'. the details of phase ii are presented in figure 1 .'],C02-1144deepfigures-results.json,algorithms,C02-1144
C02-1158.pdf-Figure1.png,Figure 1: Previous works.,"[u'different parts are replaced by variables to generalize translation examples as shown in (1) of Figure 1 . However, the number of different parts of the two SL sentences must be same', u'as shown in (3a) of Figure 1 . Moreover, the system performs Inductive Learning. By using Inductive Learning, the abstract translation rules', u'. by using inductive learning, the abstract translation rules are acquired by performing phased extraction of different parts as shown in figure 1 (3b) and (3c).', u'. in all methods shown in figure 1 , the condition of acquisition of translation rules is that two similar translation examples must exist.']",C02-1158deepfigures-results.json,algorithms,C02-1158
C04-1020.pdf-Figure1.png,Figure 1. Coherence graph for (10).,[],C04-1020deepfigures-results.json,algorithms,C04-1020
C04-1020.pdf-Figure3.png,Figure 3. Coherence graph for (12).,"[u'. the resulting coherence structure, shown in figure 3 , contains a crossed dependency between {3, 1} and {2, 0-1}.']",C04-1020deepfigures-results.json,algorithms,C04-1020
C04-1024.pdf-Figure3.png,Figure 3: Extraction of the 1-bits from a bit vector,[u'. figure 3 shows how the 1-bits would be extracted from a 4-bit word v and stored in the set s.'],C04-1024deepfigures-results.json,algorithms,C04-1024
C04-1024.pdf-Figure4.png,Figure 4: Parse forest with two analyses for A,[],C04-1024deepfigures-results.json,algorithms,C04-1024
C04-1024.pdf-Figure5.png,Figure 5: Parse forest generation,[u'. the parse forest is built by the function parse shown in figure 5 .'],C04-1024deepfigures-results.json,algorithms,C04-1024
C04-1079.pdf-Figure3.png,Figure 3. Framework for extracting discussion Issues.,"[u'In Figure 3 , we present the general framework of the algorithm. In this framework we divide the']",C04-1079deepfigures-results.json,algorithms,C04-1079
C04-1079.pdf-Figure4.png,Figure 4. Oracle 1 heuristic rules,"[u'The oracle in Figure 4 attempts to choose the best sentence, retrieving a single sentence. Rule 2 attempts to encode']",C04-1079deepfigures-results.json,algorithms,C04-1079
C04-1079.pdf-Figure5.png,Figure 5. Oracle 2 heuristic rules,"[u"". if all methods return different answers, then choose centroid method's answer figure 5 .""]",C04-1079deepfigures-results.json,algorithms,C04-1079
C04-1134.pdf-Figure2.png,Figure 2. BiFrameNet ontology induction,[],C04-1134deepfigures-results.json,algorithms,C04-1134
C04-1134.pdf-Figure3.png,Figure 3. BiFrameNet example sentence induction,[],C04-1134deepfigures-results.json,algorithms,C04-1134
C04-1171.pdf-Figure2.png,Figure 2. Pseudo-code of descriptor generation,[],C04-1171deepfigures-results.json,algorithms,C04-1171
C04-1172.pdf-Figure3.png,Figure 3: Examples of embedding,[u'. figure 3 .'],C04-1172deepfigures-results.json,algorithms,C04-1172
C04-1172.pdf-Figure4.png,Figure 4: An example with a gap,"[u'Consider Figure 4 . It shows how one can look for objects inside ""gaps"" in other objects. In', u'. the query in figure 4 searches for structures of this kind.']",C04-1172deepfigures-results.json,algorithms,C04-1172
C04-1172.pdf-Figure5.png,Figure 5: Example with agreement,"[u'. in figure 5 , the ""as"" keyword gives a name (""w1"") to the noun inside the np, and this name can then be used inside the adjective in the adjp to specify agreement.']",C04-1172deepfigures-results.json,algorithms,C04-1172
C04-1172.pdf-Figure6.png,Figure 6: Hebrew example,"[u'. the query in figure 6 finds 234 instances, showing that the pattern was not uncommon, and inspection of the results show that the verb most often agrees with the first member of the compound subject.']",C04-1172deepfigures-results.json,algorithms,C04-1172
C04-1179.pdf-Figure1.png,Fig. 1. The sequence of steps on a sample sentence.,"[u'We define the problem into three subsequent processes (see Figure 1 ): 1) sentence segmentation 2) frame element identification, and 3) semantic role tagging for the']",C04-1179deepfigures-results.json,algorithms,C04-1179
C04-1179.pdf-Figure3.png,"Fig. 3. The framework of the re-ranking method with an actual system output. (1) contains different number of segmentations depending on each sentence, (2) has mn number of lists when we obtain m possible segmentations in (1) and we get n-best FE identifications, (3) has mnn number of lists when we get n-best role classifications given mn lists (4) shows finally chosen output.",[u'. figure 3 shows the overall framework of this task.'],C04-1179deepfigures-results.json,algorithms,C04-1179
C04-1191.pdf-Figure4.png,Figure 4: Sample rule from the general speech part classifier.,[u'. see figure 4 .'],C04-1191deepfigures-results.json,algorithms,C04-1191
C08-1046.pdf-Figure3.png,Figure 3: Pseudo code of training example generation procedure.,"[u'As shown in Figure 3 , for each dependent, we generate pairs of the correct head and all other candidate']",C08-1046deepfigures-results.json,algorithms,C08-1046
C08-1046.pdf-Figure4.png,Figure 4: Pseudo code of parsing algorithm.,"[u'This algorithm is shown in Figure 4 . The overall parsing process moves from right to left. On selecting the head for', u'. in figure 4 , the array ""head"" stores the parsed results and ensures that only noncrossing candidate heads are taken into consideration.']",C08-1046deepfigures-results.json,algorithms,C08-1046
C08-1047.pdf-Figure1.png,Figure 1: An example of a transliterated cuneiform tablet from Ur III,[],C08-1047deepfigures-results.json,algorithms,C08-1047
C08-1122.pdf-Figure1.png,Figure 1. The Framework of CollabRank,[u'. figure 1 gives the framework of the proposed approach.'],C08-1122deepfigures-results.json,algorithms,C08-1122
C08-1125.pdf-Figure1.png,Figure 1.The domain adaptation algorithm,[u'. then we perform transductive learning as described in figure 1 .'],C08-1125deepfigures-results.json,algorithms,C08-1125
C08-1137.pdf-Figure5.png,Figure 5. Reordering template extraction algorithm. The p,"[u'. the simple algorithm is illustrated in figure 5 , and some reordering templates are shown in tabl e 2.', u'. b figure 5 .']",C08-1137deepfigures-results.json,algorithms,C08-1137
C08-1140.pdf-Figure2.png,Figure 2: A modified K-means clustering method.,[],C08-1140deepfigures-results.json,algorithms,C08-1140
C08-2026.pdf-Figure2.png,"Figure 2: CYK-style parsing algorithm for (2,2)-BRCG.","[u'A CYK-style algorithm is presented for (2,2)-BRCG in Figure 2 ; it is assumed, w.l.o.g, that if the same variable occurs twice in the LHS', u'that iterate over the string, i.e. the four for loops and the two s in Figure 2 .', u') s(0, n), (S, 1) t(0, m) then return success else failure Figure 2 : CYK-style parsing algorithm for (2,2)-BRCG.']",C08-2026deepfigures-results.json,algorithms,C08-2026
C10-1023.pdf-Figure1.png,Figure 1. High-level algorithm of the synthesis approach,[u'. figure 1 describes the high-level algorithm of our approach.'],C10-1023deepfigures-results.json,algorithms,C10-1023
C10-1023.pdf-Figure2.png,Figure 2. Segment selection algorithm,[u'. figure 2 describes the segment selection algorithm.'],C10-1023deepfigures-results.json,algorithms,C10-1023
C10-1024.pdf-Figure2.png,"Figure 2.  (n1, n2, , ) algorithm","[u'. figure 2 shows the algorithm.', u'( + delta(nch 1 , nch 2 , , )); END FOR RETURN mult_delta; END Figure 2 . (n 1 , n 2 , , ) algorithm 4 Performance Improving Factors']",C10-1024deepfigures-results.json,algorithms,C10-1024
C10-1030.pdf-Figure1.png,Figure 1: Error taxonomy,[],C10-1030deepfigures-results.json,algorithms,C10-1030
C10-1030.pdf-Figure2.png,Figure 2: Error generation procedure,[],C10-1030deepfigures-results.json,algorithms,C10-1030
C10-1034.pdf-Figure4.png,Figure 4. PageRank Algorithm for Calculating Popularity Score for Users,[u'. figure 4 shows the pagerank algorithm for calculating popularity scores for twitter users.'],C10-1034deepfigures-results.json,algorithms,C10-1034
C10-1034.pdf-Figure6.png,Figure 6. Advanced Greedy Feature Selection Algorithm,[u'. figure 6 shows the feature selection approach.'],C10-1034deepfigures-results.json,algorithms,C10-1034
C10-1081.pdf-Figure2.png,"Figure 2: Decoding algorithm for the standard Tree-to-String transducer. leftw/rightw denote the left/right boundary word of s. c1, c2 denote the descendants of v, ordered based on RHS of t.","[u'. the bottom-up decoding algorithm for the tts transducer is sketched in figure 2 .', u'. to simplify the description, we assume in figure 2 that a bigram language model is used and all the tts templates are binarized.']",C10-1081deepfigures-results.json,algorithms,C10-1081
C10-1089.pdf-Figure3.png,Figure 3: Pseudo-code for sentence simplification.,"[u'until r is null 13: until r is null 14: return S Figure 3 : Pseudo-code for sentence simplification.', u'a sentence, we apply rules repeatedly until no more applications are possible as presented in Figure 3 . After one application of one rule, the simplified sentence is re-parsed before attempting to']",C10-1089deepfigures-results.json,algorithms,C10-1089
C10-1134.pdf-Figure1.png,Figure 1: Algorithm of computing SO of documents,"[u""The baseline algorithm is illustrated by the non-italic part in Figure 1 , where we set the same parameters with Wan' s approach: PosValue=1, NegValue=-2, q=2, =2."", u'added the disambiguation of DSAAs to the algorithm, as illustrated by the italic part in Figure 1 . When a word is a DSAA, compute its SO with the proposed integrated method,']",C10-1134deepfigures-results.json,algorithms,C10-1134
C10-1143.pdf-Figure1.png,Figure 1. The proposed SC-EM algorithm,[u'. the detailed algorithm is given in figure 1 .'],C10-1143deepfigures-results.json,algorithms,C10-1143
C10-1143.pdf-Figure2.png,Figure 2. Generating the soft-labeled set SL,"[u'. the detailed algorithm is given in figure 2 .', u'. figure 2 .']",C10-1143deepfigures-results.json,algorithms,C10-1143
C10-1143.pdf-Figure3.png,Figure 3. Distributional context extraction,"[u'. given a relevant corpus r, the document d i for each feature expression e i in l (or u) is generated using the algorithm in figure 3 .', u'1, 2, , |S i |; // duplicates are kept as it is not union Figure 3 . Distributional context extraction For example, a feature expression from L (or U) is e']",C10-1143deepfigures-results.json,algorithms,C10-1143
C10-1147.pdf-Figure2.png,Figure 2: The maximum metric score training (MMST) algorithm,"[u'. initially, the tree has only one node: the root (node 1 in the figure).']",C10-1147deepfigures-results.json,algorithms,C10-1147
C10-2022.pdf-Figure1.png,Figure 1: EditDistanceSMS,"[u'The EditDistance SM S (Figure 1 ) compares the Consonant Skeletons', u'Step 2. remove all vowels in t //(painting pntng, threat thrt) return t End Figure 1 : EditDistance SM S']",C10-2022deepfigures-results.json,algorithms,C10-2022
C10-2022.pdf-Figure2.png,Figure 2: Sample SMS queries,[u'. a list l e i is created for each token s i using terms in a dic-hv u cmplted ure prj rprt d ddline fr sbmission of d rprt hs bn xtnded i wil be lte by 20 mns d docs shd rech u in 2 days thnk u for cmg 2 d prty figure 2 : sample sms queries tionary d e consisting of clean english words.'],C10-2022deepfigures-results.json,algorithms,C10-2022
C10-2025.pdf-Figure3.png,Figure 3: Algorithm for skeleton-to-hypothesis alignment,[u'. figure 3 shows the algorithm for skeleton-to-hypothesis alignment.'],C10-2025deepfigures-results.json,algorithms,C10-2025
C10-2025.pdf-Figure4.png,Figure 4: Hybrid decoding algorithm,[u'. the algorithm is fully demonstrated in figure 4 .'],C10-2025deepfigures-results.json,algorithms,C10-2025
C10-2026.pdf-Figure2.png,Figure 2. The Dependent Ranked List Generation Algorithm (represented using python syntax).,"[u'. figure 2 shows the steps of the transformation algorithm.', u' Figure 2 . The Dependent Ranked List Generation Algorithm (represented using python syntax).']",C10-2026deepfigures-results.json,algorithms,C10-2026
C10-2031.pdf-Figure1.png,Figure 1: Preposition prediction results,"[u'. esting figure included in figure 1 , namely the accuracy of the human agreement with the original text, averaged over two english native-speakers.']",C10-2031deepfigures-results.json,algorithms,C10-2031
C10-2084.pdf-Figure5.png,Figure 5. Semi-supervised training for HP-DITG.,[u'. the sketch of the semi-supervised training is shown in figure 5 .'],C10-2084deepfigures-results.json,algorithms,C10-2084
C10-2125.pdf-Figure1.png,Figure 1: Relaxation labeling algorithm,[u'. figure 1 shows the pseudo-code of the relaxation algorithm.'],C10-2125deepfigures-results.json,algorithms,C10-2125
C10-2161.pdf-Figure1.png,Figure 1: Pseudo-code for the SGD algorithm.,[u'. the pseudo-code for the sgd algorithm is shown in figure 1 .'],C10-2161deepfigures-results.json,algorithms,C10-2161
C12-1116.pdf-Figure6.png,FIGURE 6  Learned affix identification rule example,[u'The two example rules in Figure 6 show that the prefixes'],C12-1116deepfigures-results.json,algorithms,C12-1116
C12-1116.pdf-Figure7.png,FIGURE 7  Suffix segmentation algorithm,[],C12-1116deepfigures-results.json,algorithms,C12-1116
P06-1066.pdf-Figure2.png,Figure 2: Reordering Example Extraction Algorithm.,[u'. the reordering example extraction algorithm is shown in figure 2 .'],P06-1066deepfigures-results.json,algorithms,P06-1066
P06-1080.pdf-Figure2.png,Figure 2: A function that determines how large a window size should be.,"[u'. figure 2 depicts an algorithm that determines how large -grams should be used.', u'. the main stream of this algorithm is equivalent to that in figure 2 .']",P06-1080deepfigures-results.json,algorithms,P06-1080
P06-1080.pdf-Figure3.png,Figure 3: A function that determines how small a window size should be used.,[u'. figure 3 shows an algorithm which determines how deeply the shrinking is occurred.'],P06-1080deepfigures-results.json,algorithms,P06-1080
P06-1080.pdf-Figure4.png,Figure 4: A function that determines the changing window size of -grams.,"[u'. in this paper, an expanding is checked prior to a shrinking as shown in figure  4 .']",P06-1080deepfigures-results.json,algorithms,P06-1080
Y07-1024.pdf-Figure5.png,Figure 5: TBL templates,"[u'environment is similar with a context window. If we enlarge the context window by 4, Figure 5 is changed into', u'.  what the feature map in figure 5 specifies is any clusters with consonant and diphthong [ye] cannot be placed next to each other.']",Y07-1024deepfigures-results.json,algorithms,Y07-1024
Y07-1024.pdf-Figure6.png,Figure 6: Example of 4 context windows TBL,[u'is changed into Figure 6 .'],Y07-1024deepfigures-results.json,algorithms,Y07-1024
Y07-1025.pdf-Figure2.png,Figure 2: Parsing Output of (4),[u'. figure 2 is a successfully parsed output of example (4).'],Y07-1025deepfigures-results.json,algorithms,Y07-1025
Y07-1025.pdf-Figure3.png,Figure 3: Abbreviated Parsing Output of (5),"[u'. let us show the parsing result of (5) in figure 3 .', u'. as seen in the representation in figure 3 the parser produced the semantic representation fo(say(john, introduce(meta v, meta v, professor), naomi)) where meta v is a meta variable, which is merged with someone in the context.']",Y07-1025deepfigures-results.json,algorithms,Y07-1025
Y07-1036.pdf-Figure3.png,Figure 3: PCE algorithm.,"[u', and the description is given in Figure 3 . Starting with two seed patterns, PCE extracted person, category tuples. The extracted tuples were']",Y07-1036deepfigures-results.json,algorithms,Y07-1036
Y07-1036.pdf-Figure4.png,"Figure 4: person, category extraction from a match.","[u'the above regular expression, the person, category tuple is extracted according to the algorithm in Figure 4 , where is_valid(category) is a function that returns true if category is a sort of', u'. we modify the procedure in figure 4 to extract ne, category, related-to, object quadruples instead of ne, category tuples.', u"". let category_str be the string containing the category, the regular expressions corresponding to each case are (we omit pos and chunk tags for readability): figure 4 ), if the current processing sentence matches one of the above regular expressions, the object is produced by removing pos tags in noun_phrase; related-to is the preposition after removing pos tags in cases a) and c); related-to is 'of' in case b), then, ne, category, related-to, object quadruples are returned instead of tuples."", u'. in the preprocessing step of the algorithm in figure 4 , all nes in this plain text collection were tagged by lingpipe 3 .']",Y07-1036deepfigures-results.json,algorithms,Y07-1036
Y07-1036.pdf-Figure5.png,Figure 5: Pattern generation procedure.,"[u'These two conditions are used in the pattern generation procedure as described in Figure 5 . In the experiments, for the simplity, threshold R and threshold D are set to']",Y07-1036deepfigures-results.json,algorithms,Y07-1036
Y08-1050.pdf-Figure2.png,Figure 2: The pseudo-code for analyzing UM verbs.,"[u'does not exist, then it must be a misspelled word or an invalid Filipino verb. Figure 2 shows the pseudo-code for analyzing ""UM-"" verbs (active) and']",Y08-1050deepfigures-results.json,algorithms,Y08-1050
Y08-1050.pdf-Figure3.png,Figure 3: The pseudo-code for analyzing -IN verbs.,"[u'shows the pseudo-code for analyzing ""UM-"" verbs (active) and Figure 3 shows the pseudo-code for analyzing ""-IN"" verbs (passive).']",Y08-1050deepfigures-results.json,algorithms,Y08-1050
Y09-1022.pdf-Figure2.png,Figure 2: Outline of the process used to train our model.,"[u'. our learning process is shown in figure 2 .', u'In the first stage of the learning process (Step (1) in Figure 2 ), we word segment and PoS tag the review articles in training data. For example,', u'. in the second stage of the training (step (2) in figure 2 ), we transform review articles into collections of features.']",Y09-1022deepfigures-results.json,algorithms,Y09-1022
Y09-1022.pdf-Figure3.png,Figure 3: Transformation algorithm.,"[u'. figure 3 shows the transformation algorithm.', u'tuned, for a given review article, features(RE) is acquired using the review-to-feature transformation algorithm in Figure 3 . Inspired by']",Y09-1022deepfigures-results.json,algorithms,Y09-1022
Y09-1023.pdf-Figure1.png,Figure 1: Preprocessing the parsed text extracted from ICE-GB,"[u'.tre files), the output of the search function from ice-gb, are shown in the left-hand side in figure 1 .']",Y09-1023deepfigures-results.json,algorithms,Y09-1023
Y09-1040.pdf-Figure2.png,Figure 2: Outline of the training process.,"[u'. our learning process is shown in figure 2 .', u'. in the first stage of the learning process (step (1) in figure 2 ), we propagate translations of each word sense (, synsets) to its inherited hypernyms (, ancestors) in wordnet.', u'. then, the word translation classification (wtc) models described in the following stage (step (2) in figure 2 ) can exploit this information to learn to classify (1) propagate translations to generate the training data (2) train hierarchical word translation classification models (3) train filtering model translations into appropriate senses.', u'. in the second stage of the learning algorithm (step (2) in figure 2 ), we train translation classification models for branching synsets with more than one direct hyponym in wordnet.', u'in Figure 2 are utilized to train the filtering model. The filtering model, a ME-based classification model, estimates']",Y09-1040deepfigures-results.json,algorithms,Y09-1040
Y09-1040.pdf-Figure3.png,Figure 3: Algorithm of translation propagation.,[u'. figure 3 shows the algorithm for propagating a translation in the wordnet hierarchy.'],Y09-1040deepfigures-results.json,algorithms,Y09-1040
Y09-1040.pdf-Figure4.png,Figure 4: Run-time classification algorithm.,[u'. we associate adequate senses with given translations using the procedure in figure 4 .'],Y09-1040deepfigures-results.json,algorithms,Y09-1040
Y09-2004.pdf-Figure1.png,Figure 1: The task of sense guessing,"[u'. the assigned semantic category is chosen from a predefined set of semantic categories (figure 1) .', u'. __________________________________________________________________ figure 1 : the task of sense guessing secondly, a pos-sense association model is developed by a sequence-labeling method.']",Y09-2004deepfigures-results.json,algorithms,Y09-2004
Y09-2004.pdf-Figure2.png,Figure 2: The baseline method,[u'The former two models are combined together (see Figure 2 ).'],Y09-2004deepfigures-results.json,algorithms,Y09-2004
Y09-2004.pdf-Figure3.png,Figure 3: The HPPS Model,"[u'According to the above observations, HPPS Model is designed as shown in Figure 3 . SS Model is running first. For words which SS Model gives no guess, give']",Y09-2004deepfigures-results.json,algorithms,Y09-2004
Y09-2051.pdf-Figure2.png,Figure 2: Argument Identification Module,[u'. the argument identifier is summarized in figure 2 .'],Y09-2051deepfigures-results.json,algorithms,Y09-2051
Y09-2051.pdf-Figure3.png,Figure 3: Argument Classification Module,[],Y09-2051deepfigures-results.json,algorithms,Y09-2051
Y10-1001.pdf-Figure1.png,"Figure 1: A positive spiral leading towards greater use of speech recognition (through repetition of steps (i), (ii), and (iii)).","[u'. to achieve this, we proposed setting into motion a positive spiral as explained in figure 1 .', u'2.0 aims to change the usage of speech recognition by setting the positive spiral of Figure 1 into motion.', u'In the following, we discuss how the beneficial spiral of Figure 1 can be put into effect while also explaining the points in', u'. in this way, it promotes understanding of speech recognition performance (step (i) in figure 1 ).', u'. in this way, users can contribute to improved performance (step (ii) in figure 1 ).', u'. in this way, we can use the wisdom of crowds to achieve a better user experience (step (iii) in figure 1 ).', u'. in this project, our goal is to set the positive spiral of figure 1 into motion by providing the podcastle web service which is based on the concepts of both web 2.0 and speech recognition research 2.0.', u'and set the positive spiral of Figure 1 into motion. Specifically, the searching and reading functions let users better understand the speech recognition', u'. specifically, the searching and reading functions let users better understand the speech recognition performance regarding podcasts (step (i) in figure 1 ), and the annotating (error correction) function allows them to contribute to improved performance (step (ii)).']",Y10-1001deepfigures-results.json,algorithms,Y10-1001
Y10-1019.pdf-Figure2.png,Figure 2: Basic Steps of GA,"[u'. the basic steps of the proposed approach, that closely follow those of the conventional ga, are shown in figure 2 .']",Y10-1019deepfigures-results.json,algorithms,Y10-1019
Y10-1036.pdf-Figure2.png,Figure 2: The FTL based term extraction approach,"[u'. the details of the ftl based term extraction approach are shown in figure 2 .', u'2 from L 2 ; 19. Add instances from L 2 to S 2 ; Figure 2 : The FTL based term extraction approach']",Y10-1036deepfigures-results.json,algorithms,Y10-1036
Y10-1040.pdf-Figure2.png,Figure 2. Outline of the GRASP Preprocessing.,"[u'. our preprocessing process is shown in figure 2 .', u'. in the second stage of the preprocessing process (step (2) in figure 2 ), we generate the shallow parsing result for each sentence.']",Y10-1040deepfigures-results.json,algorithms,Y10-1040
Y10-1040.pdf-Figure3.png,Figure 3. Evaluating Patterns at Run-Time.,"[u'GRASP retrieves and evaluates the grammar-based context of the search phrase using the procedure in Figure 3 .', u'. figure 3 .']",Y10-1040deepfigures-results.json,algorithms,Y10-1040
Y14-1034.pdf-Figure2.png,Figure 2: An example TakeTwo session and results,[u'. an example taketwo alignment for example (1) is shown in figure 2 .'],Y14-1034deepfigures-results.json,algorithms,Y14-1034
Y14-1034.pdf-Figure3.png,Figure 3: Ouline of the process to train the TakeTwo system.,[u'. our learning process is shown in figure 3 .'],Y14-1034deepfigures-results.json,algorithms,Y14-1034
Y14-1040.pdf-Figure2.png,Figure 2: Procedure for learning RBM.,"[u'. if we assume totally e epochs are performed for learning n training data using cd-k, the procedure for learning rbm can be given as in figure 2 .', u'inputs by the procedure for learning RBM (Figure 2 ) and fix its weights and offsets.', u'. train rbm 2 with the samples of the hidden layer of rbm 1 as inputs by the procedure for learning rbm (figure 2 ) and fix its weights and offsets.', u'. train rbm 3 with the samples of the hidden layer of rbm 2 as inputs by the procedure for learning rbm (figure 2 ) and fix its weights and offsets.']",Y14-1040deepfigures-results.json,algorithms,Y14-1040
Y14-1040.pdf-Figure4.png,Figure 4: Procedure for learning DBN with three RBMs.,[u'The procedure for learning the DBN with three RBMs is shown in Figure 4 .'],Y14-1040deepfigures-results.json,algorithms,Y14-1040
Y14-1040.pdf-Figure5.png,Figure 5: Baseline algorithm.,"[u'. the algorithm is shown in figure 5, where the words used for counting are those extracted from the descriptive texts in accordance with steps (1)-(4) in subsection 2.4.', u'. count the correct predicting results and compute the correct rate (precision) figure 5 : baseline algorithm.']",Y14-1040deepfigures-results.json,algorithms,Y14-1040
Y14-1041.pdf-Figure2.png,Fig 2. Pseudo code for Modified Edit Distance Algorithm,[],Y14-1041deepfigures-results.json,algorithms,Y14-1041
Y14-1062.pdf-Figure2.png,Figure 2. Outline of the process used to train the KEA system.,"[u'. figure 2 .', u'. our learning process is shown in figure 2 .', u""In the second stage of the learning algorithm (Step (2) in Figure 2 ), we estimate words' keyness in Art o and Arts based on PageRank, or specifically"", u'. tf*idf scores of step (1) in figure 2 are used for this purpose and denoted by kwprefs.', u'In the final stage of training (Step (3) in Figure  2 ), we combine word keyness scores from TF*IDF and semantic PageRank. Note that to gather']",Y14-1062deepfigures-results.json,algorithms,Y14-1062
Y14-1062.pdf-Figure3.png,Figure 3. Evaluating word keyness via semantic PageRank.,"[u'. figure 3 shows the algorithm for deriving keyness scores for article words.', u'of Figure 3 sets the one-by-v matrix KP, Step']",Y14-1062deepfigures-results.json,algorithms,Y14-1062
Y14-1062.pdf-Figure4.png,Figure 4. Aligning sentences at run-time.,[u'. kea then aligns sentences of given monolingual parallel articles using the procedure in figure 4 .'],Y14-1062deepfigures-results.json,algorithms,Y14-1062
Y14-1062.pdf-Figure5.png,Figure 5. Alignment results of a testing article pair done by (a) TF*IDF (b) KEA.,"[u"". figure 5 shows a testing article pair's sentence alignment results done by tf*idf and kea.""]",Y14-1062deepfigures-results.json,algorithms,Y14-1062
Y15-1.pdf-Figure12.png,Figure 12: Syntactico-semantic links of the,[u'. figure 12 demonstrates the unwinding of the last step of the proposed approach.'],Y15-1deepfigures-results.json,algorithms,Y15-1
Y15-1011.pdf-Figure3.png,Figure 3. The CNGRAM procedure,"[u'. figure 3 lists the steps of our main procedure, constrained n-grams (cngram), for ner.']",Y15-1011deepfigures-results.json,algorithms,Y15-1011
Y18-1087.pdf-Figure3.png,Figure 3. Training algorithm.,[],Y18-1087deepfigures-results.json,algorithms,Y18-1087
Y18-1087.pdf-Figure4.png,Figure 4. Updating embedding algorithm.,[],Y18-1087deepfigures-results.json,algorithms,Y18-1087
Y18-2004.pdf-Figure2.png,Figure 2: Lexical relations obtained from the model using the function most similar,[],Y18-2004deepfigures-results.json,algorithms,Y18-2004
1994.amta-1.18.pdf-Figure1.png,Figure 1: Design for the USC/ISI Japanese-English MT system.,"[u'. figure 1 illustrates the current japanese-english mt system, including all processing modules and knowledge resources.', u'. hax parses according to a context-free figure 1 : design for the usc/isi japanese-english mt system.', u'. (the glosser is shown in the center of figure 1 .']",1994.amta-1.18deepfigures-results.json,architecture diagram,1994.amta-1.18
1994.amta-1.6.pdf-Figure1.png,Figure 1: ILustrate Design: Lexical Component View,[u'. figure 1 : ilustrate design: lexical component view this class of entries and their scoping domains will need to be defined with respect to algorithms in that component.'],1994.amta-1.6deepfigures-results.json,architecture diagram,1994.amta-1.6
1996.amta-1.1.pdf-Figure3.png,Fig. 3. Concepts linked in the ReVerb translation memory.,"[u"". since a case must be retrieved from memory on the basis of its string/word content, each such word is defined in reverb's frame-structured memory, providing pointers to the chunks and cases in which it is employed; this situation is illustrated in figure 3 below.""]",1996.amta-1.1deepfigures-results.json,architecture diagram,1996.amta-1.1
1996.amta-1.1.pdf-Figure6.png,Fig. 6. Retrieval levels in ReVerb.,[u'can act as a useful filter for subsequent structural retrieval. This arrangement is illustrated in Figure 6 . We shall now consider each of these levels in turn:'],1996.amta-1.1deepfigures-results.json,architecture diagram,1996.amta-1.1
1996.amta-1.17.pdf-Figure1.png,Figure 1: The ZARDOZ Blackboard Architecture: A communication medium for diverse knowledge agencies.,"[u'A process-oriented view of the system is illustrated in Figure 1 , which presents the blackboard as compartmentalised into distinct panels. Task-specific knowledge agencies (composed of', u'. taking a clockwise tour around figure 1 , system operation proceeds as follows: (i) the incoming text stream is processed by a swarm of lexperts-lexical experts specified as autonomous 170 demons-which individually implement both morphological rules and heuristic measures for recognising and representing compound word constructs (, ""laptop"" can be decomposed into lap + computer and signed accordingly, by comparison with the word ""desktop"", which is known to the system as a form of computer).']",1996.amta-1.17deepfigures-results.json,architecture diagram,1996.amta-1.17
1996.amta-1.9.pdf-Figure1.png,Figure 1,[u'. figure 1 shows the architecture of the earliest logos english-source system.'],1996.amta-1.9deepfigures-results.json,architecture diagram,1996.amta-1.9
1996.amta-1.9.pdf-Figure2.png,Figure 2,[u'. figure 2 shows the architecture of logos during its second phase.'],1996.amta-1.9deepfigures-results.json,architecture diagram,1996.amta-1.9
1996.amta-1.9.pdf-Figure3.png,Figure 3,"[u'. figure 3 illustrates the third-generation system architecture.', u'. this module creates a ""macro parse"" showing the clause figure 3 structure of the sentence and uses this top-down information to complete the homograph resolution process begun in res1.', u'. it is easy to bias the source parse in favor of one target or another; within the system shown in figure 3 , one might ask if it is possible to have a pure source parse.']",1996.amta-1.9deepfigures-results.json,architecture diagram,1996.amta-1.9
1996.amta-1.9.pdf-Figure5.png,Figure 5,"[u'The fourth-generation Logos system, shown in Figure 5 , is currently being developed.']",1996.amta-1.9deepfigures-results.json,architecture diagram,1996.amta-1.9
2007.sigdial-1.41.pdf-Figure1.png,Figure 1: System Architecture,"[u'of the system are presented below. Sections 2, 3, and 4 describe the annotation client, Figure 1 : System Architecture the web-accessible data repository, and the portal to the TeraGrid, respectively, as', u'Architecture the web-accessible data repository, and the portal to the TeraGrid, respectively, as shown in Figure 1 below. Section 6 describes system availability and planned extensions to system functionality.']",2007.sigdial-1.41deepfigures-results.json,architecture diagram,2007.sigdial-1.41
2016.jeptalnrecital-jep.38.pdf-Figure1.png,FIGURE 1  Utilisation de paramtres drivs de GMMs adapts pour une apprentissage SAT de DNN-HMM.,[],2016.jeptalnrecital-jep.38deepfigures-results.json,architecture diagram,2016.jeptalnrecital-jep.38
2016.lilt-13.3.pdf-Figure2.png,FIGURE 2 General SMT system architecture.,[u'. this process is depicted in figure 2 .'],2016.lilt-13.3deepfigures-results.json,architecture diagram,2016.lilt-13.3
2017.jeptalnrecital-court.21.pdf-Figure1.png,FIGURE 1  Mthodologie dvaluation.,[],2017.jeptalnrecital-court.21deepfigures-results.json,architecture diagram,2017.jeptalnrecital-court.21
2017.jeptalnrecital-court.27.pdf-Figure1.png,"FIGURE 1  Le modle de traduction gnrique est appris sur les donnes gnriques, puis, ce modle gnrique est r-entrain avec des donnes spcifiques au domaine afin dobtenir un modle adapt au domaine (ou  la tche).",[],2017.jeptalnrecital-court.27deepfigures-results.json,architecture diagram,2017.jeptalnrecital-court.27
2017.jeptalnrecital-court.29.pdf-Figure1.png,FIGURE 1  Processus de slection des articles pour une revue systmatique.,[],2017.jeptalnrecital-court.29deepfigures-results.json,architecture diagram,2017.jeptalnrecital-court.29
2017.jeptalnrecital-recital.3.pdf-Figure4.png,Figure 4  Reprsentation de lhritage sur characterize-29.2,[],2017.jeptalnrecital-recital.3deepfigures-results.json,architecture diagram,2017.jeptalnrecital-recital.3
2018.jeptalnrecital-court.14.pdf-Figure1.png,FIGURE 1  Le principe de fonctionnement de notre systme,[u'La Figure 1 donne une vue gnrale de la mthode propose :'],2018.jeptalnrecital-court.14deepfigures-results.json,architecture diagram,2018.jeptalnrecital-court.14
2018.jeptalnrecital-demo.7.pdf-Figure1.png,FIGURE 1  Architecture globale de lOntoNotes Release 5.0 aprs lajout des correspondances pour les 261 lemmes uniques de leurs sens vers ceux du Princeton WordNet 3.0,[],2018.jeptalnrecital-demo.7deepfigures-results.json,architecture diagram,2018.jeptalnrecital-demo.7
2018.jeptalnrecital-long.7.pdf-Figure2.png,FIGURE 2  Analyse en dpendances (en haut) et reprsentation smantique (en bas) dun nonc du corpus ralises grce au parser Grew,"[u'. figure 2 , l\'expert prononce l\'nonc ""la dame qui est sur le tableau"".']",2018.jeptalnrecital-long.7deepfigures-results.json,architecture diagram,2018.jeptalnrecital-long.7
2018.jeptalnrecital-recital.5.pdf-Figure1.png,FIGURE 1  Architecture gnrale dun systme de rsum guid,[],2018.jeptalnrecital-recital.5deepfigures-results.json,architecture diagram,2018.jeptalnrecital-recital.5
2019.jeptalnrecital-deft.8.pdf-Figure2.png,FIGURE 2  Architecture de la prdiction de liaison entre cas et discussion,[],2019.jeptalnrecital-deft.8deepfigures-results.json,architecture diagram,2019.jeptalnrecital-deft.8
2019.jeptalnrecital-deft.8.pdf-Figure4.png,FIGURE 4  Architecture tache 1.  et  sont des paramtres libres.  dnote la fonction sigmode. Les vecteurs en gris sont des paramtres appris,[],2019.jeptalnrecital-deft.8deepfigures-results.json,architecture diagram,2019.jeptalnrecital-deft.8
2019.jeptalnrecital-recital.7.pdf-Figure1.png,FIGURE 1  Schma typique du systme de dialogue,[],2019.jeptalnrecital-recital.7deepfigures-results.json,architecture diagram,2019.jeptalnrecital-recital.7
2019.jeptalnrecital-recital.7.pdf-Figure3.png,FIGURE 3  Modle classique de mmoire humaine,[],2019.jeptalnrecital-recital.7deepfigures-results.json,architecture diagram,2019.jeptalnrecital-recital.7
2020.acl-demos.1.pdf-Figure5.png,Figure 5: Avatar animation synthesis: a) multi-lingual voices are cloned. b) A sequence of phonemes and their duration is drawn from the voices. c) A sequence of blendshape weights is transformed by a neural network model. d) Lip-motion is synthesized and re-targeted synchronously to avatar animation.,[],2020.acl-demos.1deepfigures-results.json,architecture diagram,2020.acl-demos.1
2020.acl-demos.11.pdf-Figure3.png,Figure 3: The architecture of GAIA multimedia knowledge extraction.,"[u'The architecture of our multimedia knowledge extraction system is illustrated in Figure 3 . The system pipeline consists of a Text Knowledge Extraction (TKE) branch and a Visual', u'As shown in Figure 3 , the Text Knowledge Extraction (TKE) system extracts entities, relations, and events from input documents.']",2020.acl-demos.11deepfigures-results.json,architecture diagram,2020.acl-demos.11
2020.acl-demos.14.pdf-Figure1.png,"Figure 1: Overview of Sta n z a s neural NLP pipeline. Sta n z a takes multilingual text as input, and produces annotations accessible as native Python objects. Besides this neural pipeline, Sta n z a also features a Python client interface to the Java CoreNLP software.","[u'models that range from tokenizing raw text to performing syntactic analysis on entire sentences (see Figure 1 ). All components are designed with processing many human languages in mind, with high-level design', u'. within a document, annotations are further stored in sentences, tokens and words in a top-down fashion (figure 1 ).']",2020.acl-demos.14deepfigures-results.json,architecture diagram,2020.acl-demos.14
2020.acl-demos.15.pdf-Figure2.png,"Figure 2: jiant pipeline stages using RoBERTa as the sentence encoder, ReCoRD and MNLI tasks as intermediate tasks, and MNLI and BoolQ as tasks for target training and evaluation. The diagram highlights that during target training and evaluation phases, copies are made of the sentence encoder model, and fine tuning and evaluation for each task are conducted on separate copies.",[],2020.acl-demos.15deepfigures-results.json,architecture diagram,2020.acl-demos.15
2020.acl-demos.16.pdf-Figure1.png,"Figure 1: The workflow of MT-DNN: train a neural language model on a large amount of unlabeled raw text to obtain general contextual representations; then finetune the learned contextual representation on downstream tasks, e.g. GLUE (Wang et al., 2018); lastly, distill this large model to a lighter one for online deployment. In the later two phrases, we can leverage powerful multi-task learning and adversarial training to further improve performance.","[u'them to small ones for online deployment.The overall workflow and system architecture are shown in Figure 1 ', u'As shown in Figure 1 , starting from the neural language model pre-training, there are three different training configurations by', u'Adversarial Training Figure 1 : The workflow of MT-DNN: train a neural language model on a large amount of']",2020.acl-demos.16deepfigures-results.json,architecture diagram,2020.acl-demos.16
2020.acl-demos.18.pdf-Figure1.png,"Figure 1: The system architecture of CLIReval. Documents from input files are separately indexed into two instances of IR systems. Generated search queries are used to query both IR instances. Search scores from REF-IR are converted to discrete relevance judgment labels as required by trec eval. Finally, CLIReval uses trec eval to calculate IR metrics.","[u'. finally, we run the queries q through this mt-ir system to obtain document scores (q, t ) ( figure 1 , left branch), which can be evaluated with respect to the relevance labels.']",2020.acl-demos.18deepfigures-results.json,architecture diagram,2020.acl-demos.18
2020.acl-demos.2.pdf-Figure1.png,Figure 1: (a) An overview of the main functionalities of TextBrewer. (b) A sketch that shows the function of adaptors inside a distiller.,"[u'. to support different models and different tasks and meanwhile stay flexible and extensible, textbrewer provides distillers to conduct the actual experiments and configuration classes to configure the behaviors of the distillers.', u'. each key explains the meaning of the corresponding value, as shown in figure 1 (b) .']",2020.acl-demos.2deepfigures-results.json,architecture diagram,2020.acl-demos.2
2020.acl-demos.24.pdf-Figure1.png,"Figure 1: PHOTON workflow. The question corrector (upper block) detects the untranslatable questions from user input, scans the confusion span(s) that need clarification or correction. The accepted question is mapped into a SQL query through a text-to-SQL model, and finally the SQL execution results are returned to the user.","[u' Figure 1 : PHOTON workflow. The question corrector (upper block) detects the untranslatable questions from user input,', u'. figure 1 shows the overall workflow of our system.']",2020.acl-demos.24deepfigures-results.json,architecture diagram,2020.acl-demos.24
2020.acl-demos.24.pdf-Figure2.png,"Figure 2: State transition map of interaction in PHOTON. States with darker background are the end states that can receive user reply, and switch to INIT state automatically. The bottom part is the system response templates in each end state.",[u'. figure 2 .'],2020.acl-demos.24deepfigures-results.json,architecture diagram,2020.acl-demos.24
2020.acl-demos.25.pdf-Figure1.png,Figure 1: An example dialog structure while SUGILITE learns a new task that contains a conditional and new concepts. The numbers indicate the sequence of the utterances. The screenshot on the right shows the conversational interface during these steps.,"[u'The system should handle a Figure 1 : An example dialog structure while SUGILITE learns a new task that contains a conditional', u'. although it does not know these concepts, it can recognize the structure of the command (, conditional), and parse each part of the command into the corresponding typed resolve functions, as shown in figure 1 .', u'. for example, for utterance 8 in figure 1 , the user does not need to demonstrate again since the system can invoke the newlylearned order starbucks function with a different parameter value (details in section 3.3).', u'. for example, the concept ""hot"" used in figure 1 can be generalize to ""arg0 is greater than arg1"" where arg0 and arg1 can be value queries or constant values of the temperature type.', u'Similarly, named value queries (resolved from resolveValue such as ""temperature"" in Figure 1) can be generalized to have different implementations depending on the task domain. In ""the temperature']",2020.acl-demos.25deepfigures-results.json,architecture diagram,2020.acl-demos.25
2020.acl-demos.25.pdf-Figure4.png,Figure 4: SUGILITEs instruction parsing and grounding process for intent clarifications illustrated on an example UI snapshot graph constructed from a simplified GUI snippet.,"[u"". this graph captures the gui elements' text labels, metainformation (including screen position, type, and package name), and the spatial (, nextto), hierarchical (, haschild), and semantic relations (, containsprice) among them (figure 4) .""]",2020.acl-demos.25deepfigures-results.json,architecture diagram,2020.acl-demos.25
2020.acl-demos.29.pdf-Figure2.png,"Figure 2: The main editor window, showing a story with start node A. The node B is selected, and so its details appear in the inspector on the right. Note that its display name is distinct from the richer game text (C). The results of using the Node Tester on the potential input battle the beast are shown in (D). They demonstrate that this utterance would cause a transition to node E, an AutoAdvance node that would immediately transition to one of two end states depending on the players previous choices.","[u'The home screen of the tool (Figure 2) consists of two panels. On the left is the graph editor, which vi- sualizes and', u'. figure 2 : the main editor window, showing a story with start node a.', u'node tester, found in the graph inspector when focused on a node (lower right in Figure 2 ). This tool allows the author to probe with a potential utterance and view the', u'. we hope that allowing flexibility in the storage medium figure 2 .']",2020.acl-demos.29deepfigures-results.json,architecture diagram,2020.acl-demos.29
2020.acl-demos.34.pdf-Figure2.png,Figure 2: All-in-one process pipelines in ESPnet-ST,[],2020.acl-demos.34deepfigures-results.json,architecture diagram,2020.acl-demos.34
2020.acl-demos.8.pdf-Figure1.png,Figure 1: System architecture of EVIDENCEMINER.,"[u'. this paper presents evidenceminer, a webbased system for textual evidence discovery for life sciences (figure 1 ).', u'. figure 1 shows the system architecture of evidenceminer.']",2020.acl-demos.8deepfigures-results.json,architecture diagram,2020.acl-demos.8
2020.acl-main.112.pdf-Figure1.png,"Figure 1: Schematic view on the methodology for generating and evaluating an emotion lexicon for a given target language based on source language supervision. Included is a toy example starting with an English VA lexicon (sunshine, nuclear, terrorism and the associated numerical scores for Valence and Arousal) and resulting in an extended German lexicon which incorporates translated entries with altered VA scores and additional entries originating from the embedding model with newly learned scores.","[u'. figure 1 gives an overview of our framework including a toy example for illustration.', u'. figure 1 ).', u'. our rationale for doing so is that a reasonably trained model should generalize well figure 1 : schematic view on the methodology for generating and evaluating an emotion lexicon for a given target language based on source language supervision.', u'. words, such as german sonnenschein for which a translational equivalent already exists in the source (""sunshine""; see figure 1 ), mainly rely on translation, while the prediction step acts as an optional refinement procedure.', u'the model already had access during training (e.g., Sonnenschein and nuklear in our example from Figure 1) ; but, on the other hand, (2) a reasonable number of instances must be available']",2020.acl-main.112deepfigures-results.json,architecture diagram,2020.acl-main.112
2020.acl-main.93.pdf-Figure2.png,"Figure 2: This figure explains the differences between an error and different concerns when mismatches occur. We show an image as nine parts grid chart (3  3). In error case, caption1 focuses on three parts while b3 cannot represent the part that matches a1. In different-concerns cases, two captions can represent well in different parts of the same image. However, a mismatch occurs since a3 and b3 attend to a different part of the image.",[u'. we introduce these two concepts in figure 2 .'],2020.acl-main.93deepfigures-results.json,architecture diagram,2020.acl-main.93
N10-1089.pdf-Figure1.png,Figure 1: Evaluation process,"[u'. this is the evaluation procedure that we chose to follow, and is shown in figure 1 .']",N10-1089deepfigures-results.json,architecture diagram,N10-1089
N10-1113.pdf-Figure1.png,Figure 1: System Architecture,[u'. figure 1 shows the architecture of our system.'],N10-1113deepfigures-results.json,architecture diagram,N10-1113
N15-1102.pdf-Figure1.png,Figure 1: Dialog system used in this work.,[u'. a schematic representation is shown in figure 1 .'],N15-1102deepfigures-results.json,architecture diagram,N15-1102
N19-1232.pdf-Figure2.png,"Figure 2: Gradients estimation in stochastic computation graphs (1) Gumbel-Softmax trick (2) The StraightThrough estimator, used for Bernoulli discrete variables, skips stochastic node by approximating dzdg  1","[u'. in general, the solution to this issue is using the straight-through (st) estimator where a biased path derivative estimator can be utilized even when z is not reparameterizable as shown in figure 2 .', u'. however, in figure 2 : gradients estimation in stochastic computation graphs (1) gumbel-softmax trick (2) the straight-through estimator, used for bernoulli discrete variables, skips stochastic node by approximating dz dg   1 backpropagation, we cannot pass gradient through the hard threshold operator, arg max, since it is non-differentiable.']",N19-1232deepfigures-results.json,architecture diagram,N19-1232
N19-1369.pdf-Figure1.png,Figure 1: Overview of Chrono Workflow,"[u'. chrono consists of 3 main modules: 1) temporal phrase extraction, 2) scate normalization, and 3) temporal disambiguation (figure 1) .']",N19-1369deepfigures-results.json,architecture diagram,N19-1369
N19-1384.pdf-Figure2.png,Figure 2: The framework for extracting partial translations from monolingual data.,"[u'The whole framework for extracting partial translations is presented in Figure 2 . To extract partial translations, we first induce a phrase table that contains phrases in']",N19-1384deepfigures-results.json,architecture diagram,N19-1384
N19-4018.pdf-Figure2.png,"Figure 2: SkillBot Architecture & Workflow: When learning via demonstration (green arrows), automatic action fulfillment (runs as a system service on each users device) first tracks and captures system level event sequence based on user operations. It outputs a bytecode file with learned event sequence for this skill and saves in database for future execution. After each user input operation, automatic NLU development identifies the list of possible slot descriptions for user to select and then generates more training utterances by leveraging existing training utterances in pre-built skills. At last, the NLU engine is updated using all generated utterances. When executing an existing skill (orange arrows), based on the parsed skill from NLU, the corresponding bytecode is retrieved to automatically execute all saved steps.",[u'. figure 2 shows skillbot architecture and work flow.'],N19-4018deepfigures-results.json,architecture diagram,N19-4018
N19-4018.pdf-Figure3.png,Figure 3: New Skill Learning in SkillBot,"[u'Action learning module has two main threads, as shown in the left part of Figure 3 . Following user demonstration, at each screen, screen UI element extractor collects all UI elements', u'invoked after each user input during demonstration. As shown in the middle part (blue) of Figure 3 , after user inputs ""San Jose"" in the example of']",N19-4018deepfigures-results.json,architecture diagram,N19-4018
N19-4019.pdf-Figure1.png,Figure 1: System overview.,"[u'. we develop a novel framework, as illustrated in figure 1 , to aggregate knowledge elements from multiple documents in multiple languages and visualize these knowledge elements in three interfaces (temporal, spatial, and entityrelation networks) which support effective multidimensional search and filtering.']",N19-4019deepfigures-results.json,architecture diagram,N19-4019
N19-4023.pdf-Figure1.png,"Figure 1: Platform Architecture, MultiBot Testing Scenario","[u'. figure 1 ) passes the user utterance to a natural language understanding (nlu) and dialogue management (dm) process.', u'. when robot actions are completed, a text message signal is sent that may either be converted from text to speech, as depicted in figure 1 , or shown in a chat window if the environment is noisy or if stealth is desired.']",N19-4023deepfigures-results.json,architecture diagram,N19-4023
N19-4023.pdf-Figure2.png,Figure 2: Behavior tree for scout instruction,"[u'. the output of the behaviors are any discovered objects of interest and the resulting state of the robot, such as position, and are provided back to the human figure 2 : behavior tree for ""scout"" instruction operator through the speech synthesis previously described.', u'. the ""scout"" instruction is presented in figure 2 .']",N19-4023deepfigures-results.json,architecture diagram,N19-4023
O00-1.pdf-Figure4.png,Fig 4. The work flow of the session clustering process.,[],O00-1deepfigures-results.json,architecture diagram,O00-1
O00-1006.pdf-Figure4.png,Fig 4. The work flow of the session clustering process.,[],O00-1006deepfigures-results.json,architecture diagram,O00-1006
O01-1005.pdf-Figure1.png,Fig. 1. Overall scheme for developing the HMM-based recognition models using modified SAMPA-C,[],O01-1005deepfigures-results.json,architecture diagram,O01-1005
O01-1011.pdf-Figure2.png,Figure 2: System architecture,[u'. the system architecture is shown in figure 2 .'],O01-1011deepfigures-results.json,architecture diagram,O01-1011
O01-2001.pdf-Figure1.png,Figure 1 The flowchart of constructing the dependency triple database.,[],O01-2001deepfigures-results.json,architecture diagram,O01-2001
O01-2003.pdf-Figure4.png,Figure 4 Flowchart of the bottom-up hierarchical approach for detecting boundaries of prosody constituents.,"[u'. figure 4 shows the flowchart of the hierarchical approach.', u'The three CARTs shown in Figure 4 were trained separately from the same training set. Only feature set 1 was used. The']",O01-2003deepfigures-results.json,architecture diagram,O01-2003
O01-2004.pdf-Figure7.png,Figure 7 System Architecture.,[u'.  an overview of the machine translation system with auto acquired translation templates is shown in figure 7 .'],O01-2004deepfigures-results.json,architecture diagram,O01-2004
O03-2.pdf-Figure2.png,Fig. 2: Schematic Representation of Conversation,[],O03-2deepfigures-results.json,architecture diagram,O03-2
O02-1.pdf-Figure4.png,Figure 4 System overview of the primitive NVEF sense-pair identifier.,[],O02-1deepfigures-results.json,architecture diagram,O02-1
O02-1002.pdf-Figure4.png,Figure 4 System overview of the primitive NVEF sense-pair identifier.,[],O02-1002deepfigures-results.json,architecture diagram,O02-1002
O02-1002.pdf-Figure5.png,Figure 5 A system overview of the NVEF sense-pair identifier.,[],O02-1002deepfigures-results.json,architecture diagram,O02-1002
O04-1013.pdf-Figure9.png,Fig. 9. Conventional book recommendation process,"[u'. after the check, the librarian needs to type the metadata into the library automation system (figure 9 ).']",O04-1013deepfigures-results.json,architecture diagram,O04-1013
O02-1.pdf-Figure5.png,Figure 5 A system overview of the NVEF sense-pair identifier.,[],O02-1deepfigures-results.json,architecture diagram,O02-1
O03-3.pdf-Figure2.png,Fig. 2: The system architecture.,[],O03-3deepfigures-results.json,architecture diagram,O03-3
O03-1011.pdf-Figure2.png,Figure 2. An illustration of n-gram extractor and CWAC agent,"[u'. in this paper, an n-gram extractor is developed to extract all n-grams (n  2 and n-gram frequency  3) from test sentences as the n-gram input for our cwac agent (see figure 2 ).', u'Online dictionary Figure 2 . An illustration of n-gram extractor and CWAC agent']",O03-1011deepfigures-results.json,architecture diagram,O03-1011
O04-1005.pdf-Figure2.1.png,Fig. 2.1. An overview of our system,[],O04-1005deepfigures-results.json,architecture diagram,O04-1005
O04-1005.pdf-Figure3.1.png,Fig. 3.1. The overview of the candidate generator,[],O04-1005deepfigures-results.json,architecture diagram,O04-1005
O04-1009.pdf-Figure1.png,Figure 1. A system overview of the meaningful word-pair (MWP) identifier.,"[u', we obtain our meaningful word-pair (MWP) identifier, (Figure 1 ). In', u'). In Figure 1 The algorithm of the MWP identifier is as follows:']",O04-1009deepfigures-results.json,architecture diagram,O04-1009
O04-1013.pdf-Figure3.png,Fig. 3. Three-tier model of WIS,[u'. from figure 3 we can see that wis replaces the position of the web browser.'],O04-1013deepfigures-results.json,architecture diagram,O04-1013
O04-1013.pdf-Figure5.png,Fig. 5. Typical workflow of an application in WIS,[],O04-1013deepfigures-results.json,architecture diagram,O04-1013
O04-2001.pdf-Figure1.png,Figure 1. The major components in the proposed CLASS algorithm.,[],O04-2001deepfigures-results.json,architecture diagram,O04-2001
O04-2002.pdf-Figure1.png,Figure 1. Architecture overview.,[],O04-2002deepfigures-results.json,architecture diagram,O04-2002
O04-3003.pdf-Figure1.png,Figure 1. A schematic diagram of a speech recognition system.,"[u'. as shown in figure 1 , a speech recognition system is composed of syllable-level and word-level matching processes, in which the acoustic model  and language model  are applied, respectively.']",O04-3003deepfigures-results.json,architecture diagram,O04-3003
O04-3004.pdf-Figure3.png,Figure 3. Diagram of textual emotion recognition module.,[u'. figure 3 shows a diagram of the textual emotion recognition module.'],O04-3004deepfigures-results.json,architecture diagram,O04-3004
O04-3005.pdf-Figure2.png,Figure 2. Features extraction algorithm of MBLPCCs,[u'The schematic flow of the proposed feature extraction method is shown in Figure 2 .'],O04-3005deepfigures-results.json,architecture diagram,O04-3005
O04-3005.pdf-Figure3.png,Figure 3. Structure of FCGMM,[u'. the structure of fcgmm is shown in figure 3 .'],O04-3005deepfigures-results.json,architecture diagram,O04-3005
O04-3005.pdf-Figure4.png,Figure 4. Structure of LCGMM,"[u'approach combines the likelihood scores of the independent GMM for each band, as illustrated in Figure 4 . We call this identifier model the likelihood combination Gaussian mixture model (LCGMM). First, the']",O04-3005deepfigures-results.json,architecture diagram,O04-3005
O04-3006.pdf-Figure1.png,Figure 1(B). Illustration of the innovative C-DSR platform.,"[u'. the c-dsr platform is embedded in the wireless network in contrast to conventional speech recognizers that are treated as input interfaces for portable devices (see figure 1 ).', u'Relatively cost high Client Devices Figure 1(B) . Illustration of the innovative C-DSR platform.']",O04-3006deepfigures-results.json,architecture diagram,O04-3006
O04-3006.pdf-Figure2.png,Figure 2. The function blocks of the C-DSR Platform.,[u'The function blocks of the C-DSR development platform are shown in Figure 2 . A wireless device equipped with the C-DSR Client connects to a remote C-DSR Server'],O04-3006deepfigures-results.json,architecture diagram,O04-3006
O04-3006.pdf-Figure3.png,Figure 3. Modules of the C-DSR engine.,[u'and can be configured according to different requests requested from the various types of clients; Figure 3 shows the modules of'],O04-3006deepfigures-results.json,architecture diagram,O04-3006
O04-3006.pdf-Figure4.png,"Figure 4. Diagram of the Dialogue System, DS.",[u'.org) and the simplified voicexml format used to describe dialogue scripts (see figure 4 ).'],O04-3006deepfigures-results.json,architecture diagram,O04-3006
O04-3006.pdf-Figure5.png,Figure 5. Illustration of the C-DSR implementation.,"[u'. now, using the proposed c-dsr solutions, thin client devices can take advantage of powerful, wireless servers to perform sophisticated speech recognition functions (see figure 5 ), including the following:  car agentretrieving map/travel/hotel information through gprs network in a car;  a personal inquiry systema portable device which can retrieve stock/weather information anywhere through a gprs network;  general-purpose remote controlin a wlan 802.11b environment, a remote control which can be used to control a tv, stereo, air-conditioner, etc.']",O04-3006deepfigures-results.json,architecture diagram,O04-3006
O05-2001.pdf-Figure1.png,Figure 1. The overall framework of the NTNU broadcast news system.,[u'. figure 1 depicts the overall framework of the broadcast news system.'],O05-2001deepfigures-results.json,architecture diagram,O05-2001
O05-2003.pdf-Figure1.png,Figure 1. Ontology construction framework,[u'. figure 1 shows a block diagram of the ontology construction process.'],O05-2003deepfigures-results.json,architecture diagram,O05-2003
O05-3006.pdf-Figure1.png,Figure 1. The development flow of the BRCC,[u'The development flow is shown in Figure 1 . We pre-processed the raw data in two steps:'],O05-3006deepfigures-results.json,architecture diagram,O05-3006
O05-3007.pdf-Figure1.png,Figure 1. The domain ontology for the news category Political,"[u'class, so the structure of the Concept Set can be treated as a class diagram. Figure 1 shows an example for our Chinese Political news domain ontology']",O05-3007deepfigures-results.json,architecture diagram,O05-3007
O05-4.pdf-Figure5.png,Figure 5. Main components of our cloze-item generator,"[u""We create cloze items in two major steps as shown in Figure 5 . Constrained by the administrator's Item Specification and Target-Dependent Item Requirements, the Sentence Retriever selects"", u'The sentence retriever shown in Figure 5 extracts qualified sentences from the corpus. A sentence must contain the desired key with the', u'. an example of the emission probability function ) | p( i i u t for the chunk tagger is shown in figure 5 .', u'. all of the steps are also shown in figure 5 .']",O05-4deepfigures-results.json,architecture diagram,O05-4
O05-4001.pdf-Figure1.png,Figure 1. A functional structure of an intelligent tutoring system,"[u"". figure 1 shows a possible functional structure of the main components of an its that uses test items to assess students' competence levels."", u'Computer-Generated Multiple-Choice Cloze Items As shown in Figure 1 , the quality and quantity of test items are crucial to the success of the', u'As indicated in Figure 1 , a major step in our approach is acquiring sentences from the Web before we']",O05-4001deepfigures-results.json,architecture diagram,O05-4001
O05-4001.pdf-Figure5.png,Figure 5. Main components of our cloze-item generator,"[u""We create cloze items in two major steps as shown in Figure 5 . Constrained by the administrator's Item Specification and Target-Dependent Item Requirements, the Sentence Retriever selects"", u'The sentence retriever shown in Figure 5 extracts qualified sentences from the corpus. A sentence must contain the desired key with the']",O05-4001deepfigures-results.json,architecture diagram,O05-4001
O05-4004.pdf-Figure5.png,Figure 5. Diagram of pronunciation variations obtained with a data-driven approach.,[u'. all of the steps are also shown in figure 5 .'],O05-4004deepfigures-results.json,architecture diagram,O05-4004
O06-1015.pdf-Figure1.png,Figure 1. Flow chart of Single-Pass Clustering,[],O06-1015deepfigures-results.json,architecture diagram,O06-1015
O06-1015.pdf-Figure5.png,Figure 5. Concept of Profile Expansion,[u'. this activity is described in figure 5 .'],O06-1015deepfigures-results.json,architecture diagram,O06-1015
O06-1015.pdf-Figure6.png,Figure 6. NQES Architecture,[u'. figure 6 .'],O06-1015deepfigures-results.json,architecture diagram,O06-1015
O06-1021.pdf-Figure4.png,Figure 4: Flowchart of the proposed training process.,[u'Statement: Given a sentence-aligned corpus The training process can be illustrated using the flowchart in Figure 4 .'],O06-1021deepfigures-results.json,architecture diagram,O06-1021
O06-2002.pdf-Figure2.png,Figure 2. A typical LVCSR system,[],O06-2002deepfigures-results.json,architecture diagram,O06-2002
O06-2003.pdf-Figure1.png,Figure 1. Implementation procedure for ME semantic topic modeling,[u'. the proposed procedure of me semantic topic modeling is illustrated in figure 1 .'],O06-2003deepfigures-results.json,architecture diagram,O06-2003
O06-3002.pdf-Figure2.png,Figure 2. The structure of the MEMM Chinese chunking model,[u'. figure 2 shows the structure of the chinese chunking model based on memm.'],O06-3002deepfigures-results.json,architecture diagram,O06-3002
O06-3004.pdf-Figure1.png,Figure 1. Block diagram of a PPR-LM LID system,"[u'A typical LID system is illustrated in Figure 1 , which shows a collection of parallel phone recognizers (PPR frontend) that serve as voice', u'partitioned into subsets to work for the parallel phone recognition (PPR) frontend as shown in Figure 1@dot ']",O06-3004deepfigures-results.json,architecture diagram,O06-3004
O06-3004.pdf-Figure2.png,Figure 2. Block diagram of a UPR-LM LID system,"[u'. as such, the ppr-lm system can be simplified as the upr-lm system with a universal phone recognition (upr) frontend as shown in figure 2 .']",O06-3004deepfigures-results.json,architecture diagram,O06-3004
O06-3004.pdf-Figure3.png,Figure 3. Block diagram of a PPR-VSM LID system,[u'categorization community to design language classifiers. An LID system with the VSM-backend is shown in Figure 3 for the PPR frontend and in'],O06-3004deepfigures-results.json,architecture diagram,O06-3004
O06-3004.pdf-Figure4.png,Figure 4. Block diagram of a UPR-VSM LID system,[u'for the PPR frontend and in Figure 4 for the UPR frontend. The VSM-backend takes as inputs n-gram statistics in the form of'],O06-3004deepfigures-results.json,architecture diagram,O06-3004
O06-3004.pdf-Figure5.png,Figure 5. Block diagram of four combinations of frontends and backends,"[u'. to gain insight into the behavior of each of the frontends and backends, it is desirable to investigate the performance of each of the four combined systems as shown in figure 5 , namely, ppr-lm, ppr-vsm, upr-lm, and upr-vsm, where the ppr/upr frontends are built on a set of universal asms.']",O06-3004deepfigures-results.json,architecture diagram,O06-3004
O06-4001.pdf-Figure1.png,Figure 1. Stage in transliteration name recognition,[u'The spoken transliteration name recognition system shown in Figure 1 accepts a speech signal denoting a foreign named entity and converts it into a character'],O06-4001deepfigures-results.json,architecture diagram,O06-4001
O03-3001.pdf-Figure2.png,Fig. 2: The system architecture.,[],O03-3001deepfigures-results.json,architecture diagram,O03-3001
1994.amta-1.25.pdf-Figure3.png,Figure 3. Japanese FAMT results,[],1994.amta-1.25deepfigures-results.json,bar charts,1994.amta-1.25
2007.sigdial-1.12.pdf-Figure6.png,Figure 6. ROC Area for n*-best and n-best (n* is represented as n=0),[],2007.sigdial-1.12deepfigures-results.json,bar charts,2007.sigdial-1.12
2007.sigdial-1.12.pdf-Figure7.png,Figure 7. ROC Area for n*-best and other n-best methods (n* is represented as n=0),[u'. figure 7 .'],2007.sigdial-1.12deepfigures-results.json,bar charts,2007.sigdial-1.12
2007.sigdial-1.18.pdf-Figure4.png,Figure 4. Completion ratio (%) per task.,"[u'. figure 4 shows the task completion rates for the various tasks as a function of all users, registered and non-registered users.']",2007.sigdial-1.18deepfigures-results.json,bar charts,2007.sigdial-1.18
2007.sigdial-1.23.pdf-Figure3.png,Figure 3: Comparing High-level Dialog Features.,"[u'. figure 3 graphically compares the means of our high-level dialog features, for both the user and subject dialog corpora.', u'shows the results for speech recognition quality, using scaled mean values as in Figure 3 . There are no statistically significant differences between the number of rejected user turns or']",2007.sigdial-1.23deepfigures-results.json,bar charts,2007.sigdial-1.23
2007.sigdial-1.23.pdf-Figure4.png,Figure 4: Comparing User Dialog Acts.,"[u'. in the figures, the mean values of each measure are scaled according to the mean values of the user corpus, in order to present all of the results on one graph.']",2007.sigdial-1.23deepfigures-results.json,bar charts,2007.sigdial-1.23
2007.sigdial-1.23.pdf-Figure5.png,Figure 5: Comparing System Dialog Acts.,[u'. figure 5) shows the distribution of the user (resp.'],2007.sigdial-1.23deepfigures-results.json,bar charts,2007.sigdial-1.23
2007.sigdial-1.23.pdf-Figure6.png,Figure 6: Comparing Speech Recognition Quality.,"[u'any significant difference in the number of system requests (S requestinfo) or confirmations (S confirm). Figure 6 shows the results for speech recognition quality, using scaled mean values as in']",2007.sigdial-1.23deepfigures-results.json,bar charts,2007.sigdial-1.23
2007.sigdial-1.23.pdf-Figure7.png,Figure 7: Comparing User Dialog Behaviors.,[u'. figure 7 shows the normalized mean values and standard errors for our user dialog behaviors.'],2007.sigdial-1.23deepfigures-results.json,bar charts,2007.sigdial-1.23
2007.sigdial-1.27.pdf-Figure1.png,Figure 1: Variation between speakers,[],2007.sigdial-1.27deepfigures-results.json,bar charts,2007.sigdial-1.27
2007.sigdial-1.27.pdf-Figure2.png,Figure 2: Variation between pronouns,[],2007.sigdial-1.27deepfigures-results.json,bar charts,2007.sigdial-1.27
2007.sigdial-1.32.pdf-Figure3.png,Figure 3. Average response times.,[u'. figure 3 shows the plot of average response times for different participants.'],2007.sigdial-1.32deepfigures-results.json,bar charts,2007.sigdial-1.32
2014.lilt-11.3.pdf-Figure2.png,FIGURE 2 Morphological ranking of morphs according to textual morph complexity in the mixed-genre corpus. Abscissa indexes increased morphological complexity.,"[u'. in this spirit, a ranking of the morphs according to their textual complexity on the morphological level is established (figure 2) .']",2014.lilt-11.3deepfigures-results.json,bar charts,2014.lilt-11.3
2014.lilt-11.3.pdf-Figure5.png,FIGURE 5 Morphological ranking of constructions according to textual construction complexity in the mixed-genre corpus. Abscissa indexes increased morphological complexity.,[u'. the differences between the morphological complexity scores of the construction manipulated texts and the original text are calculated and visualised in figure 5 .'],2014.lilt-11.3deepfigures-results.json,bar charts,2014.lilt-11.3
2014.lilt-11.3.pdf-Figure6.png,FIGURE 6 Syntactic ranking of constructions according to textual construction complexity in the mixed-genre corpus. Abscissa indexes increased syntactic complexity.,"[u'is an indicator for the amount of complexity a given construction adds to the original. Figure 6 displays the ranking of the constructions according to their syntactic complexity contributions, i.e. the difference']",2014.lilt-11.3deepfigures-results.json,bar charts,2014.lilt-11.3
2015.jeptalnrecital-demonstration.6.pdf-Figure1.png,"FIGURE 1: Comparison of the quantity of correct found NE, noise and references",[],2015.jeptalnrecital-demonstration.6deepfigures-results.json,bar charts,2015.jeptalnrecital-demonstration.6
2015.jeptalnrecital-demonstration.6.pdf-Figure2.png,"FIGURE 2: Comparison of the precision, recall and Fmeasure",[],2015.jeptalnrecital-demonstration.6deepfigures-results.json,bar charts,2015.jeptalnrecital-demonstration.6
2015.jeptalnrecital-long.2.pdf-Figure3.png,FIGURE 3  Distribution des fonctions caractristiques (nombre de fonctions selon leur nombre doccurrences ; chelle logarithmique sur les deux axes) sur les donnes ESTER et distribution des fonctions utilises dans le modle.,[],2015.jeptalnrecital-long.2deepfigures-results.json,bar charts,2015.jeptalnrecital-long.2
2015.jeptalnrecital-long.2.pdf-Figure4.png,FIGURE 4  Distribution des fonctions caractristiques sans indication de label (nombre de fonctions selon leur nombre doccurrences ; chelle logarithmique sur les deux axes) sur les donnes ESTER et distribution des fonctions utilises dans le modle sans indication de classe.,[],2015.jeptalnrecital-long.2deepfigures-results.json,bar charts,2015.jeptalnrecital-long.2
2016.jeptalnrecital-jep.1.pdf-Figure7.png,FIGURE 7 : VOT des diffrentes occlusives selon la hauteur vocalique pour chaque locuteur,[],2016.jeptalnrecital-jep.1deepfigures-results.json,bar charts,2016.jeptalnrecital-jep.1
2016.jeptalnrecital-jep.20.pdf-Figure4.png,FIGURE 4: Distribution des genres dans le corpus.,[],2016.jeptalnrecital-jep.20deepfigures-results.json,bar charts,2016.jeptalnrecital-jep.20
2016.jeptalnrecital-jep.22.pdf-Figure3.png,Figure 3 : dure moyenne des prolongations normales produites par les locuteurs normo-fluents et les prolongations svres prsentes dans la parole des sujets qui bgaient.,[u'. la figure 3 montre que les prolongations de sons perues comme normales durent 526 ms en moyenne.'],2016.jeptalnrecital-jep.22deepfigures-results.json,bar charts,2016.jeptalnrecital-jep.22
2016.jeptalnrecital-jep.22.pdf-Figure4.png,Figure 4 : nombre moyen de rptitions lors des disfluences catgorises comme normales ( gauche) et comme svres ( droite).,[u'La Figure 4 prsente ainsi le nombre moyen de rptitions en cas de disfluences normales et pathologiques. On'],2016.jeptalnrecital-jep.22deepfigures-results.json,bar charts,2016.jeptalnrecital-jep.22
2016.jeptalnrecital-jep.55.pdf-Figure3.png,"FIGURE 3 : Coefficient de corrlation des matrices de voyelles des apprenants et des locuteurs natifs,  T1 (gris fonc) et  T2 (gris clair) pour chaque groupe. (Les astrisques indiquent le niveau de significativit dans diffrences : * p < .05 ; ** p < .01 ; *** p < .001)",[],2016.jeptalnrecital-jep.55deepfigures-results.json,bar charts,2016.jeptalnrecital-jep.55
2016.jeptalnrecital-jep.60.pdf-Figure1.png,"FIGURE 1: Nombre moyen de mots dissyllabiques produits en tche spontane et en tche de lecture des populations CTRL, PARK, SLA, ATAX.","[u'. le nombre moyen de mots dissyllabiques produits sont globalement comparables entre les populations, comme le montre la figure 1 .']",2016.jeptalnrecital-jep.60deepfigures-results.json,bar charts,2016.jeptalnrecital-jep.60
2016.jeptalnrecital-long.22.pdf-Figure2.png,FIGURE 2  Rsultats obtenus avec le classifieur Adaboost compars aux 10 soumissions des participants de la campagne TAC EDL 2015.,[],2016.jeptalnrecital-long.22deepfigures-results.json,bar charts,2016.jeptalnrecital-long.22
2016.jeptalnrecital-recital.6.pdf-Figure7.png,FIGURE 7  Scores de difficult de diffrentes anaphores au sein dun mme document. Les deux anaphores entoures sont celles de la figure 8. On a identifi les anaphores par leur mot forme et leur identifiant dans le corpus ANCOR Centre,[],2016.jeptalnrecital-recital.6deepfigures-results.json,bar charts,2016.jeptalnrecital-recital.6
2016.lilt-13.2.pdf-Figure2.png,"FIGURE 2 Class distribution per annotator (A1: blue, A2: red, A3: grey).",[],2016.lilt-13.2deepfigures-results.json,bar charts,2016.lilt-13.2
2016.lilt-13.3.pdf-Figure5.png,FIGURE 5 Comparison of results obtained in the classification Experiments. The blue represents the bounded class and the red represents the unbounded class.,[u'. a graphical summary of this comparison is given in figure 5 .'],2016.lilt-13.3deepfigures-results.json,bar charts,2016.lilt-13.3
2018.jeptalnrecital-court.41.pdf-Figure2.png,"FIGURE 2: Matchs contenant au moins une tiquette mal prdite en fonction de la taille du match. Selon les jeux de donnes, le taux derreur sur lensemble du corpus est compris entre 5% et 15%.",[],2018.jeptalnrecital-court.41deepfigures-results.json,bar charts,2018.jeptalnrecital-court.41
2018.jeptalnrecital-deft.9.pdf-Figure1.png,FIGURE 1  Rpartition des prdictions par label de rfrence du modle BLSTM de la tche 1,[],2018.jeptalnrecital-deft.9deepfigures-results.json,bar charts,2018.jeptalnrecital-deft.9
2019.ccnlg-1.1.pdf-Figure10.png,Figure 10: Accomplishment (value) scores for each game. Error bars indicate one standard deviation.,[],2019.ccnlg-1.1deepfigures-results.json,bar charts,2019.ccnlg-1.1
2019.ccnlg-1.1.pdf-Figure12.png,Figure 12: Originality (novelty) scores for each game. Error bars indicate one standard deviation.,[u'. figure 12 : originality (novelty) scores for each game.'],2019.ccnlg-1.1deepfigures-results.json,bar charts,2019.ccnlg-1.1
2019.ccnlg-1.1.pdf-Figure13.png,Figure 13: Unpredictability (surprise) scores for each game. Error bars indicate one standard deviation.,[],2019.ccnlg-1.1deepfigures-results.json,bar charts,2019.ccnlg-1.1
2019.ccnlg-1.1.pdf-Figure8.png,Figure 8: Unpredictability (surprise) scores for each game. Error bars indicate one standard deviation.,[],2019.ccnlg-1.1deepfigures-results.json,bar charts,2019.ccnlg-1.1
2019.jeptalnrecital-court.15.pdf-Figure4.png,FIGURE 4  Pourcentage des prdictions corriges et celles falsifies en introduisant le prapprentissage sur le corpus de la langue standard par rapport  linitialisation alatoire.,[],2019.jeptalnrecital-court.15deepfigures-results.json,bar charts,2019.jeptalnrecital-court.15
2019.jeptalnrecital-court.17.pdf-Figure2.png,FIGURE 2  Prcision de chaque classifieur de type de questions aprs la validation croise.,[],2019.jeptalnrecital-court.17deepfigures-results.json,bar charts,2019.jeptalnrecital-court.17
2019.jeptalnrecital-tia.5.pdf-Figure2.png,Figure 2: Entropies of different layers of termino-conceptual structure,"[u': Entropies of different layers of termino-conceptual structure Figure 2 : Entropies of different layers of termino-conceptual structure not necessarily be connected. Classification graphs are', u'. to facilitate the analysis, figure 2 gives the corresponding barplot.', u'. exceptions are determinant-oriented specification and classificatory structures (fourth and the rightmost bars in figure 2 , respectively) in computer science, and to some extent agriculture.']",2019.jeptalnrecital-tia.5deepfigures-results.json,bar charts,2019.jeptalnrecital-tia.5
2019.jeptalnrecital-tia.5.pdf-Figure3.png,Figure 3: Entropies of specification structure and of reference graphs,"[u'. figure 3 shows the barplot of the entropies of the six domains, with the bars from left to right showing: nucleus-oriented specification entropy of terminology, determinant-oriented specification entropy of terminology, entropy of er specification graph, nucleus-oriented entropy of ba specification graph, determinantoriented entropy of ba specification graph, entropy of ws specification graph.', u'. figure 3 , we can observe that, in all the domains, entropies of both nucleus-oriented and determinant-oriented specification structures of terminologies are smaller than the entropies of er, determinant-oriented ba and ws graphs.']",2019.jeptalnrecital-tia.5deepfigures-results.json,bar charts,2019.jeptalnrecital-tia.5
2020.acl-demos.20.pdf-Figure2.png,Figure 2: BLEU scores for Finnish-English translation models trained with data that is pruned based on different ranking orders. The reported BLEU values show the mean of six translation models. The 100-mark bar shows the score when using the whole ParaCrawl corpus for training.,[u'. figure 2 provides an overview of the results for finnish to english.'],2020.acl-demos.20deepfigures-results.json,bar charts,2020.acl-demos.20
2020.acl-demos.20.pdf-Figure3.png,Figure 3: BLEU scores for English-Finnish translation models.,[],2020.acl-demos.20deepfigures-results.json,bar charts,2020.acl-demos.20
2020.acl-demos.32.pdf-Figure3.png,Figure 3: Example of the distribution of the techniques as used by two media and on two different topics. Note that the scales are different.,"[u""and doubt, whereas Fox News has a higher preference for flag waving and slogans. Next, Figure 3c shows the propaganda techniques used by Fox News when covering the Khashoggi's Murder, which has"", u""covering the Khashoggi's Murder, which has a very similar technique distribution to the plot in Figure 3b .""]",2020.acl-demos.32deepfigures-results.json,bar charts,2020.acl-demos.32
2020.acl-main.113.pdf-Figure3.png,Figure 3: Distribution of DA scores for Estonian-English 1K and English-Czech datasets,[u'. figure 3 shows the distribution of da scores for estonian-english 1k and english-czech datasets.'],2020.acl-main.113deepfigures-results.json,bar charts,2020.acl-main.113
2020.acl-main.117.pdf-Figure2.png,Figure 2: Number of white space separated tokens in training questions (title plus body.) and answers (for answerable questions only). The bin at 200 also contains all questions longer than 200 tokens.,[],2020.acl-main.117deepfigures-results.json,bar charts,2020.acl-main.117
2020.acl-main.117.pdf-Figure3.png,Figure 3: Number of white space separated tokens in devtest questions (title plus body) and answers (for answerable questions only).,[],2020.acl-main.117deepfigures-results.json,bar charts,2020.acl-main.117
2020.acl-main.123.pdf-Figure3.png,Figure 3: The distribution of the support scores on the English Gigaword dataset.,[],2020.acl-main.123deepfigures-results.json,bar charts,2020.acl-main.123
2020.acl-main.123.pdf-Figure4.png,Figure 4: The distribution of the support scores on JAMUL.,[],2020.acl-main.123deepfigures-results.json,bar charts,2020.acl-main.123
2020.acl-main.151.pdf-Figure1.png,"Figure 1: Average results of our best-performing metric, together with reference-based BLEU on WMT17.",[],2020.acl-main.151deepfigures-results.json,bar charts,2020.acl-main.151
2020.acl-main.158.pdf-Figure5.png,"Figure 5: Evaluation results on all models, split across test suite circuits.","[u'We show the circuit-level breakdown of absolute SG scores for all models (including off-the-shelf) in Figure 5 . In general, the models that obtain high SG scores on average (as in']",2020.acl-main.158deepfigures-results.json,bar charts,2020.acl-main.158
2020.acl-main.158.pdf-Figure6.png,"Figure 6: SG score on the pairs of test suites with and without intervening modifiers: Center Embedding, Cleft, MVRR, NPZ-Ambiguous, and NPZ-Object.","[u""In Figure 6 we plot models' average scores on these five test suites (dark bars) and their minimally""]",2020.acl-main.158deepfigures-results.json,bar charts,2020.acl-main.158
2020.acl-main.159.pdf-Figure1.png,Figure 1: Plural class productions by item.,"[u'; roughly 80% of its productions are /-e/, relative to 45% of speaker productions (Figure 1) . In contrast, the model rarely predicts /-(e)n/, which speakers use 30% of the time.']",2020.acl-main.159deepfigures-results.json,bar charts,2020.acl-main.159
2020.acl-main.159.pdf-Figure2.png,Figure 2: Distribution of plural classes by rank in ED model output.,[u'. figure 2 shows the distribution of plural classes in the top 5 most likely forms predicted by the model for each m95 word.'],2020.acl-main.159deepfigures-results.json,bar charts,2020.acl-main.159
2020.acl-main.164.pdf-Figure4.png,"Figure 4: Number of votes expert raters made for each label as a function of number of tokens observed. As raters observe more tokens, their predictions become more confident.","[u"". figure 4 shows that 'possibly human' is by far the most frequent answer upon observing 16 tokens, and as more tokens are observed raters gravitate towards 'definitely human' or 'definitely machine."", u'. figure 4 also shows how raters for the most part default to guessing short excerpts are human-written, and as the excerpts are extended, raters use the extra evidence available to revise their guess.']",2020.acl-main.164deepfigures-results.json,bar charts,2020.acl-main.164
2020.acl-main.169.pdf-Figure2.png,Figure 2: Probability of occurrence for 10 of the most common 30 words in the P0 and P9 data buckets,[],2020.acl-main.169deepfigures-results.json,bar charts,2020.acl-main.169
2020.acl-main.181.pdf-Figure2.png,"Figure 2: Accuracies of predictors on AAN triples in the held-out test data, with 95% confidence intervals shown.","[u'our predictors in predicting held-out adjective orders in the Common Crawl test set, visualized in Figure 2a . We find that the pattern of results depends on whether predictors are estimated based', u'and visualized in Figure 2b . When estimating based on wordforms, the best predictors are subjectivity and PMI, although the']",2020.acl-main.181deepfigures-results.json,bar charts,2020.acl-main.181
2020.acl-main.193.pdf-Figure4.png,Figure 4: Performance of CONNET variants of decoupling phase (DP) and aggregation phase (AP).,[],2020.acl-main.193deepfigures-results.json,bar charts,2020.acl-main.193
2020.acl-main.197.pdf-Figure2.png,Figure 2: Score breakdown by degree of agreement.,[u'same; 4) 3/1/1 three annotations are the same and the other two annotations are different. Figure 2 summarizes the results in terms of both accuracy and KL-divergence:'],2020.acl-main.197deepfigures-results.json,bar charts,2020.acl-main.197
2020.acl-main.199.pdf-Figure3.png,"Figure 3: Results on Science (Eurovoc) domain: The average Precision, Recall and F-score values and their standard error values. It is clear that addition of Residual Layer and SC Layer lowers the variance of the results.","[u'. furthermore, in figure 3 , we randomly choose science (eurovoc) domain as the one to report the error-bars (corresponding to the standard-deviation values) for our experiments.']",2020.acl-main.199deepfigures-results.json,bar charts,2020.acl-main.199
2020.acl-main.201.pdf-Figure2.png,"Figure 2: Train and test accuracy (P@1) for BLI on MUSE; Projection-based CLWE underfit the training dictionary (gray), but retrofitting to the training dictionary overfits (pink). Adding a synthetic dictionary balances between training and test accuracy (orange).","[u'. in practice, we use a non-zero  for regularization, but the updated clwe still have perfect training bli accuracy (figure 2 ).', u'We first compare BLI accuracy on both training and test dictionaries (Figure 2 ). We use CSLS to translate words with default parameters. The original projection-based CLWE have']",2020.acl-main.201deepfigures-results.json,bar charts,2020.acl-main.201
2020.acl-main.201.pdf-Figure3.png,"Figure 3: For each CLWE, we report accuracy for document classification (left) and unlabeled attachment score (UAS) for dependency parsing (right). Compared to the original embeddings (gray), retrofitting to the training dictionary (pink) improves average downstream task scores, confirming that fully exploiting the training dictionary helps downstream tasks. Adding a synthetic dictionary (orange) further improves test accuracy in some languages.","[u'Dependency parsing with RCSLS Figure 3 : For each CLWE, we report accuracy for document classification (left) and unlabeled attachment score', u""Although training dictionary retrofitting lowers BLI test accuracy, it improves both downstream tasks' test accuracy (Figure 3 ). This confirms that over-optimizing the test BLI accuracy can hurt downstream tasks because training""]",2020.acl-main.201deepfigures-results.json,bar charts,2020.acl-main.201
2020.acl-main.210.pdf-Figure6.png,Figure 6: Number of annotations by frequent annotators,[],2020.acl-main.210deepfigures-results.json,bar charts,2020.acl-main.210
2020.acl-main.217.pdf-Figure2.png,"Figure 2: Performance of all models relative to Baseline Cascade ( = 0) across our 3 resource conditions. Cascaded models in orange, end-to-end models in purple. Our proposed models yield improvements across all three conditions, with a widening margin under low-resource conditions for the phone cascade.","[u"". our phone end-to-end model not only shows less of a decrease in performance across figure 2 : performance of all models relative to 'baseline cascade' ( = 0) across our 3 resource conditions."", u'. the phone cascade performs still better, with marked improvements across all conditions over all other models (see figure 2 ).', u'. our hybrid cascade uses an asr model with phone-informed downsampling and bpe targets   ably compared to phone featured models -as shown in figure 2 , both the phone cascade and phone end-to-end models outperform the hybrid cascade at lower-resource conditions, by up to 10 bleu at 20 hours.']",2020.acl-main.217deepfigures-results.json,bar charts,2020.acl-main.217
2020.acl-main.217.pdf-Figure3.png,Figure 3: Phone Cascade Robustness: using phone labels in place of BPE as the text source for downstream MT. Comparing performance across our three data conditions and phone label qualities.,"[u'. figure 3 compares the impact of phone quality on the performance of phone cascades trained on our high, medium, and low-resource conditions.', u'. figure 3 : phone cascade robustness: using phone labels in place of bpe as the text source for downstream mt.', u"". for the phone cascade models compared in figure 3 , we collapse adjacent consecutive phones with the same label,  when three consecutive frames have been aligned to the same phone label 'b b b' we have reduced the sequence to a single phone 'b' for translation.""]",2020.acl-main.217deepfigures-results.json,bar charts,2020.acl-main.217
2020.acl-main.217.pdf-Figure4.png,Figure 4: Phone End-to-End Robustness: trainable embeddings for phone labels are concatenated to framelevel filterbank features. Comparing performance across three data conditions and phone label qualities.,"[u"". figure 4 compares the results of these factored models using phone features of differing qualities, with 'gold' alignments as an upper bound."", u'phone cascade and speech features of the baseline end-to-end model, but unlike the phone cas- Figure 4 : Phone End-to-End Robustness: trainable embeddings for phone labels are concatenated to framelevel filterbank features.']",2020.acl-main.217deepfigures-results.json,bar charts,2020.acl-main.217
2020.acl-main.219.pdf-Figure4.png,"Figure 4: Human evaluations on IMAGE-CHAT. Engagingness win rates of pairwise comparisons between human utterances and TRANSRESNETRET (ResNet152 or ResNeXt-IG-3.5B) or TRANSRESNETGEN , comparing over the rounds of dialogue.",[],2020.acl-main.219deepfigures-results.json,bar charts,2020.acl-main.219
2020.acl-main.219.pdf-Figure5.png,"Figure 5: IGC Evaluations. The best model from Mostafazadeh et al. (2017) is compared to our best TRANSRESNETRET and TRASNRESNETGEN models. On the left, annotators ratings of responses from the models are shown as a percentage of the annotators ratings of human responses. On the right, BLEU-4 scores on the response task are shown.",[u'. we then scale that by the score of human authored figure 5 : igc evaluations.'],2020.acl-main.219deepfigures-results.json,bar charts,2020.acl-main.219
2020.acl-main.222.pdf-Figure1.png,"Figure 1: Human evaluations on Image Chat and Wizard of Wikipedia (WoW), comparing existing state of the art models with our All Tasks MT conversational agent. Engagingness win rates are statistically significant in all three matchups (binomial test, p < .05).","[u'The results, given in Figure 1 , show our method outperforming the existing state of the art generative models on all', u'. in figure 1 , we compare our all tasks mt image+seq2seq model to existing baselines from both tasks; to produce those outputs, we used beam search with a beam size of 10 and tri-gram blocking.']",2020.acl-main.222deepfigures-results.json,bar charts,2020.acl-main.222
2020.acl-main.244.pdf-Figure1.png,Figure 1: Pretrained Transformers often have smaller IID/OOD generalization gaps than previous models.,"[u'. figure 1 : pretrained transformers often have smaller iid/ood generalization gaps than previous models.', u"". for instance, figure 1 shows that the lstm model declined by over 35%, while roberta's generalization performance in fact increases.""]",2020.acl-main.244deepfigures-results.json,bar charts,2020.acl-main.244
2020.acl-main.244.pdf-Figure2.png,"Figure 2: Generalization results for sentiment analysis and reading comprehension. While IID accuracy does not vary much for IMDb sentiment analysis, OOD accuracy does. Here pretrained Transformers do best.","[u'we find that model distillation can reduce robustness, as evident in our DistilBERT results in Figure 2 . This highlights that testing model compression methods for BERT', u"". see figure 2 's results.""]",2020.acl-main.244deepfigures-results.json,bar charts,2020.acl-main.244
2020.acl-main.244.pdf-Figure3.png,"Figure 3: The IID/OOD generalization gap is not improved with larger models, unlike in computer vision.",[u'. figure 3 shows that larger bert and al-  bert models do not reduce the generalization gap.'],2020.acl-main.244deepfigures-results.json,bar charts,2020.acl-main.244
2020.acl-main.244.pdf-Figure4.png,"Figure 4: We feed in OOD examples from out-of-distribution datasets (20 Newsgroups, Multi30K, etc.) to SST-2 sentiment classifiers and report the False Alarm Rate at 95% Recall. A lower False Alarm Rate is better. Classifiers are repurposed as anomaly detectors by using their negative maximum softmax probability as the anomaly score OOD examples should be predicted with less confidence than IID examples. Models such as BoW, word2vec averages, and LSTMs are near random chance; that is, previous NLP models are frequently more confident when classifying OOD examples than when classifying IID test examples.","[u'. partial results are in figure 4 , and full results are in the appendix.']",2020.acl-main.244deepfigures-results.json,bar charts,2020.acl-main.244
2020.acl-main.245.pdf-Figure4.png,"Figure 4: Histogram of |B(x)| for SST-2 and RTE. SST-2 has the highest percentage of inputs x where |B(x)| = 1, while RTE has the least. On both datasets, |B(x)| < 9 for most x, and |B(x)| = 1 on a plurality of inputs.","[u'. figure 4 plots the distribution of |b  (x)|, across test examples in sst-2 and rte, where b  (x) is the set of encodings that are mapped to by some perturbation of x.']",2020.acl-main.245deepfigures-results.json,bar charts,2020.acl-main.245
2020.acl-main.252.pdf-Figure1.png,Figure 1: Number of parallel and monolingual training samples in millions for each language in WMT training corpora.,[u'. the distribution of our parallel and monolingual data is depicted in figure 1 .'],2020.acl-main.252deepfigures-results.json,bar charts,2020.acl-main.252
2020.acl-main.263.pdf-Figure2.png,"Figure 2: Comparison of inflectional distributions for SpanBERTSQuAD 2. The adversarial distributions include only examples that degrade model performance. To make the best use of limited space, we omit the RBR, RBS, and NNPS tags since they do not vary much across distributions. Full figures in Appendix D.","[u'. figure 2a illustrates the overall distributional differences in inflection occurrence between the original and adversarial examples found by morpheus for squad 2.0.', u'times each inflection is used in this adversarial dataset, giving us the inflectional distribution in Figure 2a .', u"". algorithm 2 in appendix c details our approach and figure 2b depicts the training set's inflectional distribution before and after this procedure.""]",2020.acl-main.263deepfigures-results.json,bar charts,2020.acl-main.263
2020.acl-main.263.pdf-Figure4.png,Figure 4: Effect of shuffling the inflection list on the adversarial distribution. We observe that shuffling the inflection list induces a more uniform inflectional distribution by reducing the higher frequency inflections and boosting the lower frequency ones.,"[u'. in order to increase overall inflectional variation in the set of adversarial examples, getinflections shuffles the generated list of inflections before returning it (see figure 4 in appendix).', u'. figure 4 : effect of shuffling the inflection list on the adversarial distribution.']",2020.acl-main.263deepfigures-results.json,bar charts,2020.acl-main.263
2020.acl-main.263.pdf-Figure5.png,Figure 5: Full versions of Figure 2,[],2020.acl-main.263deepfigures-results.json,bar charts,2020.acl-main.263
2020.acl-main.267.pdf-Figure5.png,"Figure 5: Class distributions of datasets. For IMDB, Amazon and Yelp, darker colors indicate higher ratings.",[],2020.acl-main.267deepfigures-results.json,bar charts,2020.acl-main.267
2020.acl-main.283.pdf-Figure4.png,Figure 4: Results on tail labels in nDCG@k.,"[u'. figure 4 shows the results of the five deep learning based mlc methods,  xml-cnn, sgm, reggnn, nlp-cap and hypercaps.']",2020.acl-main.283deepfigures-results.json,bar charts,2020.acl-main.283
2020.acl-main.283.pdf-Figure5.png,"Figure 5: Results of ablation test on EUR-LEX57K in P@k. L denotes local capsules, G denotes global capsules, H denotes HDR.","[u'. figure 5 shows the results on eur-lex57k in terms of p@k with k = 1, 3, 5.']",2020.acl-main.283deepfigures-results.json,bar charts,2020.acl-main.283
2020.acl-main.286.pdf-Figure4.png,Figure 4: Top-1 sensitivity by diseases.,[],2020.acl-main.286deepfigures-results.json,bar charts,2020.acl-main.286
2020.acl-main.286.pdf-Figure5.png,"Figure 5: The accuracy of ECNN-PGM-E using different types of features. Gyn and Res represent gynaecology and respiration, respectively. MI and Occ are mutual information and occurrence, respectively.",[u'. we randomly select 50 testing samples per department whose top-1 diagnosis prediction is correct and generate the explanation for the diagnosis prediction with res-top1 gyn-top3 res-top3 figure 5 : the accuracy of ecnn-pgm-e using different types of features.'],2020.acl-main.286deepfigures-results.json,bar charts,2020.acl-main.286
2020.acl-main.289.pdf-Figure4.png,Figure 4: Comparative results of our variant model that removes the clause pair representation learning and ranking component (denoted as RANKCP w/o Rank) and our full model RANKCP.,[],2020.acl-main.289deepfigures-results.json,bar charts,2020.acl-main.289
2020.acl-main.307.pdf-Figure2.png,Figure 2: Mean classification accuracy percentages (with SD in parentheses) over 10 replications.,[u'. figure 2 summarizes the complete experimental results.'],2020.acl-main.307deepfigures-results.json,bar charts,2020.acl-main.307
2020.acl-main.309.pdf-Figure3.png,"Figure 3: An ablation study to see the performance of models trained with reduced explicit negative examples (token-level and construction-level). One color represents the same models across plots, except the last bar (construction-level), which is different for each plot.","[u'Results Figure 3 is the main results. Across models, we restrict the evaluation on four nonlocal dependency constructions,']",2020.acl-main.309deepfigures-results.json,bar charts,2020.acl-main.309
2020.acl-main.331.pdf-Figure3.png,Figure 3: BERT for news representation.,[],2020.acl-main.331deepfigures-results.json,bar charts,2020.acl-main.331
2020.acl-main.345.pdf-Figure3.png,Figure 3: Comparison of phone focus across layers for various accents.,"[u'. figure 3 shows this quantity, averaged over all phones in all the utterances for each accent.', u'seen to reduce the phone focus the most, uniformly across all accents (as shown in Figure 3 , it is also seen to segregate accent information the most, recovering accent information ""lost""']",2020.acl-main.345deepfigures-results.json,bar charts,2020.acl-main.345
2020.acl-main.365.pdf-Figure2.png,"Figure 2: Evolution of usage type distributions in the period 19102009, generated with occurrences of coach, tenure, curtain and disk in COHA (Davies, 2012). The legends show sample usages per identified usage type.","[u"". for example, occurrences of curtain are clustered into four usage types (figure 2c ): two of these correspond to a literal interpretation of the word as a hanging piece of cloth ('curtainless windows', 'pulled the curtain closed') whereas the other two indicate metaphorical interpretations of curtain as any barrier that excludes the free exchange of information or communication ('the curtain on the legal war is being raised')."", u"". this holds, for example, for the word tenure, whose usages in phrases such as 'tenure-track faculty position' are present in two distinct usage types (see figure 2b )."", u'. this is the case, for example, for coach, where the frequency decrease of a usage type is gradual and caused by technological evolution (see figure 2a ).', u'. the usage type capturing tenure of a landed property becomes obsolete; however, we obtain a positive mean ed caused by the appearance of a new usage type (the third type in figure 2b ).', u"". as an example, figure 2d shows how, starting from the 1950's and as a result of technological innovation, the word disk starts to be used to denote also optical disks while beforehand it referred only to generic flat circular objects."", u'. figure 2c allows us to see when the metaphorical meaning related to the historically charged expression iron curtain is acquired.', u'. for example, curtain yields a rather low apd score due to the low relative frequency of the novel usage (figure 2c) .']",2020.acl-main.365deepfigures-results.json,bar charts,2020.acl-main.365
2020.acl-main.370.pdf-Figure2.png,Figure 2: Comparison of A-distance of different models.,[u'. results are shown in figure  2 .'],2020.acl-main.370deepfigures-results.json,bar charts,2020.acl-main.370
2020.acl-main.371.pdf-Figure1.png,Figure 1: Argument coverage per number of key points.,[u'. figure 1 examines the impact of the number of key points on argument coverage.'],2020.acl-main.371deepfigures-results.json,bar charts,2020.acl-main.371
2020.acl-main.372.pdf-Figure1.png,"Figure 1: Our emotion categories, ordered by the number of examples where at least one rater uses a particular label. The color indicates the interrater correlation.","[u', and Figure 1 . See Appendix B for more details on our multi-step taxonomy selection procedure.', u'. figure 1 shows the distribution of emotion labels.', u'. figure 1 shows that gratitude, admiration and amusement have the highest and grief and nervousness have the lowest interrater correlation.', u'. we find that those emotions that are highly significantly associated with certain tokens ( gratitude with ""thanks"", amusement with ""lol"") tend to have the highest interrater correlation (see figure 1 ).']",2020.acl-main.372deepfigures-results.json,bar charts,2020.acl-main.372
2020.acl-main.372.pdf-Figure4.png,Figure 4: Softmax weights of each BERT layer when trained on our dataset.,"[u'. we find that all layers are similarly important for our task, with center of gravity = 6.19 (see figure 4 ).']",2020.acl-main.372deepfigures-results.json,bar charts,2020.acl-main.372
2020.acl-main.372.pdf-Figure5.png,Figure 5: Number of emotion labels per example before and after filtering the labels chosen by only a single annotator.,"[u'. figure 5 shows the number of emotion labels per example before and after we filter for those labels that have agreement.', u'. in practice, since most of our examples only has a single label (see figure 5 ), our confusion matrix is very similar to one calculated for a single-label classification task.', u'though the training data only includes a subset of the labels that have agreement (see Figure 5 ).']",2020.acl-main.372deepfigures-results.json,bar charts,2020.acl-main.372
2020.acl-main.375.pdf-Figure4.png,"Figure 4: UAS accuracy for the average models (BERT 13, ELMo 3) on incoming dependencies of different part-of-speech categories.",[u'. figure 4 shows probe accuracy for different models (bert/elmo) and syntactic representations (ud/sud) when attaching words of specific part-of-speech categories to their heads.'],2020.acl-main.375deepfigures-results.json,bar charts,2020.acl-main.375
2020.acl-main.378.pdf-Figure1.png,Figure 1: Reranking 100 samples of dev set sentences generated by discriminative non-incremental model.,"[u'The results in Figure 1 show that the gap between incremental and non-incremental models is around one point of F1-score.', u'. figure 1 shows that while mbr does not make any significant difference for the non-incremental model, it makes a huge difference for the incremental models.']",2020.acl-main.378deepfigures-results.json,bar charts,2020.acl-main.378
2020.acl-main.39.pdf-Figure6.png,Figure 6: SeqAcc for the Location Attender and best baseline on the Long Lookup Tables task (10 runs).,[u'. figure 6 shows the seqacc of the location attender against the strongest baseline (transformer attention).'],2020.acl-main.39deepfigures-results.json,bar charts,2020.acl-main.39
2020.acl-main.39.pdf-Figure7.png,Figure 7: SeqAccBE for the Location Attender and best baseline on the Long Lookup Tables task (10 runs).,"[u'. figure 7 shows that the model outputs are always correct but that it often terminates decoding too soon, which we will refer to as the <eos> problem.']",2020.acl-main.39deepfigures-results.json,bar charts,2020.acl-main.39
2020.acl-main.39.pdf-Figure8.png,Figure 8: SeqAccBE (5 runs) for the Mix Attender and best baseline on the reversed lookup tables (reverse) and lookup tables with noisy start (noisy).,"[u'. figure 8 shows that when considering seqaccbe, the mix attender is able to extrapolate well in the noisy setting and a little in the reverse setting.']",2020.acl-main.39deepfigures-results.json,bar charts,2020.acl-main.39
2020.acl-main.487.pdf-Figure2.png,Figure 2: Frequency with which word suggestions from BERT produce negative sentiment score.,[u'. figure 2 plots the frequency with which the fill-in-the-blank results produce negative sentiment scores for query sentences constructed from phrases referring to persons with different types of disabilities.'],2020.acl-main.487deepfigures-results.json,bar charts,2020.acl-main.487
2020.acl-main.591.pdf-Figure4.png,"Figure 4: Accuracy breakdown w.r.t. constituent height in unbiased trees derived from the syntactic task distances in our model (top) and the language modeling distances (bottom). A constituent is considered as correct if its boundaries correspond to a true constituent. The constituents heights are those in the predicted tree. Since constituents that represent the whole sentence always have correct boundaries, they are excluded from the calculation.","[u'. in the top part of the figure, we see the parse produced from the l syd distances of our model, in the middle the tree produced the l lm distances and, on the bottom, the gold standard tree.']",2020.acl-main.591deepfigures-results.json,bar charts,2020.acl-main.591
2020.acl-main.622.pdf-Figure3.png,Figure 3: Performance on the development set of the CoNLL-2012 dataset with various number of speakers. F1(Speaker as feature): F1 score for the strategy that treats speaker information as a mention-pair feature. F1(Speaker as input): F1 score for our strategy that treats speaker names as token input. Frequency: percentage of documents with specific number of speakers.,[u'strategy in Wiseman et al. documents according to the number of their constituent speakers in Figure 3 . Results show that the proposed strategy performs significantly better on documents with a larger'],2020.acl-main.622deepfigures-results.json,bar charts,2020.acl-main.622
2020.acl-main.627.pdf-Figure7.png,Figure 7: Performances on different argument label.,[u'. figure 7 shows the results.'],2020.acl-main.627deepfigures-results.json,bar charts,2020.acl-main.627
2020.acl-main.627.pdf-Figure9.png,Figure 9: Performances on different argument label.,[],2020.acl-main.627deepfigures-results.json,bar charts,2020.acl-main.627
2016.jeptalnrecital-jep.27.pdf-Figure1.png,"FIGURE 1: valeurs de F1 par groupe (enfants CI et NH), par condition (production et rptition), et par degr de hauteur de la voyelle",[],2016.jeptalnrecital-jep.27deepfigures-results.json,boxplots,2016.jeptalnrecital-jep.27
2016.jeptalnrecital-jep.27.pdf-Figure3.png,"FIGURE 3 : valeurs de F2 par groupe (enfants CI et NH), par condition (production et rptition), et",[],2016.jeptalnrecital-jep.27deepfigures-results.json,boxplots,2016.jeptalnrecital-jep.27
2016.jeptalnrecital-jep.43.pdf-Figure4.png,FIGURE 4 : valeur des formants F2 et largeur de bande pour les 3 voyelles aux 2 temps opratoires,[],2016.jeptalnrecital-jep.43deepfigures-results.json,boxplots,2016.jeptalnrecital-jep.43
2016.jeptalnrecital-jep.60.pdf-Figure5.png,FIGURE 5: Dures de S1 (en blanc) et de S2 (en gris) des mots dissyllabiques dans la tche de lecture pour les 12 locuteurs SLA.,[],2016.jeptalnrecital-jep.60deepfigures-results.json,boxplots,2016.jeptalnrecital-jep.60
2016.jeptalnrecital-jep.64.pdf-Figure2.png,"FIGURE 2 : F1 de V1 (ajustes par le modle, moyenne et erreur type) en fonction de laperture de V2, du corpus et des interfrences orthographiques.","[u"". comme illustr figure 2 , l'effet du corpus sur l'hv interagit avec les contraintes orthographiques : si l'on ne considre que les voyelles v 1 non marques orthographiquement, l'hv ressort comme plus importante dans le corpus de parole conversationnelle par rapport au corpus journalistique.""]",2016.jeptalnrecital-jep.64deepfigures-results.json,boxplots,2016.jeptalnrecital-jep.64
2016.jeptalnrecital-jep.64.pdf-Figure4.png,"FIGURE 4 : F1 de V1 (ajustes par le modle, moyenne et erreur type) en fonction laperture de V2 et de la prsence ou non dun schwa sousjacent.","[u'Elle montre galement une interaction significative entre ApertureV 2 et SchwaIntervocalique (p<0.05), illustre par la Figure 4 . On remarque que les modulations de F1 en fonction de V 2 sont plus']",2016.jeptalnrecital-jep.64deepfigures-results.json,boxplots,2016.jeptalnrecital-jep.64
2016.jeptalnrecital-jep.65.pdf-Figure1.png,"Figure 1 : Distribution des rangs (de 1  72, en ordonnes) obtenus par chaque locuteur (Sp1, Sp3 ; Sp5 & Sp7) en fonction des types de scores (L2, Zr & AX). Les points reprsentent le rang moyen de chaque locuteur pour les 18 noncs imits.","[u'. la figure 1 montre la distribution des rangs pour les trois types de scores, qui donneront par la suite lieu  un calcul de corrlation.']",2016.jeptalnrecital-jep.65deepfigures-results.json,boxplots,2016.jeptalnrecital-jep.65
2016.jeptalnrecital-jep.67.pdf-Figure3.png,"FIGURE 3  Dure moyenne des voyelles en fonction du style de parole (laboratoire, prpar, spontan)",[],2016.jeptalnrecital-jep.67deepfigures-results.json,boxplots,2016.jeptalnrecital-jep.67
2016.jeptalnrecital-jep.68.pdf-Figure2.png,"FIGURE 2: Graphique en bote  moustache reprsentant le ratio des mdianes des temps de rponse pour les essais des mots modifis par la prononciation compose, dans les diffrentes conditions dexprimentation (en laboratoire ou  lextrieur) pour les deux groupes tests (adultes et enfants).",[u'. tous les ratios sont significativement diffrents de 0 pour tous les groupes (voir figure 2) .'],2016.jeptalnrecital-jep.68deepfigures-results.json,boxplots,2016.jeptalnrecital-jep.68
2020.acl-main.699.pdf-Figure2.png,"Figure 2: Letter-value plot (Hofmann et al., 2017) showing the distribution of citation ages in the corpus, grouped by year of publication. The solid black lines denote the median, boxes correspond to quantiles.","[u'papers. 8 3 Analysis 3.1 Are more recently published papers citing more recently published papers? Figure 2 shows the distribution of the age of cited articles with respect to the year in', u'shows the distribution of citation ages, analogous to Figure 2 , but separately for each publication venue.']",2020.acl-main.699deepfigures-results.json,boxplots,2020.acl-main.699
2020.acl-main.699.pdf-Figure8.png,"Figure 8: Letter-value plot (Hofmann et al., 2017) considering only the oldest citation per paper among all papers published in a given year. The solid black lines denote the median, boxes correspond to quantiles.","[u'.1 oldest citation per paper figure 8 shows the distribution of the oldest citation per paper in our dataset.', u'. figure 8 shows that this is not really the case: the majority of papers in our dataset include a citation of age 15 or older.']",2020.acl-main.699deepfigures-results.json,boxplots,2020.acl-main.699
I17-1022.pdf-Figure1.png,Figure 1: The feature density of different types of embedding on CSLB Concept Property Norms dataset.,"[u'In Figure 1 that summarizes the result, the proposed embedding method shows higher averages and lower deviations of']",I17-1022deepfigures-results.json,boxplots,I17-1022
W10-1203.pdf-Figure3.png,Figure 3: Profile plot of MAP,"[u'and Figure 3 summarize the results for the full set of topics. Each row in', u'. figure 3 describes the distribution of map across the three text normalization techniques and three query models.']",W10-1203deepfigures-results.json,boxplots,W10-1203
W11-1415.pdf-Figure2.png,"Figure 2: Box plots showing the minimum, first quantile, median, third quantile, the maximum and the outliers for the scores assigned to each text","[u'. figure 2 clearly shows the variability in the scores assigned to the texts.', u'. a resulting text pair then consists figure 2 : box plots showing the minimum, first quantile, median, third quantile, the maximum and the outliers for the scores assigned to each text of two texts, accompanied with an assessment that designates which of the two texts is easier than the other one, and to what degree.']",W11-1415deepfigures-results.json,boxplots,W11-1415
W12-0703.pdf-Figure5.png,"Figure 5: Figure of box plots for different inference iterations i and m = 1000, T = 100,  = 50/T ,  = 0.1, r = 1 .","[u'. starting from 5 iterations, error rates do not change much, see figure 5 .']",W12-0703deepfigures-results.json,boxplots,W12-0703
W12-0703.pdf-Figure6.png,"Figure 6: Box plot for several inference runs r, to assign the topics to a word with m = 1000, i = 100, T = 100,  = 50/T ,  = 0.1.","[u'. the box plot for several evaluated values of r is shown in figure 6 .', u'. figure 6 : box plot for several inference runs r, to assign the topics to a word with m = 1000, i = 100, t = 100,  = 50/t ,  = 0.1.']",W12-0703deepfigures-results.json,boxplots,W12-0703
W12-0703.pdf-Figure7.png,"Figure 7: Box plot using the mode method d = true with several inference iterations i with m = 500, T = 100,  = 50/T ,  = 0.1.",[u'. the impact of this method on error and variance is illustrated in figure 7 .'],W12-0703deepfigures-results.json,boxplots,W12-0703
W12-0703.pdf-Figure8.png,"Figure 8: Box plot for several alpha values withm = 500, i = 100, T = 100,  = 0.1, r = 1.","[u'For values, shown in Figure 8 , we can see that the recommended value for T = 100 , = 0.5']",W12-0703deepfigures-results.json,boxplots,W12-0703
W12-0703.pdf-Figure9.png,"Figure 9: Box plot for several beta values  with m = 500, i = 100, T = 100,  = 50/T , r = 1.","[u'. regarding variance, no patterns within the stable range emerge, see figure 9 .']",W12-0703deepfigures-results.json,boxplots,W12-0703
W13-4011.pdf-Figure2.png,Figure 2: Duration (in seconds) of each lexical type,[],W13-4011deepfigures-results.json,boxplots,W13-4011
W13-4011.pdf-Figure3.png,Figure 3: Number of feedback items per speaker,[u'. figure 3 illustrates the total figures of feedback per speaker.'],W13-4011deepfigures-results.json,boxplots,W13-4011
W13-4011.pdf-Figure4.png,Figure 4: Distribution of the lexical items,[],W13-4011deepfigures-results.json,boxplots,W13-4011
W14-0201.pdf-Figure5.png,Figure 5: SASSI Usability scores.,"[u'. figure 5 shows results separated into the six dimensions system response accuracy (sra), likeability (like), cognitive demand (cog dem), annoyance (ann), habitability (hab), and speed.']",W14-0201deepfigures-results.json,boxplots,W14-0201
W14-0204.pdf-Figure4.png,Figure 4: Driving distraction while using a multimodal search UI.,"[u'. the graph in figure 4 shows averaged results for the final undistracted drive and for the first and second distracted driving tasks (reflecting the order of the tasks, not their types).']",W14-0204deepfigures-results.json,boxplots,W14-0204
W14-0212.pdf-Figure4.png,Figure 4: User answer response delay under three conditions.,[u'. the average response delay (from the end of the recall question to the button press) per condition across all subjects is shown in figure 4 .'],W14-0212deepfigures-results.json,boxplots,W14-0212
W14-2515.pdf-Figure2.png,"Figure 2: Accuracy of classifier trained and tested on balanced set contrasting agreed upon Twitter users of a given role, against users pulled at random from the 1% stream.",[u'. results are shown in figure 2 .'],W14-2515deepfigures-results.json,boxplots,W14-2515
W14-2515.pdf-Figure3.png,Figure 3: Results of positive vs negative by verb. Given that a user writes a tweet containing I interview . . . or Interviewing . . . we are about 75% accurate in identifying whether or not the user is a Radio/Podcast Host.,[],W14-2515deepfigures-results.json,boxplots,W14-2515
W14-3207.pdf-Figure1.png,"Figure 1: Box and whiskers plot of proportion of tweets each user has (y-axis) matching various LIWC categories. Each bar represents one LIWC category for one condition  PTSD in purple, depression in blue, SAD in orange, bipolar in red and control in gray. Anxiety occurs an order of magnitude less often than the others, so its proportion is on the right y-axis (and thus not comparable to the others). Statistically significant deviations from control users are denoted by asterisks.","[u'. figure 1 shows the proportion of tweets from each user that scores positively on various liwc categories (, have at least one word from that category).', u'. furthermore, when taken in combination with the different patterns exhibited by the groups as seen in figure 1 , this correlation is not solely attributable to liwc categories either.']",W14-3207deepfigures-results.json,boxplots,W14-3207
W14-33.pdf-Figure10.png,"Figure 10: The result of clustering by TrueSkill model with 25K training data. Dashed lines separate systems with non-overlapping rank ranges, splitting the data into clusters.","[u'and Figure 10 is the same, further corroborating the stability and accuracy of the TrueSkill model even with']",W14-33deepfigures-results.json,boxplots,W14-33
W14-33.pdf-Figure9.png,"Figure 9: The result of clustering by TrueSkill model with 1K training data from WMT13 French-English. The boxes range from the lower to upper quartile values, with means in the middle. The whiskers show the full range of each systems rank after the bootstrap resampling.","[u'. figure 9 and 10 present the result of clustering two different size of training data (1k and 25k pairwise comparisons) on the trueskill model, which indicates that the rank ranges become narrow and generate clusters reasonably as the number of training samples increases.', u'is based on Expected Wins. One noteworthy observation is that the ranking of systems between Figure 9 and']",W14-33deepfigures-results.json,boxplots,W14-33
W14-3301.pdf-Figure10.png,"Figure 10: The result of clustering by TrueSkill model with 25K training data. Dashed lines separate systems with non-overlapping rank ranges, splitting the data into clusters.","[u'and Figure 10 is the same, further corroborating the stability and accuracy of the TrueSkill model even with']",W14-3301deepfigures-results.json,boxplots,W14-3301
W14-3301.pdf-Figure9.png,"Figure 9: The result of clustering by TrueSkill model with 1K training data from WMT13 French-English. The boxes range from the lower to upper quartile values, with means in the middle. The whiskers show the full range of each systems rank after the bootstrap resampling.","[u'. figure 9 and 10 present the result of clustering two different size of training data (1k and 25k pairwise comparisons) on the trueskill model, which indicates that the rank ranges become narrow and generate clusters reasonably as the number of training samples increases.', u'is based on Expected Wins. One noteworthy observation is that the ranking of systems between Figure 9 and']",W14-3301deepfigures-results.json,boxplots,W14-3301
W14-54.pdf-Figure5.png,Figure 5: A boxplot comparing the number of actions between scenes from Experiment 1 and 2. Paired plots with the same colour refer to corresponding scenes (continued from Figure 5).,"[u'. paired plots with the same colour refer to corresponding scenes (continued in figure 5 ).', u'. 7 figure 5 : a boxplot comparing the number of actions between scenes from experiment 1 and 2.', u'. paired plots with the same colour refer to corresponding scenes (continued from figure 5 ).', u'. figure 5 shows a distribution of ""male-person"" label scores.']",W14-54deepfigures-results.json,boxplots,W14-54
W15-19.pdf-Figure6.png,Figure 6: Per item distribution of distances between misspelled words and target hypotheses,"[u'.question figure 6 : per item distribution of distances between misspelled words and target hypotheses aspell reported 21 (4%) correctly spelled words as misspelled and suggested a correction (false positives).', u'. figure 6 shows the distribution of cosine and normalized damerau-levenshtein distances (ndl) to target hypotheses with linear trend lines.', u'Proceedings of the 4th workshop on NLP for Computer Assisted Language Learning at NODALIDA 2015 Figure 6 . Using the Talebob Portation Tool Kit.']",W15-19deepfigures-results.json,boxplots,W15-19
W15-19.pdf-Figure7.png,Figure 7: Per score distribution of distances between misspelled key concepts and target hypotheses for two items,"[u'distances between misspelled key concepts and target hypotheses for two items For these responses, in Figure 7 we show the distribution of the distances to the target hypotheses between score points. Most', u'. based on figure 7 the 3-gram cosine distance yields a pattern that best distinguishes between the three score points.']",W15-19deepfigures-results.json,boxplots,W15-19
W15-19.pdf-Figure8.png,Figure 8: Per score distribution of distances between normalized responses and reference responses,[u'of distances between normalized learner and reference responses for all the items are shown in Figure 8 . Items clustered by score-point are ordered as in'],W15-19deepfigures-results.json,boxplots,W15-19
2015.jeptalnrecital-long.18.pdf-Figure5.png,FIGURE 5: Score dattribution sur le corpus EBG-40.,"[u""Le score d'AA est calcul sur trois corpus : EBG-40 ( Figure 5 ), LIB-40 (""]",2015.jeptalnrecital-long.18deepfigures-results.json,confusion matrix,2015.jeptalnrecital-long.18
2015.jeptalnrecital-long.18.pdf-Figure6.png,FIGURE 6: Score dattribution sur le corpus LIB-40.,"[u'), LIB-40 ( Figure 6 ) et MIXT-80']",2015.jeptalnrecital-long.18deepfigures-results.json,confusion matrix,2015.jeptalnrecital-long.18
2015.jeptalnrecital-long.18.pdf-Figure7.png,FIGURE 7: Score dattribution sur le corpus MIXT-80.,"[u'. chaque figure est constitue de quatre matrices pour chaque trait : les rptitions maximales (motifs), les n-grammes, les rptitions maximales pondres par leur longueur (motif s len ) et les rptitions maximales pondres par les rptitions maximales du 2 me ordre (motif s 2 nd ).']",2015.jeptalnrecital-long.18deepfigures-results.json,confusion matrix,2015.jeptalnrecital-long.18
S19-2110.pdf-Figure1.png,Figure 1: The confusion matrix for DeepModel+val in subtask A,[],S19-2110deepfigures-results.json,confusion matrix,S19-2110
S19-2111.pdf-Figure4.png,"Figure 4: Sub-task A, HAD-Tubingen LSTM + Hashtag parsing.",[],S19-2111deepfigures-results.json,confusion matrix,S19-2111
S19-2111.pdf-Figure5.png,"Figure 5: Sub-task B, HAD-Tubingen LSTM + Preprocessing.",[],S19-2111deepfigures-results.json,confusion matrix,S19-2111
S19-2111.pdf-Figure6.png,"Figure 6: Sub-task C, HAD-Tubingen LSTM + Preprocessing + Lexical lookup with Pronouns.",[],S19-2111deepfigures-results.json,confusion matrix,S19-2111
S19-2112.pdf-Figure2.png,Figure 2: Confusion matrix for sub-task C and the best performing model SVM,[u'. figure 2 shows the confusion matrix for the best performing system.'],S19-2112deepfigures-results.json,confusion matrix,S19-2112
S19-2116.pdf-Figure1.png,"Figure 1: Sub-task A,RNN method","[u'. in rnn method, there is more type i error (see figure 1 ) which means the model classifies some non-offensive sentences as offensive ones.']",S19-2116deepfigures-results.json,confusion matrix,S19-2116
S19-2116.pdf-Figure2.png,"Figure 2: Sub-task A,MSOC method","[u'. however, since there are still some offensive words appeared in dataset that are not defined in the dictionary, there is still much type ii error (see figure 2) .']",S19-2116deepfigures-results.json,confusion matrix,S19-2116
S19-2116.pdf-Figure3.png,"Figure 3: Sub-task B,RNN method","[u'and Figure 3 , 4) when categorizing the types of offense. This is because usually targeted offensive language']",S19-2116deepfigures-results.json,confusion matrix,S19-2116
S19-2116.pdf-Figure4.png,"Figure 4: Sub-task B,MSOC method",[],S19-2116deepfigures-results.json,confusion matrix,S19-2116
S19-2116.pdf-Figure5.png,"Figure 5: Sub-task C,RNN method","[u"".(see figure  5 ) the main reason of this result is 'oth' class is not as characteristic as other two classes and the partition of this class is the smallest as well.""]",S19-2116deepfigures-results.json,confusion matrix,S19-2116
S19-2116.pdf-Figure6.png,"Figure 6: Sub-task C,MSOC method",[u'. (see figure 6 ) this may contribute to the predefined dictionary and sentence structure.'],S19-2116deepfigures-results.json,confusion matrix,S19-2116
S19-2117.pdf-Figure2.png,"Figure 2: Confusion matrix for Sub-task A, JTML CodaLab CNN model.",[u'. figure 2 shows the confusion matrix for the results with our cnn model.'],S19-2117deepfigures-results.json,confusion matrix,S19-2117
S19-2118.pdf-Figure4.png,Figure 4: Confusion matrix of CNN-glove model for Sub-task A,[],S19-2118deepfigures-results.json,confusion matrix,S19-2118
S19-2118.pdf-Figure5.png,Figure 5: Confusion matrix of RNN-LSTM model for Sub-task B,[],S19-2118deepfigures-results.json,confusion matrix,S19-2118
S19-2118.pdf-Figure6.png,Figure 6: Confusion matrix of ML model for Subtask C,[],S19-2118deepfigures-results.json,confusion matrix,S19-2118
S19-2119.pdf-Figure1.png,Figure 1: Confusion matrix of sub task A,[u'. weighting the feature set using tf-idf also did not work well as it de- figure 1 : confusion matrix of sub task a creases the accuracy of the system.'],S19-2119deepfigures-results.json,confusion matrix,S19-2119
S19-2119.pdf-Figure2.png,Figure 2: Confusion matrix of sub task B,[u'. figure 2 shows the confusion matrix of the system.'],S19-2119deepfigures-results.json,confusion matrix,S19-2119
S19-2119.pdf-Figure3.png,Figure 3: Confusion matrix of sub task C,[u'. we can easily detect and study the error from the confusion matrix given in figure 3 .'],S19-2119deepfigures-results.json,confusion matrix,S19-2119
S19-2120.pdf-Figure2.png,Figure 2: Confusion matrix of our best performed model for Sub-task A (biLSTM with specific configuration - Run2A),"[u'. the confusion matrix of our best performed model for the first task (see figure 2) illustrates that between the two classes, not (not offensive) class achieves the best result where the majority of the data being correctly classified.']",S19-2120deepfigures-results.json,confusion matrix,S19-2120
S19-2120.pdf-Figure3.png,Figure 3: Confusion matrix of our best performed model for Sub-task B (biLSTM with specific configuration - Run3B),[u'that our best performed model has the highest precision for class TIN as shown in Figure 3 . The confusion matrix of our best performed model for sub-task C can be seen'],S19-2120deepfigures-results.json,confusion matrix,S19-2120
S19-2120.pdf-Figure4.png,Figure 4: Confusion matrix of our best performed model for Sub-task C (biLSTM with specific configuration - Run3C),[u'. the confusion matrix of our best performed model for sub-task c can be seen in figure 4 .'],S19-2120deepfigures-results.json,confusion matrix,S19-2120
S19-2122.pdf-Figure3.png,Figure 3: Confusion Matrix for MIDAS submission 2 for Sub-task A.,[u'. figure 3 presents the confusion matrix of our submission for sub-task a.'],S19-2122deepfigures-results.json,confusion matrix,S19-2122
S19-2127.pdf-Figure2.png,"Figure 2: Confusion Matrix of the OLID gold test set, sub-task A. Depicted are instances and normalized values.",[u'. figure 2 shows the confusion matrix of our submitted predictions for the semeval shared task.'],S19-2127deepfigures-results.json,confusion matrix,S19-2127
S19-2128.pdf-Figure3.png,Figure 3: Confusion Matrix shows results for Sub-task A using Bidirectional GRU,[],S19-2128deepfigures-results.json,confusion matrix,S19-2128
S19-2128.pdf-Figure4.png,Figure 4: Confusion Matrix shows results for Sub-task B using Bidirectional GRU,[],S19-2128deepfigures-results.json,confusion matrix,S19-2128
S19-2128.pdf-Figure5.png,Figure 5: Confusion Matrix shows results for Sub-task C using Bidirectional LSTM,[],S19-2128deepfigures-results.json,confusion matrix,S19-2128
S19-2132.pdf-Figure2.png,Figure 2: SubTask A: Confusion Matrix for 2D-CNN with Word2Vec embeddings,[],S19-2132deepfigures-results.json,confusion matrix,S19-2132
S19-2132.pdf-Figure4.png,Figure 4: SubTask C: Confusion Matrix for 1D-CNN with GloVe,[],S19-2132deepfigures-results.json,confusion matrix,S19-2132
S19-2133.pdf-Figure1.png,"Figure 1: Sub-task A, garain CodaLab 528038 (BiDirectional LSTM)",[],S19-2133deepfigures-results.json,confusion matrix,S19-2133
S19-2133.pdf-Figure3.png,"Figure 3: Sub-task C, garain CodaLab 535813 (BiDirectional LSTM)",[],S19-2133deepfigures-results.json,confusion matrix,S19-2133
S19-2136.pdf-Figure1.png,Figure 1: Confusion matrix of soft voting ensemble model (Model A in Table 4) for Sub-Task B.,"[u'confusion matrices of predictions acquired by our best models as released by organizers. Sub-Task B. Figure 1 shows that our model has higher precision for the targeted threats, which is also clear', u'. figure  1 also shows that our model has slightly higher false negatives as compared to false positives.']",S19-2136deepfigures-results.json,confusion matrix,S19-2136
S19-2136.pdf-Figure2.png,Figure 2: Confusion matrix of soft voting ensemble model (Model 1 in Table 6) for Sub-Task C.,[u'Sub-Task C We visualize model errors in Figure 2@dot Figure 2 shows that our model has'],S19-2136deepfigures-results.json,confusion matrix,S19-2136
S19-2138.pdf-Figure1.png,Figure 1: The UM-IU@LING confusion matrix for subtask A.,"[u'. the confusion matrix in figure 1 further illustrates the error pattern of our classifier, which more often misclassified offensive tweets as being not offensive.']",S19-2138deepfigures-results.json,confusion matrix,S19-2138
S19-2138.pdf-Figure2.png,Figure 2: Confusion matrix for the SVM classifier for subtask C.,[u'. the confusion matrix in figure 2 indicates that our classi- @user yeah thanks to your nobel emmy award winning idiot chief flip flopping on everything from iran to gun control.'],S19-2138deepfigures-results.json,confusion matrix,S19-2138
S19-2139.pdf-Figure1.png,"Figure 1: a) Sub-task A, LSTM (epoch=5, dropout= 0.2), b) Sub-task B, LSTM (dropout=0.2, epochs=20), c) Sub-task C, LSTM(dropout=0.2, epochs=50)","[u'. best results are highlighted in bold ink in tables and confusion matrix for them is also shown in figure 1 for sub-tasks a, b and c.']",S19-2139deepfigures-results.json,confusion matrix,S19-2139
S19-2140.pdf-Figure3.png,Figure 3: Confusion matrix of the UTFPR-Scratch model on the test set.,[u'. the confusion matrix of utfpr-reuse in figure 3 shows that the main reason behind this poor showing was the large amount of false negatives predicted.'],S19-2140deepfigures-results.json,confusion matrix,S19-2140
S19-2141.pdf-Figure2.png,"Figure 2: SubTask A, Ensemble - No additional data",[],S19-2141deepfigures-results.json,confusion matrix,S19-2141
S19-2142.pdf-Figure1.png,"Figure 1: Sub-task A, YNU-HPCC CodaLab 528232","[u'. in addition, from the confusion matrix in figure 1 , it is observed that when the classifier predicts two classes of labels, namely not and off, it is more specific to the not label, and the precision for the not label is higher than that for the off label.']",S19-2142deepfigures-results.json,confusion matrix,S19-2142
S19-2142.pdf-Figure3.png,"Figure 3: Sub-task C, YNU-HPCC CodaLab 536705","[u'. additionally, as shown in figure  3 , among the ind, oth, and grp labels, the highest recall and precision are for the ind labels, and the lowest are for the oth labels.']",S19-2142deepfigures-results.json,confusion matrix,S19-2142
S19-2143.pdf-Figure2.png,Figure 2: Confusion matrix of K-max pooling CNN model for Sub-task A,"[u'0.8453 The confusion matrix of our model prediction results in Sub-task A is shown in Figure 2 . There are 620 NOT tags in the test dataset, and 240 OFF tags. As']",S19-2143deepfigures-results.json,confusion matrix,S19-2143
S19-2144.pdf-Figure4.png,"Figure 4: Sub-task C, Logistic Regression - Count - Stopwords Removal","[u', and confusion matrix Figure 4 .']",S19-2144deepfigures-results.json,confusion matrix,S19-2144
S19-2181.pdf-Figure1.png,"Figure 1: First experiment accuracy:0.9125, 0 is not bias,1 is bias.",[u'. figure 1 shows the result of our first experiments.'],S19-2181deepfigures-results.json,confusion matrix,S19-2181
S19-2181.pdf-Figure2.png,"Figure 2: Second experiment accuracy:0.6077, 0 is not bias,1 is bias.",[],S19-2181deepfigures-results.json,confusion matrix,S19-2181
S19-2213.pdf-Figure1.png,Figure 1: Confusion matrix training data,[u'best model (ULMFiT) on the training data as shown by the confusion matrix presented in Figure 1 . We specially look at the predictions made by our model that falls into the'],S19-2213deepfigures-results.json,confusion matrix,S19-2213
U15-1009.pdf-Figure1.png,"Figure 1. Lex plots for all texts at 20, 50, 100, and 300 dimensions","[u'. figure 1 demonstrates that the motifs for this text begin to disappear at 20 dimensionswhereas at 50 dimensions, the motifs in the 100and 300-dimension plots are darker, but still clearly visible.']",U15-1009deepfigures-results.json,confusion matrix,U15-1009
U15-1009.pdf-Figure2.png,Figure 2: Motifs in 300-dimension Lex plot of Domestic violence report,[],U15-1009deepfigures-results.json,confusion matrix,U15-1009
U15-1009.pdf-Figure3.png,Figure 3. Local cohesion motifs in 300- dimension Lex plot of the Education report,[],U15-1009deepfigures-results.json,confusion matrix,U15-1009
U17-1006.pdf-Figure3.png,"Figure 3: Confusion matrix showing the rates of substitution errors between tones (as a percentage, normalized per row).",[u'. figure 3 shows the most common tone substitution mistakes for  in the test set.'],U17-1006deepfigures-results.json,confusion matrix,U17-1006
W03-2002.pdf-Figure6.png,"Figure 6: Comparison between a 11x11 Use viewpoint thematic map and a 16x16 Use viewpoint thematic map through map extracts: the 11x11 map extract is presented at the left, the 16x16 map extract is presented at the right. On the figure, the focus is given machine oil topic. The comparison highlights, as an example, that the logical surrounding of this topic is more precisely defined in the 16x16 map (optimal quality) than in the 11x11 map (lower quality). Moreover, in the 11x11 map, the topic machine oil has been derived in a more fuzzy scope topic named machine and vehicles.","[u'. as it is shown in the figure 6, high quality maps are usually characterized by more precise topic labels and smaller average size of their logical areas.']",W03-2002deepfigures-results.json,confusion matrix,W03-2002
W14-1616.pdf-Figure1.png,Figure 1: Example Chinese-English sentence pair with word alignments shown as filled grid squares.,"[u'. for the segment pairs <b t kolv jnq, take it into account> in figure 1 , the more common structure is <b .', u'. for example, in the sentence pair in figure 1 , <b t , take it> is a phrase but not a minimal phrase, as it contains smaller phrases of <b , take> and <t , it>.', u'We will contribute the code to Moses. Figure 1 , the MP sequence is shown in', u'., t 6 extracted from the sentence pair in figure 1 .']",W14-1616deepfigures-results.json,confusion matrix,W14-1616
W14-32.pdf-Figure4.png,"Figure 4: Pearsons r correlations between various analytics, color indicates the strength of statistically significant correlations, or 0 (aquamarine) otherwise. Bonferroni corrected, each comparison is significant only if <0.0002). Rows and columns represent the analytics in the same order, so the diagonal is self-correlation.",[],W14-32deepfigures-results.json,confusion matrix,W14-32
W14-33.pdf-Figure6.png,Figure 6: Heat map for the ratio of pairwise judgments across the full cross-product of systems used in the first 20% of TrueSkill model.,"[u'. across this figure and the next two, systems are sorted along each axis by the final values of  inferred by trueskill during training, and the heat of each square is proportional to the percentage of judgments obtained between those two systems.', u'to BLEU, for a pair of systems for which differences in scores are significant, while Figure 6 shows the same for ME-TEOR but for a pair of systems with no significant difference', u'a pair of systems with a small but significant score difference according to BLEU, and Figure 6 shows the same for METEOR applied to a pair of systems where no significant difference', u'for BLEU and Figure 6 for METEOR. Instead of computing counts with reference to the actual statistic, the line through']",W14-33deepfigures-results.json,confusion matrix,W14-33
W14-33.pdf-Figure7.png,Figure 7: Heat map for the ratio of pairwise judgments across the full cross-product of systems used in the last 20% of TrueSkill model.,"[u'pairs selected by TrueSkill for the first 20% of its matches chosen during training, while Figure 7 presents a heat map of the last 20%. The contrast is striking: whereas the judgments', u'. figure 7 : human evaluation pairwise significance tests for spanish-to-english systems (colored cells denote scores for system row being significantly greater than system column.', u'. figure 7 shows pairwise significance test results for fluency, adequacy and the combination of the two tests, for all pairs of spanish-to-english systems.', u'. for example, in figure 7 , adequacy scores of system b are not significantly greater than those of systems c, d and e, while fluency scores for system b are significantly greater than those of the three other systems.']",W14-33deepfigures-results.json,confusion matrix,W14-33
W14-33.pdf-Figure8.png,Figure 8: Automatic metric pairwise randomized significance test results for Spanish-to-English systems (colored cells denote scores for System row significantly greater than System column).,"[u'. figure 8 show the number of clusters according to the increase of training data for three models.', u', METEOR and TER . Figure 8 shows the outcome of pairwise randomized significance tests for each metric for Spanishto-English systems, and']",W14-33deepfigures-results.json,confusion matrix,W14-33
W14-3627.pdf-Figure2.png,"Figure 2: METEOR X-ray alignment of the sentence in table 6. The left side is the output of the one-step system, the right side is the output of the two-step system, and the top is the reference. The shaded cells represent matches between the reference and the one-step system, and the dots represent matches between the reference and the twostep system.","[u'. its meteor x-ray alignment is illustrated in figure 2 .', u'the results from the core and the results from the first step of the two-step Figure 2 : METEOR X-ray alignment of the sentence in table 6. The left side is the', u'and Figure 2 in Section 6.3, the transliteration of ""Liberia"" in the output of the two-step system matches']",W14-3627deepfigures-results.json,confusion matrix,W14-3627
W14-3627.pdf-Figure3.png,"Figure 3: A comparison of the output of the one-step domain and dialect adaptation system (left column) and the two-step domain and dialect adaptation system (right column), both built on top of the phrase-based core. The top is the reference sentence.","[u'and Figure 3 show the output for a sentence from the Egyptian test set from the two different', u'. in figure 3 , the results from the one-step and two-step adaptation systems are almost the same, except that the two-step adaptation system (which scored 2.8 bleu higher than the one-step system overall) has one more word correct (the second word).', u'Step System rAjl TEn tsEh fy knys yhwdy fy mwskwA Figure 3 : A comparison of the output of the one-step domain and dialect adaptation system (left']",W14-3627deepfigures-results.json,confusion matrix,W14-3627
W15-20.pdf-Figure4.png,Figure 4: Disagreement for noun senses in BLOG.,"[u'. these relations are illustrated in figure 4 .', u'. figure 4 : the lexical unit general evokes four frames without motivating as many entries in the lexicon.']",W15-20deepfigures-results.json,confusion matrix,W15-20
W15-20.pdf-Figure5.png,Figure 5: Disagreement for noun senses in PARLIAMENT.,"[u'Residence People_by_jurisdiction Figure 5 : When meaning potentials evoke frames in close relation to each other, vagueness may be']",W15-20deepfigures-results.json,confusion matrix,W15-20
W15-29.pdf-Figure7.png,Figure 7: Categories treemap with surprise density and negative density sliders both set to > 0.4.,"[u'. figure 7 shows the number of users in bins of 50 tweets.', u'. on the other hand, figure 7 shows categories with surprise and negative densities each greater than 0.4.']",W15-29deepfigures-results.json,confusion matrix,W15-29
W15-4627.pdf-Figure3.png,Figure 3: Histogram of Correctness and Preference for Experiment 1 averaged across story (lower is better),[],W15-4627deepfigures-results.json,confusion matrix,W15-4627
W15-54.pdf-Figure3.png,"Figure 3: The matrix of pairwise Q-coefficient values between our feature types, displayed as a heat map. Smaller values indicate lower dependence between their predictions.","[u'. the results for the analysis are shown as a heat map in figure 3 .', u'. we illustrate the algorithm using the example shown in figure 3 .']",W15-54deepfigures-results.json,confusion matrix,W15-54
W16-1003.pdf-Figure4.png,Figure 4: Confusion matrix for activities without using an ontology.,"[u'. in fact, a primary source of misclassifications on this dataset is pick up and throw being mistaken for bend, as shown in figure  4 .']",W16-1003deepfigures-results.json,confusion matrix,W16-1003
W16-1616.pdf-Figure3.png,Figure 3: Example of a pair of questions that is correctly predicted as similar by the first (top) and second (bottom) bidirectional LSTMs. The dark blue squares represent areas of high similarity.,[u'the cosine similarities between word vector representations of words in questionquestion pairs or question-answer pairs. Figure  3 shows the heatmaps for the first bidirectional LSTM (top) and the second bidirectional LSTM (bottom)'],W16-1616deepfigures-results.json,confusion matrix,W16-1616
W16-1616.pdf-Figure4.png,Figure 4: Example of a pair of questions that is incorrectly predicted as similar by the first bidirectional LSTM (top) and correctly predicted by the the second bidirectional LSTM (bottom). The dark blue squares represent areas of high similarity.,"[u'smaller values to the non-important words (e.g., ""please post"") while highlighting important words (e.g., ""admit""). Figure 4 shows the heatmaps for the first bidirectional LSTM (top) and the second bidirectional LSTM (bottom)']",W16-1616deepfigures-results.json,confusion matrix,W16-1616
W16-2001.pdf-Figure5.png,"Figure 5: SAX-MINDIST(left) and f0-Euclidean (right) Distance matrix of 1920 Mandarin tones sorted by tone category. Tones are ordered by tone categories along the x- and y-axis. Origin at top left corner (i.e., on both axis data points are ordered by 480 instances of tone 1, tone 2, tone 3, and tone 4 when moving away from origin).","[u'Finally, we plot distance matrixes in Figure 5 , which may give a hint as to why SAX is a more effective representation', u'SAX is a more effective representation than the f 0 -Hertz vectors in clustering: In Figure 5 we can clearly see that the lower dimension SAX-MINDIST distance reflects the intrinsic structure of']",W16-2001deepfigures-results.json,confusion matrix,W16-2001
W16-2206.pdf-Figure4.png,Figure 4: A translation example produced by our system. The shown German sentence is preordered.,"[u'. figure 4 : a translation example produced by our system.', u""We show an example from the GermanEnglish task in Figure 4 , along with the alignment path. The reference translation is 'and the proposal has been""]",W16-2206deepfigures-results.json,confusion matrix,W16-2206
W16-2301.pdf-Figure7.png,"Figure 7: Significance test results for pairs of systems competing in the news domain translation task (cs-en, de-en, fi-en), where a green cell denotes a significantly higher DA adequacy or fluency score for the system in a given row over the system in a given column, Combined results show overall conclusions when adequacy is primarily used to rank systems with fluency used to break ties between systems tied with respect to adequacy.",[],W16-2301deepfigures-results.json,confusion matrix,W16-2301
W16-2301.pdf-Figure8.png,"Figure 8: Significance test results for pairs of systems competing in the news domain translation task (ro-en, ru-en, tr-en), where a green cell denotes a significantly higher DA adequacy or fluency score for the system in a given row over the system in a given column, Combined results show overall conclusions when adequacy is primarily used to rank systems with fluency used to break ties between systems tied with respect to adequacy.",[],W16-2301deepfigures-results.json,confusion matrix,W16-2301
W16-2301.pdf-Figure9.png,"Figure 9: Significance test results for pairs of systems competing in the news domain translation task (en-ru), where a green cell denotes a significantly higher DA adequacy score for the system in a given row over the system in a given column.",[],W16-2301deepfigures-results.json,confusion matrix,W16-2301
W16-2302.pdf-Figure1.png,"Figure 1: German-to-English (de-en), Finnish-to-English (fi-en) and Romanian-to-English (ro-en) system-level metric significance test results for human assessment variants; green cells denote a significant increase in correlation with human assessment for the metric in a given row over the metric in a given column according to Williams test; RR = standard WMT relative ranking for translation task systems only; DA = direct assessment of translation adequacy; DA Hybrids = direct assessment with hybrid super-sampling. 208","[u'Correlations of metric scores with human assessment of the large set of hybrid systems are Figure 1 : German-to-English (de-en), Finnish-to-English (fi-en) and Romanian-to-English (ro-en) system-level metric significance test results for human']",W16-2302deepfigures-results.json,confusion matrix,W16-2302
W16-2302.pdf-Figure10.png,"Figure 10: HUME segment-level metric significance test results (himl2015): Green cells denote a significant win for the metric in a given row over the metric in a given column according to Williams test for difference in dependent correlation; Winning metrics are those not significantly outperformed by any other (en-cs: CHRF3; en-de: BEER, CHRF3, CHRF2, MPEDA, CHRF1; en-pl: BEER, CHRF1, MPEDA, CHRF2; en-ro: CHRF3).",[u'. full pairwise significance test results for all metrics are shown in figure 10 .'],W16-2302deepfigures-results.json,confusion matrix,W16-2302
W16-2302.pdf-Figure2.png,"Figure 2: Russian-to-English (ru-en), Turkish-to-English (tr-en) and English-to-Russian (en-ru) systemlevel metric significance test results for human assessment variants; green cells denote a significant increase in correlation for the metric in a given row over the metric in a given column according to Williams test; RR = standard WMT relative ranking for translation task systems only; DA = direct assessment of translation adequacy; DA Hybrids = direct assessment with hybrid super-sampling.","[u'. 208 figure 2 : russian-to-english (ru-en), turkish-to-english (tr-en) and english-to-russian (en-ru) systemlevel metric significance test results for human assessment variants; green cells denote a significant increase in correlation for the metric in a given row over the metric in a given column according to williams test; rr = standard wmt relative ranking for translation task systems only; da = direct assessment of translation adequacy; da hybrids = direct assessment with hybrid super-sampling.']",W16-2302deepfigures-results.json,confusion matrix,W16-2302
W16-2302.pdf-Figure4.png,Figure 4: English-to-Czech (en-cs) system-level metric significance test results; a green cell corresponds to a significant increase in correlation for the metric in a given row over the metric in a given column according to Williams test; RR = standard WMT relative ranking; RR + TT = standard WMT relative ranking for translation and tuning task systems.,"[u'. figure 4 : english-to-czech (en-cs) system-level metric significance test results; a green cell corresponds to a significant increase in correlation for the metric in a given row over the metric in a given column according to williams test; rr = standard wmt relative ranking; rr + tt = standard wmt relative ranking for translation and tuning task systems.', u'and Czech in Figure 4 . No significance tests are provided for IT domain Bulgarian and Basque, as all metrics', u'. for instance, evaluation of english-to-czech this year suggests that wordf, bleu and nist outperform chrf under evaluation against rr both with and without tuning systems (figure 4) on the news domain, whereas we have seen the exact op-posite last year.']",W16-2302deepfigures-results.json,confusion matrix,W16-2302
W16-2302.pdf-Figure5.png,Figure 5: English-to-German (en-de) system-level metric significance test results; a green cell corresponds to a significant increase in correlation for the metric in a given row over the metric in a given column according to Williams test; RR = standard WMT relative ranking.,"[u', German in Figure 5 and Czech in']",W16-2302deepfigures-results.json,confusion matrix,W16-2302
W16-2302.pdf-Figure6.png,"Figure 6: English-to-Finnish (en-fi), English-to-Romanian (en-ro) and English-to-Turkish (en-tr) systemlevel metric significance test results; a green cell corresponds to a significant increase in correlation for the metric in a given row over the metric in a given column according to Williams test; RR = standard WMT relative ranking.",[u'. figure 5: english-to-german (en-de) system-level metric significance test results; a green cell corresponds to a significant increase in correlation for the metric in a given row over the metric in a given column according to williams test; rr = standard wmt relative ranking.'],W16-2302deepfigures-results.json,confusion matrix,W16-2302
W16-2302.pdf-Figure7.png,Figure 7: System-level metric ittest2016 significance test results for differences in metric correlation with human assessment for remaining out-of-English language pairs evaluated with relative ranking (RR) human assessment.,"[u'competing metrics evaluated on IT domain systems for Spanish, Dutch and Portuguese are shown in Figure 7 , German in', u'and also Figure 7 that MOSESBLEU does not belong to the winners for several target languages (Czech, German, Dutch),', u'. figure 7 : system-level metric ittest2016 significance test results for differences in metric correlation with human assessment for remaining out-of-english language pairs evaluated with relative ranking (rr) human assessment.']",W16-2302deepfigures-results.json,confusion matrix,W16-2302
W16-2302.pdf-Figure8.png,Figure 8: Direct Assessment (DA) segment-level metric significance test results for to-English language pairs (newstest2016): Green cells denote a significant win for the metric in a given row over the metric in a given column according to Williams test for difference in dependent correlation.,[],W16-2302deepfigures-results.json,confusion matrix,W16-2302
W16-2302.pdf-Figure9.png,Figure 9: Direct Assessment (DA) segment-level metric significance test results for English to Russian (newstest2016): Green cells denote a significant win for the metric in a given row over the metric in a given column according to Williams test for difference in dependent correlation.,[],W16-2302deepfigures-results.json,confusion matrix,W16-2302
W16-25.pdf-Figure4.png,Figure 4: Accuracy of all functions on space s5.,[u'Baselines: Figure 4 shows the success of all of the analogy functions in recovering the intended analogy target'],W16-25deepfigures-results.json,confusion matrix,W16-25
W16-25.pdf-Figure5.png,"Figure 5: Comparison across spaces. The leftmost panel shows the accuracy of ADD, and the next two panels show the improvement in accuracy of ADD over the baselines.","[u'. yet the breakdown of the results by category ( figure 5 ) shows that the similarity in average performance across the spaces obscures differences across categories: s 2 performed much better than s 10 in some of the morphological inflection categories (, .']",W16-25deepfigures-results.json,confusion matrix,W16-25
W16-2503.pdf-Figure4.png,Figure 4: Accuracy of all functions on space s5.,[u'Baselines: Figure 4 shows the success of all of the analogy functions in recovering the intended analogy target'],W16-2503deepfigures-results.json,confusion matrix,W16-2503
W16-2503.pdf-Figure5.png,"Figure 5: Comparison across spaces. The leftmost panel shows the accuracy of ADD, and the next two panels show the improvement in accuracy of ADD over the baselines.","[u'. yet the breakdown of the results by category ( figure 5) shows that the similarity in average performance across the spaces obscures differences across categories: s 2 performed much better than s 10 in some of the morphological inflection categories (, .', u'was already captured by the IGNORE-A baseline in s 10 than in s 2 ( Figure 5) ']",W16-2503deepfigures-results.json,confusion matrix,W16-2503
W16-48.pdf-Figure5.png,Figure 5: B2 Confusion Matrices,"[u'Although the overall results seem similar, the visualization in Figure 5 indicate that the distances based on both LSTM autoencoders indicate a smoother change compared to']",W16-48deepfigures-results.json,confusion matrix,W16-48
W16-48.pdf-Figure6.png,Figure 6: C Closed Confusion Matrices,"[u'. figure 6 presents similar analyses for dialects of germany.', u': Task 2 dialects F1-score Based on Figure 6 and 7, we note that our systems perform in a very similar behavior under the']",W16-48deepfigures-results.json,confusion matrix,W16-48
W16-48.pdf-Figure7.png,Figure 7: C Open Confusion Matrices,[],W16-48deepfigures-results.json,confusion matrix,W16-48
W16-4804.pdf-Figure3.png,Figure 3: A Confusion Matrices,[],W16-4804deepfigures-results.json,confusion matrix,W16-4804
W16-4804.pdf-Figure4.png,Figure 4: B1 Confusion Matrices,[],W16-4804deepfigures-results.json,confusion matrix,W16-4804
W16-4804.pdf-Figure5.png,Figure 5: B2 Confusion Matrices,[],W16-4804deepfigures-results.json,confusion matrix,W16-4804
W16-4804.pdf-Figure6.png,Figure 6: C Closed Confusion Matrices,"[u'. p p p p p p p p p figure 6 and 7, we note that our systems perform in a very similar behavior under the open and closed settings, which is due to the small number of added features under the open settings as opposed to the closed.']",W16-4804deepfigures-results.json,confusion matrix,W16-4804
W16-4821.pdf-Figure1.png,Figure 1: The confusion matrix of the system as in run1 (Table 1).,"[u'. figure 1 shows the confusion matrix of the system as described in run1.', u'shows the confusion matrix of the system as described in run1. Figure 1 : The confusion matrix of the system as in run1']",W16-4821deepfigures-results.json,confusion matrix,W16-4821
W16-4821.pdf-Figure2.png,Figure 2: The confusion matrix of the system as in run1 (Table 2).,"[u'. Figure 2 and', u'mistakes are made, while MSA is the one that is most accurately recognized. Comparing between Figure 2 and']",W16-4821deepfigures-results.json,confusion matrix,W16-4821
W16-4821.pdf-Figure3.png,Figure 3: The confusion matrix of the system as in run3 (Table 3).,"[u'and Figure 3 , the system misclassified all Arabic varieties with each other with different confusion degrees. Gulf', u'and Figure  3 shows that using extra training data has reduced the classification confusion in most cases, except']",W16-4821deepfigures-results.json,confusion matrix,W16-4821
W16-4824.pdf-Figure3.png,Figure 3: Run3 confusion matrix using CNN multi-class classification,[],W16-4824deepfigures-results.json,confusion matrix,W16-4824
W16-4825.pdf-Figure2.png,Figure 2: Graphical depiction of the confusion matrix for hltcoe run 1 (test set A  closed training).,[],W16-4825deepfigures-results.json,confusion matrix,W16-4825
W16-4827.pdf-Figure1.png,Figure 1: Confusion matrix for test set A (closed training),"[u'. the confusion matrix on figure 1 hints at a lower classification accuracy concerning the three-way concurrencies, spanish in particular.']",W16-4827deepfigures-results.json,confusion matrix,W16-4827
W16-4828.pdf-Figure1.png,Figure 1: Confusion matrix for our best model,[],W16-4828deepfigures-results.json,confusion matrix,W16-4828
W17-0503.pdf-Figure1.png,"Figure 1: Normalized confusion matrix of POS annotation with the correct pos-tag on the y-axis, pos-tag assigned by TreeTagger on the x-axis.",[u'. figure 1 shows a confusion matrix with the correct pos-tags from the gold standard on the y-axis and the pos-tag assigned by treetagger on the xaxis.'],W17-0503deepfigures-results.json,confusion matrix,W17-0503
W17-0503.pdf-Figure2.png,"Figure 2: Confusion matrix of POS annotation with the correct pos-tag on the y-axis, pos-tag assigned by TreeTagger on the x-axis.","[u'. as the figures are not normalized, only highly frequent observations are visible, and the shading is directly linked to the overall impact of the error.']",W17-0503deepfigures-results.json,confusion matrix,W17-0503
W17-2340.pdf-Figure3.png,Figure 3: Confusion matrix of trigger classes with abbreviations mentioned in Table 1,"[u"". we can also infer from the confusion matrix shown in figure 3 that positive regulation, negative regulation and regulation among general category and planned category triggers are causing many false positives and false negatives thus degrading the model's performance.""]",W17-2340deepfigures-results.json,confusion matrix,W17-2340
W17-2401.pdf-Figure2.png,Figure 2: Accuracy rate (from 0 to 1) in the pairwise classification using the frequency of the 20 most frequent words.,"[u'extracted the frequency of the 20 most frequent words, and then used a SVM classifier. Figure 2 shows the accuracies for the traditional features, and', u'A careful examination of Figure 2 and 3 reveals that for some cases, except the squares with lighter colors, our results', u'0.8 0.9 0.9 0.9 0.9 0.9 0.6 1 0.7 1 0.9 0.9 0.8 1 0.8 Figure 2 : Accuracy rate (from 0 to 1) in the pairwise classification using the frequency of']",W17-2401deepfigures-results.json,confusion matrix,W17-2401
W17-2401.pdf-Figure3.png,Figure 3: Accuracy rate (from 0 to 1) in the pairwise classification using network features extracted from mesoscopic networks.,"[u'shows the accuracies for the traditional features, and Figure 3 illustrates the pairwise classification accuracies when mesoscopic networks were used to model each text, we']",W17-2401deepfigures-results.json,confusion matrix,W17-2401
W19-5211.pdf-Figure10.png,Figure 10: Frequency-based classification accuracy on states from the ENDE decoder.,[],W19-5211deepfigures-results.json,confusion matrix,W19-5211
W19-5211.pdf-Figure11.png,Figure 11: Frequency-based classification accuracy on states from the ENDE decoder + lexical shortcuts.,[],W19-5211deepfigures-results.json,confusion matrix,W19-5211
W19-5211.pdf-Figure12.png,Figure 12: Frequency-based classification accuracy on states from the ENRU encoder.,[],W19-5211deepfigures-results.json,confusion matrix,W19-5211
W19-5211.pdf-Figure13.png,Figure 13: Frequency-based classification accuracy on states from the ENRU encoder + lexical shortcuts.,[],W19-5211deepfigures-results.json,confusion matrix,W19-5211
W19-5211.pdf-Figure14.png,Figure 14: Frequency-based classification accuracy on states from the ENRU decoder.,[],W19-5211deepfigures-results.json,confusion matrix,W19-5211
W19-5211.pdf-Figure15.png,Figure 15: Frequency-based classification accuracy on states from the ENRU decoder + lexical shortcuts.,[],W19-5211deepfigures-results.json,confusion matrix,W19-5211
W19-5211.pdf-Figure16.png,Figure 16: POS-based classification accuracy on states from the ENDE encoder.,[],W19-5211deepfigures-results.json,confusion matrix,W19-5211
W19-5211.pdf-Figure16-23.png,Figures 16-23.,[],W19-5211deepfigures-results.json,confusion matrix,W19-5211
W19-5211.pdf-Figure17.png,Figure 17: POS-based classification accuracy on states from the ENDE encoder + lexical shortcuts.,[],W19-5211deepfigures-results.json,confusion matrix,W19-5211
W19-5211.pdf-Figure18.png,Figure 18: POS-based classification accuracy on states from the ENDE decoder.,[],W19-5211deepfigures-results.json,confusion matrix,W19-5211
W19-5211.pdf-Figure19.png,Figure 19: POS-based classification accuracy on states from the ENDE decoder + lexical shortcuts.,[],W19-5211deepfigures-results.json,confusion matrix,W19-5211
W19-5211.pdf-Figure9.png,Figure 9: Frequency-based classification accuracy on states from the ENDE encoder + lexical shortcuts.,[],W19-5211deepfigures-results.json,confusion matrix,W19-5211
2020.scil-1.11.pdf-Figure1.png,Figure 1: Time-indexing conventions for a finite-state machine.,[u'. the time indexing convention is shown in figure 1 .'],2020.scil-1.11deepfigures-results.json,graph,2020.scil-1.11
2020.scil-1.11.pdf-Figure2.png,"Figure 2: SL2 PDFA of ab,  = {a, b, c}",[],2020.scil-1.11deepfigures-results.json,graph,2020.scil-1.11
2020.scil-1.11.pdf-Figure4.png,"Figure 4: LT2 PDFA of Some-ab,  = {a, b, c}",[],2020.scil-1.11deepfigures-results.json,graph,2020.scil-1.11
2020.scil-1.11.pdf-Figure5.png,"Figure 5: LTT2 PDFA of One-ab,  = {a, b, c}",[],2020.scil-1.11deepfigures-results.json,graph,2020.scil-1.11
2020.scil-1.11.pdf-Figure7.png,"Figure 7: PT2 PDFA of Some-a . . . b,  = {a, b, c}",[],2020.scil-1.11deepfigures-results.json,graph,2020.scil-1.11
2020.scil-1.21.pdf-Figure2.png,Figure 2: The factor oxyn under multiple relations.,[u'. figure 2 shows the automaton constructed for the factor oxyn.'],2020.scil-1.21deepfigures-results.json,graph,2020.scil-1.21
2020.scil-1.21.pdf-Figure3.png,"Figure 3: The factors xy, oxy, and xyn under C.",[u'. figure 3 shows the less-anchored versions of oxyn.'],2020.scil-1.21deepfigures-results.json,graph,2020.scil-1.21
2020.scil-1.21.pdf-Figure4.png,Figure 4: The factor oxy under C .,[u'. figure 4 shows this transformation applied to the factor oxy.'],2020.scil-1.21deepfigures-results.json,graph,2020.scil-1.21
2020.scil-1.25.pdf-Figure1.png,Figure 1: MT-FST for 1-to-1 slot-filling.,"[u'k = [1, 1, 1]. The function is modeled by the deterministic asynchronous MT-FST in Figure 1 ', u'To understand why the function is [1,1,1]-MISL, consider its MT-FST in Figure 1 . Besides the initial and final state, there is only one state q 1 .', u""output turZim is generated with the same [1,1,1]-MISL function that's modeled by the MT-FST in Figure 1 . A sample derivation is provided in the appendix."", u'. it is modeled by the same mt-fst in figure 1 but with the additional transition arc: [c,  o , n] : [+1, 0, 0] : between q 1 , q 1 .', u': Derivation of kutib using the MT-FST in Figure 1 .', u'to generate k<t>usib. This function is [1,1,1]-MISL. It is computed by the same MT-FST in Figure 1 but with the additional transition arc:', u': Derivation of turZim using the MT-FST in Figure 1 .', u'using the [1,1,1]-MISL MT-FST from Figure 1 .']",2020.scil-1.25deepfigures-results.json,graph,2020.scil-1.25
2020.scil-1.25.pdf-Figure3.png,Figure 3: MT-FST for 1-to-1 slot-filling with final consonant deletion,"[u': Derivation of muGnit using the MT-FST in Figure 3 Current C-tape V-tape T-tape Output Output State Symbol String 1. q 0 oksbn ouin oCtVCVCn', u'is muGnit with final consonant deletion. This function is modeled by the [1,1,1]-MISL MT-FST in Figure 3 , illustrated with the derivation in']",2020.scil-1.25deepfigures-results.json,graph,2020.scil-1.25
2020.scil-1.25.pdf-Figure4.png,Figure 4: MT-FST for 1-to-1 slot-filling with preassociated affixes,"[u'root C=ksb and vocalism V=ui, the output is ktusib. A [1,1,1]-MISL MT-FST is provided in Figure 4 along with a sample derivation in [+1,+1,+1]:', u'along with a sample derivation in [+1,+1,+1]: Figure 4 : MT-FST for 1-to-1 slot-filling with preassociated affixes']",2020.scil-1.25deepfigures-results.json,graph,2020.scil-1.25
2020.scil-1.25.pdf-Figure5.png,Figure 5: MT-FST for 1-to-many slot-filling with final spread of vowels,"[u'root C=ktb, the output is katab. This function is modeled with the [1,2,1]-MISL MT-FST in Figure 5 , illustrated with a sample derivation in', u': Derivation of katab using the MT-FST in Figure 5 Current C-tape V-tape T-tape Output Output State Symbol String 1. q 0 oktkn ouin oCVC']",2020.scil-1.25deepfigures-results.json,graph,2020.scil-1.25
2020.scil-1.25.pdf-Figure6.png,Figure 6: MT-FST for 1-to-many slot-filling with medial spread of consonants,"[u'. this is modeled by the [2,1,1]-misl mt-fst in figure 6 .', u': Derivation of kuttik using the MT-FST in Figure 6 q0 start']",2020.scil-1.25deepfigures-results.json,graph,2020.scil-1.25
2020.scil-1.26.pdf-Figure1.png,Figure 1: MT-FST for Mende,"[u'This function is computed by the deterministic asynchronous MT-FST in Figure (1) . It uses two input tapes: a tone tape T and a vowel tape V.', u'q3 (L, ) q f Figure 1 : MT-FST for Mende marks the read input symbols on the input string w i', u'of HL + felama over its tone-vowel tiers HL + eaa with the MT-FST in Figure 1 show that they are all MISL. Example MT-FSTs and derivations for cases b,c are in']",2020.scil-1.26deepfigures-results.json,graph,2020.scil-1.26
2020.scil-1.26.pdf-Figure4.png,Figure 4: MT-FST for Hausa,"[u'initial spread, e.g. f ([LH, VVV])=VVV. This function is modeled by the [2,1]-MISL MT-FST in Figure 4 , with a sample derivation in']",2020.scil-1.26deepfigures-results.json,graph,2020.scil-1.26
2020.scil-1.26.pdf-Figure5.png,Figure 5: MT-FST for Rimi,"[u'f ([hHi, VhViVV]=VVVV. This function is modeled by the [1,2]-MISL MT-FST in Figure 5 , with a sample derivation in']",2020.scil-1.26deepfigures-results.json,graph,2020.scil-1.26
2020.scil-1.26.pdf-Figure6.png,Figure 6: MT-FST for Zigulu,"[u'. this function is modeled by the [1,3]-misl mt-fst in figure 6 , with a sample derivation in 7.']",2020.scil-1.26deepfigures-results.json,graph,2020.scil-1.26
2020.scil-1.26.pdf-Figure7.png,Figure 7: MT-FST for Bemba,"[u'f ([hHi,VhViVV])=VVVV. This function is modeled by the [1,2]-MISL MT-FST in Figure 7 , with a sample derivation in']",2020.scil-1.26deepfigures-results.json,graph,2020.scil-1.26
2020.scil-1.26.pdf-Figure8.png,Figure 8: MT-FST for Arusa,"[u' Figure 8 , with a sample derivation in 9. The FST reads the input from right-toleft using', u': Derivation of f ([hHihHi,hV V(VVV]=V VVVV in Arusa with the MT-FST in Figure 8 ']",2020.scil-1.26deepfigures-results.json,graph,2020.scil-1.26
2020.scil-1.4.pdf-Figure2.png,"Figure 2: Fragment of first transducer, from MSPs to abstract morphemes. (For compactness, only one MSP per morphosyntactic category is shown.)",[],2020.scil-1.4deepfigures-results.json,graph,2020.scil-1.4
2020.scil-1.4.pdf-Figure3.png,"Figure 3: Fragment of second transducer, from abstract morphemes to characters. Only the lexical entry for M1=1, only three steps in the linear chain, and only the character h are shown.",[u'The second transducer is a lexicon (Figure 3 ) which maps each abstract morpheme to a phonological underlying form. Cotterell et al. implement'],2020.scil-1.4deepfigures-results.json,graph,2020.scil-1.4
2020.scil-1.4.pdf-Figure4.png,"Figure 4: Fragment of third transducer responsible for altering i to y. X stands for any character, V for any vowel and C for any consonant.","[u'. the third transducer (figure 4 ) implements phonological rules.', u'; the transducer for assimilation resembles the one in Figure 4 . We use = 1000 as a bias parameter to penalize both phonological rules (vowel']",2020.scil-1.4deepfigures-results.json,graph,2020.scil-1.4
2020.scil-1.56.pdf-Figure1.png,Figure 1: A 2-I-2-TISL SFST for LHOR (top) and a 2-ISL SFST for its tier projection (bottom).,[],2020.scil-1.56deepfigures-results.json,graph,2020.scil-1.56
2020.scil-1.56.pdf-Figure4.png,Figure 4: Left: Sample metrical grids for the two-letter Dybos Rule. Right: 2-ISL SFST implementing the level-2 rule for the two-letter Dybos Rule.,"[u'23 and 1 is given by the 2-ISL SFST shown in the right panel of Figure 4 . Following the MGT analysis of LHOR stress, 1 serves to mark all heavy syllables,']",2020.scil-1.56deepfigures-results.json,graph,2020.scil-1.56
2020.scil-1.61.pdf-Figure1.png,"Figure 1: Aari sibilant harmony as an OPT1 function, where ? denotes any non-sibilant","[u'. for example, consider figure 1 which shows how a hypothetical opt 1 function would model the aari sibilant harmony from above.', u'. if this is indeed an appropriate definition of the osp k functions, an automata-theoretic characterization as in figure 1 seems likely achievable.']",2020.scil-1.61deepfigures-results.json,graph,2020.scil-1.61
2020.sigdial-1.32.pdf-Figure1.png,Figure 1: A dialogue graph using a state machine approach with NATEX to dialogue management.,"[u'. 2 additionally, several types of error checking are performed before runtime such as: figure 1 : a dialogue graph using a state machine approach with natex to dialogue management.', u'. in our framework, state transitions alternate between the user and the system to track turn taking, and are defined by natex (figure 1 ).', u'. similarly, developers can specify a catch-all ""error transition"" (error in figure 1 ) to handle cases where no transition\'s natex returns a match.', u'. the following demonstrates the streamlined json syntax for specifying transitions s 1 , s 3 , u 1 , u 2 , and u 3 in figure 1 .']",2020.sigdial-1.32deepfigures-results.json,graph,2020.sigdial-1.32
2020.tacl-1.23.pdf-Figure1.png,"Figure 1: Graphical model showing the factorization of our noisy channel model where yi indicates the ith target language sentence andxi indicates the ith source language sentence. In the prior (top) the target sentences (the yis) only influence the corresponding source sentence and therefore can be learned and modeled independently, but at test time (bottom), when the target is not observed, each yi depends on every xi.","[u'. thus conceived, this is a generative model of parallel documents that makes a particular independence assumption; we illustrate the corresponding graphical model on the top of figure 1 .', u'. the (in)dependencies that are present in the posterior distribution are shown in the bottom of figure 1 .']",2020.tacl-1.23deepfigures-results.json,graph,2020.tacl-1.23
C00-1023.pdf-Figure1.png,Figure 1: An example of a Bayesian network and the probabilities at each node that de ne the relationships between a node and its parents. The equation at the bottom shows how the distribution across all of the variables is computed.,[],C00-1023deepfigures-results.json,graph,C00-1023
C00-1023.pdf-Figure2.png,"Figure 2: The structure of the belief network that represents the selectional preference of great#1. The leaf nodes are the nouns within the training set, and the intermediate nodes re ect the ISA hierarchy from WordNet. The probabilities at each node are used to disambiguate novel adjective-noun pairs.",[],C00-1023deepfigures-results.json,graph,C00-1023
C00-1023.pdf-Figure3.png,Figure 3: Query of the great#1 belief network to infer the probability of ood being modi ed by great#1. The left branch of the network has been omitted for clarity.,[],C00-1023deepfigures-results.json,graph,C00-1023
C00-1054.pdf-Figure10.png,Figure 10: Result from speech recognizer,"[u'. in our example case it yields the word sequence email this person and that organization ( figure 10) .', u'with the result lattice from speech recognition (Figure 10) , yielding transducer Gest-SpeechFST']",C00-1054deepfigures-results.json,graph,C00-1054
C00-1054.pdf-Figure3.png,Figure 3: Multimodal three-tape FSA,"[u'translates into the FSA in Figure 3 , a three-tape finite state device capable of composing two input streams into a single', u'. instead of having the specific values in the fsa, we have the transitions "":e 1 :e 1 , "":e 2 :e 2 , "":e 3 :e 3 : : : in each location where content needs to be transferred from the gesture tape to the meaning tape (see figure 3) .', u'. in order to implement our approach, we convert the three-tape fsa (figure 3) into an fst, by decomposing the transition symbols into an input component (gw ) and output component m, thus resulting in a function, t :g w ! m.']",C00-1054deepfigures-results.json,graph,C00-1054
C00-1054.pdf-Figure5.png,Figure 5: Gesture finite-state machine,[u'the gesture input is unambiguous and the Gesture finite state machine will be as in Figure 5 . If the gestural input involves gesture recognition or is otherwise ambiguous it is represented'],C00-1054deepfigures-results.json,graph,C00-1054
C00-1054.pdf-Figure7.png,Figure 7: Transducer relating gesture and speech (R:G!W ),"[u'. the domain of this function t can be further curried to result in a transducer that maps r:g ! w (figure 7) .', u'. (figure 7) .']",C00-1054deepfigures-results.json,graph,C00-1054
C00-1054.pdf-Figure8.png,Figure 8: GestLang Transducer,"[u'. the result of this composition is a transducer gest-lang (figure 8 ).', u'that we removed in the projection step before recognition. This is achieved by composing Gest-Lang (Figure 8) with the result lattice from speech recognition']",C00-1054deepfigures-results.json,graph,C00-1054
C00-1054.pdf-Figure9.png,Figure 9: Projection of Output tape of GestLang Transducer,"[u'. in order to use this information to guide the speech recognizer, we then take a projection on the output tape (speech) of gest-lang to yield a finite-state machine which is used as a language model for speech recognition (figure 9 ).']",C00-1054deepfigures-results.json,graph,C00-1054
C02-1033.pdf-Figure2.png,Figure 2  Automaton for topic shift detection,[u'The transition from one state to another follows the automaton of Figure 2 according to three parameters: -its current state; -the similarity between the context of the focus'],C02-1033deepfigures-results.json,graph,C02-1033
C02-1151.pdf-Figure1.png,Figure 1: Conceptual view of entities and relations,"[u'. conceptually, the entities and relations can be viewed, taking into account the mutual dependencies, as the labeled graph in figure 1 , where the nodes represent entities ( phrases) and the links denote the binary relations between the entities.', u'. the number of features on which this learning problem depends could be huge, and they can be of different granularity and based on previous learned predicates ( pos), as caricatured using the ""network-like"" structure in figure 1 .']",C02-1151deepfigures-results.json,graph,C02-1151
C04-1022.pdf-Figure1.png,Figure 1: Standard backoff path for a 4-gram language model over words (left) and backoff graph for 4-gram over factors (right).,"[u'. this can be visualized as a backoff path (figure 1(a) ).', u'. in this case, several backoff paths are possible, which can be summarized in a backoff graph (figure 1(b) ).']",C04-1022deepfigures-results.json,graph,C04-1022
C04-1022.pdf-Figure2.png,Figure 2: Generation of Backoff Graph from production rules selected by the gene 10110.,"[u'. the choice of rules used to generate the backoff graph is encoded in a binary string, with 1 indicating the use and 0 indicating the non-use of a rule, as shown schematically in figure 2 .']",C04-1022deepfigures-results.json,graph,C04-1022
C04-1027.pdf-Figure1.png,"Figure 1: The more likely path in this FSA segment is given by the choice of resign(D,E,F ) : 0.15, followed by cperson(E) : 0.64 and finally succeed(G,C,E) : 0.26. This can be interpreted as follows: If a person C is elected, another person E has resigned and C succeeds E",[u'The automaton of Figure 1 incorporates the following rules:'],C04-1027deepfigures-results.json,graph,C04-1027
C04-1027.pdf-Figure2.png,"Figure 2: Here the more likely path is provided by the sequence: cperson(C) : 0.32, director(C,D) : 0.08, of(D,E) : 0.13, company(E) : 1, succeed(F,C,G) : 0.25, cperson(F ) : 1. This can be read as: If a person C is elected director of a company E then C succeeds another person G.","[u""The automaton of Figure 2 provides for rules such as: 'If a person is elected chairman of a company E""]",C04-1027deepfigures-results.json,graph,C04-1027
C04-1058.pdf-Figure1.png,Figure 1: Piped architecture with n-fold partitioning.,"[u'. figure 1 illustrates the ntpc architecture.', u'. during the evaluation phase, depicted in the lower portion of figure 1 , the test set is first labeled by the base model.']",C04-1058deepfigures-results.json,graph,C04-1058
C04-1080.pdf-Figure1.png,Figure 1: Graphical Structure of Traditional HMM Tagger (top) and Contextualized HMM Tagger (bottom),"[u'. this change in structure, which we will call a contextualized hmm, is depicted in figure 1 .']",C04-1080deepfigures-results.json,graph,C04-1080
C04-1096.pdf-Figure4.png,Figure 4: A simplified graph with a group vertex for the situation shown in Figure 1,"[u'If we consider a perceptual group as an ordinary object as shown in Figure 4 , their algorithm is applicable. It will be able to handle not only intra-group relations', u'handle not only intra-group relations (e.g., the edges with labels ""front"", ""middle"", and ""back"" in Figure 4 ) but also inter-group relations (e.g., the edge from ""Group 1"" to ""', u'in Figure 4 ). However, introducing perceptual groups as vertices makes it difficult to design the cost function.', u'front_of right_of back_of back_of left_of front middle back right_of right_of right_of Figure 4 : A simplified graph with a group vertex for the situation shown in']",C04-1096deepfigures-results.json,graph,C04-1096
C08-1027.pdf-Figure1.png,Figure 1: Example word lattice.,[u'. figure 1 shows the word lattice for the example in table 3.'],C08-1027deepfigures-results.json,graph,C08-1027
C08-1056.pdf-Figure1.png,Figure 1: Transducing phone sequences into word sequences with a dictionary,[u'. this mechanism is illustrated on figure 1 .'],C08-1056deepfigures-results.json,graph,C08-1056
C08-1062.pdf-Figure1.png,Figure 1 Positive and Negative Reinforcement,"[u'. in this paper, we study two kinds of reinforcement, namely positive and negative reinforcement, among two document collections, as illustrated in figure 1 .', u'In Figure 1 , ""A"" and ""B"" denote two document collections about the same topics (""A"" is the']",C08-1062deepfigures-results.json,graph,C08-1062
C08-1092.pdf-Figure1.png,Figure 1. Cycle topology,[],C08-1092deepfigures-results.json,graph,C08-1092
C08-1092.pdf-Figure2.png,Figure 2. Asymmetric ring topology,[],C08-1092deepfigures-results.json,graph,C08-1092
C08-1092.pdf-Figure3.png,Figure 3. Symmetric ring topology,[],C08-1092deepfigures-results.json,graph,C08-1092
C08-1099.pdf-Figure3.png,"Figure 3: Graphical representation of a Hierarchic Hidden Markov Model. Circles denote random variables, and edges denote conditional dependencies. Shaded circles are observations.",[u'. a graphical representation of an hhmm with three levels is shown in figure 3 .'],C08-1099deepfigures-results.json,graph,C08-1099
C08-1126.pdf-Figure1.png,Figure 1: Schematic structure of a general graph. Example taken from Sedgewick (1990).,[],C08-1126deepfigures-results.json,graph,C08-1126
C08-1126.pdf-Figure2.png,Figure 2: Schematic star-like structure of a wordnet.,[],C08-1126deepfigures-results.json,graph,C08-1126
C08-1126.pdf-Figure3.png,Figure 3: A path through the wordnet.,[],C08-1126deepfigures-results.json,graph,C08-1126
C08-1126.pdf-Figure4.png,Figure 4: Node classes. White: inner nodes; dark gray: tree nodes; dark gray with thick border: root nodes; light gray: leaf nodes.,[],C08-1126deepfigures-results.json,graph,C08-1126
D09-1110.pdf-Figure2.png,Figure 2: A compact forest containing both the source and derived sentences of Figure 1. Parts of speech are omitted.,"[u'A compact forest F represents a set of dependency trees. Figure 2 shows an example of a compact forest, containing both the source and derived sentences of', u'. in figure 2 , we may choose to connect the root either to the left see, resulting in the source passive sentence, or to the right see, resulting in the derived active sentence.', u'. the black part of figure 2 corresponds to the initial forest.', u'. this is illustrated by the sharing of yesterday in figure 2 .']",D09-1110deepfigures-results.json,graph,D09-1110
D09-1110.pdf-Figure3.png,"Figure 3: A compact forest representing the 23 sentences derivable from the sentence children are fond of candies using the following three rules: childrenkids, candiessweets, and X is fond of YX likes Y.","[u'This set of trees, termed embedded trees, comprise the set of trees represented by F. Figure 3 shows another example for a compact', u'shows another example for a compact Figure 3 : A compact forest representing the 2 3 sentences derivable from the sentence ""children are', u"". figure 3) , the rule 'x is fond of yx likes y' will be matched and applied only once, rather than four times (for each combination of matching x and y )."", u'. for example, the three rule matches presented in figure 3 are independent.', u'. thus, the resulting size is o(|t |+k), as we can see from figure 3 .']",D09-1110deepfigures-results.json,graph,D09-1110
D10-1037.pdf-Figure2.png,"Figure 2: A graphical depiction of the generative process for a labeled document at training time (See Section 3); shaded nodes indicate variables which are observed at training time. First the latent underlying content structure T is drawn. Then, the document text s is drawn conditioned on the content structure utilizing content parameters . Finally, the observed task labels for the document are modeled given s and T using the task parameters . Note that the arrows for the task labels are undirected since they are modeled discriminatively.",[],D10-1037deepfigures-results.json,graph,D10-1037
D11-1077.pdf-Figure1.png,Figure 1: Bayesian Network for MWE identification,"[u'. over these nodes we impose the structure depicted graphically in figure 1 .', u'which we train a Bayesian Network whose structure reflects manually-crafted linguisticallymotivated knowledge, as depicted in Figure 1 The linguistically-motivated features defined in Section 3.2 are clearly helpful in the classification task: the']",D11-1077deepfigures-results.json,graph,D11-1077
D11-1092.pdf-Figure4.png,Figure 4. Paths are established between different concepts if they share values of same feature types <bold underlined>,"[u'. as shown in figure 4 , different concepts are connected if they share same values of same types of features, namely, there exists a path that connects one concept to another.', u'. figure 4 it is easier to comprehend the difference between feature combination and integration.']",D11-1092deepfigures-results.json,graph,D11-1092
D11-1112.pdf-Figure1.png,"Figure 1: Deterministic automaton that accepts all strings over alphabet {a, b, c} with infix abac.","[u'. this construction is illustrated in figure 1 .', u'. see for example figure 1 , with cycles running through states t 0 , .']",D11-1112deepfigures-results.json,graph,D11-1112
D11-1122.pdf-Figure1.png,"Figure 1: Simplified example of an argument-instance graph. All pairs of vertices with non-zero similarity are connected through edges that are weighted with a similarity score (vi,v j). Upon updating the label for a vertex all neighboring vertices propagate their label to the vertex being updated. The score for each label is determined by summing together the weighted votes for that label and the label with the maximal score is chosen.","[u'. figure 1 shows an example of a graph for a verb with five argument instances (vertices a-e).', u'. consider again figure 1 .']",D11-1122deepfigures-results.json,graph,D11-1122
D11-1122.pdf-Figure2.png,"Figure 2: The update rule (Equation 2) can be understood as choosing a minimal edge-cut, thereby greedily maximizing intra-cluster similarity and minimizing intercluster similarity. Assuming equal weight for all edges above, label 3 is chosen for the vertex being updated such that the sum of weights of edges crossing the cut is minimal.",[u'. this is illustrated in figure 2 .'],D11-1122deepfigures-results.json,graph,D11-1122
D11-1127.pdf-Figure1.png,Figure 1: PDA Examples: (a) Non-regular PDA accepting {anbn|n  N}. (b) Regular (but not bounded-stack) PDA accepting ab. (c) Bounded-stack PDA accepting ab and (d) its expansion as an FSA.,"[u'. figure 1 shows non-regular, regular and bounded-stack pdas.', u'. Figure 1d show the result of the algorithm when applied to the PDA of', u'show the result of the algorithm when applied to the PDA of Figure 1c .']",D11-1127deepfigures-results.json,graph,D11-1127
D11-1135.pdf-Figure1.png,"Figure 1: Rel-LDA model. Shaded circles are observations, and unshaded ones are hidden variables. A document consists of N tuples. Each tuple has a set of features. Each feature of a tuple is generated independently from a hidden relation variable r.",[u'. figure 1 shows the graphical representation of rel-lda.'],D11-1135deepfigures-results.json,graph,D11-1135
D13-1002.pdf-Figure5.png,Figure 5: Graph derived from discourse annotation in Figure 4.,[u'is given in Figure 5 . Each text fragment t i maps to a vertex n i in the graph.'],D13-1002deepfigures-results.json,graph,D13-1002
D13-1004.pdf-Figure1.png,"Figure 1: Plate diagram depicting the morphology model (adapted from Goldwater et al. (2006)). Hyperparameters have been omitted for clarity. The left-hand plate depicts the base distribution P0; note that the morphological analyses lk are generated deterministically as (tk,sk, fk). The observed words wi are also deterministic given zi = k and lk, since wi = sk fk.","[u'initially empty tables. Upon entering, each customer is seated at a table k with probability Figure 1 : Plate diagram depicting the morphology model (adapted from', u'. figure 1 illustrates how the full pyp morphology model generates the observed sequence of word tokens.']",D13-1004deepfigures-results.json,graph,D13-1004
D13-1004.pdf-Figure2.png,"Figure 2: Plate diagram depicting the joint model. Hyperparameters have been omitted for clarity. The L-shaped plate contains the tokens, while the square plates contain the morphological analyses. The t are latent tags, zi is an assignment to a morphological analysis lk = (sk, fk), and wi is the observed word. T is the number of distinct tags, and Kt the number of tables used by tag type t.","[u'The full model (Figure 2 ) combines the latent tag sequence with the morphology model. Tag tokens are generated conditioned', u'. the distribution of customers figure 2 : plate diagram depicting the joint model.']",D13-1004deepfigures-results.json,graph,D13-1004
D13-1008.pdf-Figure3.png,"Figure 3: Example DAGs, built from the cluster containing the words never and gladly.",[u'. sample graphs are shown in figure 3 .'],D13-1008deepfigures-results.json,graph,D13-1008
D13-1012.pdf-Figure2.png,"Figure 2: (a) An example citation network. (b) Graphical model for TIR on the example network, collapsing out  but retaining topics . Influence variables and hyperparameters not shown for simplicity.","[u'. in the figure, an edge in (a) from c to d corresponds to a citation of c by d.']",D13-1012deepfigures-results.json,graph,D13-1012
D13-1119.pdf-Figure1.png,Figure 1: A search space for word segmenter,"[u'. in section 4 we de- figure 1 : a search space for word segmenter scribe our co-training experiments.', u'contains the words {} and all single Chinese characters, the search space is illustrated in Figure 1 ']",D13-1119deepfigures-results.json,graph,D13-1119
D13-1130.pdf-Figure1.png,Figure 1: An example of a lattice L.,[],D13-1130deepfigures-results.json,graph,D13-1130
D13-1138.pdf-Figure1.png,Figure 1: The factored translation model equivalent to our approach. The generation step assigns all probability mass to a single event: pgen(c(e)|e) = 1.,"[u'mathematically equivalent to a special case of the factored translation framework, which is shown in Figure 1 . The generation step from target word e to its target class c(e) assigns all']",D13-1138deepfigures-results.json,graph,D13-1138
D13-1171.pdf-Figure1.png,Figure 1: Sentiment expressed across an entity.,[],D13-1171deepfigures-results.json,graph,D13-1171
D14-1037.pdf-Figure1.png,"Figure 1: Portion of the word association graph for part of the sample sentence in Table 3. (d: distance, w: edge weight).","[u'is shown in Figure 1 .', u'.2) figure 1 : portion of the word association graph for part of the sample sentence in  an edge is created between two nodes in the graph, if the corresponding word pair ( token/pos pair) are contextually associated.']",D14-1037deepfigures-results.json,graph,D14-1037
D14-1037.pdf-Figure2.png,Figure 2: Sample nodes and edges from the word association graph.,"[u"". for example, figure 2 shows the edges that would be derived let'sl startv thisd morningn wp ad beatifula smilen .""]",D14-1037deepfigures-results.json,graph,D14-1037
D14-1037.pdf-Figure3.png,"Figure 3: A portion of the graph that includes the OOV token beatiful, its neighbors and the candidate nodes that each neighbor is connected to. Thick lines show the edge list with relative weights.","[u'. figure 3 represents a portion from the graph where the neighbors and candidates of the oov node ""beatiful"" are shown.', u'. as shown in figure 3 , there are 11 lines representing the edges between the neighbors of the oov token and the candidate nodes.', u'. as described in the previous section the edge weights are computed based on the frequency figure 3 : a portion of the graph that includes the oov token ""beatiful"", its neighbors and the candidate nodes that each neighbor is connected to.', u'The edge weights of the edges in EL(o 2 ) are shown in Figure 3 . The edges that are connected to the OOV neighbor ""w"" have smaller edge weights']",D14-1037deepfigures-results.json,graph,D14-1037
D14-1120.pdf-Figure2.png,Figure 2. Tweet label design.,[u'. figure 2 illustrates our annotation for each tweet.'],D14-1120deepfigures-results.json,graph,D14-1120
D15-1099.pdf-Figure1.png,"Figure 1: When training the classifier for the second label, the feature (the bold lines) consists of only the origin feature and the prediction for the first label. In this time, it is impossible to model the dependencies between the second label and the third label.","[u'. it means that we only model the dependencies between the first label and the sec- figure 1 : when training the classifier for the second label, the feature (the bold lines) consists of only the origin feature and the prediction for the first label.']",D15-1099deepfigures-results.json,graph,D15-1099
D15-1163.pdf-Figure4.png,"Figure 4: Sensitivity issue in graph propagation for translations. Lager is a translation candidate for stock, which is transferred to majority after 3 iterations.","[u'in a part of an English para-stock bank margin majority stock Lager iter1 iter2 iter3 Figure 4 : Sensitivity issue in graph propagation for translations. ""Lager"" is a translation candidate for ""stock"",']",D15-1163deepfigures-results.json,graph,D15-1163
D15-1297.pdf-Figure1.png,"Figure 1. The Graphical representation of JEAM. All the latent variables are marked in white, and all the observed variables are marked in gray.","[u'relations in EA, e.g., Eq(1) and Eq(2). We draw the graphical representation of JEAM in Figure 1 , and show the notations of this paper in', u'In Figure 1 , y denotes the sentiment orientation of the author, which is a latent variable in']",D15-1297deepfigures-results.json,graph,D15-1297
D16-1224.pdf-Figure1.png,Figure 1: AMR graph for The boy wants to go.,"[u'. shown in figure 1 , the nodes of an amr graph ( ""boy"", ""go-01"" and ""want-01"") represent concepts, and the edges ( ""arg0"" and ""arg1"") represent relations between concepts.', u'To show a brief example, consider the AMR in Figure 1 and the following rules,', u'. for example, for concept ""w/want-01"" in figure 1 , we generate concept rules such as ""(w/want-01) ||| want"", ""(w/want-01) ||| wants"", ""(w/want-01) ||| wanted"" and ""(w/want-01) ||| wanting"".']",D16-1224deepfigures-results.json,graph,D16-1224
D16-1232.pdf-Figure4.png,Figure 4: Graphical model of the utterance model.,"[u'an n-gram-based distribution over V that is defined in an identical way to (1) and Figure 4 : Graphical model of the utterance model.', u'. figure 4 presents the graphical model of our utterance model.']",D16-1232deepfigures-results.json,graph,D16-1232
D17-1005.pdf-Figure3.png,"Figure 3: Graphical model of oc,is correctness",[],D17-1005deepfigures-results.json,graph,D17-1005
D17-1020.pdf-Figure4.1.png,"Figure 4.1: Sentence graphs for summarization. G: sentence affinity graph constructed from the document cluster. GA: sentence augmented graph with an absorbing vertex s0. Cmax equals toC(s1) indicating that sentence s1 has the maximum conductance, so there is no edge (s1, s0).",[],D17-1020deepfigures-results.json,graph,D17-1020
D17-1020.pdf-Figure4.2.png,"Figure 4.2: Sentence augmented graphs for summarization in two successive iterations. GA(K): augmented graph in the iteration K. Virtual summary V = {s1, s3, s5}, which is constructed from x in the iteration (K1) by producingSummary. D = diag([C(s1), Cmax, C(s3), Cmax, C(s5)]). GA(K + 1): augmented graph in the iteration (K+1). V = {s1, s4, s5}, which is constructed from x in the iteration K by producingSummary. D = diag([C(s1), Cmax, Cmax, C(s4), C(s5)]). In both cases, Cmax equals to C(s1) indicating that sentence s1 has the maximum conductance.",[],D17-1020deepfigures-results.json,graph,D17-1020
D17-1108.pdf-Figure1.png,Figure 1: The desired event temporal graph for Ex1. Reverse TLINKs such as hurt is after ripping are omitted for simplicity.,[],D17-1108deepfigures-results.json,graph,D17-1108
D17-1108.pdf-Figure2.png,"Figure 2: The human-annotation for Ex1 provided in TE3, where many TLINKs are missing due to the annotation difficulty. Solid lines: original human annotations. Dotted lines: TLINKs inferred from solid lines. Dashed lines: missing relations.",[],D17-1108deepfigures-results.json,graph,D17-1108
D17-1133.pdf-Figure2.png,Figure 2: Intra-sentential structure CRF with pairwise modeling.,"[u'A linear-chain CRF for intra-sentential discourse parsing is shown in Figure 2 . Here, C = {c 0 , , c t , , c n }']",D17-1133deepfigures-results.json,graph,D17-1133
D17-1133.pdf-Figure3.png,Figure 3: Intra-sentential relation CRF with pairwise modeling.,[u'. figure 3 depicts the relation crf for intrasentential parsing.'],D17-1133deepfigures-results.json,graph,D17-1133
D18-1152.pdf-Figure1.png,"Figure 1: A two-state WFSA B described in 2. It is closely related to several models studied in this paper (4.1). Bold circles indicate initial states, and double circles final states, which are associated with final weights. Arrows represent transitions, labeled by the symbols  they consume, and the weights as a function of . Arcs not drawn are assumed to have weight 0. For brevity, 8 means 8 2 , with  being the alphabet.","[u'8/() Figure 1 : A two-state WFSA B described in 2. It is closely related to several models', u'. for instance, we will show in 4 that the wfsa diagrammed in figure 1 has strong connections to several of the models mentioned above.', u'. figure 1 diagrams a wfsa b, consisting of two states.', u"". let's recall the wfsa b (figure 1 and example 5) using the real semiring hr, +, , 0, 1i.""]",D18-1152deepfigures-results.json,graph,D18-1152
D18-1152.pdf-Figure2.png,Figure 2: A three-state WFSA C discussed in 4.2.,"[u'. figure 2 diagrams a wfsa c, augmenting b with another state.', u'. compared to c (figure 2) , f uses q 1 as a second final state, aiming to capture both unigram and bigram patterns, since a path is allowed to stop at q 1 after consuming one input.']",D18-1152deepfigures-results.json,graph,D18-1152
D18-1152.pdf-Figure3.png,Figure 3: WFSA D1 discussed in 4.3. Two initial states q1 and q4 are used here.,"[u'. figure 3 diagrams one of them, d 1 .']",D18-1152deepfigures-results.json,graph,D18-1152
D18-1152.pdf-Figure4.png,"Figure 4: A WFSA F that combines both unigram and bigram features (5.1). Two final states q1 and q2 are used, with weights 1 and 2, respectively.","[u'. figure 4 diagrams a 4-state wfsa f.', u'. the ""-transition allows for skipping the figure 4 : a wfsa f that combines both unigram and bigram features ( 5.1).']",D18-1152deepfigures-results.json,graph,D18-1152
D18-1222.pdf-Figure3.png,Figure 3: An inference example of TransC.,"[u'As shown in Figure 3 , New York City is an instance and others are concepts. The solid lines represent']",D18-1222deepfigures-results.json,graph,D18-1222
D18-1236.pdf-Figure1.png,Figure 1: Illustration of the dual learning system of Logician and Orator.,"[u'. following rewards are introduced into the proposed dual learning system, and the relationships between them are shown in figure 1 .']",D18-1236deepfigures-results.json,graph,D18-1236
D19-3027.pdf-Figure10.png,"Figure 10: Alternation variant of Fig. 9. (q1, q2, q3) is (2C, 21, 2B). Considering that 102C and 1021 are frequently used, the convenient v-key is assigned instead the natural Romanization by a.",[],D19-3027deepfigures-results.json,graph,D19-3027
D19-3027.pdf-Figure11.png,"Figure 11: Alternation in Fig. 8 with an extra branch. (q1, q2, q3, q4) is (1E, 3F, 29, 2A). Here, s is a natural Romanization for 101E, whereas 103F, 1029, and 102A are extremely obscure.",[],D19-3027deepfigures-results.json,graph,D19-3027
D19-3027.pdf-Figure12.png,"Figure 12: Alternation by h and g. (, q1, q2, q3, q4) can be (t, 10, 11, 0B, 0C), and (d, 12, 13, 0D, 0E). Both t and d are the natural Romanization, and h is also a part of the Romanization.",[],D19-3027deepfigures-results.json,graph,D19-3027
D19-3027.pdf-Figure13.png,"Figure 13: Most complex alternation. (q1, q2, q3, q4, q5, q6) is (14, 04, 0A, 0F, 09, 4C). Here, n, ng and ny are the natural Romanization for 1014, 1004, and 100A, respectively. Other alternated characters are rare.",[],D19-3027deepfigures-results.json,graph,D19-3027
D19-3027.pdf-Figure14.png,"Figure 14: Doubled and looped alternation. (, q1, q2) can be (j, 38, 37), and (f, 3A, 39). Here, 1038 and 103A are remarkably frequent marks; hence convenient j- and f-keys are assigned, respectively.",[],D19-3027deepfigures-results.json,graph,D19-3027
D19-3027.pdf-Figure15.png,"Figure 15: Combination of Figs. 12 and 14. (, q1, q2, q3, q4) can be (i, 2D, 2E, 23, 24), and (u, 2F, 30, 25, 26). Here, i and u are the natural Romanization for the corresponding characters.",[],D19-3027deepfigures-results.json,graph,D19-3027
D19-3027.pdf-Figure16.png,"Figure 16: Alternation in Fig. 14 with an extra branch. (q1, q2, q3, q4) is (31, 32, 27, 4F). Here, e is a natural Romanization for 1031, 1032 and 1027, whereas 104F is an abbreviated mark derived from 1027.",[],D19-3027deepfigures-results.json,graph,D19-3027
D19-3027.pdf-Figure7.png,Figure 7: Overall configuration of the automaton.,[],D19-3027deepfigures-results.json,graph,D19-3027
D19-3027.pdf-Figure8.png,"Figure 8: Simplest case. When 2 is h, (1, q1, q2) can be (k, 00, 01), (g, 02, 03), (c, 05, 06), (z, 07, 08), (p, 15, 16), and (b, 17, 18). When 2 is g, (1, q1, q2) is (m, 19, 36). When 2 = 1, (1, q1, q2) can be (y, 3B, 1A), (w, 3D, 1C), and (h, 3E, 1D). All 1 are natural Romanization. When 2 is h, it is also a part of the Romanization.",[],D19-3027deepfigures-results.json,graph,D19-3027
D19-3027.pdf-Figure9.png,"Figure 9: Two-step alternation. When (1, 2) is (l, g), (q1, q2, q3) is (1C, 20, 4E). Here, l is a natural Romanization for 101C and 1020, whereas 104E is a special abbreviated mark with l as onset. When (1, 2) is (r, r), (q1, q2, q3) is (3C, 1B, 4D), respectively. Here, r is a natural Romanization for 103C and 101B, whereas 104D is a special abbreviated mark with r as onset.",[u'. q s q 1 q 2 q 3 q e  1  2 q g q q figure 9 : two-step alternation.'],D19-3027deepfigures-results.json,graph,D19-3027
2007.sigdial-1.12.pdf-Figure4.png,Figure 4. ROC curves from n*-best and n-best,[u'. figure 4 shows the roc curves obtained by the different methods in case 1.'],2007.sigdial-1.12deepfigures-results.json,Line graph_chart,2007.sigdial-1.12
2007.sigdial-1.48.pdf-Figure3.png,Figure 3: Graph showing the number of agenda tree leaf nodes after each observation during a training run performed on a single dialogue.,[],2007.sigdial-1.48deepfigures-results.json,Line graph_chart,2007.sigdial-1.48
2007.sigdial-1.48.pdf-Figure4.png,Figure 4: Graph showing a monotonous increase in log probability L() after each iteration of the EM algorithm.,[],2007.sigdial-1.48deepfigures-results.json,Line graph_chart,2007.sigdial-1.48
2014.lilt-9.5.pdf-Figure2.png,"FIGURE 2: Mixture-based composition, such as the additive model illustrated in the left panel, takes distributional vectors representing two words and mixes them (e.g., adds their components) to obtain the representation of a phrase. In a function-based model, such as the one illustrated on the right, one of the two words is not a vector, but a function exerting an action on the argument vector to move it to a new position in semantic space.",[u'. an approximate geometric intuition for the dierence between a mixture and a functional approach is given in figure 2 .'],2014.lilt-9.5deepfigures-results.json,Line graph_chart,2014.lilt-9.5
2015.jeptalnrecital-court.1.pdf-Figure1.png,"FIGURE 1  PL(k) with 500 L-BFGS iterations, k=1,3,5,7,9,12,15 compared with MIRA in reranking.",[u'. results are shown in figure 1 .'],2015.jeptalnrecital-court.1deepfigures-results.json,Line graph_chart,2015.jeptalnrecital-court.1
2015.jeptalnrecital-court.7.pdf-Figure1.png,"FIGURE 1: Rsultats pour diffrentes mesures de similarits FIGURE 2: Scores ROBO selon le paramtre k,  fix  0.8",[],2015.jeptalnrecital-court.7deepfigures-results.json,Line graph_chart,2015.jeptalnrecital-court.7
2015.jeptalnrecital-long.1.pdf-Figure5.png,FIGURE 5  Conversion graphme-phonme. Taux derreur en fonction de lindice de la dcision : modle gauche-droite ( gauche) et modle ordre libre ( droite),[],2015.jeptalnrecital-long.1deepfigures-results.json,Line graph_chart,2015.jeptalnrecital-long.1
2015.jeptalnrecital-long.18.pdf-Figure3.png,"FIGURE 3: volution du nombre de motifs (chelle logarithmique) en fonction de lordre des rptitions maximales (LIB-40, EBG-40, MIXT-40 et MIXT-80).",[u'des dix chantillons de la validation croise. Les rsultats sont diffrents de ceux de la Figure 3 '],2015.jeptalnrecital-long.18deepfigures-results.json,Line graph_chart,2015.jeptalnrecital-long.18
2015.jeptalnrecital-long.2.pdf-Figure6.png,FIGURE 6  Courbe dapprentissage sur les donnes CoNLL2002 : taux de prcision selon le cot dannotation en mots (chelle log),[],2015.jeptalnrecital-long.2deepfigures-results.json,Line graph_chart,2015.jeptalnrecital-long.2
2015.jeptalnrecital-long.2.pdf-Figure7.png,FIGURE 7  Courbe dapprentissage sur les donnes CoNLL2000 : taux de prcision selon le cot dannotation en mots (chelle log),[],2015.jeptalnrecital-long.2deepfigures-results.json,Line graph_chart,2015.jeptalnrecital-long.2
2015.jeptalnrecital-long.2.pdf-Figure8.png,FIGURE 8  Courbe dapprentissage sur les donnes SensEval-2 : taux de prcision selon le cot dannotation en mots (chelle log),[],2015.jeptalnrecital-long.2deepfigures-results.json,Line graph_chart,2015.jeptalnrecital-long.2
2015.jeptalnrecital-long.2.pdf-Figure9.png,FIGURE 9  Courbe dapprentissage sur les donnes Nettalk : taux de prcision (mots correctement phontiss) selon le cot dannotation en nombre de lettres (chelle log),[],2015.jeptalnrecital-long.2deepfigures-results.json,Line graph_chart,2015.jeptalnrecital-long.2
2015.jeptalnrecital-long.6.pdf-Figure2.png,FIGURE 2  Performances de diverses configurations de la mthode ZSSP en termes de F-score en fonction du nombre de dialogues utilises pour ladaptation.,[],2015.jeptalnrecital-long.6deepfigures-results.json,Line graph_chart,2015.jeptalnrecital-long.6
2015.jeptalnrecital-long.6.pdf-Figure3.png,FIGURE 3  Capacit de gnralisation de lapproche ZSSP sur la corpus de test DSTC2 exprime en termes de F-score sur la dtection d actes de dialogue gnriques (i.e. acttype(champ)) en fonction du pourcentage dexemples de valeurs retires dans K,[],2015.jeptalnrecital-long.6deepfigures-results.json,Line graph_chart,2015.jeptalnrecital-long.6
2015.jeptalnrecital-long.7.pdf-Figure3.png,"FIGURE 3  Les 2 000 premires itrations (burn in) de lchantillonnage pour 4 rles, avec les attributs ajouts  miparcours (50 %, ligne continue), ou sans les attributs (ligne pointille)","[u'. on voit par exemple  la figure 3 , qui montre l\'volution des probabilits de certains lments  mesure des itrations de burn in, que la probabilit de la relation ""kill:dobj"" diminue dans le rle correspondant aux victimes, de mme que celle de l\'entit ""terrorist"" dans ce mme rle.']",2015.jeptalnrecital-long.7deepfigures-results.json,Line graph_chart,2015.jeptalnrecital-long.7
2015.jeptalnrecital-long.9.pdf-Figure1.png,FIGURE 1  Distribution des notes dans le corpus videogamesdaily selon les diffrentes caractristiques (les notes sont arrondies  la valeur entire la plus proche).,[],2015.jeptalnrecital-long.9deepfigures-results.json,Line graph_chart,2015.jeptalnrecital-long.9
2015.jeptalnrecital-recital.2.pdf-Figure1.png,Figure 1: Taux dexactitude sur les noms du jeu de test par Figure 2: Taux dexactitude sur les verbes du jeu de test utilisation de lalgorithme de Lesk tendu par utilisation de lalgorithme de Lesk tendu,[],2015.jeptalnrecital-recital.2deepfigures-results.json,Line graph_chart,2015.jeptalnrecital-recital.2
2015.jeptalnrecital-recital.2.pdf-Figure3.png,Figure 3: Taux dexactitude sur les noms du jeu de test par Figure 4: Taux dexactitude sur les verbes du jeu de test slection alatoire de 30% sur les dpendances syntaxiques sur lensemble des dpendances syntaxiques,[],2015.jeptalnrecital-recital.2deepfigures-results.json,Line graph_chart,2015.jeptalnrecital-recital.2
2015.lilt-12.2.pdf-Figure4.png,FIGURE 4 Graph of stylistic change for Tiresias,"[u'. in the second passage, he describes a lavish scene in which a woman from a privileged social class undertakes her elaborate grooming ritu the narrator\'s presentation is ironic: the description of ""the chair she sat in, like a burnished throne"", is deliberately overblown, and serves to show how desperately out of touch this privileged woman is with the realities of modern life figure 4 graph of stylistic change for tiresias figure 5 graph of stylistic change for marie outside her dressing-room.']",2015.lilt-12.2deepfigures-results.json,Line graph_chart,2015.lilt-12.2
2015.lilt-12.2.pdf-Figure5.png,FIGURE 5 Graph of stylistic change for Marie,"[u'."" in her third passage (35-41), she remembers a moment of greater emotional intensity with an old lover: figure 6 graph of stylistic change for crazy prufrock ""yet when we came back, late, from the hyacinth garden, / your arms full, and your hair wet, i could not / speak, and my eyes failed, i was neither / living nor dead, and i knew nothing, / looking into the heart of light, the silence.']",2015.lilt-12.2deepfigures-results.json,Line graph_chart,2015.lilt-12.2
2015.lilt-12.2.pdf-Figure6.png,FIGURE 6 Graph of stylistic change for Crazy Prufrock,"[u'character Crazy Prufrock is to some degree defined in terms of his stylistic variation (see Figure 6 ). His second (60-76), twelfth (207-214), and fifteenth (367-377) passages provide a representative sample of']",2015.lilt-12.2deepfigures-results.json,Line graph_chart,2015.lilt-12.2
2015.lilt-12.6.pdf-Figure2.png,FIGURE 2 ROC curves on the medium size (left) and large (right) data sets. The embedded box is a zoom over the top-left corner.,"[u'. to check that this is actually the case, we plot the roc (receiver operating characteristic) curve in figure 2 .']",2015.lilt-12.6deepfigures-results.json,Line graph_chart,2015.lilt-12.6
2016.jeptalnrecital-demo.10.pdf-Figure2.png,FIGURE 2  Prcision et rappel.,[],2016.jeptalnrecital-demo.10deepfigures-results.json,Line graph_chart,2016.jeptalnrecital-demo.10
2016.jeptalnrecital-jep.43.pdf-Figure5.png,"FIGURE 5 : superposition des spectres  long terme, en pr-op. (noir) et post-op. (rouge)",[],2016.jeptalnrecital-jep.43deepfigures-results.json,Line graph_chart,2016.jeptalnrecital-jep.43
2016.jeptalnrecital-jep.44.pdf-Figure3.png,FIGURE 3  Evolution de la F-mesure en fonction du nombre de neurones des couches caches du MLP sur BUCKEYE-DEV,[],2016.jeptalnrecital-jep.44deepfigures-results.json,Line graph_chart,2016.jeptalnrecital-jep.44
2016.jeptalnrecital-jep.44.pdf-Figure4.png,FIGURE 4  Courbe DET avec un CNN en fonction du voisinage considr sur BUCKEYE-DEV,[],2016.jeptalnrecital-jep.44deepfigures-results.json,Line graph_chart,2016.jeptalnrecital-jep.44
2016.jeptalnrecital-jep.49.pdf-Figure3.png,FIGURE 3: Impact de sur leffort utilisateur (cots) cumul.,[],2016.jeptalnrecital-jep.49deepfigures-results.json,Line graph_chart,2016.jeptalnrecital-jep.49
2016.jeptalnrecital-jep.70.pdf-Figure2.png,"FIGURE 2  volution du cot sur train, du cot et prcision sur val, au cours des 100 premires itrations dapprentissage. La performance sur le corpus de test PHON-IM est indique par une droite horizontale (88,9%).",[],2016.jeptalnrecital-jep.70deepfigures-results.json,Line graph_chart,2016.jeptalnrecital-jep.70
2016.jeptalnrecital-jep.79.pdf-Figure1.png,FIGURE 1  Estimations des dures moyennes (log) des phases douverture et de fermeture pour chaque cycle mandibulaire mesur. Les fermetures sont significativement plus longues que les ouvertures.,[],2016.jeptalnrecital-jep.79deepfigures-results.json,Line graph_chart,2016.jeptalnrecital-jep.79
2016.jeptalnrecital-jep.79.pdf-Figure7.png,FIGURE 7  Estimations du dplacement vertical de la mandibule (log) dans les phases douverture et de fermeture pour chaque syllabe.,[],2016.jeptalnrecital-jep.79deepfigures-results.json,Line graph_chart,2016.jeptalnrecital-jep.79
2016.jeptalnrecital-poster.27.pdf-Figure1.png,FIGURE 1  Indice silhouette de 2  50 k,[],2016.jeptalnrecital-poster.27deepfigures-results.json,Line graph_chart,2016.jeptalnrecital-poster.27
2017.lilt-15.3.pdf-Figure2.png,FIGURE 2: Comparing effect of vocabulary size across architectures.,[],2017.lilt-15.3deepfigures-results.json,Line graph_chart,2017.lilt-15.3
2017.lilt-15.3.pdf-Figure3.png,"FIGURE 3: Results of testing the effect of various hyper-parameters. The reference (solid blue line) is an LSTM architecture, vocabulary size 10000, training set 90%, single layer, 150 memory units, no drop out.",[],2017.lilt-15.3deepfigures-results.json,Line graph_chart,2017.lilt-15.3
2017.lilt-15.3.pdf-Figure4.png,"FIGURE 4: Results for a configuration with best parameters values: LSTM RNN with 2 layers of 1350 units, dropout rate 0.1, vocabulary size 100k, training on 90%, and lexical embedding dimension size 150",[],2017.lilt-15.3deepfigures-results.json,Line graph_chart,2017.lilt-15.3
2017.lilt-15.3.pdf-Figure5.png,FIGURE 5: Comparing LSTM trained language model (with voc. size 100 and 1000 units) for the two methods of predicting verb number. The solid blue line represents our (supervised) benchmark LSTM RNN.,[],2017.lilt-15.3deepfigures-results.json,Line graph_chart,2017.lilt-15.3
2017.lilt-15.3.pdf-Figure6.png,FIGURE 6: Number of test examples per number of attractors,[],2017.lilt-15.3deepfigures-results.json,Line graph_chart,2017.lilt-15.3
2018.jeptalnrecital-recital.9.pdf-Figure1.png,Figure 1 : Performances MAP selon les segments,"[u'. or, nous pouvons constater figure 1 que leader rank se dtache positivement de page rank lorsque la taille des segments crot.']",2018.jeptalnrecital-recital.9deepfigures-results.json,Line graph_chart,2018.jeptalnrecital-recital.9
2018.lilt-16.1.pdf-Figure1.png,"FIGURE 1 Results for long-distance dependency task for LSTM (red), GRU (yellow) and RUSS (blue).",[],2018.lilt-16.1deepfigures-results.json,Line graph_chart,2018.lilt-16.1
2018.lilt-16.1.pdf-Figure2.png,FIGURE 2 Results for a single RUSS layer.,[],2018.lilt-16.1deepfigures-results.json,Line graph_chart,2018.lilt-16.1
2018.lilt-16.1.pdf-Figure4.png,"FIGURE 4 Results for LSTM, 160 units (red) and GRU (yellow)",[],2018.lilt-16.1deepfigures-results.json,Line graph_chart,2018.lilt-16.1
2018.lilt-16.1.pdf-Figure7.png,FIGURE 7 Results for LSTM+RUSS (purple) and GRU+RUSS (green),[],2018.lilt-16.1deepfigures-results.json,Line graph_chart,2018.lilt-16.1
2019.jeptalnrecital-deft.6.pdf-Figure3.png,FIGURE 3  MAP et Prcision rang N pour diffrentes tailles de corpus dapprentissage pour lautoencodeur),[],2019.jeptalnrecital-deft.6deepfigures-results.json,Line graph_chart,2019.jeptalnrecital-deft.6
2019.jeptalnrecital-deft.8.pdf-Figure3.png,FIGURE 3  Courbe dapprentissage pour la tache 2,[],2019.jeptalnrecital-deft.8deepfigures-results.json,Line graph_chart,2019.jeptalnrecital-deft.8
2019.jeptalnrecital-long.10.pdf-Figure2.png,FIGURE 2  Graphique montrant lannotation optimale sur la base de notre chantillon,"[u'. figure 2 , quand x vaut 0).', u"". figure 2 -graphique montrant l'annotation optimale sur la base de notre chantillon ces rsultats paraissent acceptables, mme s'ils sont loins de correspondre  la situation idale dcrite ci-dessus."", u'. nous pouvons observer sur la figure 2 que la configuration qui se rapproche le plus des pourcentages optimaux est celle o la difficult des ep est d\'un niveau infrieur  ceux dcrits dans polylexfle, en particulier pour la classe ""gale"".']",2019.jeptalnrecital-long.10deepfigures-results.json,Line graph_chart,2019.jeptalnrecital-long.10
2020.acl-demos.21.pdf-Figure1.png,"Figure 1: Precision and F0.5 at various noise levels, averaged across the five datasets.","[u'precision and recall equally, precision five times more than recall, and precision ten times more. Figure 1 shows average precision and F 0.5 scores across the five datasets, and', u'. figure 1a also shows that the neighborhood activation filter gives a large boost to precision over all three noise-detection algorithms, and the feature neighborhood filter gives a smaller but still observable benefit.', u'. from the graph in figure 1d , it is apparent that activation neighborhood filtering has a benefit to f 0.5 at low error rates but declines relative to the other systems as the error rate increases, crossing at error rates near 15%.', u'. however, for next-best noise, harf suffered a dramatic loss in recall when error rates exceeded about 12% (figure 1d ), leading it to have low overall f 0.5 .']",2020.acl-demos.21deepfigures-results.json,Line graph_chart,2020.acl-demos.21
2020.acl-demos.21.pdf-Figure6.png,"Figure 6: F0.5 at various noise levels, averaged across the five datasets.",[],2020.acl-demos.21deepfigures-results.json,Line graph_chart,2020.acl-demos.21
2020.acl-demos.28.pdf-Figure3.png,Figure 3: Results of re-training BERT funniness regression as more data becomes available in FunLines.,"[u'. the results, shown in figure 3 , suggest that the model gets increasingly accurate in its funniness estimations over time as more data becomes available, and thus its feedback to the user improves over time.']",2020.acl-demos.28deepfigures-results.json,Line graph_chart,2020.acl-demos.28
2020.acl-demos.42.pdf-Figure6.png,Figure 6: Label Efficiency. We choose commonlyused supervised baselines for comparison.,[],2020.acl-demos.42deepfigures-results.json,Line graph_chart,2020.acl-demos.42
2020.acl-demos.7.pdf-Figure3.png,"Figure 3: Economy comparison: Recall vs number of patterns, for the different representations.",[u'. figure 3 plots the achieved recall against the number of patterns.'],2020.acl-demos.7deepfigures-results.json,Line graph_chart,2020.acl-demos.7
2020.acl-main.1.pdf-Figure2.png,Figure 2: Validation performance in early training on synthetic speech,[],2020.acl-main.1deepfigures-results.json,Line graph_chart,2020.acl-main.1
2020.acl-main.112.pdf-Figure2.png,Figure 2: Silver evaluation results in Pearsons r. Languages (x-axis) are sorted according to mean correlation.,[u'. figure 2 displays the results of our silver evaluation.'],2020.acl-main.112deepfigures-results.json,Line graph_chart,2020.acl-main.112
2020.acl-main.119.pdf-Figure4.png,Figure 4: SMATCH scores with different numbers of inference steps. Sentences are grouped by length.,"[u'. the results on amr 2.0 are shown in figure 4 (solid line).', u'. for a closer study on the effect of the inference steps with respect to the lengths of input sentences, we group sentences into three classes by length and also show the individual results in figure 4 (dashed lines).']",2020.acl-main.119deepfigures-results.json,Line graph_chart,2020.acl-main.119
2020.acl-main.119.pdf-Figure6.png,Figure 6: SMATCH scores with different beam sizes.,[u'. we vary the beam size and plot the curve in figure 6 .'],2020.acl-main.119deepfigures-results.json,Line graph_chart,2020.acl-main.119
2020.acl-main.14.pdf-Figure4.png,Figure 4: The effect of encoder layers number.,[u'. figure 4 shows that the f 1 scores are increasing until three encoder layers.'],2020.acl-main.14deepfigures-results.json,Line graph_chart,2020.acl-main.14
2020.acl-main.15.pdf-Figure3.png,"Figure 3: Attention density ratio R(p) for NMT and TTS tasks under different p with and without knowledge distillation, where KD means knowledge distillation.",[u'. the results are shown in figure 3 .'],2020.acl-main.15deepfigures-results.json,Line graph_chart,2020.acl-main.15
2020.acl-main.26.pdf-Figure2.png,Figure 2: Analysis for proposition of instances.,[],2020.acl-main.26deepfigures-results.json,Line graph_chart,2020.acl-main.26
2020.acl-main.31.pdf-Figure2.png,"Figure 2: Test performance and gain with different percent of training data ranging from 0.005 to 1 on MR. The less data in training, the more new words in test.","[u'. in addition, a tendency of test performance and gain with different percentages of training data on mr is illustrated as figure 2 .']",2020.acl-main.31deepfigures-results.json,Line graph_chart,2020.acl-main.31
2020.acl-main.31.pdf-Figure4.png,Figure 4: Accuracy with varying interaction steps.,[u'. figure 4 exhibits the performance of texting with a varying number of the graph layer on mr and ohsumed.'],2020.acl-main.31deepfigures-results.json,Line graph_chart,2020.acl-main.31
2020.acl-main.31.pdf-Figure5.png,Figure 5: Accuracy with varying graph density.,[u'. figure 5 illustrates the performance as well as the graph density of texting with a varying window size on mr and ohsumed.'],2020.acl-main.31deepfigures-results.json,Line graph_chart,2020.acl-main.31
2020.acl-main.34.pdf-Figure1.png,Figure 1: Number denotes the number of content or function words that were randomly masked in each sentence of the WMT14 English-to-German translation task.,"[u'evaluate this, we randomly masked content or function words with UNK in a source sentence. Figure 1 shows that the BLEU scores of the test set decreased much * Corresponding author more']",2020.acl-main.34deepfigures-results.json,Line graph_chart,2020.acl-main.34
2020.acl-main.34.pdf-Figure5.png,Figure 5: BLEU scores of Trans.base+SCWAContLoss on the EN-DE and ZH-EN test sets with different number of function words T .,[u'. figure 5 shows the results of trans.'],2020.acl-main.34deepfigures-results.json,Line graph_chart,2020.acl-main.34
2020.acl-main.35.pdf-Figure1.png,Figure 1: PPL for each explanation method on TRANSFORMER over the IWSLT DeEn dataset with different k value.,[u'. figure 1 depicts the effects of k for transformer on deen task.'],2020.acl-main.35deepfigures-results.json,Line graph_chart,2020.acl-main.35
2020.acl-main.36.pdf-Figure1.png,"Figure 1: (a) The convergence speed of the encoder and the decoder. (b) The performance when adding noise to the encoder input, encoder output and decoder input in the inference stage of a basic NAT model.","[u'. the convergence speed is illustrated by the bleu score along with the training steps, as shown in figure 1(a) .', u'. this experiment is conducted on a basic 5-layer encoder and decoder nat model, and the results are illustrated in figure 1(b) .']",2020.acl-main.36deepfigures-results.json,Line graph_chart,2020.acl-main.36
2020.acl-main.37.pdf-Figure4.png,Figure 4: Subject-Verb Agreement Analysis. X-axis and y-axis represent subject-verb distance in words and the accuracy respectively.,"[u'. results are shown in figure 4 .', u'. figure 4 shows that our approach can improve the accuracy of long-distance subject-verb dependencies, especially for cases where there are more than 10 tokens between the verb and the corresponding subject when comparing the ""base+pr"" with the ""transformer big"".']",2020.acl-main.37deepfigures-results.json,Line graph_chart,2020.acl-main.37
2020.acl-main.39.pdf-Figure4.png,Figure 4: Soft staircase activation function.,"[u'following ""soft-staircase"" 5 to force the weights of the step size to be approximately integers (Figure 4) :']",2020.acl-main.39deepfigures-results.json,Line graph_chart,2020.acl-main.39
2020.acl-main.4.pdf-Figure2.png,Figure 2: Performance of the RoBERTa evaluator w.r.t amount of supervised training data (6.2).,"[u'. figure 2 shows that, with only around 100 samples, the roberta evaluator can reach performance close to the result obtained using the entire 720 samples.']",2020.acl-main.4deepfigures-results.json,Line graph_chart,2020.acl-main.4
2020.acl-main.52.pdf-Figure2.png,Figure 2: Comparison of CDL and CDL-DL for Emotion-acc on the validation set.,"[u'. as shown in figure 2 , cdl accelerates the learning effectively and consistently outperforms cdl-dl.']",2020.acl-main.52deepfigures-results.json,Line graph_chart,2020.acl-main.52
2020.acl-main.56.pdf-Figure4.png,Figure 4: Trends of AOR (Average Off-topic Recall) on seen and unseen prompts with datasize variation.,[],2020.acl-main.56deepfigures-results.json,Line graph_chart,2020.acl-main.56
2020.acl-main.57.pdf-Figure2.png,Figure 2: The per-turn accuracy of different methods on the test set during fine-tuning with 1 dialog adaptation where the target domain is restaurant.,"[u'adaptation process, we present the fine-tuning curves for different methods with 1 dialog adaptation in Figure 2 . As it can be seen, MDS achieves the best accuracy at the beginning and']",2020.acl-main.57deepfigures-results.json,Line graph_chart,2020.acl-main.57
2020.acl-main.59.pdf-Figure3.png,Figure 3: Learning curves of the interaction between the user agent and the system agent.,[u'. figure 3 : learning curves of the interaction between the user agent and the system agent.'],2020.acl-main.59deepfigures-results.json,Line graph_chart,2020.acl-main.59
2020.acl-main.59.pdf-Figure5.png,Figure 5: Learning curves of the interaction between the benchmark user policy and each system agent.,[],2020.acl-main.59deepfigures-results.json,Line graph_chart,2020.acl-main.59
2020.acl-main.59.pdf-Figure6.png,Figure 6: Learning curves of the interaction between each user agent and the benchmark system policy.,[],2020.acl-main.59deepfigures-results.json,Line graph_chart,2020.acl-main.59
2020.acl-main.6.pdf-Figure2.png,Figure 2: Bleu improvements on Wizard-of-Wikipedia.,"[u"". as all bleu metrics are shown in figure 2 , we can find that the improvement of result increasing with the augment of bleu's grams, which means the dialog response produced via model kic is more in line with the real distribution of ground-truth response in the phrase level, and the better improvement on higher gram's bleu reflects the model have preferable readability and fluency.""]",2020.acl-main.6deepfigures-results.json,Line graph_chart,2020.acl-main.6
2020.acl-main.61.pdf-Figure4.png,Figure 4: The accumulated attention weights of documents tokens on RAM T and CMR on Case 1 in Fig. 3. We only show top tokens in both methods here.,[],2020.acl-main.61deepfigures-results.json,Line graph_chart,2020.acl-main.61
2020.acl-main.62.pdf-Figure3.png,Figure 3: Effects of dynamics model,[],2020.acl-main.62deepfigures-results.json,Line graph_chart,2020.acl-main.62
2020.acl-main.65.pdf-Figure2.png,"Figure 2: The Meteor scores of ESD on Oxford test dataset with different M and K, where M is the number of discrete latent variables used in ESD, and K is the number of categories.","[u'performances of several models with different number of latent variables, and plot the result in Figure 2 . Overall, setting multiple latent variables given the same categories achieves noticeable improvements over M=1,']",2020.acl-main.65deepfigures-results.json,Line graph_chart,2020.acl-main.65
2020.acl-main.65.pdf-Figure3.png,Figure 3: Comparison between LOG-CaD and ESDdef with different parameter .  controls how much we prefer content words over function words. Larger  implies we prefer content words more.,[u'. figure 3 shows the results.'],2020.acl-main.65deepfigures-results.json,Line graph_chart,2020.acl-main.65
2020.acl-main.66.pdf-Figure4.png,"Figure 4: Pinskers inequality, our bound, and the total variation squared of parameter estimates for different parameter estimates (c = 0.2). As shown, loss truncation can significantly improve bounds over Pinskers inequality and, in this case, has a nearly identical minimizer to directly minimizing total variation.","[u""and 0 1 2 3 4 5 6 0 2 4 Pinsker's Loss-truncated (ours) TV^2 Figure 4 : Pinsker's inequality, our bound, and the total variation squared of parameter estimates for different"", u"". figure 4 shows the objective function value implied by the tv loss, log loss (pinsker's bound), and our c-truncated bound as a function of the gaussian mean.""]",2020.acl-main.66deepfigures-results.json,Line graph_chart,2020.acl-main.66
2020.acl-main.66.pdf-Figure5.png,"Figure 5: HUSE-D vs HUSE-Q for loss truncation, truncation + rejection sampling, and baselines. The red line shows the best achievable frontier via ensembling. Truncation and rejection outperform all baselines.",[u'(which can be achieved via ensembling) dominates the baselines on both quality and diversity ( Figure 5 ). Rejection sampling decreases overall HUSE score because it is designed to only return high'],2020.acl-main.66deepfigures-results.json,Line graph_chart,2020.acl-main.66
2020.acl-main.67.pdf-Figure5.png,Figure 5: BLEU variation between models with different orders K with respect to AMR graph size.,"[u'. figure 5 : bleu variation between models with different orders k with respect to amr graph size.', u'. figure 5 shows the result.']",2020.acl-main.67deepfigures-results.json,Line graph_chart,2020.acl-main.67
2020.acl-main.67.pdf-Figure6.png,Figure 6: BLEU variation between models with different Ke with respect to size of AMR graph and (left) and reentrancy numbers (right).,"[u'. we evaluate our models with order figure 6 : bleu variation between models with different k e with respect to size of amr graph and (left) and reentrancy numbers (right).', u'. performance test on different partitions of amr graph size (figure 6 , left) also suggests that relationships of edges are helpful when the graph becomes larger.', u'. as shown in figure 6 (right), the gap becomes wide when the number of reentrancies grows to 5.']",2020.acl-main.67deepfigures-results.json,Line graph_chart,2020.acl-main.67
2020.acl-main.70.pdf-Figure2.png,Figure 2: The performance of different text steam clustering algorithm over time (in thousand points) in terms of NMI measure.,"[u'. additionally, we also evaluate the performance of each algorithm over different time-stamps of the stream (see figure 2 ).', u'. furthermore, to investigate the performance over time, we plot the performance of   all algorithms over time in figure 2 .']",2020.acl-main.70deepfigures-results.json,Line graph_chart,2020.acl-main.70
2020.acl-main.70.pdf-Figure3.png,"Figure 3: The sensitivity analysis with different parameters, including ,  and .","[u'. from figure 3a , we can observe the effect of , which ranges from 9e 3 to 9e 1 .', u'. figure 3b shows the performance on different values of , which ranges from 1e 4 to 1e 2 .', u'. figure 3c shows effect of  ranges from 9e 4 to 9e 6 .']",2020.acl-main.70deepfigures-results.json,Line graph_chart,2020.acl-main.70
2020.acl-main.72.pdf-Figure7.png,"Figure 7: Hit ratio (|Ui|/|C|) in terms of each iteration number by LR, +Fd(p), and HB. The upper and lower graphs start with one and three seeds, respectively.","[u'. figure 7 directly describes the score differences with different alphas by showing the hit ratios defined as |u i |/|c| in terms of each iteration number for lr, +fd(p), and hb.']",2020.acl-main.72deepfigures-results.json,Line graph_chart,2020.acl-main.72
2020.acl-main.73.pdf-Figure3.png,Figure 3: Topic specialization scores for each level.,[u'. figure 3 presents the average topic specialization scores for each level.'],2020.acl-main.73deepfigures-results.json,Line graph_chart,2020.acl-main.73
2020.acl-main.76.pdf-Figure4.png,"Figure 4: Average runtimes of each model according to the number of words on STS and reranking tasks, subscripted as sts and rrk, respectively.","[u'. figure 4 shows that the t-ta exhibits faster runtimes than the bilm, and the gap between the t-ta and bilm increases as the sentence becomes longer.', u'For the visual clarity of Figure 4 , we omit the runtime results of the uniLM, which is as fast as the', u'. considering figure  4 , however, the cpu-only environment and gpuaugmented environment show a similar tendency: the longer the sentence is, the more significant the difference in the runtime between the t-ta and the bilm.']",2020.acl-main.76deepfigures-results.json,Line graph_chart,2020.acl-main.76
2020.acl-main.76.pdf-Figure6.png,Figure 6: Runtimes according to the number of words for the biLM and T-TA in the GPU-augmented environment.,[u'. figure 6 shows the average runtimes of the bilm and the t-ta for the number of words in a sentence.'],2020.acl-main.76deepfigures-results.json,Line graph_chart,2020.acl-main.76
2020.acl-main.8.pdf-Figure2.png,Figure 2: Test scores compared against the number of dialog turns given as context prior to generating samples for GCC-DEC (355M) and GCC-NRC (355M).,[u'. in figure 2 we delve deeper into the results of the ground truth experiments and display labeler preference as a function of conversation length.'],2020.acl-main.8deepfigures-results.json,Line graph_chart,2020.acl-main.8
2020.acl-main.81.pdf-Figure2.png,Figure 2: The test curves for sentence-level correction metrics with and without SpellGCN w.r.t. the number of training epochs on SIGHAN 2015.,[u'. the comparison in figure 2 ).'],2020.acl-main.81deepfigures-results.json,Line graph_chart,2020.acl-main.81
2020.acl-main.84.pdf-Figure3.png,"Figure 3: Learning curve for 20%, 40%, 60%, 80%, and 100% of the data on database MOVIEDATA as part of OTTA. We compare the scores for the training with and without token alignment.",[u'. figure 3 displays the learning curves for the moviedata database with and without the token assignment.'],2020.acl-main.84deepfigures-results.json,Line graph_chart,2020.acl-main.84
2020.acl-main.92.pdf-Figure4.png,Figure 4: Syntactic pattern diversity of various corpora,"[u'. figure 4 shows that there are 87%, 54%, 46% and 33% identical syntactic patterns (these numbers are the percentages of mwps with =0 w.']",2020.acl-main.92deepfigures-results.json,Line graph_chart,2020.acl-main.92
2020.acl-main.92.pdf-Figure5.png,Figure 5: Lexicon usage diversity of various corpora: test-set versus training-set,"[u'. figure 5 shows the ld between the test-set and the trainingset for various corpora.', u'Last, in Figure 5 , MathQA actually possesses the highest CLD between its official test-set and training-set: 0.85, surprisingly']",2020.acl-main.92deepfigures-results.json,Line graph_chart,2020.acl-main.92
2020.acl-main.92.pdf-Figure6.png,Figure 6: Syntactic pattern diversity of various corpora: test-set versus training-set,[u'. figure 6 shows the sd between the testset and the training-set for different corpora.'],2020.acl-main.92deepfigures-results.json,Line graph_chart,2020.acl-main.92
2020.acl-main.92.pdf-Figure7.png,"Figure 7: Lexicon usage diversity measured only within the test-set for various corpora. Here, the suffix -test denotes that it is measured only within the test-set.","[u'. figure 7 illustrates that the cld=0.27 of mathqa measured within the test set is actually low in comparison with the corresponding clds of other corpora (, the means are 0.57 for asdiv-a and asdiv corpora).']",2020.acl-main.92deepfigures-results.json,Line graph_chart,2020.acl-main.92
2020.acl-main.94.pdf-Figure1.png,Figure 1: BLI performance in the comparable setting. The target window size is fixed and the source window size is varied.,[u'. figure 1 shows the result of the four languages.'],2020.acl-main.94deepfigures-results.json,Line graph_chart,2020.acl-main.94
2020.acl-main.94.pdf-Figure2.png,Figure 2: BLI performance for each PoS in the comparable setting.,"[u'other PoS, and thus are expected to show a high correlation with the window size. Figure 2 shows the scores for each']",2020.acl-main.94deepfigures-results.json,Line graph_chart,2020.acl-main.94
2020.acl-main.94.pdf-Figure3.png,Figure 3: BLI performance in the comparable setting.,"[u'. figure 3 summarizes the result of the same domain setting.', u'Firstly, compared to the same-domain settings (Figure 3) , the scores are lower by around 0.1 to 0.2 points across the languages and']",2020.acl-main.94deepfigures-results.json,Line graph_chart,2020.acl-main.94
2020.acl-main.94.pdf-Figure5.png,Figure 5: BLI performance for each PoS in the different domain setting.,[u'. figure 5 summarizes the results for each pos.'],2020.acl-main.94deepfigures-results.json,Line graph_chart,2020.acl-main.94
2020.acl-main.94.pdf-Figure6.png,Figure 6: BLI performance with the top 500 frequent and rare words in the comparable setting.,[u'The results of the comparable setting in each language are shown in Figure 6 . The scores for the frequent words (top500) are notably higher than the rare words'],2020.acl-main.94deepfigures-results.json,Line graph_chart,2020.acl-main.94
2020.acl-main.94.pdf-Figure7.png,Figure 7: BLI performance on the top 500 frequent and rare words in the different domain setting.,"[u'In the different domain settings, as shown in Figure 7 , the rare words, in turn, suffer from larger window sizes, especially for Fr and']",2020.acl-main.94deepfigures-results.json,Line graph_chart,2020.acl-main.94
2020.acl-main.94.pdf-Figure8.png,Figure 8: Downstream evaluations in the comparable settings. SA: sentiment analysis; DC: document classification; DP: dependency parsing. The window sizes of both the source and target embeddings are varied.,[u'. the results from all the three tasks are presented in figure 8 .'],2020.acl-main.94deepfigures-results.json,Line graph_chart,2020.acl-main.94
2016.jeptalnrecital-jep.14.pdf-Figure2.png,FIGURE 2 : Distribution des dpartements o les sujets avaient pass la plupart de leur vie.,[],2016.jeptalnrecital-jep.14deepfigures-results.json,maps,2016.jeptalnrecital-jep.14
2020.acl-main.415.pdf-Figure1.png,Figure 1: The 635 languages of our corpus geo-located with mean Mel Cepstral Distortion (MCD) scores.,[],2020.acl-main.415deepfigures-results.json,maps,2020.acl-main.415
W10-21.pdf-Figure7.png,"Figure 7: WALS map of the languages and their behavior with respect to SPA. The color indicates the number of self-succession  values which are negative, i.e., which adhere to the SPA principle. Color mapping is from blue (conforming to SPA) to red. The numbers in square brackets indicate the number of languages in this group.","[u'. we transformed the results of our spa statistics for each language in the asjp database that is also included in the wals database into a wals map (figure 7) .', u'. in figure   7 , the fact that those languages with fewer negative diagonal cells are plotted on top of those with a higher number slightly distorts the overall picture that most languages adhere to the principle.', u', in contrast to Figure 7 , shows the 3,200 languages we investigated more closely and not just the ones included', u'of automatically assessing and visualizing linguistic features across a wide range of languages, thus al- (Figure 7) . Locations of nonconforming languages are highlighted with red boxes. Note that the number of']",W10-21deepfigures-results.json,maps,W10-21
W10-21.pdf-Figure8.png,Figure 8: Density equalized distribution of the languages with respect to SPA. The area of the geographic regions corresponds to the number of languages in that location  represented by dots. Overlap is avoided using pixel-placement. The color mapping corresponds to the one used in the WALS map (Figure 7). Locations of nonconforming languages are highlighted with red boxes. Note that the number of languages in this map is about twice the number in the WALS map (7).,"[u'. in figure 8 , we show the density equalized distortion by cartograms and the overlap-free representation of the data points using pixel placement.', u'an artifact of the database that we used or if they manifest an areal feature. Figure 8 , in contrast to']",W10-21deepfigures-results.json,maps,W10-21
W10-2110.pdf-Figure7.png,"Figure 7: WALS map of the languages and their behavior with respect to SPA. The color indicates the number of self-succession  values which are negative, i.e., which adhere to the SPA principle. Color mapping is from blue (conforming to SPA) to red. The numbers in square brackets indicate the number of languages in this group.","[u'. we transformed the results of our spa statistics for each language in the asjp database that is also included in the wals database into a wals map (figure 7) .', u'. in figure   7 , the fact that those languages with fewer negative diagonal cells are plotted on top of those with a higher number slightly distorts the overall picture that most languages adhere to the principle.', u', in contrast to Figure 7 , shows the 3,200 languages we investigated more closely and not just the ones included', u'of automatically assessing and visualizing linguistic features across a wide range of languages, thus al- (Figure 7) . Locations of nonconforming languages are highlighted with red boxes. Note that the number of']",W10-2110deepfigures-results.json,maps,W10-2110
W10-2110.pdf-Figure8.png,Figure 8: Density equalized distribution of the languages with respect to SPA. The area of the geographic regions corresponds to the number of languages in that location  represented by dots. Overlap is avoided using pixel-placement. The color mapping corresponds to the one used in the WALS map (Figure 7). Locations of nonconforming languages are highlighted with red boxes. Note that the number of languages in this map is about twice the number in the WALS map (7).,"[u'. in figure 8 , we show the density equalized distortion by cartograms and the overlap-free representation of the data points using pixel placement.', u'an artifact of the database that we used or if they manifest an areal feature. Figure 8 , in contrast to']",W10-2110deepfigures-results.json,maps,W10-2110
W10-22.pdf-Figure8.png,Figure 8: Dialect varieties detected by k-means clustering algorithm based on the first 20 sound correspondences in the first dimension.,"[u'. the results of the 2-way, 3-way and 4-way clustering are given in figure 8 .']",W10-22deepfigures-results.json,maps,W10-22
W10-22.pdf-Figure9.png,Figure 9: Dialect varieties detected by k-means clustering algorithm based on all word transcriptions.,[u'. in figure 9 we present the dialect divisions that we get if the distances between the sites are calculated using whole word transcriptions instead of only the 20 most prominent sound correspondences.'],W10-22deepfigures-results.json,maps,W10-22
W10-2206.pdf-Figure5.png,"Figure 5: [A]-[@] (left), [o]-[u] (middle), [e]-[i] (right) sound correspondences.","[u'In Figure 5 we see the geographical distribution of the first three extracted correspondences.', u'. on the maps in figure 5 we represent it with the black line that roughly divides bulgaria into east and west.']",W10-2206deepfigures-results.json,maps,W10-2206
W10-2206.pdf-Figure6.png,"Figure 6: [d]-[dj] (left), [v]-[vj] (middle), [r]-[rj] (right) sound correspondences.",[u'sound correspondences that involve soft consonants and their counterparts have the same east-west distribution (see Figure 6 ). In the first dimension we find the following consonants and their pal- In some'],W10-2206deepfigures-results.json,maps,W10-2206
W10-2206.pdf-Figure7.png,"Figure 7: [S]-[C] (left), [ > ]-[ > C] (middle), [Z]-[] (right) sound correspondences.",[],W10-2206deepfigures-results.json,maps,W10-2206
W10-2206.pdf-Figure8.png,Figure 8: Dialect varieties detected by k-means clustering algorithm based on the first 20 sound correspondences in the first dimension.,"[u'. the results of the 2-way, 3-way and 4-way clustering are given in figure 8 .']",W10-2206deepfigures-results.json,maps,W10-2206
W10-2206.pdf-Figure9.png,Figure 9: Dialect varieties detected by k-means clustering algorithm based on all word transcriptions.,[u'. in figure 9 we present the dialect divisions that we get if the distances between the sites are calculated using whole word transcriptions instead of only the 20 most prominent sound correspondences.'],W10-2206deepfigures-results.json,maps,W10-2206
W10-2305.pdf-Figure1.png,Figure 1: Distribution of GTRP varieties including province names,[u'. the geographic distribution of the gtrp varieties including province names is shown in figure 1 .'],W10-2305deepfigures-results.json,maps,W10-2305
W10-2305.pdf-Figure3.png,"Figure 3: Geographic visualization of the clustering including dendrogram. The shades of grey in the dendrogram correspond with the map (e.g., the Limburg varieties can be found at the bottomright).","[u'. figure 3 shows a dendrogram visualizing the obtained hierarchy as well as a geographic visualization of the clustering.', u'It is clear that the geographical results of the hierarchical approach (Figure 3) are comparable to the results of the flat clustering approach', u'. the shades of grey are identical to the shades of grey in figure 3 .']",W10-2305deepfigures-results.json,maps,W10-2305
W10-2305.pdf-Figure4.png,Figure 4: Geographic visualization of the flat clustering reported in Wieling and Nerbonne (2009). The shades of grey are identical to the shades of grey in Figure 3.,"[u'. for comparison, figure 4 shows the visualization of four clusters based on the flat clustering approach of .', u'are comparable to the results of the flat clustering approach (Figure 4) of . 4 How- ever, despite the Frisian area (top-left) being identical, we clearly observe', u'as a substitute for the external ranking method, we correlated the absolute values of the Figure 4 : Geographic visualization of the flat clustering reported in . The shades of grey are']",W10-2305deepfigures-results.json,maps,W10-2305
W10-4316.pdf-Figure6.png,Figure 6. Interface of our system in an AMT assignment.,"[u'. each assignment is a scenario of finding a particular restaurant, as shown in figure 6 .', u'. figure 6 .']",W10-4316deepfigures-results.json,maps,W10-4316
W11-4646.pdf-Figure1.png,Figure 1: Villages in Kinnaur where data collection was conducted,[],W11-4646deepfigures-results.json,maps,W11-4646
W11-4646.pdf-Figure2.png,Figure 2: Dialect groups according to our study,[],W11-4646deepfigures-results.json,maps,W11-4646
W12-02.pdf-Figure6.png,Figure 6: Map representing the second dimension of multi-dimensional scaling (same experiment as Fig. 4).,"[u'(4 x 11 matrix) 1,5sec, Figure 6 (4 x 12 matrix) 1,7sec.', u'. in figure 6 , for example, from estwn there is an example of a closed set for nouns.', u'allows to distinguish Northwestern from Southeastern variants, while the second dimension ( Figure 6 ) distinguishes Northeastern from Southwestern variants. Instead of +-shaped dialect divisions put forward by traditional']",W12-02deepfigures-results.json,maps,W12-02
W12-0210.pdf-Figure3.png,"Figure 3: Geographic localization of the Archimob texts, according to the place of residence of the interviewed persons. The colors represent the linguistic distance between texts; they correspond to the colors used in the dendrogram of Figure 2.","[u'. 7 figure 3 localizes the data points on a geographical map.', u'closely corresponds to their geographic location (as illustrated in Figure 3) : the major North-South divisions as well as some East-West divisions are clearly recovered.']",W12-0210deepfigures-results.json,maps,W12-0210
W12-0210.pdf-Figure5.png,Figure 5: Map representing the first dimension of multi-dimensional scaling (same experiment as Fig. 4).,[],W12-0210deepfigures-results.json,maps,W12-0210
W12-0210.pdf-Figure6.png,Figure 6: Map representing the second dimension of multi-dimensional scaling (same experiment as Fig. 4).,"[u'allows to distinguish Northwestern from Southeastern variants, while the second dimension ( Figure 6 ) distinguishes Northeastern from Southwestern variants. Instead of +-shaped dialect divisions put forward by traditional']",W12-0210deepfigures-results.json,maps,W12-0210
W12-0211.pdf-Figure2.png,Figure 2: Six dialect groups in Dutch speaking area.,"[u'clustering of the distance matrix for Dutch varieties with six clusters, which we present in Figure 2 .']",W12-0211deepfigures-results.json,maps,W12-0211
W12-0211.pdf-Figure3.png,"Figure 3: Dutch dialect area based on the pronunciation of words (a) vrijdag, (b) wonen, (c) durven, (d) wegen, (f) heet and (e) gisteren selected as characteristic of respective areas.","[u'. in figure 3 we present maps of dutch language area that are based on the pronunciations of the best scoring words for each of the six groups of sites.', u'. in maps in figure 3 we present the first extracted dimension, which always explains most of the variation in the data.', u'. figure 3 reveal that the best scoring word does indeed identify the cluster in question.', u'. for example, the map in figure 3 (a) reveals that based on the pronunciation of word vrijdag the frisian-speaking area is internally homogeneous and distinct from the rest of the sites.', u"". in figure 3(b) we present the analysis of a distance matrix based on the pronunciation of the word wonen 'live' that was found to be relevant for the low saxon area.""]",W12-0211deepfigures-results.json,maps,W12-0211
W12-0211.pdf-Figure4.png,Figure 4: Two dialect groups in Germany.,[u'In Figure 4 we present the two largest groups in the cluster analysis of the distances obtained using'],W12-0211deepfigures-results.json,maps,W12-0211
W12-0211.pdf-Figure5.png,Figure 5: First MDS dimensions based on the pronunciation of words (a) weisse and (b) gefahre.,"[u'. the word weisse shows only small differences within the north, which is illustrated by the lightcolored northern part of germany in figure 5 (a).', u'. the map in figure 5(b) shows an even clearer split highlighting the high german area based on the best ranked word found by our method.', u'. this word shows also low variation in the low german area (second best scored), which is also clearly visible in figure 5(b) .']",W12-0211deepfigures-results.json,maps,W12-0211
W12-0214.pdf-Figure1.png,Figure 1: Omaha skewing polysemies in Northern Australia,"[u'. these show distributions of this polysemy (with different forms of kinship terms) in various areas (figure 1 , see mcconvell, in press).', u'. note that one of the languages with an omaha skewing pattern in figure 1 is ayabadhu in eastern cape york peninsula.']",W12-0214deepfigures-results.json,maps,W12-0214
W12-0214.pdf-Figure2.png,Figure 2: distribution *kaal MB > MBS > spouse,[u'. now look at the distribution of cognates of the root for mb in ayabadhu (kaala) in figure 2 .'],W12-0214deepfigures-results.json,maps,W12-0214
W13-05.pdf-Figure3.png,Figure 3: Jeju Island,"[u'. again from the title of the figure, we understand that the oval shape refers to jeju island.', u'. but photographs like figure 3 do not have any place names or street numbers at all.', u'. 9 unlike the aerial photograph of jeju, figure 3 , this new figure has names for several locations: (1) mt.', u'connecting the airport and Jungmun Resort. The two figures offer different types of geographic information: Figure 3 shows the elevation of each part of the island, while', u'. the parsing process removes all markup (in this case, the speaker identifiers and the square bracket overlap notation) and generates the text shown in figure 3 and a collection of rdf annotations which will be discussed below.']",W13-05deepfigures-results.json,maps,W13-05
W13-05.pdf-Figure7.png,Figure 7: Jeju Google Earth,"[u'Here is a map of the same island, Figure 7 Jeju Google Earth 10 , with the old romanized name ""Cheju do"" of the Jeju']",W13-05deepfigures-results.json,maps,W13-05
W13-05.pdf-Figure8.png,Figure 8: How to Get to the Sistine Chapel,"[u'Finally, consider a map for the Sistine Chapel in the Vatican, Figure 8 : How to Get to the Sistine Chapel. 12 This map guides one from Piazza']",W13-05deepfigures-results.json,maps,W13-05
W13-07.pdf-Figure1.png,Fig. 1. An example route segment from A to B. The squares represent the landmarks in the contexts of A and B. L represents a landmark referred to by the user (a supermarket).,[],W13-07deepfigures-results.json,maps,W13-07
W13-0702.pdf-Figure1.png,Fig. 1. An example route segment from A to B. The squares represent the landmarks in the contexts of A and B. L represents a landmark referred to by the user (a supermarket).,[],W13-0702deepfigures-results.json,maps,W13-0702
W13-4503.pdf-Figure3.png,Figure 3: The Supply Request Map of #99japan.,"[u'By the project activity used the rescue map and the supply request map (Figure 3) , The project had reporting and supporting activities more than 200 points on the rescue']",W13-4503deepfigures-results.json,maps,W13-4503
W13-4506.pdf-Figure3.png,Figure 3: Users location distribution before the earthquake,"[u""In order to express a spatial spread of people's going-home behavior, Figure 3 , 4 shows the spatial dstribution of users' location of before the earthquake and the"", u"". as an overall trend, office distribution and house distribution are spatially different, and home distribution is spread in figure 3 : users' location distribution before the earthquake the direction of the suburban area.""]",W13-4506deepfigures-results.json,maps,W13-4506
W13-4506.pdf-Figure4.png,Figure 4: Users location distribution in the morning on March 12,[],W13-4506deepfigures-results.json,maps,W13-4506
W14-0203.pdf-Figure1.png,Figure 1: Wizard of Oz interface - Google Satellite Map and StreetView,"[u"". google streetview showed the tourist's point of view (see figure 1) .""]",W14-0203deepfigures-results.json,maps,W14-0203
W14-0205.pdf-Figure2.png,Figure 2: The 9 selected tasks .,"[u'. nine destinations within the city of edinburgh were selected to be the tasks to complete (the task is to arrive to each destination, from a common starting point, see figure 2 ).']",W14-0205deepfigures-results.json,maps,W14-0205
W14-0210.pdf-Figure1.png,"Figure 1: Visualization of individual tracks A, B and C used in the experiment. The intended path is shown by solid line. Path shown by dashed line shows the real path leading to the point, where the participant got lost  yellow exclamation mark  and from where the participant was navigated back to the path.","[u'In track A the participant was asked to navigate to the Vaclavska passage, see Figure 1 . The navigation instruction were changed so that the two streets (Trojanova and Jenstejnska) were', u'track B the participant was asked to navigate through the park to the restaurant, see Figure 1 . The navigation instruction were changed so that the two junctions were skipped and the', u'participant was asked to navigate through the building from the entrance to the yard, see Figure 1 . The navigation instructions were changed so that instead of taking stairs down, the participant']",W14-0210deepfigures-results.json,maps,W14-0210
W14-3601.pdf-Figure1.png,Figure 1: Different Arabic Dialects in the Arab World (http://en.wikipedia.org/wiki/Arabic_dialects),"[u'. there are considerable geographical distinctions between das within countries, across country borders, and even between cities and villages as shown in figure 1 .']",W14-3601deepfigures-results.json,maps,W14-3601
W14-4908.pdf-Figure1.png,Figure 1: When an example location mention (e.g. Dublin) is clicked the map adjusts to show all potential location candidates that exist in the gazetteer for this place name.,"[u'All location mentions displayed in the text interface are highlighted in colour (see Figure 1 ). Those in red (e.g. Dublin) have one or more potential candidates in the gazetteer,', u'. for example, when clicking on the first location mention (dublin) shown in figure 1 , the map adjusts to the central point of all 42 candidate locations.', u'In the case of Dublin (see Figure 1) , the user would then zoom into the correct Dublin to select a candidate and', u'. in this case, it is clear from the context that the text refers to the capit it might not always be as clearcut to choose figure 1 : when an example location mention ( dublin) is clicked the map adjusts to show all potential location candidates that exist in the gazetteer for this place name.', u'mentions"") in the box located at the top of right corner of the map (see Figure 1 ). Once there are only green or blue location mentions left in the text, the']",W14-4908deepfigures-results.json,maps,W14-4908
W14-4908.pdf-Figure2.png,Figure 2: Example candidate for the location mention River Liffey and its gazetteer entry information shown in a popup.,[u'In some cases there is only one candidate that can be selected (see Figure 2 ). The user can zoom to the correct location pin which when selected shows a'],W14-4908deepfigures-results.json,maps,W14-4908
W14-4908.pdf-Figure3.png,Figure 3: Choosing between multiple candidates for the same location mention.,"[u'-the Gaelic name for Dublin and its gazetteer entry referring to the administrative division (see Figure 3 ). The gazetteer information in the popup can assist the user to make a choice.', u'different location types for essentially the same place (e.g. see the example for Dublin in Figure 3 ).']",W14-4908deepfigures-results.json,maps,W14-4908
W15-1810.pdf-Figure2.png,Figure 2: An example segment for the utterance: I continue in a this direction down the steps [L1] towards the arch [L2] A and B indicate the start and the goal position respectively. The lines indicate the speakers direction and field of view.,"[u'The recorded speech was transcribed and segmented into utterances, and aligned with the GPS signal. Figure 2 shows an example utterance, the GPS coordinates (the points A and B) indicate where the', u'. figure 2) .']",W15-1810deepfigures-results.json,maps,W15-1810
W15-1810.pdf-Figure4.png,"Figure 4: Granularity in OpenStreetMap: an intersection consisting of many street segments and nodes where they meet. The highlighted nodes inside the circle are all part of an intersection. The highlighted street segments (1-4) belong to the same named street, that is also mapped with a footway and a cycleway running next to it (indicated by the discontinuous lines)","[u'. figure 4 : granularity in openstreetmap: an intersection consisting of many street segments and nodes where they meet.', u'. figure 4 shows a major intersection, containing many street segments and nodes where they meet.', u'. in figure 4 , each thick black line corresponds to the segment of a street that stretches further in both directions, and has a pedestrian way mapped next to it.', u'. at this step, potential referents can be added for entities that make up a unit,  nodes of an intersection as depicted in figure 4 .', u'. in particular this is true in situations where the user conceptualizes her surroundings differently from how the database is organized (as in figure 4) .']",W15-1810deepfigures-results.json,maps,W15-1810
W15-2904.pdf-Figure2.png,"Figure 2: Average sentiment of states in the USA, averaged across three years, from 2012 to 2014.",[u'. the sentiment for all states averaged across the tweets from the three years is shown in figure 2 .'],W15-2904deepfigures-results.json,maps,W15-2904
W16-0805.pdf-Figure1.png,Figure 1: An overhead image of the Second Life Island,[u'. figure 1 provides an overhead view of the island.'],W16-0805deepfigures-results.json,maps,W16-0805
W16-1721.pdf-Figure1.png,Figure 1: Distribution of References in WoTR-DocGeo,"[u'Geographic summaries of the annotations are given in Figure 1 trated in a number of areas that saw heavy fighting, such as in Virginia, South']",W16-1721deepfigures-results.json,maps,W16-1721
W16-1721.pdf-Figure2.png,Figure 2: Distribution of Toponyms in WOTR-Topo,[],W16-1721deepfigures-results.json,maps,W16-1721
2015.jeptalnrecital-court.38.pdf-Figure2.png,FIGURE 2  Motifs frquents : Impact du Gap maximum des motifs sur la taille du vocabulaire et le score de classification. Le support minimal est fix  5. La longueur maximum est fix  4.,"[u'.  en figure 2 et 3, sont montrs les rsultats de classification pour les motifs frquents.', u'. la figure 2 montre que, pour ce corpus, le meilleur gap maximum est de 4 ou 5, et la figure 3 montre que la meilleure longueur maximum est de 2 ou 3.']",2015.jeptalnrecital-court.38deepfigures-results.json,pareto,2015.jeptalnrecital-court.38
2015.jeptalnrecital-court.38.pdf-Figure3.png,FIGURE 3  Motifs frquents : Impact de la longueur maximum des motifs sur la taille du vocabulaire et le score de classification. Le support minimal est fix  5. Le Gap maximum est fix  4.,[],2015.jeptalnrecital-court.38deepfigures-results.json,pareto,2015.jeptalnrecital-court.38
2015.jeptalnrecital-court.38.pdf-Figure4.png,FIGURE 4  Motifs -libres : Impact de la -libert sur la taille du vocabulaire et le score de classification. Le support minimal est fix  0.05%,"[u'. en figure 4 , on peut voir que plus le  est lev, plus le nombre de motifs extraits sera rduit et plus le score de classification sera bon.']",2015.jeptalnrecital-court.38deepfigures-results.json,pareto,2015.jeptalnrecital-court.38
W10-3302.pdf-Figure6.png,Figure 6: The precision of is-a links classified by the depth in the constructed category hierarchy,"[u'results were encouraging but we decided to use the manual alignment results for subsequent experiments. Figure 6 : The precision of is-a links classified by the depth in the constructed category hierarchy', u'. figure 6 shows the precisions of is-a links classified by the depth in the constructed category hierarchy.']",W10-3302deepfigures-results.json,pareto,W10-3302
2020.acl-main.597.Dataset.pdf-Figure1.png,"Figure 1: Declension classes, their conditional entropies (H), and their mutual information quantities (I) with form (W ), meaning (V ), and declension class (C), given gender (G) in German and Czech. H(W | G) and H(V | G) correspond to the overall uncertainty over forms and meaning given genderestimating these values falls outside the scope of this paper.","[u'.), but those that do not are + figure 1 : declension classes, their conditional entropies (h), and their mutual information quantities (i) with form (w ), meaning (v ), and declension class (c), given gender (g) in german and czech.', u'(i.e., distributional semantic vector) and between declension class and form (i.e., orthographic form), as in Figure 1 . We select two Indo-European languages (Czech and German) that have declension classes. We find', u'. figure 1 provides a graphical summary for this section until this point.', u'Jeff Parker for advice on citation forms. Thanks to Ana Paula Seraphim for helping beautify Figure 1@dot ']",2020.acl-main.597.Datasetdeepfigures-results.json,venn diagram,2020.acl-main.597.Dataset
2020.acl-main.597.pdf-Figure1.png,"Figure 1: The conditional entropies (H) and mutual information quantities (MI) of form (W ), meaning (V ), and declension class (C), given gender (G) in German and Czech.","[u'(i.e., distributional semantic vector) and between declension class and form (i.e., orthographic form), as in Figure 1 . We select two Indo-European languages (Czech and German) that have declension classes. We find', u'. figure 1 provides a graphical summary for this section until this point.', u'Jeff Parker for advice on citation forms. Thanks to Ana Paula Seraphim for helping beautify Figure 1@dot ']",2020.acl-main.597deepfigures-results.json,venn diagram,2020.acl-main.597
W12-0207.pdf-Figure1.png,Figure 1: Scientific disciplines in the SciTex corpus,"[u'. fankhauser, 2010), an english corpus which contains full english scientific journal articles from nine disciplines (see figure 1 ).', u'. figure 1 ), we analyse whether the contact disciplines (b-subcorpora) are more similar to computer science (a-subcorpus), the discipline of origin (c-subcorpus) or distinct from both (a and c).']",W12-0207deepfigures-results.json,venn diagram,W12-0207
W12-3001.pdf-Figure1.png,Figure 1: Hypergraph with 7 vertices (axioms) and 4 edges (conflict sets). Both the maximum degree of the hypergraph and the size of the largest edge are 3.,"[u'. figure 1 depicts a hypergraph with 7 vertices and 4 edges.', u'. here, we have that figure 1 depicts the corresponding conflict hypergraph.', u'. for instance, the conflict hypergraph in figure 1 can be decomposed into the sub-hypergraphs induced by the partition of the nodes {{v 1 , v 2 , v 3 , v 4 , v 5 }, {v 6 }, {v 7 }}.']",W12-3001deepfigures-results.json,venn diagram,W12-3001
W12-3001.pdf-Figure2.png,Figure 2: A knowledge base fragment with object A Einstein and its properties. Some of the minimal conflicts between property assertions (edges in the graph) are indicated by hyperedges.,[u'. figure 2 depicts a subset of the minimal conflict sets in the academic domain of prospera involving the object albert einstein.'],W12-3001deepfigures-results.json,venn diagram,W12-3001
W16-2912.pdf-Figure1.png,"Figure 1: Venn diagram: number of terms in the generated vocabularies from the three approaches: UMLS, WordNet with linguistic heuristics (WNling), and word2vec.","[u'. figure 1 shows a venn diagram with the results from the three vocabulary expansion approaches (umls, wnling, word2vec).']",W16-2912deepfigures-results.json,venn diagram,W16-2912
W16-2912.pdf-Figure2.png,"Figure 2: Venn diagram: number of unique terms from the generated vocabularies found in the evaluation data: UMLS, WordNet with linguistic heuristics (WNling), and word2vec.","[u'comparison of the coverage between the generated vocabularies is depicted in the Venn diagram in Figure 2 . Overall, the number of matched terms is higher for the larger vocabularies (WN-Ling, word2vec)']",W16-2912deepfigures-results.json,venn diagram,W16-2912
W16-2920.pdf-Figure8.png,Figure 8: The overlap of true positives among the investigated models was low.,[u'. figure 8 details this observation by showing the coverage difference between the models described here.'],W16-2920deepfigures-results.json,venn diagram,W16-2920
W16-2921.pdf-Figure2.png,Figure 2: Number of sentences annotated by different annotators in the testing dataset.,[u'. figure 2 shows the number of sentences annotated by each annotator individually and by more than one annotator in the testing dataset.'],W16-2921deepfigures-results.json,venn diagram,W16-2921
2020.acl-main.48.pdf-Figure4.png,Figure 4: GCAN ablation analysis in Accuracy.,[u'. the results are presented in figure 4 .'],2020.acl-main.48deepfigures-results.json,word cloud,2020.acl-main.48
W10-07.pdf-Figure10.png,"Figure 10: MTurk workers were asked which word cloud they thought best represented returned the results of a query, in this case Buffy the Vampire Slayer.",[u'. figure 10 shows the final query interface.'],W10-07deepfigures-results.json,word cloud,W10-07
W11-1514.pdf-Figure11.png,Figure 11: Cinderella - Godfather Death: Relative salience word cloud of joy.,[],W11-1514deepfigures-results.json,word cloud,W11-1514
W11-1514.pdf-Figure4.png,Figure 4: Hamlet - As You Like It: relative-salience word cloud for trust words.,[],W11-1514deepfigures-results.json,word cloud,W11-1514
W11-1514.pdf-Figure5.png,Figure 5: Hamlet - As You Like It: relative-salience word cloud for sadness words.,[],W11-1514deepfigures-results.json,word cloud,W11-1514
W11-1709.pdf-Figure11.png,Figure 11: Suicide notes - hate mail: relative-salience word cloud for disgust.,[u'. figure 11 depicts a relativesalience word cloud of disgust words in the hate mail corpus with respect to the suicide notes corpus.'],W11-1709deepfigures-results.json,word cloud,W11-1709
W11-1709.pdf-Figure13.png,Figure 13: Emails by women - emails by men: relativesalience word cloud of trust.,[u'. figure 13 shows the relative-salience word cloud of these trust words.'],W11-1709deepfigures-results.json,word cloud,W11-1709
W11-1709.pdf-Figure15.png,Figure 15: Emails to women - emails to men: relativesalience word cloud of joy.,[u'. figure 15 shows the relative-salience word cloud of joy.'],W11-1709deepfigures-results.json,word cloud,W11-1709
W11-1709.pdf-Figure17.png,Figure 17: Emails by men to women - email by men to men: relative-salience word cloud of anticipation.,[u'. figure 17 shows the relative-  salience word cloud of these anticipation words.'],W11-1709deepfigures-results.json,word cloud,W11-1709
W11-1709.pdf-Figure19.png,Figure 19: Emails by women to women - emails by women to men: relative-salience word cloud of sadness.,[],W11-1709deepfigures-results.json,word cloud,W11-1709
W11-1709.pdf-Figure21.png,Figure 21: Emails by men to men - emails by women to women: relative-salience word cloud of fear.,[],W11-1709deepfigures-results.json,word cloud,W11-1709
W11-1709.pdf-Figure8.png,Figure 8: Love letters corpus - hate mail corpus: relativesalience word cloud for joy.,[u'. figure 8 depicts a relative-salience word cloud of joy words in the love letters corpus with respect to the hate mail corpus.'],W11-1709deepfigures-results.json,word cloud,W11-1709
W12-3612.pdf-Figure3.png,Figure 3: A word cloud generated from all words marked as body location.,[],W12-3612deepfigures-results.json,word cloud,W12-3612
W12-3612.pdf-Figure4.png,Figure 4: A word cloud generated from all words marked as primary morphology.,[],W12-3612deepfigures-results.json,word cloud,W12-3612
W12-41.pdf-Figure4.png,Figure 4: The association results from the word Novel via our social tagging recommender system.,"[u'. for instance, it could ""trigger"" a large amount of semantic related tags from a given word: novel (figure 4) .', u'. figure 4 : the association results from the word ""novel"" via our social tagging recommender system.']",W12-41deepfigures-results.json,word cloud,W12-41
W12-4108.pdf-Figure4.png,Figure 4: The association results from the word Novel via our social tagging recommender system.,"[u'. for instance, it could ""trigger"" a large amount of semantic related tags from a given word: novel (figure 4) .', u'. figure 4 : the association results from the word ""novel"" via our social tagging recommender system.']",W12-4108deepfigures-results.json,word cloud,W12-4108
W13-42.pdf-Figure6.png,Figure 6: Tag Cloud,[],W13-42deepfigures-results.json,word cloud,W13-42
W14-3112.pdf-Figure2.png,Figure 2: The visualization utilizes a group-in-abox-inspired layout to represent the topic model as a nested network graph.,"[u'. figure 2 shows the visualization for a topic model generated for a 1,000 document nsf dataset.']",W14-3112deepfigures-results.json,word cloud,W14-3112
W14-3112.pdf-Figure3.png,"Figure 3: The user has hovered over the mostcentral topic in the layout, which is the most connected topic. The hovered topic is outlined, and the topic name is highlighted in turquoise. The topic names of the related topics are also highlighted.","[u'. as demonstrated in figure 3 , a user can hover over a topic to see the related topics 4 .', u'The visualization presented here provides a novel way to explore topic models with incorporated Figure 3 : The user has hovered over the mostcentral topic in the layout, which is the']",W14-3112deepfigures-results.json,word cloud,W14-3112
W14-3112.pdf-Figure4.png,Figure 4: The visualization where the user has hovered over a word of interest. The same word is highlighted turquoise in other topics.,"[u'. hovering over a word node highlights the same word in other topics as shown in figure 4 .', u'. figure 4 : the visualization where the user has hovered over a word of interest.']",W14-3112deepfigures-results.json,word cloud,W14-3112
W14-3112.pdf-Figure5.png,"Figure 5: The edit mode for the visualization. From this mode, the user can add words, remove words, or rename the topic.","[u'required by the ITM algorithm, the visualization has an edit mode, which is shown in Figure 5 . Ongoing work includes developing appropriate visual operations to support the following model-editing operations:', u'. figure 5 : the edit mode for the visualization.']",W14-3112deepfigures-results.json,word cloud,W14-3112
W14-32.pdf-Figure5.png,Figure 5: Top ten topics most positively correlated with depression (from r = .14 at top to r = .11 at bottom). All are significant at a Bonferronicorrected threshold of p < 0.001. Word size corresponds to prevalence within the topics.,[],W14-32deepfigures-results.json,word cloud,W14-32
W14-3214.pdf-Figure4.png,"Figure 4: The 100 ngrams most correlated with DDep (ranging from r = .05 to r = .10). All are significant at a Bonferroni-corrected threshold of p < 0.001. Ngram size corresponds to correlation strength (larger words are more distinguishing). Color corresponds to relative frequency (red if frequent, blue moderate, grey infrequent).","[u'. figure 4 shows the 100 ngrams most highly correlated with depression score across the 21,913 facebook users in our dataset writing at least 1,000 words.', u'. in line with survey seasonal trends and the broader literature, we found that languagebased predictions of depression were higher in the winter than the summer, suggesting that our figure 4 : the 100 ngrams most correlated with ddep (ranging from r = .']",W14-3214deepfigures-results.json,word cloud,W14-3214
W14-3214.pdf-Figure5.png,Figure 5: Top ten topics most positively correlated with depression (from r = .14 at top to r = .11 at bottom). All are significant at a Bonferronicorrected threshold of p < 0.001. Word size corresponds to prevalence within the topics.,"[u'As illustrated in Figure 5 , extending the words and phrase results, automatically derived topics demonstrate substantial overlap with the', u'. given such striking descriptive results, future work might try to detect depression associ- figure 5 : top ten topics most positively correlated with depression (from r = .']",W14-3214deepfigures-results.json,word cloud,W14-3214
2007.sigdial-1.19.pdf-Figure1.png,Figure 1: A test subject describes a task while performing it.,[u'. a test subject sits in front of a table with several objects ( a cup and a plant) on it that can be utilized for different manipulative actions (figure 1 ).'],2007.sigdial-1.19deepfigures-results.json,natural images,2007.sigdial-1.19
2007.sigdial-1.32.pdf-Figure1.png,Figure 1. Driving simulator.,"[u'driving simulator with a 180 field of view and a motion base, as shown in Figure 1 . The simulator presented a city scenario with two-lane (one lane for each direction) roads']",2007.sigdial-1.32deepfigures-results.json,natural images,2007.sigdial-1.32
2016.jeptalnrecital-jep.68.pdf-Figure1.png,FIGURE 1: Capture dcran de liPad pendant le jeu pour un essai avec le couple banane/chaussette,[],2016.jeptalnrecital-jep.68deepfigures-results.json,natural images,2016.jeptalnrecital-jep.68
2016.jeptalnrecital-poster.13.pdf-Figure1.png,FIGURE 1: Mouvement de plusieurs articulateurs simultanment,[],2016.jeptalnrecital-poster.13deepfigures-results.json,natural images,2016.jeptalnrecital-poster.13
2016.jeptalnrecital-poster.13.pdf-Figure3.png,Figure 3: Les deux groupes invariants de formes,[],2016.jeptalnrecital-poster.13deepfigures-results.json,natural images,2016.jeptalnrecital-poster.13
2018.jeptalnrecital-long.1.pdf-Figure4.png,FIGURE 4  Le camra doculomtrie et son calibrage avec 9 points de fixation (tape 1).,[],2018.jeptalnrecital-long.1deepfigures-results.json,natural images,2018.jeptalnrecital-long.1
2018.jeptalnrecital-long.13.pdf-Figure3.png,FIGURE 3  Exemples dimages pour lesquelles le classifieur a utilis uniquement les caractristiques visuelles pour corriger le rattachement de la prposition (en rouge).,"[u'. les figure 3 .', u'. dans la figure 3 .', u'. dans la figure 3 .']",2018.jeptalnrecital-long.13deepfigures-results.json,natural images,2018.jeptalnrecital-long.13
2018.jeptalnrecital-long.13.pdf-Figure4.png,FIGURE 4  Exemples dimages pour lesquelles le classifieur a mal corrig le rattachement de la prposition (en rouge) avec des caractristiques visuelles uniquement.,[],2018.jeptalnrecital-long.13deepfigures-results.json,natural images,2018.jeptalnrecital-long.13
2018.jeptalnrecital-long.13.pdf-Figure5.png,FIGURE 5  Exemples dimages avec un rattachement correct en utilisant des caractristiques visuelles pour les prpositions en rouge alors que lutilisation de traits lexicaux choisit un mauvais gouverneur.,[],2018.jeptalnrecital-long.13deepfigures-results.json,natural images,2018.jeptalnrecital-long.13
2019.jeptalnrecital-recital.11.pdf-Figure1.png,Figure 1 : Captures dimage dune traduction observe de lundi dans une vido du corpus,[],2019.jeptalnrecital-recital.11deepfigures-results.json,natural images,2019.jeptalnrecital-recital.11
2020.acl-demos.11.pdf-Figure1.png,Figure 1: An example of cross-media knowledge fusion and a look inside the visual knowledge extraction.,"[u'. taking figure 1 as an example, the text entity extraction system extracts the nominal mention troops, but is unable to link or relate that due to a vague textual context.', u'. some knowledge elements in a document may not be explicitly mentioned in the text, but will appear visually, such as the ukrainian flag in figure 1 .']",2020.acl-demos.11deepfigures-results.json,natural images,2020.acl-demos.11
2020.acl-demos.11.pdf-Figure4.png,"Figure 4: Examples of visual entity linking, based on face recognition, landmark recognition and flag recognition.","[u'. for example, the visual entity in figure 4 (a) is linked to the wikipedia entry rudy giuliani 9 .', u'. for example, the visual entity in figure 4 (b) is linked to the wikipedia entry maidan square 10 finally, to recognize geopolitical entities, we train a cnn to classify flags into a predetermined list of entities, such as all the nations in the world, for detection in our system.', u'. take figure 4 (c) as an example.']",2020.acl-demos.11deepfigures-results.json,natural images,2020.acl-demos.11
2020.acl-demos.11.pdf-Figure5.png,"Figure 5: The two green bounding boxes are coreferential since they are both linked to Kirstjen Nielsen, and two red bounding boxes are coreferential based on face features. The yellow bounding boxes are unlinkable and also not coreferential to other bounding boxes.","[u'. take figure 5 as an example.', u'. figure 5 : the two green bounding boxes are coreferential since they are both linked to ""kirstjen nielsen"", and two red bounding boxes are coreferential based on face features.']",2020.acl-demos.11deepfigures-results.json,natural images,2020.acl-demos.11
2020.acl-demos.31.pdf-Figure1.png,Figure 1: Tracking emotion states and engagement levels using multi-modal information.,[u'. figure 1 illustrates an example of our system tracking emotion states and engagement levels.'],2020.acl-demos.31deepfigures-results.json,natural images,2020.acl-demos.31
2020.acl-main.219.pdf-Figure1.png,"Figure 1: Some samples from the IMAGE-CHAT training set. For each sample we asked humans to engage in a conversation about the given image, where the two speakers, A and B, each have a given provided style.","[u""A and one for speaker A: There's probably more lame pavement on the other side! Figure 1 : Some samples from the IMAGE-CHAT training set. For each sample we asked humans to"", u'. some examples from the training set are given in figure 1 .']",2020.acl-main.219deepfigures-results.json,natural images,2020.acl-main.219
2020.acl-main.219.pdf-Figure10.png,"Figure 10: Long-form conversation with the model. The model is given a style here, while the human is not.",[],2020.acl-main.219deepfigures-results.json,natural images,2020.acl-main.219
2020.acl-main.230.pdf-Figure2.png,Figure 2: Example of bounding boxes.,[u'. figure 2 shows an example.'],2020.acl-main.230deepfigures-results.json,natural images,2020.acl-main.230
2020.acl-main.233.pdf-Figure4.png,"Figure 4: Nearest neighbors retrieved using memory states. Top row shows the query, the 3 rows below it are the top-3 nearest neighbors.","[u'. we show an example in figure 4 , the neighbors mostly show related activities.']",2020.acl-main.233deepfigures-results.json,natural images,2020.acl-main.233
2020.acl-main.234.pdf-Figure4.png,"Figure 4: Image-caption pairs corresponding to noun tokens estimated as most concrete (bottom 5%) in our 1, sWS, cME variant. We also report the number of occurrences in the MSCOCO training set.","[u'c ME variant, the 1d variant closest to the original model, as a concreteness estimate. Figure 4 shows the most concrete nouns, and']",2020.acl-main.234deepfigures-results.json,natural images,2020.acl-main.234
2020.acl-main.234.pdf-Figure5.png,"Figure 5: Image-caption pairs corresponding to noun tokens estimated as least concrete (bottom 5%) in our 1, sWS, cME variant. We also report the number of occurrences in the MSCOCO training set.","[u'shows the most concrete nouns, and Figure 5 shows the least concrete nouns. We selected nouns from the top (bottom) 5% of the']",2020.acl-main.234deepfigures-results.json,natural images,2020.acl-main.234
2020.acl-main.306.pdf-Figure1.png,Figure 1: Two examples for Multimodal Named Entity Recognition (MNER). Named entities and their entity types are highlighted.,[],2020.acl-main.306deepfigures-results.json,natural images,2020.acl-main.306
2020.acl-main.349.pdf-Figure1.png,Figure 1: Examples of multimodal tweets. The nonsarcasm (a) shows the users affection for the beautiful trees with positive sentiment; and (b) is a sarcastic tweet where the text word perfect contrasts sharply with the rainy weather in the image,[],2020.acl-main.349deepfigures-results.json,natural images,2020.acl-main.349
2020.acl-main.358.pdf-Figure4.png,"Figure 4: The correct probabilities of four triples in DB100K and their reverse triples. The probabilities P1, P2 and P4 are corresponding to the scoring functions f1, f2 and f4, respectively.",[u'. figure 4 shows the correct probabilities of some triples.'],2020.acl-main.358deepfigures-results.json,natural images,2020.acl-main.358
2020.acl-main.401.pdf-Figure1.png,Figure 1: Example to show that sentiment and emotion of the speaker can influence sarcasm detection,"[u'. figure 1) .', u'."" figure 1 : example to show that sentiment and emotion of the speaker can influence sarcasm detection even though sentiment, emotion, and sarcasm are related, sarcasm was treated separately from its other counterparts in the past due to its complexity and its high dependency on the context.', u'. we consider three sentiment classes, namely positive, negative and neutr for the example in figure 1 , the implicit sentiment would be negative, whereas explicit sentiment is positive.', u'. in the example of figure 1 , the implicit emotion of the speaker would be disgust while the explicit emotion is happy.']",2020.acl-main.401deepfigures-results.json,natural images,2020.acl-main.401
2020.acl-main.425.pdf-Figure1.png,"Figure 1: Two images described by the same bag of visual words but illustrating different NC concepts (i.e., high jump and pole vault).","[u'. consider, for instance, figure 1 .', u'. for instance, both images in figure 1 could be mapped to the concept competition as well.', u'. for instance, the images in figure 1 could never be used as negative examples for competition because of the hyponymy relation connecting this concept to high jump and pole vault.']",2020.acl-main.425deepfigures-results.json,natural images,2020.acl-main.425
2020.acl-main.425.pdf-Figure2.png,"Figure 2: A few examples from BabelPic, both gold (G) and silver (S).",[u'. a few examples from ba-belpic (both gold and silver) are shown in figure 2 .'],2020.acl-main.425deepfigures-results.json,natural images,2020.acl-main.425
2020.acl-main.463.pdf-Figure1.png,Figure 1: Photo stimuli 1 (L) and 2 (R),"[u'. at test time, we present the model with inputs consisting of an utterance and a photograph, like how many dogs in the picture are jumping? or kim saw this picture and said ""what a cute dog!"" what is cute? and the photos in figure 1 , where the appropriate answers are a number or a region of the photo, respectively.']",2020.acl-main.463deepfigures-results.json,natural images,2020.acl-main.463
2020.acl-main.583.pdf-Figure1.png,Figure 1: Output of a coherence-aware model for various coherence relations. Content that establishes the intended relation is underlined. (Photo credit: Blue Destiny / Alamy Stock Photo) Visible: horse and rider jumping a fence. Meta: horse and rider jumping a fence during a race. Subjective: the most beautiful horse in the world.,"[u'. for example, along with descriptive captions figure 1 : output of a coherence-aware model for various coherence relations.']",2020.acl-main.583deepfigures-results.json,natural images,2020.acl-main.583
2020.acl-main.583.pdf-Figure2.png,"Figure 2: We use a constrained set of coherence relations to summarize the structural, logical and purposeful relationships between the contributions of text and the contributions of images. Multiple coherence relations can be found simultaneously. (Imagecaption pairs are chosen from the Conceptual Caption dataset; photo credits: Dmytro Zinkevych; Shutterstock user yauhenka; Danilo Hegg; Andre Seale)","[u'. for completeness, we also present in figure 2 (d) an example of an imagecaption pair that does not fall into any of the above categories (and it is therefore labeled irrelevant).']",2020.acl-main.583deepfigures-results.json,natural images,2020.acl-main.583
2020.acl-main.583.pdf-Figure3.png,Figure 3: Examples of imagecaption pairs in the Other category. (Photo credit: santabanta.com; Mary Sollosi),"[u'. the text in these cases is relevant to the image and the accompanying captions; in this cases, the coherence relations are marked as other-text (figure 3 ).']",2020.acl-main.583deepfigures-results.json,natural images,2020.acl-main.583
2020.acl-main.584.pdf-Figure1.png,"Figure 1: Example object from VisualGenome with annotated colour attribute. The tree is described as green, despite of challenging illumination conditions.",[],2020.acl-main.584deepfigures-results.json,natural images,2020.acl-main.584
2020.acl-main.584.pdf-Figure3.png,Figure 3: TOP-DOWN and LATE-FUSION predict the canonical colour for the depicted bush (green). BOTTOM-UP and EARLY-FUSION capture the annotated colour (purple).,[],2020.acl-main.584deepfigures-results.json,natural images,2020.acl-main.584
2020.acl-main.586.pdf-Figure4.png,Figure 4: Predictions of ViLBERT and MTL model (GT denotes ground-truth). e1 and e2 are adversarial expressions of e1 and e2 respectively.,[],2020.acl-main.586deepfigures-results.json,natural images,2020.acl-main.586
2020.acl-main.643.pdf-Figure7.png,Figure 7: The MN-GMN provides the correct answer using white and black tennis shoes.,"[u'. figure 7 shows how mn-gmn can answer a ques- tion correctly by incorporating the region-grounded captions, whereas n-gmn gives the wrong answer.']",2020.acl-main.643deepfigures-results.json,natural images,2020.acl-main.643
2020.acl-main.644.pdf-Figure1.png,"Figure 1: An example from Refer360 . Orange frames represent the field-of-view (FoV) of the follower after interpreting each instruction. Numbers in the frames represent the sequential order. Green lines show how FoVs change continuously. After each instruction, the follower changes the FoV to align with what the instruction describes. Please see Figure 2a to see the correct location of Waldo.",[u'. figure 1 presents an example scenario partial fov (a) an example scene from the refer360dataset.'],2020.acl-main.644deepfigures-results.json,natural images,2020.acl-main.644
2020.acl-main.674.pdf-Figure1.png,Figure 1: Ads Dataset: Textual and Visual Cues,"[u"". for example, in the ad of the car company in figure  1 , the words and phrases from a viewer's input 'i should buy this car because it would add some excitement to my life' can be associated with the object 'car' in the image and the phrase 'add spark to life' in the scene-text.""]",2020.acl-main.674deepfigures-results.json,natural images,2020.acl-main.674
2020.acl-main.682.pdf-Figure2.png,Figure 2: CompGuessWhat?!: Detailed description of the attributes of two different objects in the reference scene. Both the objects have a set of abstract attributes (indicated in blue) and a set of situated attributes (indicated in green).,"[u'. in this work, we propose grolla -a multitask evaluation framework for grounded language learning with attributes that expands a goal-type accessories has shaft has handle open black red center umbrella has eyes has 2 legs has 2 arms has mouth little girl person figure 2 : compguesswhat?!: detailed description of the attributes of two different objects in the reference scene.', u'. this layer consists of a collection of intentional and extensional attributes of the objects in the reference image ( figure 2 ).']",2020.acl-main.682deepfigures-results.json,natural images,2020.acl-main.682
2020.acl-main.739.pdf-Figure1.png,Figure 1: Sample tweet with text and an image. The author of the tweet possesses the cup for a few weeks or months. The tweet does not indicate a co-possession.,[u'. consider the tweet in figure 1 .'],2020.acl-main.739deepfigures-results.json,natural images,2020.acl-main.739
2020.acl-main.93.pdf-Figure1.png,"Figure 1: Intrinsic variance exists in a set of ground truth captions for an image. Differences between two references are commonly caused by two reasons: different concerns or different descriptions. Different concerns mean different expressions between references are caused by different regions of interest in an image, while different descriptions mean references focus on the same part but use different ways to explain it. Oneto-one metrics can hardly deal with the cases caused by different concerns. For example, they may regard Cand as a good caption compared with Ref1; while regard Cand as a bad caption compared with Ref2 or Ref3.",[u'. figure 1 : intrinsic variance exists in a set of ground truth captions for an image.'],2020.acl-main.93deepfigures-results.json,natural images,2020.acl-main.93
2020.ai4hi-1.1.pdf-Figure1.png,"Figure 1: Automatically generated labels assigned to FSA-OWI color photographs by the Mask R-CNN instance object classification algorithm (X101-FPN) (Wu et al., 2019). For each of the eight selected object types, the five images from the FSA-OWI color photographs that are most predicted to contain the given category are shown. All categories were estimated to exist with probability greater than 80%. The plane and horse categories seem to have correctly identified the objects in their five respective images, and two of the cow images are in fact cows (the others are horses). The remaining categories seem to be all false detections. Many mistakes are hard to explain, such as the row of skateboard objects.","[u'approach is based on the detection of regions of the image containing elements described as Figure 1 : Automatically generated labels assigned to FSA-OWI color photographs by the Mask R-CNN instance object', u'with preselected categories on historic images typically produces a mix of correct and false annotations. Figure 1 shows the results of a popular object detection algorithm to the FSA-OWI collection']",2020.ai4hi-1.1deepfigures-results.json,natural images,2020.ai4hi-1.1
2020.ai4hi-1.1.pdf-Figure2.png,"Figure 2: Three detected captions for three FSA-OWI photographs using the Show, attend and tell model (Xu et al., 2015). The first provides a caption that matches the image and the third produces a caption that is very similar to the image. The second correctly identifies the subject of a woman in the frame but mistakenly believes she is holding a microphone. The final image produces an annotation that incorrectly labels the people as giraffes.","[u""false detections. Many mistakes are hard to explain, such as the row of skateboard objects. Figure 2 : Three detected captions for three FSA-OWI photographs using the 'Show, attend and tell' model"", u'methods too-often produce nonsensical results that make them difficult to deploy directly in an archive. Figure 2 show the results of one popular caption algorithm applied to photographs from the 1940s']",2020.ai4hi-1.1deepfigures-results.json,natural images,2020.ai4hi-1.1
2020.ai4hi-1.1.pdf-Figure4.png,"Figure 4: Seven selected stuff types and the people category shown with the five images from the FSA-OWI color photographs that are most predicted to contain the given type. Uses the ResNet+FPN model provided by the Detectron2 model zoo (Wu et al., 2019). The only labels that appears the be falsely detected are in the third and fifth bridge images, where construction equipment is falsely believed to be a bridge.","[u'. an example of these are shown in figure 4 .', u'. each photograph was tagged with detected objects and labelled with any object that appeared with at least a probability of 85% figure 4 : seven selected stuff types and the people category shown with the five images from the fsa-owi color photographs that are most predicted to contain the given type.', u'Recall Both the close-analysis of the annotations in Figure 4 illustrate the efficacy of ""stuff"" region-based annotations for adding structured data to historic image data.']",2020.ai4hi-1.1deepfigures-results.json,natural images,2020.ai4hi-1.1
2020.ai4hi-1.3.pdf-Figure1.png,Figure 1: Some examples of traditional artistic styles.,"[u'. figure 1 shows 5 different art styles, along with a brief description.']",2020.ai4hi-1.3deepfigures-results.json,natural images,2020.ai4hi-1.3
2020.ai4hi-1.3.pdf-Figure2.png,Figure 2: Some examples of Outsider Art paintings.,"[u'. the condition of ""non-traditional"" or, more specifically, ""outsider"" artist applies to people who have very little contact with the mainstream art world and for this reason have developed extreme unconventional ideas based on spontaneous inventions (see figure 2a -b).', u'. from a stylistic point of view, outsider artists paint obsessively repetitive images or themes (see figure 2c ).', u'. for that reason, subjects such as sexuality and eroticism can erupt in the most raw, emphatic and uncontrolled way (see figure 2d ).']",2020.ai4hi-1.3deepfigures-results.json,natural images,2020.ai4hi-1.3
2020.ai4hi-1.5.pdf-Figure1.png,Figure 1: Sample image with its metadata.,[u'. a sample image is shown in figure 1 and a snippet of the associated metadata is given the text below.'],2020.ai4hi-1.5deepfigures-results.json,natural images,2020.ai4hi-1.5
2020.alvr-1.1.pdf-Figure1.png,"Figure 1: Images in ImageNet for the synset Siberian husky. Although an Arabic synset from AWN is not available for the synset or its direct hypernym, one is available for the hypernym of the hypernym.","[u'. moreover, imagenet is structured in a way that maintains the semantic hierarchical structure of synsets in wordnet, where each image is also linked to branches of hypernyms ( figure 1 ).', u'. figure 1 : images in imagenet for the synset ""siberian husky"".']",2020.alvr-1.1deepfigures-results.json,natural images,2020.alvr-1.1
2020.alvr-1.2.pdf-Figure1.png,Figure 1: An example of scene graph for a common image from Visual Genome 200 (VG200) and VisuallyRelevant Relationship (VrR-VG) dataset.,"[u'. as shown in figure 1 ,if some of objects in an image do not belong to the dataset-specific vocabulary, objects as well as relations are omitted frequently even though they are in an image.']",2020.alvr-1.2deepfigures-results.json,natural images,2020.alvr-1.2
2020.alvr-1.5.pdf-Figure5.png,Figure 5: The image is misleading the multimodal setting to choose apple slices rather than cutting option,"[u'Images contain misleading information (see example in Figure 5 ). Image quality is low.', u'Text contains direct mentions of candidate answers. Figure 5 : The image is misleading the multimodal setting to choose apple slices rather than cutting']",2020.alvr-1.5deepfigures-results.json,natural images,2020.alvr-1.5
2020.challengehml-1.9.pdf-Figure1.png,Figure 1: Attributions overlaid on the corresponding input words. The output of the model changes from yellow to 1 which is driven by the word many.,[],2020.challengehml-1.9deepfigures-results.json,natural images,2020.challengehml-1.9
2020.challengehml-1.9.pdf-Figure2.png,Figure 2: The output of the model is driven by the word answer acting as an adversary.,[],2020.challengehml-1.9deepfigures-results.json,natural images,2020.challengehml-1.9
2020.cllrd-1.1.pdf-Figure1.png,"Figure 1: Citizen Linguist portal, LanguageARC.",[],2020.cllrd-1.1deepfigures-results.json,natural images,2020.cllrd-1.1
2020.cllrd-1.1.pdf-Figure3.png,Figure 3: LanguageARC Project menu,[],2020.cllrd-1.1deepfigures-results.json,natural images,2020.cllrd-1.1
2020.figlang-1.19.pdf-Figure1.png,Figure 1: Fire!,"[u"". let's take for example figure 1 , an image of a fire on a dark background.""]",2020.figlang-1.19deepfigures-results.json,natural images,2020.figlang-1.19
2020.figlang-1.19.pdf-Figure2.png,Figure 2: Two dragons,"[u'drawing representing a dragon, as comic book (0.28) or laptop (0.08), and the rightmost image Figure 2 , a statue of dragon, as pedestal (0.61), fountain (0.38) or palace (0.0005). In both', u"". many of vgg16's predictions in front of unknown or puzzling objects mirror those of resnet50: dam (0.22) for the modern bridge picture, pedestal (0.55) for the dragon in figure 2 , viaduct (0.24) after triumphal arch (0.33) for the bridge mirrored in the water."", u'. this possible predilection of vgg16 for shapes of elements, colour or ""style"" appears in other examples: saturn is a spotlight (0.08) and a ping pong ball (0.07) rather than a candle (0.03), and the leftmost dragon in figure 2 is labelled as jersey (0.68).']",2020.figlang-1.19deepfigures-results.json,natural images,2020.figlang-1.19
2020.figlang-1.19.pdf-Figure3.png,Figure 3: A guy.,"[u'. for similar reasons, and showing some politically incorrect bias, the model labels the person in figure 3 as prison (0.05), jean (0.02) and barrow (0.06).']",2020.figlang-1.19deepfigures-results.json,natural images,2020.figlang-1.19
2020.figlang-1.19.pdf-Figure4.png,Figure 4: The first 5 output categories of VGG16 for various pictures. Most of these pictures represent objects the network was not trained on. The reader can notice that some of the mis-classifications could work as metaphoric descriptions (as in b or f) more than others (as in a or e).,"[u'. to give the reader a first hand idea of the descriptive qualities of these mis-categorisations, we present in figure 4 a series of pictures with the first 5 categories assigned by the vgg16 model.', u'. figure 4 : the first 5 output categories of vgg16 for various pictures.']",2020.figlang-1.19deepfigures-results.json,natural images,2020.figlang-1.19
2020.figlang-1.19.pdf-Figure5.png,"Figure 5: Two elements from our dataset. A galaxy described in the human-generated caption as a jellyfish, and a building described (and commonly known) as a cucumber/gherkin.","[u'100 pictures that human users had described with a metaphor or simile as shown in Figure 5 . We exclusively selected metaphors that had only their target in the 1,000 Im-ageNet categories', u'. figure 5 : two elements from our dataset.']",2020.figlang-1.19deepfigures-results.json,natural images,2020.figlang-1.19
2020.figlang-1.19.pdf-Figure7.png,"Figure 7: Samples from the three compound visual vectors we create to serve as source, target and modifier of the metaphor Sunset is Sky on Fire.","[u'sky vector out of 10 pictures of (mainly blue) skies such as the one in Figure 7 (a). The average cosine similarity of these pictures is around 0 (the lowest possible cosine', u'created a fire vector out of 13 pictures of fire such as the one in Figure 7 (b) with average cosine similarity', u'. finally, we created a sunset vector out of 7 pictures of sunsets captioned by humans as sky on fire, as the one in figure 7 (c).']",2020.figlang-1.19deepfigures-results.json,natural images,2020.figlang-1.19
2020.figlang-1.5.pdf-Figure4.png,"Figure 4: A satirical headline generated by our model was published in the Brown Noser Satirical Newspaper, and accompanied by an article written by a human satirical writer. The article can be found here.","[u'2020 issue, Jacob Lockwood, an undergraduate satirical writer, volunteered to write an accompanying article (See Figure 4) . We are enthusiastic about the potential for future AI-Human satirical collaborations.', u'. figure 4 : a satirical headline generated by our model was published in the brown noser satirical newspaper, and accompanied by an article written by a human satirical writer.', u"". additionally, we are grateful to jacob lockwood, an editor of the brown noser satirical newspaper, for offering the necessary human talent to parlay our model's output into a full satirical article (figure 4) .""]",2020.figlang-1.5deepfigures-results.json,natural images,2020.figlang-1.5
2020.framenet-1.4.pdf-Figure5.png,Fig. 5. Screenshot of the FN-Br Webtool Multimodal Annotation Module,"[u'. nonetheless, in the video annotation, the annotator chose the people_by_origin frame, which is evoked by the object 7, as shown in figure 5 .', u'.n evokes the people_by_origin frame, which is precisely the one evoked by the object 7 in figure 5 .']",2020.framenet-1.4deepfigures-results.json,natural images,2020.framenet-1.4
2020.gamnlp-1.10.pdf-Figure1.png,Figure 1: GDX screenshot: Beginning of a trial during the training phase with visual category information human.,"[u'the participant with the virtual game environment and the background story. Screenshots are shown in Figure 1 and', u'. only during an initial training phase are visual cues shown to indicate the category of an agent (figure 1) .']",2020.gamnlp-1.10deepfigures-results.json,natural images,2020.gamnlp-1.10
2020.gamnlp-1.10.pdf-Figure2.png,Figure 2: GDX screenshot: Feedback after the end of a trial with a correct classification as alien.,"[u'and Figure 2 .', u'. after the end of a trial, feedback is provided to the player about the true category of an agent (also showing an alien figure instead of the default human figure in case the agent belonged to the alien category, cf.']",2020.gamnlp-1.10deepfigures-results.json,natural images,2020.gamnlp-1.10
2020.gamnlp-1.2.pdf-Figure1.png,"Figure 1: A game to distinguish between similar images, such as these boats, can create more meaningful labels.","[u'the images are generic and rarely provide enough information to discriminate between similar images (see Figure 1 ). In this paper, we introduce a game, ClueMeIn, to address this problem. We designed', u'. figure 1 : a game to distinguish between similar images, such as these boats, can create more meaningful labels.']",2020.gamnlp-1.2deepfigures-results.json,natural images,2020.gamnlp-1.2
2020.gamnlp-1.2.pdf-Figure3.png,Figure 3: Screenshots from the game indicating the view of Guesser (top) and Cluegiver (bottom). Players take turns in each role and have different incentives to facilitate meaningful clues.,[],2020.gamnlp-1.2deepfigures-results.json,natural images,2020.gamnlp-1.2
2020.gamnlp-1.2.pdf-Figure8.png,"Figure 8. Sometimes seemingly irrelevant facts can separate two similar images. Rocks was a label given to the image on left, but grass was a label given to the image on the right. We resolve this by assigning weights to these labels based on Cluegiver frequency",[u'. figure 8 .'],2020.gamnlp-1.2deepfigures-results.json,natural images,2020.gamnlp-1.2
2020.gamnlp-1.5.pdf-Figure1.png,"Figure 1: A screenshot of La Ghigliottina. In this case, the solution is cassa: i) cassa del cinema (cinema box office), ii) grancassa (bass drum), iii) cassa comune (petty cash), iv) battere cassa (beat the check), and v) coda alla cassa (checkout line)",[u'. in figure 1 we show an example of the game.'],2020.gamnlp-1.5deepfigures-results.json,natural images,2020.gamnlp-1.5
2020.gamnlp-1.5.pdf-Figure2.png,"Figure 2: A screenshot from Telegram in solution mode: Mago della Ghigliottina accepts the picture containing the five clues and in 1-2 seconds returns its prediction with a degree of accuracy. In this case, the system correctly guesses the solution because of the following MWEs i) conoscere alla perfezione (to perfectly know), ii) grado di perfezione (degree of perfection), iii) modello di perfezione (model of perfection), iv) ideale di perfezione (ideal of perfection), and v) perfezione divina (divine perfection).","[u'. as shown in figure 2 , mago della ghigliottina uses ocr to recognize the words from the image.']",2020.gamnlp-1.5deepfigures-results.json,natural images,2020.gamnlp-1.5
2020.gamnlp-1.6.pdf-Figure1.png,Figure 1: Game screenshot of task 1: Modifying offensive sentences.,[],2020.gamnlp-1.6deepfigures-results.json,natural images,2020.gamnlp-1.6
2020.gamnlp-1.6.pdf-Figure2.png,Figure 2: Game screenshot of task 2: Erasing graffiti with a sponge.,[],2020.gamnlp-1.6deepfigures-results.json,natural images,2020.gamnlp-1.6
2020.gamnlp-1.6.pdf-Figure3.png,Figure 3: A view of the school yard.,[],2020.gamnlp-1.6deepfigures-results.json,natural images,2020.gamnlp-1.6
2020.gamnlp-1.6.pdf-Figure4.png,Figure 4: Character customization interface.,[],2020.gamnlp-1.6deepfigures-results.json,natural images,2020.gamnlp-1.6
2020.isa-1.8.pdf-Figure4.png,Figure 4: Semantic variation of to close.,[],2020.isa-1.8deepfigures-results.json,natural images,2020.isa-1.8
2020.isa-1.8.pdf-Figure5.png,Figure 5: Frame from the scene Maria puts the objects in order.,"[u'. however, only some differences of features are mirrored by a figure 5 : frame from the scene ""maria puts the objects in order"".']",2020.isa-1.8deepfigures-results.json,natural images,2020.isa-1.8
2020.isa-1.8.pdf-Figure6.png,Figure 6: Frame from the scene Maria puts the plastic on the book.,[],2020.isa-1.8deepfigures-results.json,natural images,2020.isa-1.8
2020.jeptalnrecital-jep.48.pdf-Figure1.png,"FIGURE 1  Paire minimale MTRO ( gauche)  FAX ( droite) en LSF. Les traits pertinents de la configuration manuelle et de lemplacement sont indiqus pour chacun des signes, ainsi que lorientation obtenue  partir de ces traits.",[],2020.jeptalnrecital-jep.48deepfigures-results.json,natural images,2020.jeptalnrecital-jep.48
2020.jeptalnrecital-jep.48.pdf-Figure2.png,FIGURE 2  Signe COLRE en LSF. FIGURE 3  Signe CEINTURE en LSF.,[],2020.jeptalnrecital-jep.48deepfigures-results.json,natural images,2020.jeptalnrecital-jep.48
2020.jeptalnrecital-jep.48.pdf-Figure4.png,FIGURE 4  Signe CODA en LSF. FIGURE 5  Signe CANDIDAT en LIS.,[],2020.jeptalnrecital-jep.48deepfigures-results.json,natural images,2020.jeptalnrecital-jep.48
2020.jeptalnrecital-jep.48.pdf-Figure7.png,FIGURE 7  Signe CHMAGE en LSF. FIGURE 8  Signe OS en LSF.,[],2020.jeptalnrecital-jep.48deepfigures-results.json,natural images,2020.jeptalnrecital-jep.48
2020.lrec-1.136.pdf-Figure1.png,Figure 1: A snapshot from the Inaugural Address Speech.,[],2020.lrec-1.136deepfigures-results.json,natural images,2020.lrec-1.136
2020.lrec-1.136.pdf-Figure2.png,Figure 2: A snapshot of a beat gesture during the State of the Union Address.,[],2020.lrec-1.136deepfigures-results.json,natural images,2020.lrec-1.136
2020.lrec-1.187.pdf-Figure3.png,Figure 3: Close-up view of the thermal and video recording equipment.,[],2020.lrec-1.187deepfigures-results.json,natural images,2020.lrec-1.187
2020.lrec-1.19.pdf-Figure1.png,"Figure 1: Participants performed the task seated at the table, with the three objects placed on a table in front of them.",[],2020.lrec-1.19deepfigures-results.json,natural images,2020.lrec-1.19
2020.lrec-1.192.pdf-Figure1.png,Figure 1: (a) Recording equipment used for data collection. (b) Speakers involved in the elicitation of a drama situation.,[],2020.lrec-1.192deepfigures-results.json,natural images,2020.lrec-1.192
2020.lrec-1.204.pdf-Figure2.png,Figure 2: Transcribed TV shows.,[],2020.lrec-1.204deepfigures-results.json,natural images,2020.lrec-1.204
2020.lrec-1.25.pdf-Figure1.png,Figure 1. General design of the communication situation.,"[u'. after the film viewing was finished, the narrator, the commentator, and the reteller were seated as shown in figure 1 .']",2020.lrec-1.25deepfigures-results.json,natural images,2020.lrec-1.25
2020.lrec-1.288.pdf-Figure2.png,"Figure 2: Examples of explicit spatial relations from Visual Genome of (cat, ?, chair)","[u'. in figure 2 we see examples from the dataset for the explicit relations between cat as subject and chair as object.', u'. similarly, for the implicit relations, has is the majority relation with a frequency 167780 (figure 2 ), using the bert predictions and re-scoring the choices using the glove embedding similarity, we want to be able to predict unseen relations at test time.']",2020.lrec-1.288deepfigures-results.json,natural images,2020.lrec-1.288
2020.lrec-1.288.pdf-Figure3.png,"Figure 3: Examples of implicit spatial relations from Visual Genome of (man, ?, surfboard)",[u'. in figure 3 we see examples from the dataset for the implicit relations between man as subject and surf board as object.'],2020.lrec-1.288deepfigures-results.json,natural images,2020.lrec-1.288
2020.lrec-1.288.pdf-Figure4.png,"Figure 4: cat BENEATH chair. Even if we have never seen beneath as a relation during the training phase but have seen cat UNDER chair (Figure 2), using the BERT predictions and re-scoring the choices using the GloVe embedding similarity, we want to be able to predict unseen relations at test time.",[u'they can predict new relations never seen during the training of the spatial model (see Figure 4 ). In several cases (especially the low training data regimes) where BERT is used in'],2020.lrec-1.288deepfigures-results.json,natural images,2020.lrec-1.288
2020.lrec-1.288.pdf-Figure5.png,"Figure 5: In this example from Visual Genome,suppose we have seen (man, riding, elephant) in training. In the second image, we want to be able to predict riding, even if we have never seen the (woman, riding) combination before","[u'illustrative examples from Visual Genome that we want to handle for the unseen subject ( Figure 5 ) and unseen object (', u'(a) man RIDING elephant (b) woman RIDING elephant Figure 5 : In this example from Visual Genome,suppose we have seen (man, riding, elephant) in training.']",2020.lrec-1.288deepfigures-results.json,natural images,2020.lrec-1.288
2020.lrec-1.288.pdf-Figure6.png,"Figure 6: In this example from Visual Genome, suppose we have seen (man, riding, elephant) in training. In the second image, we want to be able to predict riding, even if we have never seen the (riding, bike) combination before","[u') and unseen object ( Figure 6 ) settings respectively. The pre-trained embeddings of the unseen subject(object) is similar to the embeddings', u'. this task is possible because bert is able to predict a much larger class of relations and using our glove-scoring metric we can decompose the scores of these (a) man riding elephant (b) man riding bike figure 6 : in this example from visual genome, suppose we have seen (man, riding, elephant) in training.']",2020.lrec-1.288deepfigures-results.json,natural images,2020.lrec-1.288
2020.lrec-1.292.pdf-Figure1.png,Figure 1: The image for the sentence 1C. Referential complexity: high (A1-O1-L1: Target entities; A2-O2-L2: Congruent distractors; and O3-L3: Incongruent distractors),"[u'among the three studies with some characters and background objects and interaction among them (see Figure 1 ). As visual complexity increases, the number of the contextual representations for the scene increases', u'the contextual representations in a triplet notation <argument, relation, predicate> for the scene illustrated in Figure 1 (*: the relevant relations for the sample sentence [1C]).']",2020.lrec-1.292deepfigures-results.json,natural images,2020.lrec-1.292
2020.lrec-1.292.pdf-Figure2.png,"Figure 2: The image for the sentence 2A and 2B. While the full image displays the fully visible target, the image part at the bottom-left corner illustrates the partially occluded target object. Referential complexity: Moderate","[u'. the target objects are visually presented as fully-visible or partially-occluded, as can be seen in figure 2 .', u'. as shown in figure 2 , each scene has a target object with a specific attribute value ( blue as color) and two other distractor objects of the same kind with different values of the same attribute ( one purple and one yellow mug).', u'the target object which has been mentioned in Sentence [2A and 2B] and depicted in Figure 2 has two different ID types: a unique object ID (S001I002E001) and a context-specific entity ID', u'. for example, the mug 1 is the target object for the scenario depicted in figure 2 , the mug 2 is the congruent distractor, and the vase 1 is the incongruent one.', u', e.g., while all the mugs in Figure 2 share the same TYPE information, they differ in COLOR, LOC NEXTTO , LOC BELOW and', u': Object Properties and Relations for all three instances of Becher (mug) displayed in Figure 2 .']",2020.lrec-1.292deepfigures-results.json,natural images,2020.lrec-1.292
2020.lrec-1.292.pdf-Figure3.png,"Figure 3: The image for the sentence 3A-B and C, Referential complexity: low","[u'. figure 3 illustrates the first condition, that conveys all possible interpretations for all the focus-of-interest fillers.']",2020.lrec-1.292deepfigures-results.json,natural images,2020.lrec-1.292
2020.lrec-1.44.pdf-Figure1.png,Figure 1: ARTranslate in Action: Exploring the evryday World around with Augmented Reality (AR) The virtual objects stick to the real objects regardless of the viewing angle.,[u'Augmented Reality (AR) can be defined as a real world Figure 1 : ARTranslate in Action: Exploring the evryday World around with Augmented Reality (AR) The virtual'],2020.lrec-1.44deepfigures-results.json,natural images,2020.lrec-1.44
2020.lrec-1.60.pdf-Figure1.png,"Figure 1: A screenshot from one of the video clips in the Margarita Dialogue Corpus, named after co-author and avatar maker Margarita Bicec.","[u'. should avatar makers use their intuition and brainstorm pairs? or should they record and transcribe real dialogues? figure 1 : a screenshot from one of the video clips in the margarita dialogue corpus, named after co-author and avatar maker margarita bicec.', u'and their corresponding video clips, as well as a number of annotated dialogue transcripts (see Figure 1 and']",2020.lrec-1.60deepfigures-results.json,natural images,2020.lrec-1.60
2020.lrec-1.61.pdf-Figure1.png,"Figure 1: Driving simulator with operator desk (Schmidt et al., 2020)","[u'are the first ones to systematically combine all areas: proactive voice assistant behavior, cognitive load, Figure 1 : Driving simulator with operator desk', u'. figure 1 depicts the setup of the driving experiment: it contained a fixed-base simulator with a 180  screen in a room with controlled light and temperature conditions.']",2020.lrec-1.61deepfigures-results.json,natural images,2020.lrec-1.61
2020.lrec-1.738.pdf-Figure2.png,Figure 2: Example of two classifiers in LIS,"[u'An example of classifiers is given in Figure 2 -which is taken from our fable -, in which the hands represent two Whole Entity']",2020.lrec-1.738deepfigures-results.json,natural images,2020.lrec-1.738
2020.lrec-1.738.pdf-Figure3.png,Figure 3: OCCHI CL-VELOCE-VS-LOC2 NPROX,"[u'. then he produces the sign in figure 3 below and narrates that the hare sees a tortoise.', u'below and narrates that the hare sees a tortoise. Figure 3 : ""OCCHI CL-VELOCE-VS-LOC2 NPROX""', u'In Figure 3 , the picture left represents the beginning of the realization of the sign and the', u"". the sign in figure 3 is a classifier: in this context, the signer's hands represent the eyes of the hare, which move to his right, toward the position in which will then be located the tortoise."", u'. therefore, the sign in figure 3 has been glossed in the lis manual signs layer as a single term: occhi cl-veloce-vs-loc2 nprox, which would be eyes cl-rapid-to-loc2 nprox glossed in english.', u'. figure 3 , for example, depicts a classifier,  a nonstandard sign, which is realized with a different handshape compared to the handshape used in the realization of the generic sign for ""eyes"" in lis.']",2020.lrec-1.738deepfigures-results.json,natural images,2020.lrec-1.738
2020.lrec-1.738.pdf-Figure4.png,Figure 4: Embodiment of the hare panting and looking right,"[u'. however, the layers that explain what the hare does ( figure 4 on the next page) are nms and agr.']",2020.lrec-1.738deepfigures-results.json,natural images,2020.lrec-1.738
2020.lrec-1.738.pdf-Figure5.png,Figure 5: Embodiment of the tortoise getting ready to start the race,"[u'and Figure 5 .', u'."" figure 5 : embodiment of the tortoise getting ready to start the race formation needed to generate acceptable sentences, so the generator may receive the relevant information from any of the slots.']",2020.lrec-1.738deepfigures-results.json,natural images,2020.lrec-1.738
2020.lrec-1.738.pdf-Figure6.png,Figure 6: Embodiment of the tortoise walking,"[u'. for example, figure 6 may represent both a tortoise that walks and a tortoise that starts walking, depending on context.']",2020.lrec-1.738deepfigures-results.json,natural images,2020.lrec-1.738
2020.lrec-1.738.pdf-Figure8.png,Figure 8: to walk (spreadthesign.com),"[u'. for example, if the signer embodying the hare used the sign in figure 8 on the next page for a specific section of the route, he would probably want to specify to the tortoise that on that section they have to walk, instead of running.', u'. however, the sign in figure 8 would not be acceptable in this context, since it is usually used to refer to entities with two legs.']",2020.lrec-1.738deepfigures-results.json,natural images,2020.lrec-1.738
2020.lrec-1.79.pdf-Figure2.png,Figure 2: Example of the scene configuration for one participant of PACO.,[],2020.lrec-1.79deepfigures-results.json,natural images,2020.lrec-1.79
2020.lrec-1.79.pdf-Figure3.png,Figure 3: Example of the merged video for one interaction of PACO.,[],2020.lrec-1.79deepfigures-results.json,natural images,2020.lrec-1.79
2020.lrec-1.85.pdf-Figure1.png,Figure 1: Two different feedbacks produced by the ECA,[u'. figure 1 illustrates two examples of feedback.'],2020.lrec-1.85deepfigures-results.json,natural images,2020.lrec-1.85
2020.lrec-1.85.pdf-Figure2.png,Figure 2: Set-up for the recording of the doctor-actor and patient-actor dialog. A green background has been used to enable us to integrate the scene in a virtual environment.,"[u'. figure 2 : set-up for the recording of the doctor-actor and patient-actor dialog.', u'. figure 2 illustrates the set-up for the recording.']",2020.lrec-1.85deepfigures-results.json,natural images,2020.lrec-1.85
2020.lrec-1.876.pdf-Figure1.png,Figure 1: An example of the players view in Minecraft. Player 829 is holding an item (pickaxe) and standing under the tree. The white nameplate is added afterwards and is not displayed in the game.,[u'. figure 1 shows a snapshot of the in-game scene.'],2020.lrec-1.876deepfigures-results.json,natural images,2020.lrec-1.876
2020.lrec-1.876.pdf-Figure3.png,Figure 3: Snapshot of the annotation tool. The bottom buttons are labelled with dialogue acts. Unannotated utterances from the partner are coloured with yellow and underlined. The white nameplates are added afterwards and are not displayed in the game.,"[u'. a toolbar is displayed above the native minecraft chat interface, as shown in figure 3 .']",2020.lrec-1.876deepfigures-results.json,natural images,2020.lrec-1.876
2020.lrec-1.93.pdf-Figure1.png,Figure 1: The collaborative assembly task. The instructor (on the right) guides the builder (on the left) how to assemble an IKEA stool.,[u'. figure 1: the collaborative assembly task.'],2020.lrec-1.93deepfigures-results.json,natural images,2020.lrec-1.93
2020.lrec-1.95.pdf-Figure1.png,Figure 1: Illustration I,[],2020.lrec-1.95deepfigures-results.json,natural images,2020.lrec-1.95
2020.lrec-1.95.pdf-Figure2.png,Figure 2: Illustration II,[],2020.lrec-1.95deepfigures-results.json,natural images,2020.lrec-1.95
2020.lrec-1.95.pdf-Figure3.png,Figure 3: Session in progress (Original Scene),[u'. figure 3 depicts the configuration of the recording.'],2020.lrec-1.95deepfigures-results.json,natural images,2020.lrec-1.95
2020.nlpmc-1.7.pdf-Figure2.png,Figure 2: Illustration of the 68 obtained and 14 used facial landmarks.,[],2020.nlpmc-1.7deepfigures-results.json,natural images,2020.nlpmc-1.7
2020.onion-1.1.pdf-Figure1.png,"Figure 1: Creation of the Christ prototype. First four canonical images of Christ are morphed pairwise, and then the pairs are morphed.","[u'. of the holy face and we produced a ""christ prototype"" by morphing them (figure 1 ).', u'. figure 1) , each presented one time on the left and one time on the right side.']",2020.onion-1.1deepfigures-results.json,natural images,2020.onion-1.1
2020.onion-1.1.pdf-Figure2.png,"Figure 2: Prototypes created from participants. All created by pairwise morphing. Upper row: Female, Human and Male prototypes. The lower row shows the effect of adding the Christ prototype.",[u'. the male and female prototypes were also combined to produce a human prototype ( figure 2 ).'],2020.onion-1.1deepfigures-results.json,natural images,2020.onion-1.1
2020.onion-1.1.pdf-Figure3.png,"Figure 3: Individual participants morphed with the Christ prototype. The alternating rows show first male, then female participants. The morphed pictures consist of 80% Christ prototype and 20% individual picture.",[u'. the 16 individualized christ images consisted of 80% christ and 20% the image of the participant (figure 3 ).'],2020.onion-1.1deepfigures-results.json,natural images,2020.onion-1.1
2020.onion-1.3.pdf-Figure1.png,Figure 1: Screen shot from one of the video recordings showing combined almost frontal camera views,"[u'. for the work presented here we used a version of the recordings in which both speakers are being viewed almost frontally, and the two views are combined in a singled video as shown in figure 1 .']",2020.onion-1.3deepfigures-results.json,natural images,2020.onion-1.3
2020.onion-1.5.pdf-Figure1.png,Figure 1: OpenBCI headband as worn for EEG data collection during our study.,[u'. the setup is depicted in figure 1 .'],2020.onion-1.5deepfigures-results.json,natural images,2020.onion-1.5
2020.rail-1.1.pdf-Figure1.png,Figure 1 & 2: Photograph of Elsie Vaalbooi (standing) and her mother Marie ||Qoesi (sitting) circa 1911.,"[u'people from the area. During this process, Elsie Vaalbooi recognised a photograph of herself ( Figure 1 ) and another of her mother']",2020.rail-1.1deepfigures-results.json,natural images,2020.rail-1.1
2020.rail-1.1.pdf-Figure3.png,Figure 3: Photograph of Elsie Vaalbooi and her son Petrus Vaalbooi taken shortly before Elsie died in 1997.,"[u'. elsie was one of the last speakers of n|uu and today her son, petrus vaalbooi (figure 3) , is the traditional leader of the khomani san.']",2020.rail-1.1deepfigures-results.json,natural images,2020.rail-1.1
2020.rail-1.1.pdf-Figure4.png,"Figure 4: Linguist Nigel Crawhall (sunglasses around his neck), working with Dawid Kruiper (hand pointing over page) and family on trilingual wordlists",[u'used by the Khomani San; culturally or historically important places; language work (as seen in Figure 4 ); and physical evidence of occupation in the Kalahari Transfrontier Park by the Khomani San'],2020.rail-1.1deepfigures-results.json,natural images,2020.rail-1.1
2020.rail-1.1.pdf-Figure6.png,"Figure 6: Cartographer Bill Kemp, working alongside","[u'. such fields included: visual anthropology, law, linguistics, cartography, conservation and social development (as exemplified in figure 6 ).']",2020.rail-1.1deepfigures-results.json,natural images,2020.rail-1.1
2020.rail-1.6.pdf-Figure4.png,Figure 4: Local researchers practice using ODK (Photo credit Nadia Jassim),[],2020.rail-1.6deepfigures-results.json,natural images,2020.rail-1.6
2020.sigdial-1.15.pdf-Figure3.png,Figure 3: Snapshot of dialogue experiment,"[u'. a snapshot of this dialogue experiment is shown in figure 3 .', u'.) erica figure 3 : snapshot of dialogue experiment system is the case where the robot was operated by a human operator.']",2020.sigdial-1.15deepfigures-results.json,natural images,2020.sigdial-1.15
2020.sigdial-1.16.pdf-Figure1.png,Figure 1: The blocks world apparatus setup.,[],2020.sigdial-1.16deepfigures-results.json,natural images,2020.sigdial-1.16
2016.jeptalnrecital-jep.41.pdf-Figure1.png,"FIGURE 1  Illustration de lapproche de fusion dembeddings. Les systmes monomodaux sont dabord entrans de manire indpendante, puis les activations de leurs couches caches sont concatnes pour servir dentre au rseau multimodal. La taille des embeddings est donne pour chaque modalit.",[],2016.jeptalnrecital-jep.41deepfigures-results.json,neural networks,2016.jeptalnrecital-jep.41
2017.jeptalnrecital-court.22.pdf-Figure1.png,FIGURE 1  structure du CNN sur les caractres utilis,[],2017.jeptalnrecital-court.22deepfigures-results.json,neural networks,2017.jeptalnrecital-court.22
2019.ccnlg-1.2.pdf-Figure2.png,Figure 2: The proposed framework for Discriminator-general,"[u'. this requires spending as little time as possible to train a good discriminator-speci the figure 2 : the proposed framework for discriminator-general multiple fully connected layer has fewer parameters, which means this network will converge faster than others will.']",2019.ccnlg-1.2deepfigures-results.json,neural networks,2019.ccnlg-1.2
2019.jeptalnrecital-recital.7.pdf-Figure2.png,"FIGURE 2  Modle de systme ""end-to-end""",[],2019.jeptalnrecital-recital.7deepfigures-results.json,neural networks,2019.jeptalnrecital-recital.7
2020.acl-demos.1.pdf-Figure4.png,Figure 4: Voice Cloning for Cross-lingual Text-toSpeech Synthesis.,"[u'. The architecture is illustrated in Figure 4 , we made the following augmentations on the base Tacotron 2 model:']",2020.acl-demos.1deepfigures-results.json,neural networks,2020.acl-demos.1
2020.acl-demos.32.pdf-Figure1.png,Figure 1: The architecture of our model.,"[u'. the network architecture is shown in figure 1 , and we refer to it as a multigranularity network.']",2020.acl-demos.32deepfigures-results.json,neural networks,2020.acl-demos.32
2020.acl-main.101.pdf-Figure2.png,"Figure 2: The architecture of our proposed model for table-to-text generation. To enhance the ability of generating multi-sentence faithful texts, our loss consists of three parts, including a maximum-likelihood loss (green), a latent matching disagreement loss (orange), and an optimal-transport loss (blue).",[u'. figure 2 illustrates the overall architecture of our model.'],2020.acl-main.101deepfigures-results.json,neural networks,2020.acl-main.101
2020.acl-main.121.pdf-Figure2.png,"Figure 2: Overview of our method. We first use encoder-decoder attention distribution to attend to some words and obtain the translation candidates from a probabilistic bilingual lexicon. Then a translating probability ptrans is calculated, which balances the probability of generating words from the neural distribution with that of selecting words from the translation candidates of the source text. The final distribution is obtained by the weighted sum (weighed by ptrans) of the neural distribution PN and the translation distribution PT. Best viewed in color.","[u'To achieve that goal, we propose a novel method ( Figure 2 ) that allows either generating words from the vocabulary or selecting words from the translation', u'), we introduce a novel cross-lingual summarization Figure 2 : Overview of our method. We first use encoder-decoder attention distribution to attend to some', u'. our proposed method is a hybrid between transformer and an additional translation layer, which is depicted in figure 2 and described as follows.']",2020.acl-main.121deepfigures-results.json,neural networks,2020.acl-main.121
2020.acl-main.126.pdf-Figure1.png,"Figure 1: Schematic of the CMADE workflow. CMADE contains a three-stage training pipeline to denoise selfreported ratings to train an automatic dialog comparison model: learning representation viaself-supervised dialog flow anomaly detection, fine-tuning with smoothed self-reported user ratings, denoising with data Shapley & further fine-tuning. The gray and blue rectangles in stage 1 represents system and user utterances. The red rectangle in stage 1 represents the randomly replaced system utterance for dialog flow perturbation. In stage 2 & 3, each ball represents a dialog in the training data. The number on each ball represents the dialog rating.",[u'a three-stage training pipeline to denoise self-reported ratings to train an automatic dialog comparison model. Figure 1 describes the overall pipeline:'],2020.acl-main.126deepfigures-results.json,neural networks,2020.acl-main.126
2020.acl-main.128.pdf-Figure1.png,Figure 1: Our few-shot CRF framework for slot tagging.,"[u'in few-shot sequence labeling (see Figure 1 ). In this paper, we translate the emission score of CRF into the output of', u'. for example in figure 1 , word rain and label name weather are highly related.', u'. for example in figure 1 , in the 1-shot support set, label [b-weather] occurs twice to ensure all labels appear at least once.']",2020.acl-main.128deepfigures-results.json,neural networks,2020.acl-main.128
2020.acl-main.128.pdf-Figure4.png,"Figure 4: Emission Scorer with L-TapNet. It first constructs a projection space M by linear error nulling for given domain, and then predicts a words emission score with its distance to label representation  in the projection space.","[u'As shown in Figure 4 , the emission scorer independently assigns each word an emission score with regard to each', u'Linear Error Nulling Projection Space will it rain tonight Figure 4 : Emission Scorer with L-TapNet. It first constructs a projection space M by linear error']",2020.acl-main.128deepfigures-results.json,neural networks,2020.acl-main.128
2020.acl-main.129v1.pdf-Figure2.png,Figure 2: Reduced Label Expert (RLE) architecture.,[u'. its architecture is outlined in figure 2 .'],2020.acl-main.129v1deepfigures-results.json,neural networks,2020.acl-main.129v1
2020.acl-main.129v1.pdf-Figure3.png,Figure 3: No Label Expert (NLE) architecture.,[u'. the nle architecture is outlined in figure 3 .'],2020.acl-main.129v1deepfigures-results.json,neural networks,2020.acl-main.129v1
2020.acl-main.129v2.pdf-Figure3.png,Figure 3: No Label Expert (NLE) architecture.,[u'. the nle architecture is outlined in figure 3 .'],2020.acl-main.129v2deepfigures-results.json,neural networks,2020.acl-main.129v2
2020.acl-main.134.pdf-Figure3.png,"Figure 3: An illustration of the swap model. (1) shows a configuration [1 2 | 3 4 5 6], where the stack contains 1 and 2, and the buffer contains 3, 4, 5 and 6, and the model predicts shift to move 3 from the buffer to the stack; (2) is the resulted configuration, and the model predicts swap, which moves 3 back to the buffer behind 4, as shown in (3). The solid black arrows represent the computation already done in the previous steps, and the dashed red arrows represent the new computation needed for the current step.","[u'Before Figure 3 : An illustration of the swap model. (1) shows a configuration [1 2 | 3', u'. figure 3 illustrates the model under the transition system, where the arrows to the right represent lstm  , the arrows to the left represent lstm  , and the arrows between the stack and the buffer represent the mlp.']",2020.acl-main.134deepfigures-results.json,neural networks,2020.acl-main.134
2020.acl-main.135.pdf-Figure2.png,"Figure 2: The framework of our proposed model (on the right) together with an input example (on the left). The model consists of four parts: (1) a document encoder to encode the input document, (2) a semantic graph encoder to embed the document-level semantic graph via Att-GGNN, (3) a content selector to select relevant question-worthy contents from the semantic graph, and (4) a question decoder to generate question from the semantic-enriched document representation. The left figure shows an input example and its semantic graph. Dark-colored nodes in the semantic graph are question-worthy nodes that are labeled to train the content selection task.","[u'based on semantic role labeling or dependency parsing, and connected by different intra-and intersemantic relations (Figure 2 ). Semantic relations provide important clues about what contents are question-worthy and what reasoning should', u'. figure 2 : the framework of our proposed model (on the right) together with an input example (on the left).', u'. figure 2 shows the general architecture of the proposed model, including three modules: semantic graph construction, which builds the dpor srl-based semantic graph for the given input; semantic-enriched document representation, employing a novel attention-enhanced gated graph neural network (att-ggnn) to learn the semantic graph representations, which are then fused with the input document to obtain graph-enhanced document representations; and joint-task question generation, which generates deep questions via joint training of node-level content selection and wordlevel question decoding.', u'The left side of Figure 2 shows an example of the DP-based semantic graph. Compared with SRLbased graphs, DP-based ones typically', u'information: a phrase is often represented as a single node in the semantic graph (cf Figure 2 as an example); therefore its constituting words are aligned with the same node representation; (3)', u'. we formulate this as a node classification task, , deciding whether each node should be involved in the process of asking, , appearing in the reasoning chain for raising a deep question, exemplified by the dark-colored nodes in figure 2 .']",2020.acl-main.135deepfigures-results.json,neural networks,2020.acl-main.135
2020.acl-main.136.pdf-Figure2.png,"Figure 2: An overview of the proposed CASREL framework. In this example, there are three candidate subjects detected at the low level, while the presented 0/1 tags at high level are specific to the first subject Jackie R. Brown, i.e., a snapshot of the iteration state when k = 1 is shown as above. For the subsequent iterations (k = 2, 3), the results at high level will change, reflecting different triples detected. For instance, when k = 2, the high-level orange (green) blocks will change to 0 (1), respectively, reflecting the relational triple (Washington, Capital of, United States Of America) led by the second candidate subject Washington.","[u'. corresponding to the two steps, the cascade decoder consists of two modules as illustrated in figure 2 : a subject tagger; and a set of relationspecific object taggers.', u'. figure 2 : an overview of the proposed casrel framework.', u'. for example, as shown in figure 2 , the nearest end token to the first start token ""jackie"" is ""brown"", hence the detected result of the first subject span will be ""jackie r.', u'. as figure 2 shows, it consists of a set of relation-specific object taggers with the same structure as subject tagger in low level module for all possible relations.', u'. therefore, the object tagger for relation ""work in"" will not identify the span of ""washington"", , the output of both start and end position are all zeros as shown in figure 2 .']",2020.acl-main.136deepfigures-results.json,neural networks,2020.acl-main.136
2020.acl-main.139.pdf-Figure1.png,Figure 1: Illustration of the weak supervision approach.,[u'. figure 1 illustrates this process.'],2020.acl-main.139deepfigures-results.json,neural networks,2020.acl-main.139
2020.acl-main.140.pdf-Figure2.png,"Figure 2: Probing task setup. In the first step, we train a RE model (sentence encoder and relation classifier) on a dataset D. In the second step, we fix the encoder and for each probing task train a classifier on the encoder representations {sj}j=1,...,|D| of all sentences in D. The probing classifier performance indicates how well the sentence representations encode the information probed by the classifier, e.g. the entity type of the tail relation argument.","[u'single sentence) into a fixed-size representation, before applying a fully connected relation classification layer ( Figure 2 ). A single input is represented as a sequence of T tokens {w t }']",2020.acl-main.140deepfigures-results.json,neural networks,2020.acl-main.140
2020.acl-main.145.pdf-Figure1.png,"Figure 1: A comparison of the encoder blocks in the standard transformer (a) and our novel modification, the convtransformer (b), which uses 1D convolutions to facilitate character interactions.","[u' Figure 1 : A comparison of the encoder blocks in the standard transformer', u'and a novel variant that we call the convtransformer (Figure 1, Section 3) . The convtransformer uses convolutions to facilitate interactions among nearby character representations.', u'transformer, but we adapt each encoder block to include an additional subblock. The sub-block ( Figure 1, b ), inspired from']",2020.acl-main.145deepfigures-results.json,neural networks,2020.acl-main.145
2020.acl-main.16.Dataset.pdf-Figure2.png,"Figure 2: The Transformer based PLuGS model. The text on the input side is used for the translation and multimodal translation experiments with the Multi30K dataset. For image captioning, no text input is provided.",[],2020.acl-main.16.Datasetdeepfigures-results.json,neural networks,2020.acl-main.16.Dataset
2020.acl-main.16.Dataset.pdf-Figure3.png,"Figure 3: Captions dependence on the Stabilizer. The target-language caption is conditioned on the Stabilizer through the Masked Self-Attention in the decoder, and on the input image through the Encoder-Decoder attention that attends to the outputs of the last encoder layer. Note that in this figure, FF stands for the feed forward network, Voc stands for the (fixed) text vocab, and Emb stands for the (trainable) text embeddings.",[],2020.acl-main.16.Datasetdeepfigures-results.json,neural networks,2020.acl-main.16.Dataset
2020.acl-main.16.pdf-Figure3.png,"Figure 3: Captions dependence on the Stabilizer. The target-language caption is conditioned on the Stabilizer through the Masked Self-Attention in the decoder, and on the input image through the Encoder-Decoder attention that attends to the outputs of the last encoder layer. Note that in this figure, FF stands for the feed forward network, Voc stands for the (fixed) text vocab, and Emb stands for the (trainable) text embeddings.",[],2020.acl-main.16deepfigures-results.json,neural networks,2020.acl-main.16
2020.acl-main.165.pdf-Figure1.png,Figure 1: Multi-head Scaled Dot-Product Attention.,"[u'An illustrative example of the multihead attention architecture is provided in Figure 1 . In addition to the above multi-head attention modules, each layer in the encoder and']",2020.acl-main.165deepfigures-results.json,neural networks,2020.acl-main.165
2020.acl-main.165.pdf-Figure10.png,Figure 10: Back-propagation for different embedding based methods.,[u'. figure 10 illustrates the back-propagation under different methods.'],2020.acl-main.165deepfigures-results.json,neural networks,2020.acl-main.165
2020.acl-main.165.pdf-Figure2.png,Figure 2: The Point-wise Linear Transformations are applied at the word-level.,"[u"". recall that the pointwise linear transformations in the multi-head attention module w i,q 's, w i,k 's, w i,v 's and w o are applied to each word separately and identically, as shown in figure 2 .""]",2020.acl-main.165deepfigures-results.json,neural networks,2020.acl-main.165
2020.acl-main.165.pdf-Figure3.png,"Figure 3: Word-level mixing with 3 domains. For simplicity, we omit the subscripts Q, i.","[u'An illustrative example is presented in Figure 3 . For other linear transformations, we applied the domain mixing scheme in the same way.']",2020.acl-main.165deepfigures-results.json,neural networks,2020.acl-main.165
2020.acl-main.165.pdf-Figure4.png,"Figure 4: Illustration of Our Multi-domain NMT Model: Normalization and residual connection are omitted for simplicity. For all other detail, please refer to Vaswani et al. (2017).","[u'. since the words have different representations at each layer, the corresponding domain proportions at each layer are also different, as shown in figure 4 .']",2020.acl-main.165deepfigures-results.json,neural networks,2020.acl-main.165
2020.acl-main.165.pdf-Figure5.png,Figure 5: Computational graph for training the domain proportion layers.,[u'. so the gradient propagation is cut off between the transformer and the domain proportion as figure 5 shows.'],2020.acl-main.165deepfigures-results.json,neural networks,2020.acl-main.165
2020.acl-main.20.pdf-Figure1.png,Figure 1: The conceptual illustration of the proposed HCVAE model encoding and decoding question and its corresponding answer jointly. The dashed line refers to the generative process of HCVAE.,"[u'c) are their conditional priors following an isotropic Gaussian distribution and a categorical distribution ( Figure 1-(a) ). We decompose the latent space of question and answer, since the answer is always', u'. specifically, we encode all context tokens as 0s, except for the tokens which are part of answer span (highlighted words of context in figure 1 -(a) or -(c)), which we encode as 1s.', u'p (z y |z x , c) to model context-dependent priors (the dashed lines in Figure 1-(a) ). To obtain the parameters of isotropic Gaussian N (, 2 I) for p (z', u'. then, we feed the two hidden representations into mlp to obtain the parameters of gaussian distribution,  and  (upper right corner in figure 1-(a) ).', u'. then, we feed the hidden representation and z x into mlp to compute the parameters  of categorical distribution (lower right corner in figure 1-(a) ).', u'. answer generation networks since we consider extractive qa, we can factorize p  (y|z y , c) into p  (y s |z y , c) and p  (y e |z y , c), where y s and y e are the start and the end position of an answer span (highlighted words in figure 1-(b) ), respectively.', u'. for encoding, we use pre-trained bert to encode the answer-specific context into the contextualized word embedding, and then use a two-layer bi-lstm to encode it into the hidden representation (in figure 1-(c) ).', u'of s j , using the j-th decoder hidden representation d j R dx (in Figure 1-(c) ). Then, we feed d j and s j into MLP with maxout activation']",2020.acl-main.20deepfigures-results.json,neural networks,2020.acl-main.20
2020.acl-main.21.pdf-Figure2.png,Figure 2: Illustration of answer embeddings and an answer-attention head for the forth sentence in Table 1.,"[u'ans k , two more components are added into the answer-info encoder, as shown in Figure 2 . First, we adopt the answer-tag features. For each word w i in sentence S']",2020.acl-main.21deepfigures-results.json,neural networks,2020.acl-main.21
2020.acl-main.215.pdf-Figure2.png,"Figure 2: MultiQT model illustration for two timesteps i and j. We depict the convolutional transformations fa and fs of the audio and character temporal softmax inputs into the respective modality encodings z(i)a and z (i) s , along with the corresponding receptive fields and strides: ra, sa and rs, ss. The convolutions are followed by multimodal fusion and finally dense layers g and h to predict the question labels y(i) and y(j).",[u'. the model is illustrated in figure 2 .'],2020.acl-main.215deepfigures-results.json,neural networks,2020.acl-main.215
2020.acl-main.22.pdf-Figure2.png,Figure 2: Rearrangement aware paraphrasing (REAP) model. The gray area corresponds to the standard transformer encoder-decoder system. Our model adds position embeddings corresponding to the target reordering to encoder outputs. The decoder attends over these augmented encodings during both training and inference.,"[u'. these orderings are encoded for our encoder-decoder paraphrase model (reap, for rearrangement aware paraphrasing) by way of po-sition embeddings, which are added to the source sentence encoding to specify the desired order of generation (see figure 2 ).', u'. this model is pictured in the gray block of figure 2 .', u'. note that since the architecture of the transformer model is non-recurrent, it adds position embeddings to the input word embeddings in order to indicate the correct sequence of the words in both x and y (see figure 2 ).', u'The architecture for our model P (y | x, r) is outlined in Figure 2 . Consider an encoder-decoder architecture with a stack of M layers in the encoder', u'. the specialpurpose position embeddings are added to the output of this layer (see figure 2 ): e = e m + p e r .']",2020.acl-main.22deepfigures-results.json,neural networks,2020.acl-main.22
2020.acl-main.22.pdf-Figure6.png,Figure 6: Source Order reWriting (SOW) model. Our model encodes order preference MONOTONE or FLIP through position embeddings added to the encoder output.,[u'C SOW Model Architecture Figure 6 provides an overview of the SOW seq2seq model. We add POS tag embeddings (or cor-'],2020.acl-main.22deepfigures-results.json,neural networks,2020.acl-main.22
2020.acl-main.224.pdf-Figure2.png,"Figure 2: The architecture of the proposed DUALENC model. The input triples are converted as a graph and then fed to two GCN encoders for plan and text generation (Planner and Graph Encoder, top center). The plan is then encoded by an LSTM network (Plan Encoder, bottom center). Finally an LSTM decoder combines the hidden states from both the encoders to generate the text (Text Decoder, middle right).",[u'. the architecture of our dual encoding method is shown in figure 2 .'],2020.acl-main.224deepfigures-results.json,neural networks,2020.acl-main.224
2020.acl-main.229.pdf-Figure2.png,"Figure 2: The BABYWALK agent has a memory buffer storing its past experiences of instructions xm, and its trajectory ym. When a new BABY-STEP xm is presented, the agent retrieves from the memory a summary of its experiences as the history context. It takes actions conditioning on the context (as well as its state st and the previous action at). Upon finishing following the instruction. the trajectory ym is then sent to the memory to be remembered.","[u'follow a ""micro instruction"" x m (i.e., a short sequence of instructions, to which we Figure 2 : The BABYWALK agent has a memory buffer storing its past experiences of instructions x']",2020.acl-main.229deepfigures-results.json,neural networks,2020.acl-main.229
2020.acl-main.266.pdf-Figure2.png,Figure 2: Proposed generative model for clustering images of a symbol by typeface. Each mixture component c corresponds to a learnable template Tk. The  variables warp (spatially adjust) the original template T to T . This warped template is then further transformed via the z variables to T via an expressive neural filter function parametrized by .,[u'. figure 2 depicts our model.'],2020.acl-main.266deepfigures-results.json,neural networks,2020.acl-main.266
2020.acl-main.269.pdf-Figure1.png,"Figure 1: Illustration of SSANs that select a subset of input elements with an additional selector network, on top of which self-attention is conducted. In this example, the word talk performs attention operation over input sequence, where the words Bush, held and Sharon are chosen as the truly-significant words.","[u'the selective mechanism on SANs by introducing an additional selector, namely SSANs, as illustrated in Figure 1 . The selector aims to select a subset of elements from the input sequence, on', u'Bush held a talk with Sharon Figure 1 : Illustration of SSANs that select a subset of input elements with an additional selector']",2020.acl-main.269deepfigures-results.json,neural networks,2020.acl-main.269
2020.acl-main.27.pdf-Figure1.png,Figure 1: Comment generation frameworks. Different types are denoted as different colors and shapes in (b).,[],2020.acl-main.27deepfigures-results.json,neural networks,2020.acl-main.27
2020.acl-main.27.pdf-Figure2.png,Figure 2: TAG Encoder and Decoder framework.,"[u'The Type-restricted Decoder, as shown in the right part of Figure 2 , takes the original toke-typetree T x, and its semantic representation from encoder as input']",2020.acl-main.27deepfigures-results.json,neural networks,2020.acl-main.27
2020.acl-main.282.pdf-Figure3.png,"Figure 3: The architecture of Hyperbolic and Co-graph Representation method (HyperCore). In the Poincare ball Bn, we show the embeded code hierarchy (i.e., tree-like hierarchical structure). The dots li (i = 1, 2, 3) on the treelike hierarchical structure and triangles mi (i = 1, 2, 3) in the Poincare ball denote hyperbolic code embeddings and hyperbolic document representations, respectively.","[u'. she was transferred from an outside hospital figure 3 : the architecture of hyperbolic and co-graph representation method (hypercore).', u'. figure 3 shows the overall architecture of our proposed model.']",2020.acl-main.282deepfigures-results.json,neural networks,2020.acl-main.282
2020.acl-main.284.pdf-Figure5.png,Figure 5: Model architecture for segmenting technical support problems.,[u'. figure 5 presents an overview of our model.'],2020.acl-main.284deepfigures-results.json,neural networks,2020.acl-main.284
2020.acl-main.286.pdf-Figure1.png,Figure 1: The architecture of the proposed framework.,"[u'. figure 1 shows the architecture of the proposed framework.', u'. ecnn-pgm-* are the combined methods while ecnn-pgm-e is the proposed method with ecnn and bayesian network ensembles in figure 1 .']",2020.acl-main.286deepfigures-results.json,neural networks,2020.acl-main.286
2020.acl-main.288.pdf-Figure2.png,Figure 2: Overview of the proposed joint framework for emotion-cause pair extraction.,"[u'. the overall architecture of the proposed method is shown in figure 2 .', u'. figure 2 : overview of the proposed joint framework for emotion-cause pair extraction.']",2020.acl-main.288deepfigures-results.json,neural networks,2020.acl-main.288
2020.acl-main.288.pdf-Figure3.png,Figure 3: Two simplified versions of 2D transformer for emotion-cause pair interaction.,"[u'. to alleviate the computational load, we furthermore propose two variants of the standard 2d transformer in the following two subsections: 1) window-constrained 2d transformer and 2) cross-road 2d transformer, as shown in figure 3 .']",2020.acl-main.288deepfigures-results.json,neural networks,2020.acl-main.288
2020.acl-main.33.pdf-Figure1.png,"Figure 1: Our model consists of a classifier, discriminator, and shared text encoder. The main task learns classification, while the auxiliary task gives negative supervision to generate distinct representations for sentences with different labels.","[u'Cold Main Task Auxiliary Task Figure 1 : Our model consists of a classifier, discriminator, and shared text encoder. The main task', u'2 Multitask Learning Framework Figure 1 shows an overview of our multitask learning framework that consists of main and auxiliary tasks.']",2020.acl-main.33deepfigures-results.json,neural networks,2020.acl-main.33
2020.acl-main.37.pdf-Figure1.png,Figure 1: The Transformer Translation Model. Residual connection and Layer normalization are omitted for simplicity.,"[u'shown in Figure  1 , which significantly outperforms the previous recurrent sequence-to-sequence approach and can be efficiently computed in']",2020.acl-main.37deepfigures-results.json,neural networks,2020.acl-main.37
2020.acl-main.39.pdf-Figure2.png,Figure 2: Attender in a recurrent seq2seq.,"[u'the accessed vector. v s attender(k s , q t ) = V t (1) Figure 2 illustrates attention in a recurrent seq2seq', u'. the model has never been modified during our experimentation and is schematized in figure 2 .']",2020.acl-main.39deepfigures-results.json,neural networks,2020.acl-main.39
2020.acl-main.48.pdf-Figure1.png,Figure 1: The architecture of our GCAN model.,[],2020.acl-main.48deepfigures-results.json,neural networks,2020.acl-main.48
2020.acl-main.78.pdf-Figure1.png,"Figure 1: The framework of the proposed SemiORC. denotes the matrix multiplication. In the training process, the predicted labels yi and observed labels yi are used to compute the classification loss for labeled data.","[u'proceed with the details of our model SemiORC, and the overall architecture is shown in Figure 1 . In a nutshell, SemiORC consists of an encoder, a decoder and a semi-supervised classifier.']",2020.acl-main.78deepfigures-results.json,neural networks,2020.acl-main.78
2020.acl-main.82.pdf-Figure1.png,Figure 1: Architecture of Soft-Masked BERT,[],2020.acl-main.82deepfigures-results.json,neural networks,2020.acl-main.82
2020.acl-main.83.pdf-Figure1.png,Figure 1: Lexical Units Attention Model.,"[u""LUs for the frame, according to target word T in the given sentence, shown in Figure 1 . More specifically, we compute the weighted sum of target word T's representation and other""]",2020.acl-main.83deepfigures-results.json,neural networks,2020.acl-main.83
2020.acl-main.83.pdf-Figure2.png,Figure 2: Frame Relation Attention Model.,"[u'. as such, we propose a novel fra model, which takes advantage of f-to-f relations to get much richer semantic information, shown in figure 2 .']",2020.acl-main.83deepfigures-results.json,neural networks,2020.acl-main.83
2020.acl-main.83.pdf-Figure4.png,Figure 4: Frame Integration Representation Model.,"[u'In Figure 4, c k (k=1, 2, 3) is the input. We first compute its matrix representation c t k , with columns']",2020.acl-main.83deepfigures-results.json,neural networks,2020.acl-main.83
2020.acl-main.96.pdf-Figure1.png,Figure 1: The overall HAN architecture along with the switching features in the final layer.,[u'. switching features: we include the switching features to the pre-final fully-connected layer of han to observe if this harnesses additional benefits (see figure 1 ).'],2020.acl-main.96deepfigures-results.json,neural networks,2020.acl-main.96
2020.acl-main.97.pdf-Figure2.png,"Figure 2: Overview of DTE. DTE consists of two parts: tree comment network (the left) and decision tree model (the right), which is used to evaluate the credibility of each node in the tree comment network for discovering evidence.","[u'. we first build a tree network based on hierarchical comments, as shown in the left of figure 2 .', u'. three factors from the perspective of content and meta data of comments figure 2 : overview of dte.', u'conditions to measure node credibility of tree comments, as shown in the grey part in Figure 2 .']",2020.acl-main.97deepfigures-results.json,neural networks,2020.acl-main.97
2020.acl-main.98.pdf-Figure3.png,Figure 3: The architecture of our multi-goal driven conversation generation framework (denoted as MGCG).,"[u'Framework overview The overview of our framework MGCG is shown in Figure 3 . The goalplanning module outputs goals to proactively and naturally lead the conversation. It first', u'As shown in Figure 3 (a), we divide the task of goal planning into two sub-tasks, goal completion estimation, and', u'. as shown in figure 3 (b), our response ranker consists of five components: a context-response representation module (c-r encoder), a knowledge representation module (knowledge encoder), a goal representation module (goal encoder), a knowledge selection module (knowledge selector), and a matching module (matcher).', u'. as shown in figure 3 (c), our generator consists of five components: a context encoder, a knowledge encoder, a goal encoder, a knowledge selector, and a decoder.']",2020.acl-main.98deepfigures-results.json,neural networks,2020.acl-main.98
N10-1141.pdf-Figure2.png,"Figure 2: Model combination applied to a phrase-based (pb) and a hierarchical model (h) includes four steps. (1) shows an excerpt of the bigram feature function for each component, (2) depicts the result of conjoining a phrase lattice with a hierarchical forest, (3) shows example hyperedge features of the combination model, including bigram features vni and system indicators i, and (4) gives training and decoding objectives.",[u'. the entire sequence is illustrated in figure 2 .'],N10-1141deepfigures-results.json,neural networks,N10-1141
N12-1005.pdf-Figure2.png,Figure 2: The architecture of a SOUL Neural Network language model in the case of a 4-gram model.,"[u'The SOUL model, represented on Figure 2 , is thus the same as for the standard model up to the output layer.', u'. a subsequent modification of the soul architecture was thus performed to make up for ""mixed"" contexts: rather than projecting all the context words or phrases into the same continuous space (using the matrix r, see figure 2 ), we used two different projection matrices, one for each language.']",N12-1005deepfigures-results.json,neural networks,N12-1005
N16-1045.pdf-Figure2.png,Figure 2: Architecture of the proposed neural tweet representation model.,"[u'The overall structure of our representation learning model is shown in Figure 2 . Given a tweet, two LSTM models (Section 5.1) are used to capture its sequential', u'. a deep lstm is built by stacking multiple lstm layers, with the output sequence of one layer forming the input sequence for the next, as shown in figure 2 .']",N16-1045deepfigures-results.json,neural networks,N16-1045
N16-1045.pdf-Figure3.png,"Figure 3: LSTM-based text embedding for word vectors x1, x2, . . . , xn.",[u'. figure 3 illustrates the memory block used for our tweet representation.'],N16-1045deepfigures-results.json,neural networks,N16-1045
N16-1055.pdf-Figure1.png,"Figure 1: Our generative OCR model with the new glyph layer (bolded). The Spanish (sp.) n-gram language model (LM) generates a sequence of characters according to standard Spanish spellings, uran in this case, from the word procurando which may be written procvrado. Language-specific characterreplacement probabilities are used to generate a glyph char from each LM char, producing vra and a zero-width (elided) n. Finally, the model generates a bounding box and right-side padding (the typesetting) and a pixel-rendering of the glyph character.","[u'Typesetting Rendering Figure 1 : Our generative OCR model with the new glyph layer (bolded). The Spanish (sp.) n-gram', u'. our model ( figure 1 ) adds an additional layer to the generative model that de-couples the lm from the rendering by allowing the lm-generated character c i to be replaced by a possibly different glyph character g i which is rendered instead.']",N16-1055deepfigures-results.json,neural networks,N16-1055
N16-1056.pdf-Figure1.png,Figure 1: Sequence Labeling model for LSTM network,"[u'. we use gru with the same neural network structure as shown in figure 1 by replacing the lstm nodes with gru.', u'. we add this feature to explicitly provide a mechanism that is somewhat similar to the surrounding context that is generated in a bi-directional rnn as shown in figure 1 .']",N16-1056deepfigures-results.json,neural networks,N16-1056
N16-1076.pdf-Figure3.png,"Figure 3: An example of the transducer G, which pairs the string x=say with infinitely many possible strings y. ThisGwas created as the composition of the straight-line input automaton (Figure 1) and the transducer F (Figure 2). Thus, the state of G tracks the states of those two machines: the position in x and the most recent output character. To avoid a tangled diagram, this figure shows only a few of the states (the start state plus all states of the form i,s or i,a), with all arcs among them.","[u'2) yields a new FST, G (Figure 3) . The paths in G are in 1-1 correspondence with exactly the paths in F', u'. figure 3 : an example of the transducer g, which pairs the string x=say with infinitely many possible strings y.', u': A compact lattice of the exponentially many paths in the transducer G of Figure 3 that align input string x=say with output string y=said. To find p(y | x), we']",N16-1076deepfigures-results.json,neural networks,N16-1076
N16-1076.pdf-Figure4.png,"Figure 4: A compact lattice of the exponentially many paths in the transducer G of Figure 3 that align input string x=say with output string y=said. To find p(y | x), we must sum over these paths (i.e., alignments). The lattice is created by composing G with y, which selects all paths in G that output y. Note that horizontal movement makes progress through x; vertical movement makes progress through y. The lattices states specialize states in G so that they also record a position in y.","[u'. we say p(y | x) is the total probability of all paths in g that align x to y ( figure 4 ).', u'desired, so it can be fit to (x, y) pairs of different lengths with unknown Figure 4 : A compact lattice of the exponentially many paths in the transducer G of', u', it can be computed as the pathsum of the composition G y * (Figure 4) , divided by the pathsum of G (which gives the normalizing constant for the distribution']",N16-1076deepfigures-results.json,neural networks,N16-1076
N16-1076.pdf-Figure5.png,Figure 5: A level-1 BiLSTM reading the word x=say.,"[u'that reads x ( Figure 5 ). For each bounded-length substring x i:j , the BiLSTM produces a characterization i:j of']",N16-1076deepfigures-results.json,neural networks,N16-1076
N16-1076.pdf-Figure6.png,Figure 6: Level k > 1 of a deep BiLSTM. (We augment the shown input vectors with level k 1s input vectors.),[u'The k th -level BiLSTM ( Figure 6 ) reads a sequence of input vectors x'],N16-1076deepfigures-results.json,neural networks,N16-1076
N16-1077.pdf-Figure3.png,Figure 3: The modified encoder-decoder architecture for inflection generation. Input characters are shown in black and predicted characters are shown in red.  indicates the append operation.,"[u'. our resultant inflection generation model is shown in figure 3 .', u'. we also evaluate the utility of having an encoder which computes a representation of the input character sequence in a vector e by removing the encoder from our model in figure 3 .']",N16-1077deepfigures-results.json,neural networks,N16-1077
N16-1086.pdf-Figure2.png,"Figure 2: Our model architecture with a bidirectional LSTM encoder, coarse-to-fine aligner, and decoder.",[],N16-1086deepfigures-results.json,neural networks,N16-1086
N16-1176.pdf-Figure1.png,Figure 1: DLSTM based nonlinear feature mapping for bigram xt1xt. Three LSTM units are used to extract features from each word position. The bottom LSTM0 is used for first order feature extraction from the current word. The output from the lower LSTM unit at current word position and the memory cell from lower LSTM at previous word position are fed to the higher LSTM units. Such information propagation is highlighted in the figure by bold orange lines.,[u'Short Term Memory Figure 1 gives the basic architecture of a three-order nonlinear feature mapping in DLSTM. The bottom LSTM'],N16-1176deepfigures-results.json,neural networks,N16-1176
N16-1177.pdf-Figure1.png,"Figure 1: An example for sentence modeling. The bottom LSTM layer processes the input sentence and feed-forwards hidden state vectors at each time step. The one-dimensional wide convolution layer and the max-over-time pooling operation extract features from the LSTM output. For brevity, only one version of word embedding is illustrated in this figure.","[u'. for a single sentence (figure 1) , the lstm network processes the sequence of word embeddings to capture long-distance dependencies within the sentence.', u'. a sentence modeling example is illustrated in figure 1 .']",N16-1177deepfigures-results.json,neural networks,N16-1177
N16-1177.pdf-Figure2.png,"Figure 2: A schematic for document modeling hierarchy, which can be viewed as a variant of the one for sentence modeling. Independent LSTM networks process subsentences separated by punctuation. Hidden states of LSTM networks are averaged as the sentence representations, from which the high-level LSTM layer creates the joint meaning of sentences.","[u'. as for document modeling (figure 2 ), dscnn first applies independent lstm networks to each subsentence.', u'. figure 2 gives the schematic for the hierarchy.']",N16-1177deepfigures-results.json,neural networks,N16-1177
N16-1181.pdf-Figure1.png,"Figure 1: A learned syntactic analysis (a) is used to assemble a collection of neural modules (b) into a deep neural network (c), and applied to a world representation (d) to produce an answer.","[u'two components, trained jointly: first, a collection of neural ""modules"" that can be freely composed (Figure 1b) ; second, a network layout predictor that assembles modules into complete deep networks tailored to', u'a network layout predictor that assembles modules into complete deep networks tailored to each question (Figure 1a ).', u'). Figure 1 : A learned syntactic analysis (a) is used to assemble a collection of neural modules', u'. for full generality, we will need to solve harder problems, like transforming what cities are in georgia? (figure 1 ) into in this paper, we present a model for learning to select such structures from a set of automatically generated candidates.', u'Given a layout z, we assemble the corresponding modules into a full neural network (Figure 1c ), and apply it to the knowledge representation. Intermediate results flow between modules until an']",N16-1181deepfigures-results.json,neural networks,N16-1181
N16-1181.pdf-Figure2.png,"Figure 2: Simple neural module networks, corresponding to the questions What color is the bird? and Are there any states? (a) A neural find module for computing an attention over pixels. (b) The same operation applied to a knowledge base. (c) Using an attention produced by a lower module to identify the color of the region of the image attended to. (d) Performing quantification by evaluating an attention directly.","[u'color is the bird? might be answered in two steps: first, ""where is the bird?"" (Figure 2a ), second, ""what color is that part of the image?""', u'), second, ""what color is that part of the image?"" (Figure 2c ). This first step, a generic module called find, can be expressed as a fragment', u'. figure 2b shows how the same module can be used to focus on the entity georgia in a non-visual grounding domain; more generally, by representing every entity in the universe of discourse as a feature vector, we can obtain a distribution over entities that corresponds roughly to a logical set-valued denotation.', u'. but the logical perspective suggests a number of novel modules that might operate on attentions:  combining them (by analogy to conjunction or disjunction) or inspecting them directly without a return to feature space (by analogy to quantification, figure 2d ).', u'. figure 2 shows simple examples of composed structures, but for realistic question-answering tasks, even larger net- works are required.', u'We write the two examples in Figure 2 as respectively. These are network layouts: they specify a structure for arranging modules (and their']",N16-1181deepfigures-results.json,neural networks,N16-1181
N16-2001.pdf-Figure1.png,Figure 1: Model architecture for an example of learning semantic frames directly from verb-specific sentence. The sentence is divided into two windows. The old music deeply is in the left window and the old man is in the right window. The target verb moved is not used in the input. The input is connected to output layer. Each unit of output layer corresponds to one semantic frame of the target verb.,[],N16-2001deepfigures-results.json,neural networks,N16-2001
N18-1001.pdf-Figure1.png,"Figure 1: La-DTL framework overview: embedding and Bi-LSTM layers are shared across domains, predictors in red (upper) boxes are task-specific CRFs, with label-aware MMD and L2 constraints to perform feature representation transfer and parameter transfer.",[u'. figure 1 gives an overview of la-dtl for ner.'],N18-1001deepfigures-results.json,neural networks,N18-1001
N18-1001.pdf-Figure2.png,Figure 2: Illustration for La-MMD. MMD-y is computed between two domains hidden representations with the same ground truth label y. A linear combination is then applied to each label-wise MMD to form La-MMD and the coefficient is set as y = 1.,[u'. the illustration of la-mmd is shown in figure 2 .'],N18-1001deepfigures-results.json,neural networks,N18-1001
N18-1001.pdf-Figure3.png,Figure 3: Illustration for CRF parameter transfer.,[],N18-1001deepfigures-results.json,neural networks,N18-1001
N18-1006.pdf-Figure2.png,Figure 2: The architecture of the NMT model with an auxiliary prediction channel and an extra morphology table. This network includes only one decoder layer and one encoder layer.  shows the attention modules.,[u'. the relation between these modules and the nmt architecture is illustrated in figure 2 .'],N18-1006deepfigures-results.json,neural networks,N18-1006
N18-1044.pdf-Figure2.png,"Figure 2: Diagram of DiffTime. timevec(t) encodes temporal information as a vector. MW encodes lexical information as a matrix. The target vector for w at time t, useW (w, t), is found by combining Transw and timevec(t). Context version useC(c, t) is the same except that it has its own embedding layer.",[],N18-1044deepfigures-results.json,neural networks,N18-1044
N18-1057.pdf-Figure1.png,"Figure 1: Overview of method. We first train a noise model on a seed corpus, then apply noise during decoding to synthesize data that is in turn used to train the denoising model.",[u'. machine translation-based approaches-that instead trans- figure 1 : overview of method.'],N18-1057deepfigures-results.json,neural networks,N18-1057
N18-1057.pdf-Figure2.png,Figure 2: Model architecture used for both noising and denoising networks.,[u'. figure 2 illustrates the model architecture.'],N18-1057deepfigures-results.json,neural networks,N18-1057
N18-1057.pdf-Figure3.png,"Figure 3: Illustration of random noising with beam width 2. Darker shading indicates less probable expansions. In this example, greedy decoding would yield How are you. Applying noise penalties, however, results in the hypotheses How is you/he. Note that applying a penalty does not always result in an expansion falling off the beam.",[u'An illustration of the random noising algorithm is shown in Figure 3 . Note that although rank penalty noising should encourage hypotheses whose parents have similar scores'],N18-1057deepfigures-results.json,neural networks,N18-1057
N18-1113.pdf-Figure2.png,Figure 2: The Reinforced Co-Training framework.,[],N18-1113deepfigures-results.json,neural networks,N18-1113
N18-1113.pdf-Figure3.png,"Figure 3: The structure of Q-network. It chooses a unlabeled subset from {U1, U2, ..., UK} at each time step. The state representation is computed according to the two classifiers N -class probability distribution on the representative example Si of each subset Ui.","[u'. the q-value q(s t , a) is determined by a neural network as illustrated in figure 3 .']",N18-1113deepfigures-results.json,neural networks,N18-1113
N18-1115.pdf-Figure1.png,"Figure 1: Context Dependent Additive Recurrent Neural Network. Note that only nCARNN has the previous hidden state hm1 in its gate computation, iCARNN and sCARNN do not.",[],N18-1115deepfigures-results.json,neural networks,N18-1115
N18-1115.pdf-Figure2.png,Figure 2: CARNN for dialog.,"[u'. in our work, we do not use any feature extraction, and simply use the position encoder to encode the responses as shown in figure 2 , which depicts our architecture of carnn for dialog.']",N18-1115deepfigures-results.json,neural networks,N18-1115
N18-1115.pdf-Figure3.png,Figure 3: CARNN for context-dependent language model.,[],N18-1115deepfigures-results.json,neural networks,N18-1115
N18-1115.pdf-Figure4.png,Figure 4: CARNN for Question Answering.,"[u'. when the cross context carnn sentence encoder processes a sentence, it takes the encoding of the other sentence, encoded by a position encoder, as the controlling context (figure 4) .']",N18-1115deepfigures-results.json,neural networks,N18-1115
N18-1116.pdf-Figure1.png,"Figure 1: Forward encoder with character attention at time step l. The encoder alternates between reading word embeddings and character context vectors. cl I and clO denotes the inside and outside character-level context vectors of the l-th word, respectively.","[u'l l l l l l l l l l l l l s l Figure 1 : Forward encoder with character attention at time step l. The encoder alternates between reading', u'. figure 1 illustrates the forward encoder with character attentions.']",N18-1116deepfigures-results.json,neural networks,N18-1116
N18-1116.pdf-Figure2.png,Figure 2: Illustration of the decoder with our multiscale attention mechanism.,[u'. the multi-scale attention mechanism is built (as shown in figure 2 ) from word-level to characterlevel.'],N18-1116deepfigures-results.json,neural networks,N18-1116
N18-1117.pdf-Figure1.png,Figure 1: Comparison of dense-connected encoder and residual-connected encoder. Left: regular residual-connected encoder. Right: dense-connected encoder. Information is directly passed from blue blocks to the green block.,"[u'. figure 1 -3 show the design of our model structure by parts.', u'. as shown in figure 1 , when designing the model, the hidden size in each layer is much smaller than the hidden size of the corresponding layer in the residual-connected model.', u'dense connected layers with low dimension. In practice, we set sumlen as 5 or 6. Figure 1 and']",N18-1117deepfigures-results.json,neural networks,N18-1117
N18-1117.pdf-Figure2.png,Figure 2: Comparison of dense-connected decoder and residual-connected decoder. Left: regular residual-connected decoder. Right: dense-connected decoder. Ellipsoid stands for attention block. Information is directly passed from blue blocks to the green block.,"[u'. figure 2 shows the comparison of a dense decoder with a regular residual decoder.', u'and Figure 2 show the difference of information flow compared with a residual-based encoder/decoder. For residual-based models, each']",N18-1117deepfigures-results.json,neural networks,N18-1117
N18-1121.pdf-Figure3.png,Figure 3: Illustration of our proposed model. The context network is a differentiable network that computes context vector ct for word xt taking the whole sequence as input.  represents the operation that combines original word embedding xt with corresponding context vector ct to form context-aware word embeddings.,[],N18-1121deepfigures-results.json,neural networks,N18-1121
N18-1125.pdf-Figure2.png,Figure 2: One slice of the architecture of Neural Machine Translation based on a generic attention.,"[u'. figure 2 shows a slice of the entire architecture for nmt at timestamp i.', u'with Figure 2 , the only difference is the variable z i , which is obtained from Eq.(10-11)']",N18-1125deepfigures-results.json,neural networks,N18-1125
N18-1125.pdf-Figure3.png,Figure 3: The prediction coarse-to-fine models for target foresight information: (a) Model 1 using only the decoding hidden state si1. (b) Model 2 using a hidden state ti from a specialized RNN. (c) Models using a hidden state from a specialized RNN enhanced by the representation vector ci of x similar to Eq.(5).,"[u'. figure 3 (a) shows its architecture.', u'i = g(t i1 , y i1 ). This prediction model architecture is shown in Figure 3 ', u'. the architecture of this model is shown in figure 3 (c).', u'.(10-11) and the prediction model as shown in figure 3 .']",N18-1125deepfigures-results.json,neural networks,N18-1125
N18-1125.pdf-Figure4.png,"Figure 4: Neural machine translation with target Foresight attention. i is derived from Figure 3, zi is from Eq.(10-11), and other nodes are similar to ones in Figure 2.","[u'the entire architecture of the proposed target foresight attention based NMT (TFA-NMT), as shown in Figure 4 . Comparing', u'. Comparing Figure 4 with']",N18-1125deepfigures-results.json,neural networks,N18-1125
N18-1150.pdf-Figure1.png,Figure 1: Illustration of deep communicating agents presented in this paper. Each agent a and b encodes one paragraph in multiple layers. By passing new messages through multiple layers the agents are able to coordinate and focus on the important aspects of the input text.,"[u'across multiple collaborating encoder agents, each in charge of a different subsection of the text (Figure 1 ). Each of these agents encodes their assigned text independently, and broadcasts their encoding to']",N18-1150deepfigures-results.json,neural networks,N18-1150
N18-1150.pdf-Figure2.png,"Figure 2: Multi-agent-encoder-decoder overview. Each agent a encodes a paragraph using a local encoder followed by multiple contextual layers with agent communication through concentrated messages z(k)a at each layer k. Communication is illustrated in Figure 3. The word context vectors cta are condensed into agent context ct . Agent specific generation probabilities, pta, enable voting for the suitable out-of-vocabulary words (e.g., yen) in the final distribution.","[u'. once each agent completes encoding, they deliver their information to the decoder with a novel contextual agent attention ( figure 2 ).', u'vector by an agent attention yielding the document global agent attention distribution g t (see Figure 2 ):']",N18-1150deepfigures-results.json,neural networks,N18-1150
N18-1150.pdf-Figure3.png,"Figure 3: Multi-agent encoder message passing. Agents b and c transmit the last hidden state output (I) of the current layer k as a message, which are passed through an average pool (Eq. (6)). The receiving agent a uses the new message z(k)a as additional input to its next layer.",[u'. communication is illustrated in figure 3 .'],N18-1150deepfigures-results.json,neural networks,N18-1150
N18-1151.pdf-Figure1.png,Figure 1: The overall structure of our keyphrase extraction framework with context encoder. Grey dotted array refer to the inputs of target posts that are also used in context encoding.,"[u'. figure 1 shows the overall structure of our keyphrase extraction framework.', u'As shown in Figure 1 , our keyphrase tagger is built upon input feature map I(), which embeds each word']",N18-1151deepfigures-results.json,neural networks,N18-1151
N18-1151.pdf-Figure2.png,Figure 2: The structure of attention-based conversation context encoder.,"[u', as shown in Figure 2 . The encoder is thus represented as']",N18-1151deepfigures-results.json,neural networks,N18-1151
N18-1151.pdf-Figure3.png,Figure 3: The structure of the conversation context encoder based on memory networks.,[u'. figure 3 illustrates its structure.'],N18-1151deepfigures-results.json,neural networks,N18-1151
2020.acl-main.173.pdf-Figure1.png,"Figure 1: Hallucinations in extreme document summarization: the abbreviated article, its gold summary and the abstractive model generated summaries (PTGEN, See et al. 2017; TCONVS2S, Narayan et al. 2018a; and, GPTTUNED, TRANS2S and BERTS2S, Rothe et al. 2020) for a news article from the extreme summarization dataset (Narayan et al., 2018a). The dataset and the abstractive models are described in Section 3 and 4. We also present the [ROUGE-1, ROUGE-2, ROUGE-L] F1 scores relative to the reference gold summary. Words in red correspond to hallucinated information whilst words in blue correspond to faithful information.","[u'. the example in figure 1 illustrates that the faithfulness and factuality are yet to be conquered by conditional text generators.', u'[66.7, 40.0, 51.9] Figure 1 : Hallucinations in extreme document summarization: the abbreviated article, its gold summary and the abstractive', u'. for example, in figure 1, ""former london mayoral candidate"" in the tconvs2s abstract and ""former london mayor"" in the trans2s abstract are hallucinations of intrinsic nature; both use terms or concepts from the document but misrepresent information from the document, making them unfaithful to the document.', u'. this can be observed in the example shown in figure 1 as the input document does not mention that the discussed ""london mayoral election"" is from ""2016""; but the abstract generated by the pretrained text generator gpt-tuned correctly predicts this information similar to the human-authored abstract.', u': Human assessment of a system generated summary for the article in Figure 1 . The annotation user interface is shown as it was shown to raters. mary with', u'shows an example assessment of a summary of an article from Figure 1 . Results from the full assessment are shown in', u'mentions ""Conservative MP Zac Goldwin"" which can not be verified from the article in Figure 1 . Here, annotators could use Wikipedia or Google Search to check that there had not']",2020.acl-main.173deepfigures-results.json,NLP text_grammar_eg,2020.acl-main.173
2020.acl-main.173.pdf-Figure3.png,Figure 3: Sample of question-answer pairs generated from hallucinated summaries that are correctly answered by their source articles. Highlighted spans in the summaries are marked as extrinsic or intrinsic hallucinations by our annotators.,[u'. we demonstrate these issues in figure 3 .'],2020.acl-main.173deepfigures-results.json,NLP text_grammar_eg,2020.acl-main.173
2020.acl-main.174.pdf-Figure1.png,Figure 1: Example of narrative structure for episode Burden of Proof from TV series Crime Scene Investigation (CSI); turning points are highlighted in color.,"[u'. figure 1 : example of narrative structure for episode ""burden of proof"" from tv series crime scene investigation (csi); turning points are highlighted in color.', u'. in figure 1 , tps are highlighted for a csi episode.', u'units). It is often assumed (Cutting, 2016) that there are six acts in a film (Figure 1) , each delineated by a turning point (arrows in the']",2020.acl-main.174deepfigures-results.json,NLP text_grammar_eg,2020.acl-main.174
2020.acl-main.183.pdf-Figure1.png,"Figure 1: Sample conversation from the BlendedSkillTalk dataset, annotated with four conversation mode types (PB: personal background; K: knowledge; S: personal situation; E: empathy). The guided (G) and unguided (U) workers are given personas and a topic. The conversation has been seeded with two utterances from a conversation sampled from WoW. When the guided worker selected one of the suggestions, it is shown in shaded grey.","[u'An example conversation from Blended-SkillTalk is shown in Figure 1 . In this example, we see that the speakers inject knowledge, empathy, and personal background,']",2020.acl-main.183deepfigures-results.json,NLP text_grammar_eg,2020.acl-main.183
2020.acl-main.187.pdf-Figure7.png,Figure 7: Crowd-sourcing instructions,[u'. figure 7 shows the instructions shown to the annotators.'],2020.acl-main.187deepfigures-results.json,NLP text_grammar_eg,2020.acl-main.187
2020.acl-main.210.pdf-Figure1.png,"Figure 1: A semi-structured premise (the table). Two hypotheses (H1, H2) are entailed by it, H3 is neither entailed nor contradictory, and H4 is a contradiction.","[u'. our work stems from the observation that we can make valid inferences about implicit information conveyed by the mere juxtaposition of snippets of text, as shown in the table describing dressage in figure 1 .', u'. for example, determining that the hypothesis h2 in figure 1 entails the premise table requires looking at multiple rows of the table, un-derstanding the meaning of the row labeled mixed gender, and also that dressage is a sport.', u'. our tables are isomorphic to key-value pairs, , in figure 1 , the bold entries are the keys, and the corresponding entries in the same row are their respective values.', u'. for example, for the table in figure 1 , the row with key equipment gets mapped to the sentence the equipment of dressage are horse, horse tack.']",2020.acl-main.210deepfigures-results.json,NLP text_grammar_eg,2020.acl-main.210
2020.acl-main.210.pdf-Figure4.png,"Figure 4: Two semi-structured premises (the tables), and three hypotheses (H1: entailment, H2: Neutral, and H3: contradiction) that correspond to each table.","[u' Figure 4 : Two semi-structured premises (the tables), and three hypotheses (H1: entailment, H2: Neutral, and H3:', u'. for example, in a table for a music group as in figure 4 , if there is a row called labels, we will assume that the labels listed in that row are the only labels associated with the group.']",2020.acl-main.210deepfigures-results.json,NLP text_grammar_eg,2020.acl-main.210
2020.acl-main.218.pdf-Figure4.png,"Figure 4: Amazon Mechanical Turk interface for validating yes-and candidates determined by the yes-and classifier. Turkers are asked to correct minor errors in grammar, spelling, and punctuation for qualifying yes-and candidates, which are then categorized as Typo/Fix.",[u'. the same turkers that extracted yes-ands from spontaneanation are invited to validate the yes-and candidates filtered out by the classifier using the interface shown in figure 4 .'],2020.acl-main.218deepfigures-results.json,NLP text_grammar_eg,2020.acl-main.218
2020.acl-main.218.pdf-Figure5.png,"Figure 5: Interface used by human evaluators to rank responses based on their quality as a yes-and, where a rank of 1 is most preferred. The correct ranking is shown for this example. The option ranked 1 is a yesbut: it does not reject a reality but rather rejects a suggestion and provides refining information that is most coherent to the prompt.","[u'. for each of these datasets, we either simply finetune on that dataset, or fine-tune and then further figure 5 : interface used by human evaluators to rank responses based on their quality as a yes-and, where a rank of 1 is most preferred.', u'."" these four responses are randomly ordered for each question to prevent evaluators from developing a bias for responses that frequently have a good or poor response in a set order, as shown in figure 5 .']",2020.acl-main.218deepfigures-results.json,NLP text_grammar_eg,2020.acl-main.218
2020.acl-main.218.pdf-Figure6.png,Figure 6: Explanation of the objective and yes-and in the yes-and guideline provided to Turkers.,[],2020.acl-main.218deepfigures-results.json,NLP text_grammar_eg,2020.acl-main.218
2020.acl-main.218.pdf-Figure7.png,Figure 7: Explanation of the label space for yes-ands and non-yes-ands and the detailed instructions for the transcription task.,[],2020.acl-main.218deepfigures-results.json,NLP text_grammar_eg,2020.acl-main.218
2020.acl-main.218.pdf-Figure8.png,Figure 8: Common mistakes that Turkers made in the early stages of data collection were corrected and added to the guidelines to aid new Turkers.,[],2020.acl-main.218deepfigures-results.json,NLP text_grammar_eg,2020.acl-main.218
2020.acl-main.218.pdf-Figure9.png,Figure 9: Annotated examples provided to Turkers for understanding the label space of the yes-and transcription task.,[],2020.acl-main.218deepfigures-results.json,NLP text_grammar_eg,2020.acl-main.218
2020.acl-main.225.pdf-Figure2.png,"Figure 2: Training examples for three baseline infilling strategies and ILM on a given artificially-masked sentence. For each strategy, we train the same architecture (GPT-2) on such examples. At both training and test time, examples are fed from left to right; anything to the left of a green target is available to the model as context when predicting the target. Precisely, LM only considers past context, and LM-Rev only considers future. LM-All considers all available context but uses long sequence lengths. Our proposed ILM considers all context while using fewer tokens.","[u'. finally, we construct the complete infilling example by concatenatingx, [sep], and y (see figure 2 for a complete example).', u'. training examples for all strategies are depicted in figure 2 .', u'. regardless of differences in the ordering and number of tokens that each strategy uses to represent a test example, ppl is always computed only for the span of tokens comprising the original sentence ( green tokens in figure 2 ).']",2020.acl-main.225deepfigures-results.json,NLP text_grammar_eg,2020.acl-main.225
2020.acl-main.225.pdf-Figure3.png,"Figure 3: Example of a short story in our STORIES dataset with its third sentence masked, and sentences infilled by different models. The sentences generated by BERT and SA models are off-topic, the sentence generated by LM model is irrelevant to the future context, while the ones generated by ILM and Human successfully account for both previous and future context.","[u', with qualitative examples in Figure 3 and Appendix E.', u'. figure 3 : example of a short story in our stories dataset with its third sentence masked, and sentences infilled by different models.']",2020.acl-main.225deepfigures-results.json,NLP text_grammar_eg,2020.acl-main.225
2020.acl-main.225.pdf-Figure4.png,Figure 4: Example of a task and instruction for human evaluation on Amazon Mechanical Turk.,"[u'model, and was asked to identify one of the five sentences generated by machine (see Figure 4 for an example). Among the 100 collected responses, we filtered out 5 responses whose annota-tion']",2020.acl-main.225deepfigures-results.json,NLP text_grammar_eg,2020.acl-main.225
2020.acl-main.243.pdf-Figure2.png,"Figure 2: Generation benchmarks. Model is given a table x consisting of semantic fields and is tasked with generating a description y1:T of this data. Two example datasets are shown. Left: E2E, Right: WikiBio.","[u', ROUGE-L Figure 2 : Generation benchmarks. Model is given a table x consisting of semantic fields and is']",2020.acl-main.243deepfigures-results.json,NLP text_grammar_eg,2020.acl-main.243
2020.acl-main.247.pdf-Figure1.png,Figure 1: Example Span Selection Instance,[u'. figure 1 illustrates an example of a span selection instance.'],2020.acl-main.247deepfigures-results.json,NLP text_grammar_eg,2020.acl-main.247
2020.acl-main.263.pdf-Figure3.png,Figure 3: Amazon Mechanical Turk UI.,[u'. figure 3 contains a screenshot of the ui we present to crowd workers.'],2020.acl-main.263deepfigures-results.json,NLP text_grammar_eg,2020.acl-main.263
2020.trac-1.7.pdf-Figure1.png,Figure 1: Examples of the posts/tweets with their labels,[],2020.trac-1.7deepfigures-results.json,NLP text_grammar_eg,2020.trac-1.7
2020.trac-1.9.pdf-Figure1.png,Figure 1: Training samples for task A (aggression identification).,[],2020.trac-1.9deepfigures-results.json,NLP text_grammar_eg,2020.trac-1.9
2020.trac-1.9.pdf-Figure2.png,Figure 2: Training samples for task B (misogynistic aggression identification).,[],2020.trac-1.9deepfigures-results.json,NLP text_grammar_eg,2020.trac-1.9
2020.wac-1.8.pdf-Figure2.png,"Figure 2: Resulting key phrase lists from optimizing on recall, precision and F-score respectively.","[u'Consider the outcome for English of the 50 phrases based on recall and precision in Figure 2 . As expected, the top 50 phrases selected based on their recall contain stopwords and', u'rt for and, automatically, the up, does, getting, is not, my rt you, it this Figure 2 : Resulting key phrase lists from optimizing on recall, precision and F-score respectively.']",2020.wac-1.8deepfigures-results.json,NLP text_grammar_eg,2020.wac-1.8
C00-1002.pdf-Figure3.png,Figure 3: Centroid precision and recall for object collocates of polysemous verbs.,[],C00-1002deepfigures-results.json,NLP text_grammar_eg,C00-1002
C00-1044.pdf-Figure1.png,Figure 1: Automatically obtained classification of a sample of 100 adjectives as gradable or not. Correct decisions (according to the COBUILD-based reference model) are indicated in bold.,[u'. figure 1 lists the gradability labels that were automatically assigned to one of the 100 random test sets using the modified prediction algorithm.'],C00-1044deepfigures-results.json,NLP text_grammar_eg,C00-1044
C00-1069.pdf-Figure1.png,"Figure 1: Example \To draw a polyline""",[],C00-1069deepfigures-results.json,NLP text_grammar_eg,C00-1069
C00-1069.pdf-Figure5.png,Figure 5: SPL expressions,"[u'. since the conceptualizations of spatial relations are di erent across english and russian, the input spl expressions diverge as shown in figure 5 ; rather than using domain model concepts, these spl expressions restrict themselves to upper model concepts in order to highlight the cross-linguistic contrast.', u'. these spl expressions also show the use of domain concepts as produced by the text planner rather than upper model concepts as in the spls in figure 5 the second di erence is handled by the generation grammars internally.']",C00-1069deepfigures-results.json,NLP text_grammar_eg,C00-1069
C00-1069.pdf-Figure6.png,Figure 6: Multilingual SPL expression for the header examples,[u'. this is shown in figure 6 by means of the standard linguistic conditionalization provided by kpml for all levels of linguistic description.'],C00-1069deepfigures-results.json,NLP text_grammar_eg,C00-1069
C00-1073.pdf-Figure1.png,Figure 1: An example dialogue with NJFun.,"[u'. a sample dialogue with nj-fun is shown in figure 1 , with system utterances labeled si and user utterances labeled ui.', u'. if njfun asks the user to explicitly verify an attribute, it is using explicit con rmation , expconf2 for the location, exempli ed by s 2 in figure 1 .', u'generates the dialogue in Figure 1 . Each row indicates the state that NJFun is in, the ac-', u': Generating the dialogue in Figure 1 .', u'tion executed in this state, the corresponding turn in Figure 1 , and the reward received. The initial state represents that NJFun will rst attempt to', u"", GreetS is also possible, generating the rst utterance in Figure 1 . After the user's response, the next state represents that NJ-Fun has now greeted the"", u'. for example, the task executed by the user in figure 1 was: you feel thirsty and want to do some winetasting in the morning.', u'. at the end of the task, njfun asked for feedback on their experience , utterance s4 in figure 1 the training phase of the experiment resulted in 311 complete dialogues not all subjects completed all tasks, for which njfun logged the sequence of states and the corresponding executed actions.', u'. figure 1 is an example dialogue using the optimal strategy.']",C00-1073deepfigures-results.json,NLP text_grammar_eg,C00-1073
C00-2139.pdf-Figure1.png,Figure 1: Example bootstrapping structure,[u'What is a family fare X What is the payload of an African Swallow X Figure 1 : Example bootstrapping structure For each sentence s 1 in the corpus:'],C00-2139deepfigures-results.json,NLP text_grammar_eg,C00-2139
C00-2139.pdf-Figure3.png,Figure 3: Ambiguous alignments,"[u'. this process reveals from 1 san francisco to dallas 2 from dallas to 1 san francisco 2 from san francisco to 1 dallas 2 from 1 dallas to san francisco 2 from san francisco 1 to dallas 2 from dallas 1 to san francisco 2 figure 3 : ambiguous alignments the groups of identical words, but it also uncovers the groups of distinct words in the sentences.']",C00-2139deepfigures-results.json,NLP text_grammar_eg,C00-2139
C00-2139.pdf-Figure4.png,Figure 4: Overlapping constituents,[u'me | z a l l ightsfrom Dallas to Boston Give me help on classes Figure 4 : Overlapping constituents by comparing against Book Delta 128 from Dallas to Boston and the'],C00-2139deepfigures-results.json,NLP text_grammar_eg,C00-2139
C00-2139.pdf-Figure5.png,Figure 5: Recursive structures learned in the ATIS corpus,[],C00-2139deepfigures-results.json,NLP text_grammar_eg,C00-2139
C00-2139.pdf-Figure6.png,Figure 6: Wrong syntactic type,[],C00-2139deepfigures-results.json,NLP text_grammar_eg,C00-2139
C00-2144.pdf-Figure3.png,Figure 3: A diagram based on Kamp and Reyle 1993,"[u' (Figure 3) , to small but useful classes for diagrams from particular areas of linguistics, such as', u'. figure 3 shows a simple example from interarbora.']",C00-2144deepfigures-results.json,NLP text_grammar_eg,C00-2144
C02-1009.pdf-Figure1.png,Figure 1: An illustration of length&lexical type error,"[u'. figure 1 : an illustration of length&lexical type error maximum number of either source sentences or target sentences allowed in each alignment unit is set to be ""4"" (, we will not consider those matchingtypes of ""5  1"", ""5  2"", ""1  5"", etc).', u'. figure 1 is an illustration of bilingual sinorama magazine texts.', u'those remaining errors generated from the proposed robust model, two error examples are given in Figure 1 . The first case shows an example of ""Length-Type Error"", in which the short sentence']",C02-1009deepfigures-results.json,NLP text_grammar_eg,C02-1009
C02-1022.pdf-Figure1.png,"Fig. 1. Some 'standard' examples of Qualia Structures, for 'food', 'sandwich', and 'pizza'","[u'. for adequately expressing this knowledge, we make use of entries in the generative lexicon (see figure 1 ).']",C02-1022deepfigures-results.json,NLP text_grammar_eg,C02-1022
C02-1022.pdf-Figure2.png,"Fig. 2. Some 'extended' examples of Qualia Structures, for special food sorts and 'table'","[u'. figure 2 , for example, shows some sorts of food associated with different expectations about how many persons typically eat them.']",C02-1022deepfigures-results.json,NLP text_grammar_eg,C02-1022
C02-1022.pdf-Figure3.png,"Fig. 3. Some 'extended' examples of Qualia Structures, for 'office', 'airline', and 'flight'","[u'. each office and each airline are supposed to employ several persons, and each person is working for one organization only, at least in his/her individual activities (this is expressed by the quantifiers single and multiple in the lexical entries in figure 3 ).', u""Making use of the TELIC role in the lexical entry of 'office', as exposed in Figure 3 , yields (SINGLE x OFFICE (AND (BOSTONIAN x) (MULTIPLE y PERSON (AND (WORK y x)"", u""The first metonymic extension, based on the lexicon entry for 'airline' (see Figure 3 ), tentatively inserts 'flights' linked to 'airline' via an ORGANIZE relation, and yields (WH x"", u""(GOAL y BOSTON)))) and the final operation based on the lexicon entry for 'flight' (see Figure 3 ) leads to a similar extension, inserting 'person' related to 'flight' via a CARRY relation:""]",C02-1022deepfigures-results.json,NLP text_grammar_eg,C02-1022
C02-1035.pdf-Figure10.png,Figure 10. Resolving ambiguity for U1,"[u'. the result of fusion is shown in figure 10 (a).', u'. in this case, a3 in figure 10 (a) indicates the information of interest is about the price of the city irvington.', u'. therefore, the content in those structures are combined using a disjunctive relation as in figure 10 (b).']",C02-1035deepfigures-results.json,NLP text_grammar_eg,C02-1035
C02-1035.pdf-Figure11.png,Figure 11. Deriving unspecified information for U3,"[u'. the motivation and task of this input is not known as in figure 11(a) .', u'(also as in Figure 11b ), MIND is able to derive Motivator and Method features from DS1 to update the', u'to derive Motivator and Method features from DS1 to update the conversation unit for U3 (Figure 11c ). As a result, this revised conversation unit provides the overall meaning that the user']",C02-1035deepfigures-results.json,NLP text_grammar_eg,C02-1035
C02-1035.pdf-Figure12.png,Figure 12. Improving alignment for U5,"[u'. furthermore, for u5 (""comparing these two houses with the previous house""), there are two attention structures (a1 and a2) created for the speech input as in figure 12 (a).', u'. although there is only one deictic gesture which points to two potential houses (figure 12b ), mind is able to figure out that this deictic gesture is actually referring to a group of two houses rather than an ambiguous single house.', u'. for the second reference of ""previous house"" (a2 in figure 12a ), based on the information captured in the temporal constraint, mind searches the conversation history and finds the most recent house explored (mls7689432).', u'. therefore, mind is able to reach an overall understanding of u5 that the user is interested in comparing three houses (as in figure 12c ).']",C02-1035deepfigures-results.json,NLP text_grammar_eg,C02-1035
C02-1035.pdf-Figure6.png,Figure 6. Temporal and visual reference constraints,"[u'. for example, as in figure 6 (a), a relative temporal constraint is used since ""the previous house"" refers to the house that precedes the current focus (anchor: current) in the conversation history.', u'. for example, constraint used in the input ""the green house"" is shown in figure 6 (b).']",C02-1035deepfigures-results.json,NLP text_grammar_eg,C02-1035
C02-1035.pdf-Figure7.png,Figure 7. Attributive data constraints,"[u'. for example, for the input ""houses under 300,000 dollars"" in figure 7(a) , manner is comparative since the constraint is about a ""less than"" relationship (relation: less-than) between the price (aspect: price) of the desired object(s) and a particular value (anchor: ""300000 dollars"").', u'. for the input ""3 largest houses"" in figure 7 (b), manner is superlative since it is about the maximum (relation: max) requirement on the size of the houses (aspect: size).', u', and Figure 7) , where intention, attention and constraints are represented in feature structures. Note that only features', u'on the complexity of user inputs, the representation can be composed by a flexible combi- Figure 7 . Attributive data constraints nation of different feature structures. Specifically, an attention structure may have']",C02-1035deepfigures-results.json,NLP text_grammar_eg,C02-1035
C02-1035.pdf-Figure8.png,Figure 8. Attention structures for U4,"[u'. the modality unit created for u4 speech input is shown in figure 8(a) .', u'. we will show in section 4.3 that the fine-grained representation in figure 8 (a) allows mind to use contexts to resolve these two references and improve alignment.', u'reference constraints are no longer present in conversation units. For example, once two references in Figure 8 (a) are resolved during multimodal understanding (details are described in Section 4.3), and MIND identifies', u'""White Plains"", it creates a conversation unit representing the overall meanings of this input in Figure 8(b) .', u'. for example, from the speech input in u4 (""show me houses with this style around here""), three attention structures are generated as shown in figure 8(a) .', u'. suppose the style is ""victorian"", then mind is able to figure out that the overall meaning of u4 is looking for houses with a victorian style and located in white plains (as shown in figure 8b ).']",C02-1035deepfigures-results.json,NLP text_grammar_eg,C02-1035
C02-1085.pdf-Figure1.png,Figure 1: Documents from the TDT1,"[u'Clinton offered the assistance of U.S. military forces in [Japan], and Washington provided the Japanese Figure 1 : Documents from the TDT1', u""The underlined words in Figure 1 denote a subject word in each document. Words marked with '{}' and '[]' refer to""]",C02-1085deepfigures-results.json,NLP text_grammar_eg,C02-1085
C02-1156.pdf-Figure4.png,Figure 4: The Commercial-Transaction schema.,[],C02-1156deepfigures-results.json,NLP text_grammar_eg,C02-1156
C02-1156.pdf-Figure5.png,"Figure 5: The Buy, Sell and Pay schemas.","[u'. as shown in figure 5 , we treat buy, sell and pay as schemas that evoke the ct schema and identify their roles with specific participants and event stages of the evoked ct schema.', u'. first, the frame element binding patterns may differ among perspectives, as illustrated by figure 5 , in which the lexical item buy identifies the actor of the transitive-action with both the customer of the ct and the agent of the money-transfer.']",C02-1156deepfigures-results.json,NLP text_grammar_eg,C02-1156
C02-1158.pdf-Figure4.png,Figure 4: Example of the acquisition of a part translation rule using the sentence translation rule.,"[u'. figure 4 2 shows an example of the acquisition of a part translation rule using the sentence translation rule.', u'. in figure 4, (thirty;30[sanju] ) as the part translation rule is acquired because ""thirty"" corresponds to the variable in the source part of sentence translation rule and ""30[sanju]"" corresponds to the variable in the target part of sentence translation rule.', u'.] as a sentence translation rule by using the part translation rule (thirty;30[sanju]) acquired in figure 4 , and @1 starts in @0 minutes.']",C02-1158deepfigures-results.json,NLP text_grammar_eg,C02-1158
C02-1158.pdf-Figure42.png,"Figure 42 shows an example of the acquisition of a part translation rule using the sentence translation rule. In Figure 4, (thirty;30[sanju]) as the part translation rule is acquired because thirty corresponds to the variable in the source part of sentence translation rule and 30[sanju] corresponds to the variable in the target part of sentence translation rule.",[],C02-1158deepfigures-results.json,NLP text_grammar_eg,C02-1158
C02-1158.pdf-Figure5.png,Figure 5: Examples of the acquisition of a sentence translation rule using the part translation rule.,"[u'. figure 5 shows examples of the acquisition of the sentence translation rules using the part translation rules.', u'. in figure 5 , the system acquires it starts in @0 minutes.', u'. figure 5 : examples of the acquisition of a sentence translation rule using the part translation rule.']",C02-1158deepfigures-results.json,NLP text_grammar_eg,C02-1158
C04-1078.pdf-Figure2.png,Figure 2: Illustration of generalizing instances,"[u'(1) Figure 2 gives five examples of original training instances for ""starting time"" in the seminar announcement domain.', u'. for each type of tagged slot (slot 0 ) such as stime in figure 2 , we accumulate all the tagged instances and align them according to the positions of slot 0 .', u'shows the generated soft pattern rules for the examples given in Figure 2 .']",C04-1078deepfigures-results.json,NLP text_grammar_eg,C04-1078
C04-1078.pdf-Figure3.png,Figure 3: An excerpt of soft pattern rules,[u'gives the proportion of occurrences of the j th token to the i th slot. Figure 3 shows the generated soft pattern rules for the examples given in'],C04-1078deepfigures-results.json,NLP text_grammar_eg,C04-1078
C04-1079.pdf-Figure1.png,Figure 1. Example summary from a conventional sentence extraction summarizer,"[u'However, in contrast to a conventional sentence extraction summary in Figure 1 , the ideal summary ought to provide sufficient information about the current state-of-affairs of the']",C04-1079deepfigures-results.json,NLP text_grammar_eg,C04-1079
C04-1079.pdf-Figure2.png,Figure 2. Example summary from our system,[u'. an example of such a summary is presented in figure 2 .'],C04-1079deepfigures-results.json,NLP text_grammar_eg,C04-1079
C04-1114.pdf-Figure1.png,Figure 1: Some semantic types,[],C04-1114deepfigures-results.json,NLP text_grammar_eg,C04-1114
C04-1128.pdf-Figure1.png,Figure 1: Sample summary obtained with sentence extraction,"[u'. phil, we can just use that as our ""press release"", right? in another subthread, on apr 12, 2001, kevin danquoit wrote: are you sending out upcoming events for this week? figure 1 : sample summary obtained with sentence extraction formation from past conversations becomes increasingly inaccessible and difficult to manage.', u'shown in Figure 1 . While this summary does include an answer to the first question, it does not']",C04-1128deepfigures-results.json,NLP text_grammar_eg,C04-1128
C04-1129.pdf-Figure1.png,"Figure 1: Example information collected for entities in the input. The canonic form of the named entity is shown in bold and the input article id in italic. IR stands for initial reference, CO for subsequent noun co-reference, PR for pronoun reference, AP for apposition and RC for relative clause.","[u'. the processed references to the same people across documents were aligned using the named entity tagger canonic name, resulting in tables similar to those shown in figure 1.']",C04-1129deepfigures-results.json,NLP text_grammar_eg,C04-1129
C04-1129.pdf-Figure2.png,Figure 2: First three sentences from a machine generated summary before/after reference regeneration.,[u'. figure 2 : first three sentences from a machine generated summary before/after reference regeneration.'],C04-1129deepfigures-results.json,NLP text_grammar_eg,C04-1129
C04-1129.pdf-Figure3.png,Figure 3: First four sentences from another machine summary before/after reference regeneration.,[u'. figure 3 : first four sentences from another machine summary before/after reference regeneration.'],C04-1129deepfigures-results.json,NLP text_grammar_eg,C04-1129
C04-1143.pdf-Figure1.png,Figure 1 Sample Embedded Answer,"[u'. emails containing a list of questions or requests for comments are often edited by the replying party and answers inserted directly inside the text of the original request, as illustrated in figure 1 .', u'. figure 1 illustrates the main two difficulties faced by the summariser in this situation.']",C04-1143deepfigures-results.json,NLP text_grammar_eg,C04-1143
C04-1158.pdf-Figure1.png,Figure 1: Example of one article in database,[u'. figure 1 is an example of the database.'],C04-1158deepfigures-results.json,NLP text_grammar_eg,C04-1158
C04-1158.pdf-Figure3.png,Figure 3: Example of calculating perplexity (PP ) and relevance score (RS),[u'. an example of calculating the relevance score is shown in figure 3 .'],C04-1158deepfigures-results.json,NLP text_grammar_eg,C04-1158
C04-1158.pdf-Figure4.png,Figure 4: Example of calculating significance score,[u'. figure 4 has an example of calculating the significance score.'],C04-1158deepfigures-results.json,NLP text_grammar_eg,C04-1158
C04-1189.pdf-Figure1.png,FIGURE 1: A fragment of an analysts session with HITIQA,"[u'. figure 1 shows a fragment of an analytical session with hitiqa; note that these questions are not aimed at factoids, despite their simple form.']",C04-1189deepfigures-results.json,NLP text_grammar_eg,C04-1189
C04-1189.pdf-Figure10.png,FIGURE 10: Fragment of an analytical session,[u'. figure 10 shows an abridged transcript from another analytical session with hitiqa.'],C04-1189deepfigures-results.json,NLP text_grammar_eg,C04-1189
C04-1189.pdf-Figure2.png,FIGURE 2: A general frame obtained from the text passage in Figure 3 (not all attributes shown).,"[u'. in the general frame, attributes have no assigned roles; they are loosely grouped around the topic (figure 2 ).']",C04-1189deepfigures-results.json,NLP text_grammar_eg,C04-1189
C04-1189.pdf-Figure3.png,FIGURE 3: A text passage from the WMD domain data,[u'. figure 3 contains an example passage from this data set.'],C04-1189deepfigures-results.json,NLP text_grammar_eg,C04-1189
C04-1189.pdf-Figure4.png,FIGURE 4: A general goal frame from the Iraq question,"[u'. goal frames generated from the question, ""has iraq been able to import uranium?"" are shown in the frame in figure 4 is simply a general frame which is invoked first.']",C04-1189deepfigures-results.json,NLP text_grammar_eg,C04-1189
C04-1189.pdf-Figure5.png,FIGURE 5: A typed goal frame from the Iraq question,"[u'. this new frame, shown in figure 5 , has three role attributes trf_to, trf_from and trf_object, plus the relation type (trf_type).', u'was obtained by comparing the Goal frame ( Figure 5 ) to a partly matching frame']",C04-1189deepfigures-results.json,NLP text_grammar_eg,C04-1189
C04-1189.pdf-Figure6.png,"FIGURE 6: A typed frame obtained from the text passage in Figure 3, in response to the Iraq question","[u"". the frame in figure 6 is scored as relevant to the user's query and included in the answer space.""]",C04-1189deepfigures-results.json,NLP text_grammar_eg,C04-1189
C04-1189.pdf-Figure7.png,FIGURE 7: The clarification dialogue detail,"[u'. this situation is illustrated in the exchange shown in figure 7 .', u'The clarification question asked by HITIQA in Figure 7 was obtained by comparing the Goal frame (', u'. if the user responds the equivalent of ""yes"" to the system clarification question in the dialogue in figure 7 , a corresponding wmddevelop frame will be added to the set of active goal frames and all wmddevelop frames obtained from text passages will be re-scored for possible inclusion in the answer.', u': A 2-conflict frame against the Iraq/uranium question that generated the dialogue in Figure 7 .', u'. figure 7 shows a portion of the answer generated by hitiqa for the iraq query.']",C04-1189deepfigures-results.json,NLP text_grammar_eg,C04-1189
C04-1189.pdf-Figure8.png,FIGURE 8: A 2-conflict frame against the Iraq/uranium question that generated the dialogue in Figure 7.,"[u') to a partly matching frame (Figure 8 ) generated from another text passage. We note first that the Goal frame for this', u'. we note first that the goal frame for this example is wmdtransfer type, while the data frame in figure 8 is wmddevelop type.']",C04-1189deepfigures-results.json,NLP text_grammar_eg,C04-1189
C04-1189.pdf-Figure9.png,Figure 9: A scenario level analytic task,[u'. figure 9 shows one of the analytical scenarios used in these workshops.'],C04-1189deepfigures-results.json,NLP text_grammar_eg,C04-1189
C04-1191.pdf-Figure1.png,Figure 1: Type definition for PhysicalDevice.,"[u'. figure 1 shows the type definition for physi-caldevice, a prototypical denotatum term for count nouns.']",C04-1191deepfigures-results.json,NLP text_grammar_eg,C04-1191
C04-1191.pdf-Figure2.png,Figure 2: Type definition for Water.,"[u'. figure 2 shows the type definition for water, a prototypical denotation for mass nouns.']",C04-1191deepfigures-results.json,NLP text_grammar_eg,C04-1191
C08-1061.pdf-Figure2.png,Figure 2. Examples of extracted hypernym,[u'. extracted examples are shown in figure 2 .'],C08-1061deepfigures-results.json,NLP text_grammar_eg,C08-1061
C08-1066.pdf-Figure1.png,Figure 1: Some projectivity operator definitions.,[u'. (figure 1 shows some example definitions.'],C08-1066deepfigures-results.json,NLP text_grammar_eg,C08-1066
C08-1116.pdf-Figure2.png,Figure 2: Instructions for scaled judging.,[],C08-1116deepfigures-results.json,NLP text_grammar_eg,C08-1116
C08-2007.pdf-Figure2.png,Figure 2: Main heuristic for identifying LCpdVs,"[u'By scanning the corpus, V1stem+V2 sequences were found given the heuristic H* specified in Figure 2 .']",C08-2007deepfigures-results.json,NLP text_grammar_eg,C08-2007
C08-2015.pdf-Figure1.png,Figure 1: Example of calculating Bayes risk,"[u'Response candidates: * With context: Figure 1 shows an example of calculating a Bayes risk (where F R = 6, Rwd Ret']",C08-2015deepfigures-results.json,NLP text_grammar_eg,C08-2015
C10-1010.pdf-Figure2.png,Figure 2: Textual content of the German Wikipedia article about Gollach. All named entities which are relevant for the river-bodyOfWater relation are highlighted. This article contains two instances for the relation between Gollach and Tauber.,[],C10-1010deepfigures-results.json,NLP text_grammar_eg,C10-1010
C10-1066.pdf-Figure5.png,Figure 5: The original text and a paraphrase,[u'. figure 5: the original text and a paraphrase for each sentence is merged with the cumulative pg from previous sentences.'],C10-1066deepfigures-results.json,NLP text_grammar_eg,C10-1066
C10-1074.pdf-Figure1.png,Figure 1. Feature based Review Summarization,"[u'. figure 1 shows a summary example for movie ""gone with the wind"".', u'. the review summary is generated as a list of structured object feature-opinion pairs, as shown in figure 1 .', u'A review summary example, generated by our methods, is shown in Figure 1 .']",C10-1074deepfigures-results.json,NLP text_grammar_eg,C10-1074
C10-1074.pdf-Figure4.png,Figure 4. Features for learning Methods,"[u'. all the features are listed in figure 4 .', u'. other features are defined as shown in figure 4 .']",C10-1074deepfigures-results.json,NLP text_grammar_eg,C10-1074
C10-1089.pdf-Figure4.png,"Figure 4: A rule-related error, a critical error, and a parser-related error. Regions removed by the rules are underlined, and target proteins are shown in bold. Predictions, applied rules, and sentence IDs are shown.","[u'. figure 4 shows the rule-related error in bioinfer, one critical error in aimed, and one parser-related error in iepa.']",C10-1089deepfigures-results.json,NLP text_grammar_eg,C10-1089
C10-1089.pdf-Figure5.png,"Figure 5: Correctly simplified cases. The first sentence is a difficult (not PPI) relation, which is typed as Similar in the BioInfer corpus.","[u'.s479) figure 5 : correctly simplified cases.', u'Our rules are applicable to sentences, with little danger of changing the relation-related mentions. Figure 5 shows three successfully simplified cases (""No Error"" cases from']",C10-1089deepfigures-results.json,NLP text_grammar_eg,C10-1089
C10-1113.pdf-Figure1.png,Figure 1: Clustered target concepts,"[u'. the noun clusters represent target concepts that we expect to be associated with the same source concept (some suggested source concepts are given in figure 1 , although the system only captures those implicitly).']",C10-1113deepfigures-results.json,NLP text_grammar_eg,C10-1113
C10-1113.pdf-Figure2.png,Figure 2: Clustered verbs (source domains),[],C10-1113deepfigures-results.json,NLP text_grammar_eg,C10-1113
C10-1113.pdf-Figure3.png,Figure 3: Sentences tagged by the system (metaphors in bold),"[u'. figure 3 : sentences tagged by the system (metaphors in bold) lated to bodily action; (3) more precise (as opposed to vague); (4) historically older.', u'. some examples of sentences annotated by the system are shown in figure 3 .']",C10-1113deepfigures-results.json,NLP text_grammar_eg,C10-1113
C10-2031.pdf-Figure2.png,Figure 2: Four prediction tasks for example (1),"[u'. figure 2 shows the four marked-up prediction tasks resulting for example (1).', u'from their eyes , and having nothing to believe -* -MASKED-* -in any more . Figure 2 : Four prediction tasks for example', u'individual queries, one query for each potential preposition. For example, the second prediction task of Figure 2 yields the cohort of nine queries in', u'. for example, in the first prediction task in figure 2 , the preposition occurs already as the second word in the sentence, thus not leaving enough context to the left of the preposition for a symmetric 7-gram.']",C10-2031deepfigures-results.json,NLP text_grammar_eg,C10-2031
C10-2031.pdf-Figure3.png,Figure 3: Cohort of nine queries resulting for the second prediction task of Figure 2,"[u'yields the cohort of nine queries in Figure 3 below, where the candidate prepositions replace the location marked by -* -MASKED-* -of. The correct']",C10-2031deepfigures-results.json,NLP text_grammar_eg,C10-2031
C10-2046.pdf-Figure5.png,Figure 5: Task scenario in the MO domain. The scenario was originally in Japanese and was translated by the authors.,"[u'. figure 5 : task scenario in the mo domain.', u'. figure 5 shows one of the task scenarios in the mo domain.']",C10-2046deepfigures-results.json,NLP text_grammar_eg,C10-2046
C10-2049.pdf-Figure3.png,Figure 3: A context modeling example.,[u'. consider the example in figure 3 .'],C10-2049deepfigures-results.json,NLP text_grammar_eg,C10-2049
C10-2066.pdf-Figure2.png,Figure 2: Example of speech understanding results in MLMU framework,[u'. figure 2 depicts an example when multiple asrs based on lms and multiple lus are used.'],C10-2066deepfigures-results.json,NLP text_grammar_eg,C10-2066
C10-2067.pdf-Figure1.png,Figure 1: RMRS generated through hybrid parsing.,"[u'. a sample output of the semantic representation generated by the deep parser is shown in figure 1 .', u'. an example of a variant of mrs, a so-called robust mrs (rmrs) has already been depicted in figure 1 .', u'In order to explain the translation process, we will analyze the RMRS depicted in Figure 1 . The RMRS was derived from the MRS of the deep unificationbased parser. We see', u'Giving this information, it should now be clear that the TFS from Figure 1 must be realized as an instance of the OWL class RMRS and that the features', u'. however, if we want to utilize morpho-syntactical information in subsequent inference steps, we have to enrich the above variable classes with further properties/roles, such as tense, pers, or num (see, , the ""structured"" variables in the structure for present v in figure 1 ).', u'. consider the sentence did bernd kiefer present a paper at ijcai 2005? from figure 1 .']",C10-2067deepfigures-results.json,NLP text_grammar_eg,C10-2067
C10-2069.pdf-Figure1.png,"Figure 1: Selected topics from the two collections (each line is one topic, with fewer than ten topic words displayed because of limited space)","[u'produced and candidates for the most representative word are selected by choosing the top-ranked words. Figure 1 : Selected topics from the two collections (each line is one topic, with fewer than', u'. sample topics from these two sets are given in figure 1 .', u'. to better understand what is happening here, consider the following topic from figure 1: health drug patient medical doctor hospital care cancer treatment disease this is clearly a coherent topic, but at least two topic words suggest themselves as labels: health and medic by way of having between 10 and 20 annotators (uniquely) label a given topic, and interpreting the multiple labellings probabilistically, we are side-stepping the inter-annotator agreement issue, but ultimately, for the best-1 evaluation, we are forced to select one term only, and consider any alternative to be wrong.']",C10-2069deepfigures-results.json,NLP text_grammar_eg,C10-2069
C10-2133.pdf-Figure1.png,Figure 1: Example of PE and MED labelings,[u'. figure 1 shows an example of pe and med labelings.'],C10-2133deepfigures-results.json,NLP text_grammar_eg,C10-2133
C10-2133.pdf-Figure2.png,Figure 2: Tagging example of a forum post,[u'. figure 2 shows an example of a forum post (which talks about allergy to dogs) being tagged using our crf model.'],C10-2133deepfigures-results.json,NLP text_grammar_eg,C10-2133
C10-2162.pdf-Figure14.png,Figure 14. Lexical entry extracted for the word /write.,"[u"", we obtain a lexical entry for the word ' /write' as shown in Figure 14 . Some Chinese constructions change the word order of sentences, such as the BA/BEI constructions.""]",C10-2162deepfigures-results.json,NLP text_grammar_eg,C10-2162
C10-2162.pdf-Figure15.png,Figure 15. Application of a lexical rule.,"[u'BA/BEI constructions. For example, by applying a lexical rule to the lexical entry template in Figure 15(a) , the moved object indi-cated by SLASH is restored into RCOMPS, and the subject introduced', u'RCOMPS, and the subject introduced by BEI in LCOMPS is restored into SUBJ (refer to Figure 15(b) ).']",C10-2162deepfigures-results.json,NLP text_grammar_eg,C10-2162
2016.jeptalnrecital-jep.22.pdf-Figure2.png,Figure 2 : Disfluences produites par les personnes qui bgaient (en pourcentages).,"[u'dans la parole des locuteurs qui bgaient mais un degr moindre, comme en atteste la Figure 2 . En effet, les fillers constituent 24% des accidents de parole produits pour cette catgorie']",2016.jeptalnrecital-jep.22deepfigures-results.json,pie chart,2016.jeptalnrecital-jep.22
2016.jeptalnrecital-recital.8.pdf-Figure2.png,FIGURE 2 : Rpartition des principaux phnomnes lexicaux parmi ceux dtects.,[],2016.jeptalnrecital-recital.8deepfigures-results.json,pie chart,2016.jeptalnrecital-recital.8
2019.jeptalnrecital-court.22.pdf-Figure2.png,FIGURE 2: La rpartition des classes dans le corpus de rfrence,"[u'. figure 2) , les opinions positives reprsentent 68% de toutes les valuations annotes, alors que les descriptions ou les intentions en font que 3% et 4%.']",2019.jeptalnrecital-court.22deepfigures-results.json,pie chart,2019.jeptalnrecital-court.22
2020.acl-main.100.pdf-Figure2.png,Figure 2: Distribution of dataset based on topics,"[u'. we then split each document into sentences, with the resultant distribution of medical topics as shown in figure 2 .']",2020.acl-main.100deepfigures-results.json,pie chart,2020.acl-main.100
2020.acl-main.100.pdf-Figure3.png,Figure 3: Distribution of testing set based on PCIO.,"[u'. the strict procedure also discards many aligned groups, leading to 675 annotations for testing, with distribution of medical pcio elements as shown in figure 3 .']",2020.acl-main.100deepfigures-results.json,pie chart,2020.acl-main.100
2020.acl-main.174.pdf-Figure4.png,Figure 4: Average composition of a CSI summary based on different crime-related aspects.,"[u'. in figure 4 we illustrate the average composition of a summary based on the different aspects seen in a crime investigation (, crime scene, victim, cause of death, perpetrator, evidence).']",2020.acl-main.174deepfigures-results.json,pie chart,2020.acl-main.174
2020.acl-main.187.pdf-Figure6.png,Figure 6: Patterns of feedback covered in our dataset. Patterns are extracted heuristically using predicates and arguments extracted from the feedback sentence. The figure shows the top 60 frequent patterns.,"[u'We visualize the distribution of feedback patterns for the top 60 most frequent patterns in Figure 6 , and label the ones shared among multiple patterns. As is shown, our dataset covers']",2020.acl-main.187deepfigures-results.json,pie chart,2020.acl-main.187
2020.acl-main.284.pdf-Figure3.png,Figure 3: Relative frequencies of each tag in the dataset.,[u'. figure 3 shows the average number of words per tag in the dataset.'],2020.acl-main.284deepfigures-results.json,pie chart,2020.acl-main.284
2020.acl-main.361.pdf-Figure3.png,"Figure 3: Evolution of evidence predictions on the development set of CoQA. From the inside to the outside, the four rings correspond to BERT-HA (iteration 0) and BERT-HA+STM (iteration 1, 2, 3), respectively.","[u'in STM, we visualized the evolution of evidence predictions on the development set of CoQA (Figure 3) . From the inside to the outside, the four rings show the statistic results of', u'. more concretely, 27% of the wrong predictions were gradually corrected with high confidence within three self-training iterations, as exemplified by instance a in figure 3 .', u'. we observed that 4% of the evidence was mistakenly revised by stm, as exemplified by instance b in figure 3 .']",2020.acl-main.361deepfigures-results.json,pie chart,2020.acl-main.361
2020.acl-main.402.pdf-Figure3.png,"Figure 3: Statistics : (a) Source across the dataset, (b) Overall speaker distribution.",[],2020.acl-main.402deepfigures-results.json,pie chart,2020.acl-main.402
2020.acl-main.427.pdf-Figure4.png,Figure 4: Frequency of each action type in the different data collection schemes described in Section 3.1.,"[u'. the action frequencies of each subset are shown in figure 4 .', u'. the resulting trees are labeled as otheraction, and their frequency in each dataset in shown in figure 4 .']",2020.acl-main.427deepfigures-results.json,pie chart,2020.acl-main.427
2020.acl-main.476.pdf-Figure1.png,Figure 1: Example of failure and party cleavages.,"[u'. for example, in figure 1b , 70% of democrats vote yea and 80% republicans vote nay on a roll call, then the bill is competitive and the disagreement between the groups is 10% (=80%-70%).', u'. for instance, consider a bill with 55% of democrats voting yea and 45% of them voting nay (figure 1c ).']",2020.acl-main.476deepfigures-results.json,pie chart,2020.acl-main.476
2020.acl-main.506.pdf-Figure3.png,Figure 3: Select results from our survey.,"[u'. a majority of the participants had ""heard"" about ""bayesian hypothesis testing"" but did not know the definition of ""bayes factor"" (figure 3 ).']",2020.acl-main.506deepfigures-results.json,pie chart,2020.acl-main.506
2020.acl-main.678.pdf-Figure4.png,Figure 4: The distributions of different temporal dimensions in the collected data.,[],2020.acl-main.678deepfigures-results.json,pie chart,2020.acl-main.678
2020.acl-main.709.pdf-Figure2.png,"Figure 2: Manual inspection of 100 random sentence pairs from our corpora (NEWSELA-AUTO and WIKIAUTO) and the existing Newsela (Xu et al., 2015) and Wikipedia (Zhang and Lapata, 2017) corpora. Our corpora contain at least 44% more complex rewrites (Deletion + Paraphrase or Splitting + Paraphrase) and 27% less defective pairs (Not Aligned or Not Simpler).","[u'. figure 2 illustrates that wiki-auto contains 75% less defective sentence pairs than the old wikilarge (zhang and lapata, 2017) dataset.']",2020.acl-main.709deepfigures-results.json,pie chart,2020.acl-main.709
2020.acl-main.709.pdf-Figure3.png,"Figure 3: Manual inspection of 100 random sentences generated by Transformerbert trained on NEWSELAAUTO and existing NEWSELA datasets, respectively.","[u'. our manual inspection ( figure 3 ) also shows that transfomer bert trained on newsela-auto performs 25% more paraphrasing and deletions than its variant trained on the previous newsela (xu et , 2015) dataset.']",2020.acl-main.709deepfigures-results.json,pie chart,2020.acl-main.709
2020.acl-main.711.Dataset.pdf-Figure3.png,Figure 3: Pie chart comparing the success rate of all the variations of our model.,"[u'. as shown in figure 3 , our best model (fm) outperforms individual ablation modules.', u'. the ablation component employing just reversal of valence is second best for sarcasticness according to figure 3 .']",2020.acl-main.711.Datasetdeepfigures-results.json,pie chart,2020.acl-main.711.Dataset
2020.acl-main.711.pdf-Figure3.png,Figure 3: Pie chart comparing the success rate of all the variations of our model.,"[u'consider this as the main criterion for the success of generating sarcasm. As shown in Figure 3 , our best model (FM) outperforms individ-', u', our best model (FM) outperforms individ- Figure 3 .']",2020.acl-main.711deepfigures-results.json,pie chart,2020.acl-main.711
2020.bionlp-1.19.pdf-Figure4.png,Figure 4: Results of expert judgement for Type-5 mismatches of the BioBERT-based NER model trained with MedMentions-st21pv dataset.,[],2020.bionlp-1.19deepfigures-results.json,pie chart,2020.bionlp-1.19
2020.bucc-1.3.pdf-Figure3.png,Figure 3: Distribution of accounts according to country,[u'. the distribution of countries is presented in figure 3 .'],2020.bucc-1.3deepfigures-results.json,pie chart,2020.bucc-1.3
2020.bucc-1.3.pdf-Figure4.png,Figure 4: Distribution of accounts according to topic,"[u'topics, for our purposes, a broad understanding of the distribution at the tweet level suffices. Figure 4 shows us the distribution of topics across profiles and']",2020.bucc-1.3deepfigures-results.json,pie chart,2020.bucc-1.3
2020.cl-1.4.pdf-Figure1.png,"Figure 1 Manual categorization of simplification transformations in sample sentences from two simplified versions in the Newsela corpus. Simp-N means sentences from the original article (version 0) automatically aligned with sentences in version-N of the same article. Extracted from Xu, Callison-Burch, and Napoles (2015).","[u'. a manual analysis of 50 random automatically aligned sentence pairs (reproduced in figure 1 ) shows a better presence and distribution of simplification transformations in the newsela corpus.', u'The statistics of Figure 1 show that there is still a preference toward compression and lexical substitution transformations, rather than']",2020.cl-1.4deepfigures-results.json,pie chart,2020.cl-1.4
2020.eamt-1.2.pdf-Figure1.png,Figure 1. Percentage of participants (not) familiar with flooding,"[u'focus of our study-was common where they lived, with 14% not knowing, as shown in Figure  1 .', u'. this is not surprising considering that almost half of them reported living in a country where flooding is common (figure 1 ).']",2020.eamt-1.2deepfigures-results.json,pie chart,2020.eamt-1.2
2020.figlang-1.23.Dataset.pdf-Figure1.png,"Figure 1: Metaphor source domains for the gun debate scenario: DISEASE and CRIME dominate the GOVTO community, whereas BARRIER and WAR are most common within the INDO community.","[u'. in figure 1 , we present the top choices of source domains for metaphors associated with the target concepts, including gun control, gun rights, and gun violence.']",2020.figlang-1.23.Datasetdeepfigures-results.json,pie chart,2020.figlang-1.23.Dataset
2020.figlang-1.23.Dataset.pdf-Figure3.png,Figure 3. Distribution of marriage equality metaphors in the progressive stance sources,"[u'metaphor distribution across the three main stances in the marriage equality domain. The first analysis (Figure 3 ,']",2020.figlang-1.23.Datasetdeepfigures-results.json,pie chart,2020.figlang-1.23.Dataset
2020.figlang-1.23.Dataset.pdf-Figure4.png,Figure 4. Distribution of marriage equality metaphors in the moderate stance sources.,[u'. Figure 4 and'],2020.figlang-1.23.Datasetdeepfigures-results.json,pie chart,2020.figlang-1.23.Dataset
2020.figlang-1.23.Dataset.pdf-Figure5.png,Figure 5. Distribution of marriage equality metaphors in the traditional stance sources.,"[u'mere 5% of metaphors in our data set, with another 20% attributed to ""neutral"" sources. Figure 5 and']",2020.figlang-1.23.Datasetdeepfigures-results.json,pie chart,2020.figlang-1.23.Dataset
2020.figlang-1.23.pdf-Figure1.png,"Figure 1: Metaphor source domains for the gun debate scenario: DISEASE and CRIME dominate the GOVTO community, whereas BARRIER and WAR are most common within the INDO community.","[u'. in figure 1 , we present the top choices of source domains for metaphors associated with the target concepts, including gun control, gun rights, and gun violence.']",2020.figlang-1.23deepfigures-results.json,pie chart,2020.figlang-1.23
2020.figlang-1.23.pdf-Figure3.png,Figure 3. Distribution of marriage equality metaphors in the progressive stance sources,"[u'metaphor distribution across the three main stances in the marriage equality domain. The first analysis (Figure 3 ,']",2020.figlang-1.23deepfigures-results.json,pie chart,2020.figlang-1.23
2020.figlang-1.23.pdf-Figure4.png,Figure 4. Distribution of marriage equality metaphors in the moderate stance sources.,[u'. Figure 4 and'],2020.figlang-1.23deepfigures-results.json,pie chart,2020.figlang-1.23
2020.figlang-1.23.pdf-Figure5.png,Figure 5. Distribution of marriage equality metaphors in the traditional stance sources.,"[u'mere 5% of metaphors in our data set, with another 20% attributed to ""neutral"" sources. Figure 5 and']",2020.figlang-1.23deepfigures-results.json,pie chart,2020.figlang-1.23
2020.figlang-1.29.pdf-Figure2.png,Figure 2: Distribution of annotation labels in COLFVID,"[u'Only 0.59 of the instances received the labels UNDECIDABLE or BOTH (see Figure 2 ), but this is hardly surprising. We nevertheless wanted to include these tags for the']",2020.figlang-1.29deepfigures-results.json,pie chart,2020.figlang-1.29
2020.finnlp-1.7.pdf-Figure4.png,Figure 4: Sentiment Results for the First Half of the Experiments,"[u'. as can be seen in figure 4 below, the three most prevalent non-gaap measures are earnings before interest, tax, depreciation, and amortization (ebitda), earnings before interest and tax (ebit), and free cash flow (fcf).']",2020.finnlp-1.7deepfigures-results.json,pie chart,2020.finnlp-1.7
2020.finnlp-1.7.pdf-Figure5.png,Figure 5: Sentiment Results for All Experiments,"[u'but the distribution is changing. When we compare the midway results with the overall results (Figure 5) , we find that while Earnings before Interest, Tax, Decpreciation and Amortization (EBITDA), Earnings before', u'have decreased by 6% and 4% respectively, while EBIT has grown by 8%, seen in Figure 5 , below. The increase in EBIT and simultaneous decrease in EBITDA suggests that companies are']",2020.finnlp-1.7deepfigures-results.json,pie chart,2020.finnlp-1.7
2020.jeptalnrecital-taln.24.pdf-Figure1.png,FIGURE 1  Rpartition des classes,[],2020.jeptalnrecital-taln.24deepfigures-results.json,pie chart,2020.jeptalnrecital-taln.24
2020.lincr-1.7.pdf-Figure2.png,Figure 2: Distribution of emotions in the corpus,[u'. figure 2 illustrates the distribution of the eight emotions in our corpus by percent.'],2020.lincr-1.7deepfigures-results.json,pie chart,2020.lincr-1.7
2020.lrec-1.114.pdf-Figure2.png,Figure 2: Distribution of metalinguistic labels in VGG.,"[u'. figure 2 illustrates the distribution of meta-linguistic labels in the corpus: half of them correspond to errors, 25% are dialectal variants and 18% are literary and obsolete forms.']",2020.lrec-1.114deepfigures-results.json,pie chart,2020.lrec-1.114
2020.lrec-1.127.pdf-Figure1.png,Figure 1: Speaker affiliations and debate sources.,[u'. the full distributions of speaker affiliations and debate sources are shown in figure 1 .'],2020.lrec-1.127deepfigures-results.json,pie chart,2020.lrec-1.127
2020.lrec-1.129.pdf-Figure1.png,Figure 1: Relation distribution,"[u'3212 relations, of which 1237 are explicit relations (39%) and 1174 are implicit relation (37%) (Figure 1) . The remaining 801 relations include Hypophora, AltLex, EntRel, and NoRel. Among these 4 kinds']",2020.lrec-1.129deepfigures-results.json,pie chart,2020.lrec-1.129
2020.lrec-1.141.pdf-Figure4.png,Figure 4: Distribution of question types based on initial word (and some multi-word expressions).,"[u'.) are with 12% only the fourth most frequent question type, after what-questions, polar questions and how-questions (see figure 4 ).']",2020.lrec-1.141deepfigures-results.json,pie chart,2020.lrec-1.141
2020.lrec-1.144.pdf-Figure1.png,Figure 1: The distribution of types of referring expressions in the chains for original and simplified texts,[u'.  the composition of the chains varies with the complexity of the texts as shown in figure 1 .'],2020.lrec-1.144deepfigures-results.json,pie chart,2020.lrec-1.144
2020.lrec-1.203.pdf-Figure3.png,Figure 3. Emotions Expressed using Rhetorical Questions,[u'. this claim is also supported by our corpus data as in figure 3 .'],2020.lrec-1.203deepfigures-results.json,pie chart,2020.lrec-1.203
2020.lrec-1.260.pdf-Figure2.png,Figure 2: Number of collected manuals in different device categories.,"[u'total, 31,601 repair manuals were collected from the iFixit API in 15 basic categories, see Figure 2 . There is a high variation in the number of steps (average=9.68, me-dian=7.00, variance =']",2020.lrec-1.260deepfigures-results.json,pie chart,2020.lrec-1.260
2020.lrec-1.306.pdf-Figure2.png,Figure 2: Distribution of annotated sentences by frame instance over corpus,[u'. in figure 2 we see a break down of the number and percentage of the corpus composed of annotated sentences by their respective frame instances.'],2020.lrec-1.306deepfigures-results.json,pie chart,2020.lrec-1.306
2020.lrec-1.320.pdf-Figure1.png,"Figure 1: Estimated proportion of pairs that are correctly aligned (category A), partially correct with more information on the Spanish side (category B), partially correct with more information on the Guarani side (category C) or incorrectly aligned (category D) for (a) the automatic method and (b) the manually curated semi-automatic method.","[u'both methods, the fully automatic one and the manually curated semi-automatic one, is shown in Figure 1@dot (a) Automatic method (b) Semi-automatic method', u'(a) Automatic method (b) Semi-automatic method Figure 1 : Estimated proportion of pairs that are correctly aligned (category A), partially correct with more', u'. this is shown in figure 1a .', u'. in this case, the evaluation process found that 84.1% of the pairs were a full match, 13.9% contained more information on the spanish side, 1.7% contained more information on the guarani side, and the pairs that were a complete mismatch dropped to only 0.3%, as shown in figure 1b .']",2020.lrec-1.320deepfigures-results.json,pie chart,2020.lrec-1.320
2020.lrec-1.345.pdf-Figure1.png,Figure 1: UNESCO 2017 World Atlas,[],2020.lrec-1.345deepfigures-results.json,pie chart,2020.lrec-1.345
2020.lrec-1.38.pdf-Figure4.png,Figure 4: Evaluation of the use for training vocabulary based on responses to the question Did you feel that the LingoGameBot helped you to improve your vocabulary?,"[u'. overall, 86% of the respondents evaluated ""open questions"" as useful, 64% evaluated ""closed questions"" as useful, and 58% of the users felt they improved their vocabulary with v-trel (see figure 4 for the distribution of answers).']",2020.lrec-1.38deepfigures-results.json,pie chart,2020.lrec-1.38
2020.lrec-1.398.pdf-Figure1.png,Figure 1: Data distribution in manually created dataset,"[u', while their respective percentages are illustrated in Figure 1 .']",2020.lrec-1.398deepfigures-results.json,pie chart,2020.lrec-1.398
2020.lrec-1.398.pdf-Figure2.png,Figure 2: Automatically created dataset in numbers,"[u': Heuristics types and the counts of the produced pairs Figure 2 : Automatically created dataset in numbers 3 and its percentages are shown in', u"": Automatically created dataset in numbers 3 and its percentages are shown in Figure 2 . We observe a very high agreement of lexicographers' choices with the results of automated""]",2020.lrec-1.398deepfigures-results.json,pie chart,2020.lrec-1.398
2020.lrec-1.398.pdf-Figure3.png,Figure 3: Strong equivalence distribution across domains in manually collected data subset.,"[u'In Figure 3 , we present the distribution of strong equivalence relation across semantic domains (lexicographer files) of']",2020.lrec-1.398deepfigures-results.json,pie chart,2020.lrec-1.398
2020.lrec-1.399.pdf-Figure1.png,Figure 1: Distribution of names with a certain length in characters,"[u'. in figure 1 , a histogram of name length in characters is displayed.']",2020.lrec-1.399deepfigures-results.json,pie chart,2020.lrec-1.399
2020.lrec-1.399.pdf-Figure2.png,Figure 2: Distribution of names over the most prominent languages (more than 30000 names),[u'. we also show the distribution of names over the most prominent languages in figure 2 .'],2020.lrec-1.399deepfigures-results.json,pie chart,2020.lrec-1.399
2020.lrec-1.409.pdf-Figure1.png,Figure 1: Text type distribution in Gigafida 2.0 by number of tokens.,"[u'and Figure 1 , Gigafida 2.0 is mainly comprised of newspapers (amost half of all words), online texts']",2020.lrec-1.409deepfigures-results.json,pie chart,2020.lrec-1.409
2020.lrec-1.414.pdf-Figure1.png,"Figure 1: Distribution of publications on machine translation between regions in ACL, COLING, EACL, NAACL and NIPS proceedings (2010-2017).","[u'of publications in top conferences and journals is very similar for North America and Europe (Figure 1 ). However, it should be noted that the trend in the last two years is']",2020.lrec-1.414deepfigures-results.json,pie chart,2020.lrec-1.414
2020.lrec-1.414.pdf-Figure3.png,Figure 3 : Distribution of publications by region for speech recognition OR text-to-speech OR speech synthesis in the Scopus DB (2010-October 2018).,"[u'.org/?q=speechlabs publications, while for 7,811 (25%) publications at least one author is from north america ( figure 3 ).']",2020.lrec-1.414deepfigures-results.json,pie chart,2020.lrec-1.414
2020.lrec-1.414.pdf-Figure4.png,Figure 4 : Distribution of publications between regions,"[u'. when the number of publications is compared between countries of our study in north america, asia and europe, the leader is asia with 4933 publications, followed by europe with 4394 publications, and north america with 2963 publications (figure 4 ).', u'. figure 4 : distribution of publications between regions when querying for ""information retrieval"" together with ""text"" or ""word"" (2010-november 2018).']",2020.lrec-1.414deepfigures-results.json,pie chart,2020.lrec-1.414
2020.lrec-1.414.pdf-Figure7.png,Figure 7 : Europe's consumption of the global HPC resources (29%) versus HPC resources supplied in Europe (5%).,"[u'. although europe consumes 29% of global hpc resources it supplies less than 5% of them ( figure 7 ).', u"". 27 figure 7 : europe's consumption of the global hpc resources (29%) versus hpc resources supplied in europe (5%).""]",2020.lrec-1.414deepfigures-results.json,pie chart,2020.lrec-1.414
2020.lrec-1.421.pdf-Figure4.png,"Figure 4: Chart showing relations by term, sorted most to least frequently occurring. Data is from 2000-2020. Note isPartWith is not represented in this chart since it is automatically generated.",[u'for these totals and Figure 4 for a breakdown of percentages of specific term use. We expect legacy data entry to'],2020.lrec-1.421deepfigures-results.json,pie chart,2020.lrec-1.421
2020.lrec-1.423.pdf-Figure2.png,Figure 2: LDC publications by source,[],2020.lrec-1.423deepfigures-results.json,pie chart,2020.lrec-1.423
2020.lrec-1.424.pdf-Figure1.png,Figure 1: Distribution of institution types in preliminary classification,"[u'about each identified establishment as well as a seven-category clustering based on institution types (see Figure 1 for a percentage representation of the preliminary institution classification). The seven institution types are: archives,']",2020.lrec-1.424deepfigures-results.json,pie chart,2020.lrec-1.424
2020.lrec-1.46.pdf-Figure2.png,Figure 2: I found the online error detection tool useful. (n=236),[],2020.lrec-1.46deepfigures-results.json,pie chart,2020.lrec-1.46
2020.lrec-1.46.pdf-Figure3.png,Figure 3: I would like to use the online error detection tool for other courses and assignments. (n=236),"[u"". figure 3 : 'i would like to use the online error detection tool for other courses and assignments.""]",2020.lrec-1.46deepfigures-results.json,pie chart,2020.lrec-1.46
2020.lrec-1.51.pdf-Figure6.png,"Figure 6: User feedback on what they liked about our system. We divided into 5 categories (i) Easy to use (Blue), (ii) Quick (Red), (iii) Capable (Yellow), (iv) Experience (Green) (v) Other (Orange).","[u'. this is summarized in figure 6 : users generally liked the concept of an nlie system (capable: 51.8%), and that dialogue created a smooth experience (easy to use: 25.3%, experience: 10.8%).']",2020.lrec-1.51deepfigures-results.json,pie chart,2020.lrec-1.51
2020.lrec-1.570.pdf-Figure2.png,Figure 2: NER types and number of instances,[u'. the entities are distributed over the types as is shown in figure 2 .'],2020.lrec-1.570deepfigures-results.json,pie chart,2020.lrec-1.570
2020.lrec-1.604.pdf-Figure2.png,Figure 2: Experiment result without metaphor,"[u'. as shown in figure 2, the mazajak tool achieved an impressive result on the sentences that do not contain any metaphors, correctly classifying the sentiment of 70% sentences with only 30% error rate.']",2020.lrec-1.604deepfigures-results.json,pie chart,2020.lrec-1.604
2020.lrec-1.604.pdf-Figure3.png,Figure 3: Experiment result with metaphors.,[],2020.lrec-1.604deepfigures-results.json,pie chart,2020.lrec-1.604
2020.lrec-1.608.pdf-Figure2.png,Figure 2: Class distribution of categories,"[u'. consequently, the number of observations per category is strongly imbalanced: pos opinion constitutes the largest percentage at 68.2%, whereas de-scription, being the smallest, at 1.8% (see figure 2 ).']",2020.lrec-1.608deepfigures-results.json,pie chart,2020.lrec-1.608
2020.lrec-1.621.pdf-Figure1.png,Figure 1: Data statistics of multi-domain annotated corpora,[u'. some statistics of this dataset are reported in figure 1 .'],2020.lrec-1.621deepfigures-results.json,pie chart,2020.lrec-1.621
2020.lrec-1.627.pdf-Figure3.png,Figure 3: Observed IAA on the annotation of T1 and T2.,"[u'In Figure 3 it can be seen how a complete agreement was reached on 485 tweets (34.1%), while', u'. figure 3 : observed iaa on the annotation of t1 and t2.']",2020.lrec-1.627deepfigures-results.json,pie chart,2020.lrec-1.627
2020.lrec-1.628.pdf-Figure2.png,Figure 2: Distribution of votes in the final version of the corpus. The numbers 1 to 5 are the different scores the annotators could assign to the humorous tweets.,[u'. figure 2 shows the general distribution of votes for tweets in the corpus.'],2020.lrec-1.628deepfigures-results.json,pie chart,2020.lrec-1.628
2020.lrec-1.650.pdf-Figure3.png,Figure 3: Allocation of the 977 CoNLL test sentences.,[],2020.lrec-1.650deepfigures-results.json,pie chart,2020.lrec-1.650
2020.lrec-1.657.pdf-Figure1.png,Figure 1: Syllables distribution of the read story,[u'. the following figure 1 describes the distribution of syllables in the corpus.'],2020.lrec-1.657deepfigures-results.json,pie chart,2020.lrec-1.657
2020.lrec-1.678.pdf-Figure1.png,"Figure 1: Distribution of verbs in the VQA dataset (Antol et al., 2015a), to be versus other verbs.","[u'. as shown in figure 1 , 43% of questions involve a verb other than ""to be in the vqa dataset.', u'verb has been captured via so-called thematic or semantic to be 57% other verbs 43% Figure 1 : Distribution of verbs in the VQA dataset']",2020.lrec-1.678deepfigures-results.json,pie chart,2020.lrec-1.678
2020.lrec-1.678.pdf-Figure2.png,Figure 2: Distribution of questions in templates. (a) covers all questions while (b) includes questions starting with question word what,[],2020.lrec-1.678deepfigures-results.json,pie chart,2020.lrec-1.678
2020.lrec-1.678.pdf-Figure3.png,Figure 3: Distribution of questions in imSituVQA. (a) covers all questions while (b) includes questions starting with question word what,[u'. figure 3 shows the distribution of imsituvqa questions according to the first question word.'],2020.lrec-1.678deepfigures-results.json,pie chart,2020.lrec-1.678
2020.lrec-1.701.pdf-Figure1.png,Figure 1: Class Proportion.,"[u'is appropriately mentioned with positive lexicon vivid. Thus, not ironic is marked for this instance. Figure 1 shows the statistics of the annotation result with respect to the distribution of different classes']",2020.lrec-1.701deepfigures-results.json,pie chart,2020.lrec-1.701
2020.lrec-1.751.pdf-Figure4.png,Figure 4: Most popular subreddits for genders,"[u'. for instance, figure 4 shows the distribution of genders in red-dust as well as the distribution of most popular subreddits for each gender.']",2020.lrec-1.751deepfigures-results.json,pie chart,2020.lrec-1.751
2020.lrec-1.761.pdf-Figure1.png,Figure 1: Platform wise distribution of the comments in the dataset.,"[u'. we then selected a random subset of 4000 comments, in total, from the three major social media platforms (comment distribution shown in figure 1 ) for manual annotation.']",2020.lrec-1.761deepfigures-results.json,pie chart,2020.lrec-1.761
2020.lrec-1.786.pdf-Figure1.png,Figure 1: Intents in artificial/real training set and VocADom@A4H test set,[u'in the training set partially contributes to boost the seq2seq model performances. As shown in Figure 1 and'],2020.lrec-1.786deepfigures-results.json,pie chart,2020.lrec-1.786
2020.lrec-1.796.pdf-Figure1.png,Figure 1: Gender Distribution in the Artie Bias Corpus.,[],2020.lrec-1.796deepfigures-results.json,pie chart,2020.lrec-1.796
2020.lrec-1.87.pdf-Figure4.png,Figure 4: Breakdown of responses in evaluation data,[u'. figure 4 shows the breakdown of responses in evaluation data.'],2020.lrec-1.87deepfigures-results.json,pie chart,2020.lrec-1.87
2020.nuse-1.pdf-Figure6.png,Figure 6: Fairy tales label distribution of sentences with unanimous inter-annotator agreement.,"[u'. also, the distribution of labels in the dataset is specified in the pie-chart depicted in figure 6 .']",2020.nuse-1deepfigures-results.json,pie chart,2020.nuse-1
2020.lrec-1.875.pdf-Figure8.png,Figure 8: The skewer-search results viewed as a pie chart for be when we choose register as the category.,[u'and Figure 8 . These graphs show at a glance that appears more frequently in the written than'],2020.lrec-1.875deepfigures-results.json,pie chart,2020.lrec-1.875
2020.lrec-1.890.pdf-Figure5.png,Figure 5: Category distribution in selected features,[u'The graph in Figure 5 shows the distribution of selected features within each feature category (see Section 3.3.).'],2020.lrec-1.890deepfigures-results.json,pie chart,2020.lrec-1.890
2020.ngt-1.24.pdf-Figure1.png,Figure 1: Profiling of the throughput during inference on newstest2018 using a 35-6 model.,"[u'. statistics show that the most time-consuming part of the decoding process is the decoder, as presented in figure 1 , so the most efficient optimization is to use a lightweight decoder.']",2020.ngt-1.24deepfigures-results.json,pie chart,2020.ngt-1.24
2020.ngt-1.24.pdf-Figure2.png,"Figure 2: Profiling results of all operations during inference before or after optimizing on newstest2018 using a 9-1 model on a 2080Ti. We performed decoding for ten times to get more convincing results. Before optimizing, the decoding time is 76.9 seconds. The combination of different optimizations reduces the time to 24.9 seconds. MM is matrix multiplication, and CopyBlocks is used in the tensor copy.","[u'. figure 2(a) shows the profiling results for different operations on gpus before optimizing.', u'layer for greedy search and other data transfers with a slight acceleration of about 10%. Figure 2 ', u' Figure 2 : Profiling results of all operations during inference before or after optimizing on newstest2018 using']",2020.ngt-1.24deepfigures-results.json,pie chart,2020.ngt-1.24
2020.osact-1.17.pdf-Figure3.png,Figure 3: Distribution of Negative and Positive Tweets after applied AraNet on Shared-Task TRAIN Data,"[u'. as shown in figure  3 , aranet assigns sensible sentiment labels to the data.']",2020.osact-1.17deepfigures-results.json,pie chart,2020.osact-1.17
2020.osact-1.5.pdf-Figure1.png,Figure 1: Ratio of sarcasm over the dialects.,[u'. figure 1 shows the ratio of sarcasm in the tweets belonging to each dialect.'],2020.osact-1.5deepfigures-results.json,pie chart,2020.osact-1.5
2020.osact-1.5.pdf-Figure2.png,Figure 2: Sentiment distribution over the sarcastic tweets.,[],2020.osact-1.5deepfigures-results.json,pie chart,2020.osact-1.5
2020.osact-1.5.pdf-Figure3.png,Figure 3: The change in sentiment labels between the original and new annotation. The labels above the charts are the original labels.,"[u'. figure 3 shows in the labels.', u'. the other reason that might have caused the labels to change figure 3 : the change in sentiment labels between the original and new annotation.']",2020.osact-1.5deepfigures-results.json,pie chart,2020.osact-1.5
2020.sltu-1.16.pdf-Figure1.png,Figure 1: PARADISEC file types,"[u'.eaf) make up the most substantial portion of transcription file types (see figure 1 ) .', u'recent years, there have been proposals to standardise formats such as XIGT (Goodman et al., Figure 1 : PARADISEC file types 2015) for Interlinear Glossed Text; and to extend XML processing tools']",2020.sltu-1.16deepfigures-results.json,pie chart,2020.sltu-1.16
2020.sltu-1.41.pdf-Figure5.png,Figure 5: Morpho-functional annotations - TC Akan corpus,"[u'Interesting in Figure 5 is in this context not so much how frequently a certain label is used as', u""Which additional features are available is shown in Figure  5 . We find form function pairs for most of the language's Tense-Aspect features, and for""]",2020.sltu-1.41deepfigures-results.json,pie chart,2020.sltu-1.41
2020.trac-1.25.pdf-Figure1.png,Figure 1: Languages in the Dataset,[u'. figure 1 5 shows the share of data in each language.'],2020.trac-1.25deepfigures-results.json,pie chart,2020.trac-1.25
C10-2016.pdf-Figure3.png,Figure 3: The distribution of POS tags based on the output EM algorithm in Chinese using the 500k dataset. Tag T-N-y% means that there are N hidden states mapped to the specific POS tag T accounting for y% of word tokens tagged with these N states by the EM algorithm.,"[u'. figure 3 shows the number of states mapping to different pos tags in chinese over the 500k data size.', u'. figure 3 : the distribution of pos tags based on the output em algorithm in chinese using the 500k dataset.']",C10-2016deepfigures-results.json,pie chart,C10-2016
C10-2016.pdf-Figure4.png,Figure 4: English tag distribution for EM using 500k dataset with 50 states mapping to the 17 pos tag set. Tag T-N-y% means that there are N hidden states mapped to the specific POS tag T accounting for y% of word tokens tagged with these N states.,"[u'. figure 4 : english tag distribution for em using 500k dataset with 50 states mapping to the 17 pos tag set.', u'. with the english 50 tag set with 500k words, we experiment with mapping the english 50 tag set result to the 17 tag set, we see that in figure  4 , 16 (of 50) states mapped to the n tag, accounting for 37% of the words in the dataset.']",C10-2016deepfigures-results.json,pie chart,C10-2016
S19-2097.pdf-Figure5.png,"Figure 5: Shared Sub-Task A, training data instance share (OFF and NOT)",[],S19-2097deepfigures-results.json,pie chart,S19-2097
S19-2097.pdf-Figure6.png,"Figure 6: Shared Sub-Task B, training data instance share (TIN and UNT)",[],S19-2097deepfigures-results.json,pie chart,S19-2097
S19-2097.pdf-Figure7.png,"Figure 7: Shared Sub-Task C, training data instance share (IND, GRP and OTH)",[],S19-2097deepfigures-results.json,pie chart,S19-2097
S19-2210.pdf-Figure5.png,Figure 5: Pie chart showing false / true positives / negatives in the final predictions on the test set.,[u'. figure 5 shows the ratios of true/false positives and true/false negatives.'],S19-2210deepfigures-results.json,pie chart,S19-2210
U14-1014.pdf-Figure2.png,Figure 2: Lines correct following un-tokenised user-interactive error correction,[u'. figure 2 illustrates the overlap between correct lines the two cots systems following untokenised user-interactive error correction.'],U14-1014deepfigures-results.json,pie chart,U14-1014
2015.lilt-12.3.pdf-Figure2.png,"FIGURE 2 Scatter plot of concreteness score vs. publication year for 88 19th century poems, 71 Imagist poems, and 100 contemporary poems. Poems published after the the Imagist movement have significantly higher concreteness scores; however, within each group of poems, there is no positive correlation between publication year and concreteness.","[u"". figure 2 shows the relationship between the poem's publication year and its concreteness.""]",2015.lilt-12.3deepfigures-results.json,scatter plot,2015.lilt-12.3
2015.lilt-12.3.pdf-Figure3.png,"FIGURE 3 Scatter plot of arousal score vs. publication year for 88 19th century poems, 71 Imagist poems, and 100 contemporary poems. Following the Imagist movement, poems scored increasingly lower on emotional arousal.","[u'. we conducted a similar time series analysis for emotional arous we found a strong negative correlation between publication year and arousal across 19th century and contemporary poems (r = 0.33, p < 0.00001), suggesting that poems published later in time tend to express lower emotional arous while there is no significant correlation between publication year and arousal among poems published before 1912 (r = 0.19, p = 0.20), the arousal scores of poems published after 1912 continued to decline (r = 0.21, p = 0.01) (figure 3 ).']",2015.lilt-12.3deepfigures-results.json,scatter plot,2015.lilt-12.3
2020.acl-demos.18.pdf-Figure5.png,"Figure 5: Scatterplot of sentence-level NDCG@10 vs sentence-level BLEU on zh-en and en-gu. For better visualization, only 300 random samples from each language direction are shown.","[u'. as we can see in figure 5 , there is no clear correlation between sentence-level ndcg@10 and sentence-level bleu scores.']",2020.acl-demos.18deepfigures-results.json,scatter plot,2020.acl-demos.18
2020.acl-main.170.pdf-Figure7.png,Figure 7: Visualization of source embeddings. Models trained on WMT14 En-Fr (4m).,"[u'. to confirm this, we reduce dimensionality of embeddings by svd and visualize (figure 7 ).']",2020.acl-main.170deepfigures-results.json,scatter plot,2020.acl-main.170
2020.acl-main.220.pdf-Figure3.png,"Figure 3: From left to right, LDA downsampled representation of BERT on Frames (Goal oriented), MultiWOZ (Goal oriented), PersonaChat (chit-chat) and DailyDialog (chit-chat)",[u'. this gives us a downsampled representation of each utterance u i which we plot as shown in figure 3 .'],2020.acl-main.220deepfigures-results.json,scatter plot,2020.acl-main.220
2020.acl-main.221.pdf-Figure9.png,Figure 9: T-SNE plots of z for four different dialogue acts using two different wKL settings.,[],2020.acl-main.221deepfigures-results.json,scatter plot,2020.acl-main.221
2020.acl-main.312.pdf-Figure2.png,Figure 2: Polarity (top) and attention scores (bottom). Scaled dot product attention is used with  = 10.,"[u'. results on using lstm or the affine transformation layer as the input encoder are similar -setting a proper value for  appears to be cruci figure 2 shows the results for polarity scores and attention scores for the first 3 datasets, when  is set to a moderate value of 10 (,  d).', u'. this is consistent with the attention scores shown in figure 2d : the attention scores of the positive tokens were generally lower than those of the negative tokens.']",2020.acl-main.312deepfigures-results.json,scatter plot,2020.acl-main.312
2020.acl-main.312.pdf-Figure3.png,"Figure 3: Polarity (left) and attention (middle) scores for SST with scaling factor  set to 100. Attention scores (right) for 20News II, with scaling factor  set to 10. Scaled dot product attention is used.","[u'. as we can see from figure 3b , when  is set to 100, the resulting attention scores for the positive tokens are smaller than those of the neutral (and negative) tokens.', u'. however, as we discussed above, when  is very large, the attention mechanism will effectively become mean pooling (we can also see from figure 3b that attentions scores of all tokens are now much smaller), and the overall model would be relying on the average polarity scores of the word tokens in the sentence for making prediction.', u'. interestingly, on the other hand, as we discussed before at the end of section 4.1, when  is large, the polarity tokens will likely end up with polarity scores of large magnitudes -a fact that can also be empirically observed in figure  3a .', u'. as shown in figure 3c , polarity tokens that are strongly associated with specific labels are still likely to have larger attention scores than those of neutral tokens.']",2020.acl-main.312deepfigures-results.json,scatter plot,2020.acl-main.312
2020.acl-main.327.pdf-Figure3.png,Figure 3: Scatter plots of human judgement scores (DA scores) and evaluation scores,"[u'hypothesis sentences, we plot scatters of evaluation scores and human judgement scores (DA scores) in Figure 3 Although in comparison, the evaluation scores of our best model XLM15 hyp+src/hyp+ref are set more']",2020.acl-main.327deepfigures-results.json,scatter plot,2020.acl-main.327
2020.acl-main.333.pdf-Figure1.png,"Figure 1: Correlation between context coherence metric c(r|q) and human ratings without and with fine-tuning of GPT-2. Note that random jitters sampled fromN (0, 0.052) are added to human ratings in visualizing scatter plots showed in this paper to overlapping points.","[u'. also, the results were compared to the previous best-performing au- figure 1 : correlation between context coherence metric c(r|q) and human ratings without and with fine-tuning of gpt-2.', u'In addition, Figure 1 details the effect of fine-Pearson Spearman GPT-2 w/o Fine-tune 0.43 0.32 GPT-2 w/ Fine-tune 0.82']",2020.acl-main.333deepfigures-results.json,scatter plot,2020.acl-main.333
2020.acl-main.333.pdf-Figure2.png,Figure 2: Correlation between response fluency metric f(r) and human ratings for GPT-2 without and with finetuning.,"[u'. in addition, figure 2 details the effect of fine-tuning.']",2020.acl-main.333deepfigures-results.json,scatter plot,2020.acl-main.333
2020.acl-main.333.pdf-Figure3.png,"Figure 3: Correlation between n-gram entropy and human ratings on the baseline dataset, WS dataset and CTG dataset.",[u'. figure 3 display correlations between normalized human ratings and corresponding n-gram entropy based on the augmented dataset.'],2020.acl-main.333deepfigures-results.json,scatter plot,2020.acl-main.333
2020.acl-main.337.pdf-Figure1.png,Figure 1: 2D t-SNE projection of GloVe vectors. The 200 plus symbols (+) represent the word vectors that can be a direct object of the verb play (positive instances) and the 1000 squares ( ) represent other word vectors (negative instances).,[u'. figure 1 shows a 2d projection of word embeddings.'],2020.acl-main.337deepfigures-results.json,scatter plot,2020.acl-main.337
2020.acl-main.343.pdf-Figure4.png,"Figure 4: Visualization in Unimodal Representations. In each subfigure, red, green, and blue points represent the unimodal representations in text, audio, and video, respectively. The first row shows the learned representations from models with the multimodal task only. The second row shows the learned representations from multi-task models. The two subgraphs in the same column contrast each other",[u'. we figure 4 : visualization in unimodal representations.'],2020.acl-main.343deepfigures-results.json,scatter plot,2020.acl-main.343
2020.acl-main.364.pdf-Figure4.png,Figure 4: Visualization of the reconstructed embeddings with their code-book assignment. Best viewed in color.,[u'. figure 4 shows the 2-dimensional projection of the reconstructed embeddings with their assigned code-books.'],2020.acl-main.364deepfigures-results.json,scatter plot,2020.acl-main.364
2020.acl-main.391.pdf-Figure1.png,Figure 1: Hidden representation visualization for encoded grammatically correct and incorrect words.,[u'. figure 1 visualizes hidden representations of bert and fine-tuned bert.'],2020.acl-main.391deepfigures-results.json,scatter plot,2020.acl-main.391
2020.acl-main.416.pdf-Figure2.png,"Figure 2: Number of n-grams in (a) GMB and (b) PMB. Red points are 4-grams, blue points are 3-grams, green points are 2-grams and black points are 1-grams.","[u'Number of n-grams Figure 2(a) shows the number of n-grams across graphs in GMB where the largest size of 4-grams', u'. figure 2(b) shows the number of n-grams across graphs in pmb where the largest size of 4-grams extracted on one graph is 2.2710 3 .']",2020.acl-main.416deepfigures-results.json,scatter plot,2020.acl-main.416
2020.acl-main.416.pdf-Figure3.png,Figure 3: Pearsons r between DSCORER (on 4-grams) and COUNTER (across systems and datasets).,[u'. figure 3 shows the 4-gram correlation between counter and dscorer.'],2020.acl-main.416deepfigures-results.json,scatter plot,2020.acl-main.416
2020.acl-main.428.pdf-Figure2.png,Figure 2: ELI5: Perplexity vs. label repeats as a function of  in the label unlikelihood objective.,[],2020.acl-main.428deepfigures-results.json,scatter plot,2020.acl-main.428
2020.acl-main.428.pdf-Figure3.png,Figure 3: ELI5: Perplexity vs. context repeats as a function of  in the context unlikelihood objective.,[],2020.acl-main.428deepfigures-results.json,scatter plot,2020.acl-main.428
2020.acl-main.447.pdf-Figure3.png,"Figure 3: Word2vec embeddings associated with 20k papers in six AI-related arXiv categories visualized using t-SNE (van der Maaten and Hinton, 2008). Example papers from two randomly selected sub-regions A and B are given in Table 7.","[u'. figure 3 : word2vec embeddings associated with 20k papers in six ai-related arxiv categories visualized using t-sne (van der maaten and hinton, 2008).', u'unique paper identifier, which can be leveraged as paper embeddings. The resulting embeddings shown in Figure 3 and', u'. these often take on the form of: ""in figure 3 , we show the relationship between a and b.', u'where Figure 3 refers to a plot displayed on a separate page. These inline references can be important']",2020.acl-main.447deepfigures-results.json,scatter plot,2020.acl-main.447
2020.acl-main.447.pdf-Figure4.png,Figure 4: Visualization of contextual representations from layer 9 of S2ORC-SCIBERT on numeric surface forms in a subsample of body text from S2ORC. Labels are heuristics based on token-level patterns.,"[u'. figure 4 : visualization of contextual representations from layer 9 of s2orc-scibert on numeric surface forms in a subsample of body text from s2orc.', u'the final 2-3 BERT layers provide embeddings that excel at predictive language modeling; as such, Figure 4 uses embeddings from layer 9 of S2ORC-SCIBERT.']",2020.acl-main.447deepfigures-results.json,scatter plot,2020.acl-main.447
2020.acl-main.47.pdf-Figure4.png,Figure 4: Correlation between the ACC-DAT rate and the rate that the ACC argument is more likely to be topicalized than DAT for each verb. Each plot corresponds to the result of each verb.,"[u'by moving the constituent to the beginning of the sentence and 20 identified by JUMAN Figure 4 : Correlation between the ACC-DAT rate and the rate that the ACC argument is more', u'Claim (ii): Figure 4 shows that the more a verb prefers the ACC-DAT order, the more ACC case tends']",2020.acl-main.47deepfigures-results.json,scatter plot,2020.acl-main.47
2020.acl-main.47.pdf-Figure5.png,"Figure 5: Change of the ACC-DAT order when the ACC argument accompanies an adverbial particle. These results indicate that the ACC argument with an adverbial particle (ACCadv) is more likely to be placed before the DAT argument. In addition, this trend differs for each particle.","[u'analyzed the trend of double object order when a specific case accompanies an adverbial particle. Figure 5 shows the result when the ACC argument accompanies an adverbial particle, and', u'. figure 5 : change of the acc-dat order when the acc argument accompanies an adverbial particle.']",2020.acl-main.47deepfigures-results.json,scatter plot,2020.acl-main.47
2020.acl-main.47.pdf-Figure6.png,"Figure 6: Change of the DAT-ACC order when the DAT argument accompanies an adverbial particle. These results indicate that the DAT argument with an adverbial particle (DATadv) is more likely to be placed before the ACC argument. In addition, this trend differs for each particle.","[u'. the left parts of these figures show the result of clm, and the right part of these figures shows the result of slm.', u'. figure 6 : change of the dat-acc order when the dat argument accompanies an adverbial particle.']",2020.acl-main.47deepfigures-results.json,scatter plot,2020.acl-main.47
2020.acl-main.479.pdf-Figure2.png,Figure 2: Correlation between empirical ratings and predictions of the BERT-LARGE LSTM+ATTENTION model on held-out test items.,[],2020.acl-main.479deepfigures-results.json,scatter plot,2020.acl-main.479
2020.acl-main.479.pdf-Figure8.png,"Figure 8: Mean inference strength ratings for items without context (new) against items with context (original), r = .68.","[u'. d results from no-context experiment figure 8 shows the correlation between the mean inference strength ratings for each item in the experiment from degen (2015) and the mean strength ratings from the new no-context experiment, discussed in section 6.']",2020.acl-main.479deepfigures-results.json,scatter plot,2020.acl-main.479
2020.acl-main.493.pdf-Figure1.png,"Figure 1: t-SNE visualization of head-dependent dependency pairs belonging to selected dependencies in English and French, projected into a syntactic subspace of Multilingual BERT, as learned on English syntax trees. Colors correspond to gold UD dependency type labels. Although neither mBERT nor our probe was ever trained on UD dependency labels, English and French dependencies exhibit cross-lingual clustering that largely agrees with UD dependency labels.","[u'. much previous analysis has been motivated by a desire to explain why bert-like models perform so well on downstream applications in the monolingual setting, which begs the question: what properties of these models make them so crosslingually effective? figure 1 : t-sne visualization of head-dependent dependency pairs belonging to selected dependencies in english and french, projected into a syntactic subspace of multilingual bert, as learned on english syntax trees.', u"". in a key departure from past work, we not only evaluate a probe's performance (on recreating dependency tree structure), but also use the probe as a window into understanding aspects of the representation that the probe was not trained on ( dependency labels; figure 1) ."", u""dependencies from one language, projected into a different language's space (Figure 1) dependencies from one language, projected into a holdout syntactic space trained on all other languages"", u'or cross-lingually, we find that dependencies of the same label share the same cluster (e.g. Figure 1 , which presents both English', u'. clusters around the edges of the figure are primarily type-based ( one cluster for the word for and another for pour), and there is insignificant overlap between clusters with parallel syntactic functions from different languages.']",2020.acl-main.493deepfigures-results.json,scatter plot,2020.acl-main.493
2020.acl-main.493.pdf-Figure10.png,"Figure 10: t-SNE visualization of syntactic differences in all languages we study, projected to 32 dimensions using PCA.","[u'. we find that projected under pca, syntactic difference vectors still cluster into major groups, and major trends are still evident ( figure 10) .', u'. figure 10 : t-sne visualization of syntactic differences in all languages we study, projected to 32 dimensions using pca.']",2020.acl-main.493deepfigures-results.json,scatter plot,2020.acl-main.493
2020.acl-main.493.pdf-Figure5.png,"Figure 5: t-SNE visualization of syntactic differences in Spanish projected into a holdout subspace (learned by a probe trained to recover syntax trees in languages other than Spanish). Despite never seeing a Spanish sentence during probe training, the subspace captures a surprisingly fine-grained view of Spanish dependencies.",[],2020.acl-main.493deepfigures-results.json,scatter plot,2020.acl-main.493
2020.acl-main.493.pdf-Figure6.png,"Figure 6: t-SNE visualization of 100,000 syntactic difference vectors projected into the cross-lingual syntactic subspace of Multilingual BERT. We exclude punct and visualize the top 11 dependencies remaining, which are collectively responsible for 79.36% of the dependencies in our dataset. Clusters of interest highlighted in yellow; linguistically interesting clusters labeled.","[u'dependencies from all languages, projected into a joint syntactic space trained on all languages ( Figure 6 )', u'. the cluster identities largely overlap with (but do not exactly agree with) dependency labels as defined by universal dependencies (figure 6) .', u'. in figure 6 , these differences are colored by gold ud dependency labels.', u't-SNE visualization of syntactic difference vectors projected into the cross-lingual syntactic subspace of Multilingual BERT (Figure 6 ), we only visualize the top 11 relations, excluding punct. This represents 79.36% of the']",2020.acl-main.493deepfigures-results.json,scatter plot,2020.acl-main.493
2020.acl-main.493.pdf-Figure7.png,"Figure 7: t-SNE visualization of dependency headdependent pairs projected into the cross-lingual syntactic subspace of Multilingual BERT. Colors correspond to gold UD dependency type labels, which are unlabeled given that there are 43 in this visualization.","[u'. in figure 7 , we visualize all 36 relations in the dataset.']",2020.acl-main.493deepfigures-results.json,scatter plot,2020.acl-main.493
2020.acl-main.493.pdf-Figure8.png,"Figure 8: t-SNE visualization of head-dependent dependency pairs belonging to selected dependencies in English and French, projected into a syntactic subspace of MBERTRAND, as learned on English syntax trees. Colors correspond to gold UD dependency type labels.","[u'. in appendix a, figure 8 , we provide a similar visualization as applied to mbertrand, finding much less cluster coherence.', u'In Figure 8 , we present a visualization akin to']",2020.acl-main.493deepfigures-results.json,scatter plot,2020.acl-main.493
2020.acl-main.493.pdf-Figure9.png,"Figure 9: Syntactic difference vectors visualized after dimensionality reduction with PCA, instead of t-SNE, colored by UD dependency types. There are no significant trends evident.",[],2020.acl-main.493deepfigures-results.json,scatter plot,2020.acl-main.493
2020.acl-main.496.Dataset.pdf-Figure5.png,Figure 5: Attention or alignment heatmaps generated by different methods on a synthetic 3020 cost matrix.,"[u'. before experimenting with the datasets, we first analyze the alignments obtained by different methods on a synthetic cost matrix in figure 5 .']",2020.acl-main.496.Datasetdeepfigures-results.json,scatter plot,2020.acl-main.496.Dataset
2020.acl-main.496.pdf-Figure5.png,Figure 5: Attention or alignment heatmaps generated by different methods on a synthetic 3020 cost matrix.,"[u'. before experimenting with the datasets, we first analyze the alignments obtained by different methods on a synthetic cost matrix in figure 5 .']",2020.acl-main.496deepfigures-results.json,scatter plot,2020.acl-main.496
2020.acl-main.50.pdf-Figure2.png,Figure 2: Top active users on the midterm topic clustered using UMAP + Mean Shift.,"[u'. figure 2 shows the top users for the ""midterm"" topic projected with umap onto the 2d plane.', u'. after the projection, we use mean shift to cluster the users as shown in figure 2 .']",2020.acl-main.50deepfigures-results.json,scatter plot,2020.acl-main.50
2020.acl-main.574.pdf-Figure3.png,"Figure 3: Visualization of learned representations on CoNLL-03 test dataset. Entity types are represented in different shapes with red for PER, blue for ORG, green for LOC and orange for MISC. Rare entities are represented using bigger points. The points with X are for the delexicalized entities.","[u'in Figure 3 . It shows 2-dimensional projections of randomly sampled 800 entities on CoNLL-03 dataset.', u'. figure 3 clearly shows separability of entities by their entity types but no separations among lowfrequency and frequent entities.', u'. results in figure 3 show that the proposed model consistently outperforms other models, especially in low-resource conditions.']",2020.acl-main.574deepfigures-results.json,scatter plot,2020.acl-main.574
2020.acl-main.595.pdf-Figure5.png,"Figure 5: t-SNE visualisation of H and H produced by the domain independent encoder and sharing encoder. Where green pointsHs. black pointsHt, blue pointsHs , red pointsHt.","[u'. as illustrated in figure 5 (green) and h t (black) have little overlap, indicating the distribution gap between different domains.']",2020.acl-main.595deepfigures-results.json,scatter plot,2020.acl-main.595
2020.acl-main.647.pdf-Figure3.png,"Figure 3: t-SNE projection of BERT representations for the profession professor (left) and for a random sample of all professions (right), before and after the projection.",[],2020.acl-main.647deepfigures-results.json,scatter plot,2020.acl-main.647
2020.acl-main.673.pdf-Figure3.png,Figure 3: Latent spaces t-SNE plots of IDEL on Yelp.,"[u'. the left side of the figure shows the style embedding space, which is well separated into two parts with different colors.', u'call this model IDEL in the following experiments. We encode the same sentences used in Figure 3 , and display the corresponding embeddings in']",2020.acl-main.673deepfigures-results.json,scatter plot,2020.acl-main.673
2020.acl-main.673.pdf-Figure4.png,Figure 4: t-SNE plots of IDEL without I(s; c).,"[u', and display the corresponding embeddings in Figure 4 . Compared with results from the original IDEL, the style embedding space (left in', u'. compared with results from the original idel, the style embedding space (left in figure 4) is not separated in a clean manner.']",2020.acl-main.673deepfigures-results.json,scatter plot,2020.acl-main.673
2020.acl-main.692.pdf-Figure1.png,Figure 1: A 2D visualization of average-pooled BERT hidden-state sentence representations using PCA. The colors represent the domain for each sentence.,"[u'. we show that these models indeed learn to cluster sentence representations to domains without further supervision ( figure 1 ), and quantify this phenomenon by fitting gaussian mixture models (gmms) to the learned representations and measuring the purity of the resulting unsupervised clustering.']",2020.acl-main.692deepfigures-results.json,scatter plot,2020.acl-main.692
2020.acl-main.692.pdf-Figure3.png,Figure 3: A 2D visualization of the unsupervised GMM clustering for the same sentences as in Figure 1.,"[u'. figure  3 also demonstrates the quality of the obtained clusters in 2d using the bert-base model, where the ellipses describe the mean and variance parameters learned for each cluster by the gmm with k = 5.', u'As can be seen in Figure 3 , in some areas the domains are somewhat overlapping in the embedding space, which may', u'. cluster proximity an interesting observation can be made with respect to the visual analysis of the domain clusters as depicted in figure 3 : as the medical cluster (in yellow), law cluster (in purple) and it cluster (in red) are close to each other in the embedding space, their cross-domain bleu scores are also higher.']",2020.acl-main.692deepfigures-results.json,scatter plot,2020.acl-main.692
2020.acl-main.692.pdf-Figure6.png,Figure 6: 2D visualizations of the unsupervised GMM-based clustering for different pretrained MLMs.,[u'. Figure 6 shows visualizations of the multi-domain dataset from additional pre-trained masked language models (BERT large and'],2020.acl-main.692deepfigures-results.json,scatter plot,2020.acl-main.692
W10-3911.pdf-Figure7.png,Figure 7. Precisionrecall distribution.,[u'. the precision-recall distribution of f 1 -scores with feature combination c is presented in figure 7 .'],W10-3911deepfigures-results.json,scatter plot,W10-3911
W12-0202.pdf-Figure4.png,Figure 4: Detailed analysis of the Memogate cluster highlighted in Figure 3 using alternative visual mappings: Sequence of spread over different countries and news sources.,[u'. in figure 4 each article mentioning memogate is represented by a colored icon.'],W12-0202deepfigures-results.json,scatter plot,W12-0202
W12-0203.pdf-Figure1.png,Figure 1: Screencap of Motion Chart for COMPUTER SCREEN,"[u'is available through this visualization, we discuss the weighted chart for the concept COMPUTER SCREEN (Figure 1 shows a screen cap, but we strongly advise to look at the interactive version on']",W12-0203deepfigures-results.json,scatter plot,W12-0203
W12-0203.pdf-Figure2.png,Figure 2: token of beeldscherm with context,"[u'. as figure 2 shows, roling over the bubbles makes the stretch of text visible in which the noun occurs (these contexts are also available in the lower right side bar).']",W12-0203deepfigures-results.json,scatter plot,W12-0203
W12-0203.pdf-Figure3.png,Figure 3: COMPUTER SCREEN tokens stratified by country,"[u'. figure 3 shows that the left-hand side,  monitor-only area of the plot, is also an all-belgian area (hovering over the be value in the legend makes the belgian tokens in the plot flash).']",W12-0203deepfigures-results.json,scatter plot,W12-0203
W12-0203.pdf-Figure4.png,Figure 4: Screencap of Motion Chart for COLLISION,[u'. figure 4 indeed shows that the right side of the chart is almost exclusively populated by botsing tokens.'],W12-0203deepfigures-results.json,scatter plot,W12-0203
W12-0209.pdf-Figure1.png,Figure 1: Linear relation between the average number of words per sentence and number of alignments per sentence,[],W12-0209deepfigures-results.json,scatter plot,W12-0209
W12-2206.pdf-Figure1.png,"Figure 1: Distribution of Gulpease index in the corpus. Document id on x axis, and Gulpease on y axis",[],W12-2206deepfigures-results.json,scatter plot,W12-2206
W12-3309.pdf-Figure4.png,Figure 4. Mean durations vs. Gusev et al. (2011) in log10 seconds,"[u'0.01), which suggests a weak positive correlation. Some of the outliers that we see in Figure 4 correspond to the following verb lemmas: freeze,']",W12-3309deepfigures-results.json,scatter plot,W12-3309
W12-4403.pdf-Figure2.png,"Figure 2: Comparing the predictions for the String Similarity for the same candidates, to the jointly-learned model. (Coding scheme: tp = true-positive, etc.) The distribution shows that while String Similarity correlates with named-entities, it is not a clean division. Note especially the mass of true-negatives in the bottom-right corner of the graph. These would be a relatively high volume of false-positives for String Similarity alone, but the model that bootstraps knowledge of context, word-shape and alignment has little trouble distinguishing them and correctly assigning them zero-probably of being an entity.",[],W12-4403deepfigures-results.json,scatter plot,W12-4403
W13-1801.pdf-Figure4.png,Figure 4: Most probable path accuracy,[u'. figure 4 shows the relative error when using the probability of the most probable path instead of the probability of the consensus string.'],W13-1801deepfigures-results.json,scatter plot,W13-1801
W13-2231.pdf-Figure1.png,"Figure 1: Distribution of [TGT, PE] and [TGT, RT] pairs plotted against the HTER.","[u'. figure 1 , which plots the distribution of positive and negative instances against hter, shows a fairly good separation between the 7 such assumption is supported by the fact that reference sentences are, by definition, free translations manually produced without any influence from the target.']",W13-2231deepfigures-results.json,scatter plot,W13-2231
W13-2231.pdf-Figure2.png,Figure 2: TGT-PE classification in post-editings and rewritings.,"[u'. 10 in figure 2 , the labelling results plotted against the hter show that there is a quite clear separation between [tgt, pe] pairs marked as post-editings (lower hter values) and pairs marked as rewritings (higher hter values).']",W13-2231deepfigures-results.json,scatter plot,W13-2231
W14-0301.pdf-Figure1.png,"Figure 1: The correlation between WQS in a sentence and its overall quality measured by : (a) BLEU, (b) TER and (c) TERp-A metrics","[u'. the results are plotted in figure 1 , where the y axis shows the ""g"" (good) word percentage, and the x axis shows bleu (1a), ter (1b) or terp-a (1c) scores.', u'axis shows BLEU (1a), TER (1b) or TERp-A (1c) scores. It can be seen from Figure 1 that the major parts of points (the densest areas) in all three cases conform the', u'parts of points (the densest areas) in all three cases conform the common tendency: In Figure 1a , the higher ""G"" percentage, the higher BLEU is; on the contrary, in', u', the higher ""G"" percentage, the higher BLEU is; on the contrary, in Figure 1b (', u'( Figure  1c ), the higher ""G"" percentage, the lower TER (TERp-A) is. We notice some outliers, i.e.', u'to represent its characteristics) and the machine learning method (to train the prediction model). Motivated Figure 1 : The correlation between WQS in a sentence and its overall quality measured by :']",W14-0301deepfigures-results.json,scatter plot,W14-0301
W14-1802.pdf-Figure1.png,"Figure 1: Overall human vs. machine scores at the participant level for Stage I (Grade K). Mean and standard deviation for human scores: (8.74, 3.1).",[],W14-1802deepfigures-results.json,scatter plot,W14-1802
W14-1802.pdf-Figure2.png,"Figure 2: Overall human vs. machine scores at the participant level for Stage II (Grades 1-2). Mean and standard deviation for human scores: (7.1, 2.5).",[],W14-1802deepfigures-results.json,scatter plot,W14-1802
W14-1802.pdf-Figure3.png,"Figure 3: Overall human vs. machine scores at the participant level for Stage III (Grades 3-5). Mean and standard deviation for human scores: (9.6, 2.3).",[],W14-1802deepfigures-results.json,scatter plot,W14-1802
W14-1802.pdf-Figure4.png,"Figure 4: Overall human vs. machine scores at the participant level for Stage IV (Grades 6-8). Mean and standard deviation for human scores: (8.3, 2.9).",[],W14-1802deepfigures-results.json,scatter plot,W14-1802
W14-1802.pdf-Figure5.png,"Figure 5: Overall human vs. machine scores at the participant level for Stage V (Grades 9-12). Mean and standard deviation for human scores: (8.9, 2.9).",[],W14-1802deepfigures-results.json,scatter plot,W14-1802
W14-3112.pdf-Figure1.png,"Figure 1: A sample GIB layout from (Rodrigues et al., 2011). The layout visualizes clusters distributed in a treemap structure where the partitions are based on the size of the clusters.",[u'. figure 1 shows a sample group-in-a-box visualization.'],W14-3112deepfigures-results.json,scatter plot,W14-3112
W16-18.pdf-Figure3.png,Figure 3: Distribution of sH  sM according to sNC in pt.,"[u'talk of, as well as speak of -a synonym not mentioned in Oxford English Dictionary. Figure 3 shows that speak of is more widely used than talk about since it may have', u'.  normalized frequency talk about (v) talk of (v) speak of (v) figure 3 : comparison between talk about, talk of and speak of.', u'. figure 3 depicts the rankings of our parameters according to their importance in a feature ablation setting.', u'. in line with results reported in figure 3 , almost all the interactions involved the model parameter.', u'. the former is quite effective at improving the quality of the annotations for these languages, while the are scores evenly distributed? figure 3 shows the widespread distribution of compositionality scores of compounds (x-axis), compared with the combination of heads and modifiers (y-axis).', u'As can be seen in Figure 3 , the average score for each compound can be reasonably approximated by the individual scores']",W16-18deepfigures-results.json,scatter plot,W16-18
W16-18.pdf-Figure4.png,Figure 4: Distribution of NC according to NC in fr.,"[u'. It has many synonym expressions; in Figure 4 we compare it with synonyms we found in the ready-made MWE list', u'. figure 4 : comparison of go wrong with its synonymous expressions.', u'. figure 4 displays the interaction between measure and model when modeling iap.', u'. figure 4 presents the standard deviation for each compound as a function of its average scores.']",W16-18deepfigures-results.json,scatter plot,W16-18
2007.sigdial-1.15.pdf-Figure2.png,"Figure 2: Hassan, a Virtual Human for Tactical Questioning, with some other components",[],2007.sigdial-1.15deepfigures-results.json,Screenshots,2007.sigdial-1.15
2007.sigdial-1.24.pdf-Figure1.png,Figure 1: DEAL user interface,"[u'(a teddy-bear is displayed, see Figure 1 The outcome of the game is affected by what the user says. For example in', u'. however, when the user points out a flaw of the object (the gui displays a teddy-bear with one ear, see figure  1 ) the seller feels obligated to give the user a better price, , his willingness increases.']",2007.sigdial-1.24deepfigures-results.json,Screenshots,2007.sigdial-1.24
2007.sigdial-1.41.pdf-Figure2.png,"Figure 2: Screenshot of the annotation client interface, with video, time-aligned textual annotations, and time series displays.",[u'. a screenshot appears in figure 2 .'],2007.sigdial-1.41deepfigures-results.json,Screenshots,2007.sigdial-1.41
2007.sigdial-1.41.pdf-Figure4.png,Figure 4: Progress of execution of programs on TeraGrid. Table lists file identifiers and status. Graph shows progress.,"[u'. software supports arbitrarily complex workflow specifications, but the current sidgrid interface provides simple support for high degrees of data-parallel processing, as well as a graphical display indicating the progress of the distributed program execution, as shown in figure  4 .']",2007.sigdial-1.41deepfigures-results.json,Screenshots,2007.sigdial-1.41
2007.sigdial-1.43.pdf-Figure1.png,Fig. 1: A screenshot of the Balloon tutor.,[u'The snapshot of Balloon tutor during the handson exercise part can be seen in Figure 1 . The Form tutor includes all the functionality of the Balloon tutor. And a form'],2007.sigdial-1.43deepfigures-results.json,Screenshots,2007.sigdial-1.43
2007.sigdial-1.43.pdf-Figure2.png,Fig. 2: A screenshot of the Form tutor.,[u'. the form tutor is shown in figure 2 .'],2007.sigdial-1.43deepfigures-results.json,Screenshots,2007.sigdial-1.43
2014.lilt-10.1.pdf-Figure1.png,FIGURE 1 Lexical acquisition interface.,[u'. figure 1 shows a screen shot of the lexical entry for detective that prepares the system to analyze compounding configurations using this word.'],2014.lilt-10.1deepfigures-results.json,Screenshots,2014.lilt-10.1
2014.lilt-9.2.pdf-Figure5.png,"FIGURE 5: Entailment Rule application visualized in the tracing tool. The upper pane displays the parse-tree generated by applying the rule. The rule description, highlighted in bold, is the first transformation of the proof, shown in the lower pane. The rule is X (is) accepted in Y ! X (is) oered in Y, and captures, for example, that if someone accepted asylum in Nigeria, than it is also known that he was oered asylum in Nigeria. This rule application is followed by transformations 2 and 3, which are syntactic rewrite rules.","[u"". in the automatic mode, shown in figure 5 , the tool presents the complete process of inference, as conducted by the system's search: the parse trees, the proof steps, the cost of each step and the final score."", u'. next, the user can apply the rule, visually examine its impact on the parse-tree, as in figure 5 , and validate that it operates as intended and does not cause undesired side-eects.']",2014.lilt-9.2deepfigures-results.json,Screenshots,2014.lilt-9.2
2015.jeptalnrecital-court.21.pdf-Figure1.png,"Figure 1. Capture dcran de la page Diko pour lentre  fracture du tibia . Diko est un outil de visualisation et de contribution en ligne pour le rseau lexical JeuxDeMots. On remarquera dans cet exemple que lentre contient  la fois des informations mdicales prcises (voir symptmes, diagnostiques etc.) et des associations dordre plus gnral (voir causes, consquences, etc.).",[],2015.jeptalnrecital-court.21deepfigures-results.json,Screenshots,2015.jeptalnrecital-court.21
2015.jeptalnrecital-court.25.pdf-Figure1.png,FIGURE 1  Outil de slection : vue gnrale,[],2015.jeptalnrecital-court.25deepfigures-results.json,Screenshots,2015.jeptalnrecital-court.25
2015.jeptalnrecital-court.25.pdf-Figure2.png,"FIGURE 2  Linterface utilise pour corriger ltiquetage. Le texte est prsent horizontalement, une ligne par token. Chaque ligne contient la forme, la solution retenue par ltiqueteur (deuxime colonne), et la liste des tiquettes possibles associes  chaque forme. Pour corriger une tiquette, lannotateur clique sur ltiquette dsire dans la liste ou saisira les traits de ltiquette si celle-ci nest pas propose.",[],2015.jeptalnrecital-court.25deepfigures-results.json,Screenshots,2015.jeptalnrecital-court.25
2015.jeptalnrecital-demonstration.10.pdf-Figure1.png,FIGURE 1 : Interface de lapplication,[],2015.jeptalnrecital-demonstration.10deepfigures-results.json,Screenshots,2015.jeptalnrecital-demonstration.10
2015.jeptalnrecital-recital.5.pdf-Figure1.png,FIGURE 1  Capture dcran du prototype de navigation jointe.,[],2015.jeptalnrecital-recital.5deepfigures-results.json,Screenshots,2015.jeptalnrecital-recital.5
2015.jeptalnrecital-recital.5.pdf-Figure2.png,FIGURE 2  Capture dcran du prototype dalignement.,[],2015.jeptalnrecital-recital.5deepfigures-results.json,Screenshots,2015.jeptalnrecital-recital.5
2016.jeptalnrecital-poster.13.pdf-Figure2.png,FIGURE 2: Pistes d'annotation (le temps s'coule ici verticalement avec le logidiel iLex),[],2016.jeptalnrecital-poster.13deepfigures-results.json,Screenshots,2016.jeptalnrecital-poster.13
2016.lilt-14.4.pdf-Figure3.png,FIGURE 3 Autocoding in UAM Corpus Tool,[],2016.lilt-14.4deepfigures-results.json,Screenshots,2016.lilt-14.4
2016.lilt-14.4.pdf-Figure4.png,FIGURE 4 Partial automatic annotation,[],2016.lilt-14.4deepfigures-results.json,Screenshots,2016.lilt-14.4
2019.jeptalnrecital-demo.5.pdf-Figure3.png,FIGURE 3  Sentiment Aware Map : Rsultats de recherche,[],2019.jeptalnrecital-demo.5deepfigures-results.json,Screenshots,2019.jeptalnrecital-demo.5
2019.jeptalnrecital-demo.5.pdf-Figure4.png,FIGURE 4  Revues utilisateurs enrichies par ABSA,[],2019.jeptalnrecital-demo.5deepfigures-results.json,Screenshots,2019.jeptalnrecital-demo.5
2019.jeptalnrecital-demo.5.pdf-Figure6.png,FIGURE 6  Configuration du profil dintention,[],2019.jeptalnrecital-demo.5deepfigures-results.json,Screenshots,2019.jeptalnrecital-demo.5
2020.acl-demos.11.pdf-Figure2.png,"Figure 2: User-facing views of knowledge networks constructed with events automatically extracted from multimedia multilingual news reports. We display the event arguments, type, summary, similar events, as well as visual knowledge extracted from the corresponding image and video.","[u"". we use russia-ukraine conflicts of 2014-2015 as a case study, and develop a knowledge exploration interface that recommends events related to the user's ongoing search based on previously-selected attribute values and dimensions of events being viewed 7 , as shown in figure 2 ."", u'. in response to user queries, the system recommends information around a primary event and its connected events from the knowledge graph (screenshot in figure 2 ).', u'. the demo is publicly available 15 with a user interface as shown in figure 2 , displaying extracted text entities and events across languages, visual entities, visual entity linking and coreference results from face, landmark and flag recognition, and the results of grounding text entities to visual entities.']",2020.acl-demos.11deepfigures-results.json,Screenshots,2020.acl-demos.11
2020.acl-demos.13.pdf-Figure2.png,"Figure 2: BENTO Web Interface. The interface can be roughly divided into three parts from left to right: tool panel, canvas panel and worksheet panel. The tool panel lists the current available tools organized in a tree view. The canvas panel contains the flowchart of the current pipeline. Every node represents a tool or dataset and each connection indicates the data flow in the pipeline.3 This figure shows an example of the pipeline for entity and relation extraction. The worksheet panel displays the content of the CodaLab worksheet such as bundles and their UUIDs.","[u'As shown in Figure 2 , the user interface of our platform is a web application that can be roughly', u'. a node, shown in the workflow figure 2 , contains several input and output ports, corresponding to the inputs and outputs of the tool.', u'linked together to form a pipeline and the connections represent the data flow during execution (Figure 2) . A connection starts from an output port and ends in an input port. An', u'. figure 2 .', u'CodaLab commands from input arguments. For example, the CodaLang expression for the node NER in Figure 2 is shown in', u': The CodaLab commands generated from the pipeline in Figure 2 . Two CodaLab commands are generated based on two steps in the pipeline, namely NER', u'to CodaLab commands according to their topological order in the graph. Take the pipeline in Figure 2 as an example, its corresponding CodaLab commands shown in', u'. the two codalab commands correspond to the two tool nodes in the pipeline of figure 2 .']",2020.acl-demos.13deepfigures-results.json,Screenshots,2020.acl-demos.13
2020.acl-demos.13.pdf-Figure3.png,"Figure 3: The CodaLang expression for the tool NER in Figure 2. The expression can be roughly split into three sections indicated by the dashed squares. The first section declares the arguments of this tool. As seen, the tool takes three bundles as inputs: config, input and pretrained model. The second section declares a constant code which is initialized with an existing bundle. The third section is a string template for generating the CodaLab command.","[u'. users can edit the tool by clicking the editor button ( ) on the top right corner and the node will be toggled to an editor interface (figure 3) .', u'is shown in Figure 3 . The configuration is composed of three sections which are highlighted with dotted squares. The', u'. the first command is generated from the tool ner based on its tool configuration in figure 3 .']",2020.acl-demos.13deepfigures-results.json,Screenshots,2020.acl-demos.13
2020.acl-demos.14.pdf-Figure3.png,"Figure 3: Sta n z a annotates a German sentence, as visualized by our interactive demo. Note am is expanded into syntactic words an and dem before downstream analyses are performed.",[u'. an example of running sta n z a on a german sentence can be found in figure 3 .'],2020.acl-demos.14deepfigures-results.json,Screenshots,2020.acl-demos.14
2020.acl-demos.17.pdf-Figure1.png,Figure 1: The screenshot of the system LinggleWrite,"[u'. for example, suggestions for ""finish"" are shown in section a of figure 1 (bottom left).', u'. the assessment is provided in the form of cefr levels 1 (a1-c2) as shown in section b of figure 1 (top right).', u'. sentences with potential errors are marked with yellow (1 possible error) or orange (2 or more possible errors) background, as shown in section c of figure 1 (center right).', u'. lingglewrite marks suspicious words with orange, red or green, suggesting to insert a word, delete the word, or replace the word respectively, as shown in section c of figure 1 (center right).', u'. an example of corrective suggestions for the sentence ""i finished school on june"" is shown in section e in figure 1 (bottom right).', u'has different query functions and operators to search word usage in context as shown in Figure 1 . These query functions enable the system to query zero, one or multiple words. For']",2020.acl-demos.17deepfigures-results.json,Screenshots,2020.acl-demos.17
2020.acl-demos.19.pdf-Figure3.png,Figure 3: The interface of the Interactive Tool.,"[u'As shown in Figure 3 , researchers can customize their dialogue system by selecting the dataset and the model of', u'. an example is shown in figure 3 , in which at first the bertnlu falsely identified the domain as restaurant.']",2020.acl-demos.19deepfigures-results.json,Screenshots,2020.acl-demos.19
2020.acl-demos.23.Dataset.pdf-Figure1.png,"Figure 1: The main web interface of our diacritization tool, showing the automatic diacritized text (A) and allowing the user to proofread and potentially correct the text. The user can navigate the words using the mouse or the left/right keys, and can select an alternate diacritization option from the listbox on the left (B) using either the mouse or the up/down keys. Changes for a given word can be marked for application over the entire text (C), and are marked in color (not shown in this example). The user can also choose to see the morphological analysis of each form (D). The resulting diacritized text can be exported to various formats (E).","[u'for the user to input a text for diacritization and refine the resulting diacritized text (Figure 1) . Our system parses the text and automatically adds diacritics throughout. Afterward, the user can']",2020.acl-demos.23.Datasetdeepfigures-results.json,Screenshots,2020.acl-demos.23.Dataset
2020.acl-demos.23.Dataset.pdf-Figure2.png,"Figure 2: Diacritization of the fictional word ! in two different contexts, with two different prefixes; the word is diacritized as expected in both contexts.",[u'. our system handles both correctly (figure 2 ).'],2020.acl-demos.23.Datasetdeepfigures-results.json,Screenshots,2020.acl-demos.23.Dataset
2020.acl-demos.23.Dataset.pdf-Figure3.png,Figure 3: Integrated Biblical quote marked with font.,[],2020.acl-demos.23.Datasetdeepfigures-results.json,Screenshots,2020.acl-demos.23.Dataset
2020.acl-demos.23.pdf-Figure1.png,"Figure 1: The main web interface of our diacritization tool, showing the automatic diacritized text (A) and allowing the user to proofread and potentially correct the text. The user can navigate the words using the mouse or the left/right keys, and can select an alternate diacritization option from the listbox on the left (B) using either the mouse or the up/down keys. Changes for a given word can be marked for application over the entire text (C), and are marked in color (not shown in this example). The user can also choose to see the morphological analysis of each form (D). The resulting diacritized text can be exported to various formats (E).","[u'for the user to input a text for diacritization and refine the resulting diacritized text (Figure 1) . Our system parses the text and automatically adds diacritics throughout. Afterward, the user can']",2020.acl-demos.23deepfigures-results.json,Screenshots,2020.acl-demos.23
2020.acl-demos.23.pdf-Figure2.png,"Figure 2: Diacritization of the fictional word ! in two different contexts, with two different prefixes; the word is diacritized as expected in both contexts.",[u'. our system handles both correctly (figure 2 ).'],2020.acl-demos.23deepfigures-results.json,Screenshots,2020.acl-demos.23
2020.acl-demos.23.pdf-Figure3.png,Figure 3: Integrated Biblical quote marked with font.,[],2020.acl-demos.23deepfigures-results.json,Screenshots,2020.acl-demos.23
2020.acl-demos.25.pdf-Figure3.png,Figure 3: The user teaches the value concept commute time by demonstrating querying the value in Google Maps. SUGILITE highlights all the duration values on the Google Maps GUI.,"[u'. when the user demonstrates a value query (, finding out the value of the temperature), sugilite highlights the gui elements showing values with the compatible types (see figure 3 ) to assist the user in finding the appropriate gui element during the demonstration.']",2020.acl-demos.25deepfigures-results.json,Screenshots,2020.acl-demos.25
2020.acl-demos.26.pdf-Figure5.png,Figure 5: Sherlock Holmes webpage demo with wikipedia knowledge example.,[],2020.acl-demos.26deepfigures-results.json,Screenshots,2020.acl-demos.26
2020.acl-demos.26.pdf-Figure6.png,Figure 6: Document Auto-completion webpage demo with user input knowledge passage.,[],2020.acl-demos.26deepfigures-results.json,Screenshots,2020.acl-demos.26
2020.acl-demos.3.pdf-Figure1.png,Figure 1: Syntactic Search System,[u'. figure 1 (next page) shows the interface of our web-based system.'],2020.acl-demos.3deepfigures-results.json,Screenshots,2020.acl-demos.3
2020.acl-demos.33.Dataset.pdf-Figure2.png,"Figure 2: The screenshot of Clinical-Coder system, the English version can be found in the appendix A. (a) gives the predicted diseases after users enter the clinical notes which contains four parts, admission situation, admission diagnosis, discharge situation and discharge diagnosis. (b1) and (b2) are the visualization of supporting information for predictions.","[u'. we refer to the non-continuous figure 2 : the screenshot of clinical-coder system, the english version can be found in the appendix a.', u'. figure 2 illustrates an example of the automatic coding for a chinese clinical note in our system (for the convenience of readers, the english version is included in the appendix a).', u'convenience of readers, the English version is included in the appendix A). The left of Figure 2 (a) is the free-text notes user entered, and the right of', u'(a) is the free-text notes user entered, and the right of Figure 2 Automatic ICD coding has recently been a research hotspot in the field of clinical medicine,', u'. figure 2 illustrates the user interface of our system.', u'. the left of figure 2(a) displays the user input.', u'. the predicted labels are presented in the list of figure 2(a) , including disease name and homologous icd codes.', u'. as shown in figure 2 (b1) , the gram in disease name is highlighted to give a hint to users if it appears in the clinical text.', u'. as shown in figure 2 (b2) , the red background is attention distribution, and the darker the color is, the more useful the word is to predict the current label.', u'A Appendix: English Version of Figure  2 (a) Since the online system is for Chinese clinical notes, we provide corresponding English version']",2020.acl-demos.33.Datasetdeepfigures-results.json,Screenshots,2020.acl-demos.33.Dataset
2020.acl-demos.33.pdf-Figure2.png,"Figure 2: The screenshot of Clinical-Coder system, the English version can be found in the appendix A. (a) gives the predicted diseases after users enter the clinical notes which contains four parts, admission situation, admission diagnosis, discharge situation and discharge diagnosis. (b1) and (b2) are the visualization of supporting information for predictions.","[u'. we refer to the non-continuous figure 2 : the screenshot of clinical-coder system, the english version can be found in the appendix a.', u'. figure 2 illustrates an example of the automatic coding for a chinese clinical note in our system (for the convenience of readers, the english version is included in the appendix a).', u'convenience of readers, the English version is included in the appendix A). The left of Figure 2 (a) is the free-text notes user entered, and the right of', u'(a) is the free-text notes user entered, and the right of Figure 2 Automatic ICD coding has recently been a research hotspot in the field of clinical medicine,', u'. figure 2 illustrates the user interface of our system.', u'. the left of figure 2(a) displays the user input.', u'. the predicted labels are presented in the list of figure 2(a) , including disease name and homologous icd codes.', u'. as shown in figure 2 (b1) , the gram in disease name is highlighted to give a hint to users if it appears in the clinical text.', u'. as shown in figure 2 (b2) , the red background is attention distribution, and the darker the color is, the more useful the word is to predict the current label.', u'A Appendix: English Version of Figure  2 (a) Since the online system is for Chinese clinical notes, we provide corresponding English version']",2020.acl-demos.33deepfigures-results.json,Screenshots,2020.acl-demos.33
2020.acl-demos.36.pdf-Figure3.png,Figure 3: The user interface of the application. a) the Discover tab lists the retrieved papers that are similar to the manuscript. b) the Read tab allows users to view papers and to highlight the sentences that are similar to the selected text in the manuscript.,"[u'formatting text, and inserting L A T E X equations, code snippets, or bullet points (Figure 3a, left) , and (2) a literature explorer encompassing multiple components, which can be accessed on their', u'(2) a literature explorer encompassing multiple components, which can be accessed on their respective tabs (Figure 3a, right) :', u'the sentences in the paper that are similar to the selected text in the manuscript (Figure 3b, right) A search can be initiated without keyword filters by clicking the ""Similar papers to the', u'paragraphs) from the manuscript, which reveals a hovering menu over the selected text (visible in Figure 3b ). Clicking on the magnifying glass icon on this menu performs a search using the', u'. alternatively, the user can select a part of the manuscript and press the marker icon on the revealed hovering menu (figure 3b ) to highlight the sentences that are most similar to the selection.', u'. the user can also add papers to the library manually by entering a b figure 3 : the user interface of the application.']",2020.acl-demos.36deepfigures-results.json,Screenshots,2020.acl-demos.36
2020.acl-demos.4.pdf-Figure1.png,Figure 1: Screenshot of the application with the card Movie Star.,[],2020.acl-demos.4deepfigures-results.json,Screenshots,2020.acl-demos.4
2020.acl-demos.4.pdf-Figure2.png,Figure 2: Screenshot of the application with the card George R. R. Martin.,[],2020.acl-demos.4deepfigures-results.json,Screenshots,2020.acl-demos.4
2020.acl-demos.40.pdf-Figure1.png,Figure 1: A query-based UI for NSTM showing two themes. The un-cropped screenshot is in Appendix C.,[],2020.acl-demos.40deepfigures-results.json,Screenshots,2020.acl-demos.40
2020.acl-demos.40.pdf-Figure4.png,Figure 4: Screenshot (taken on 29 January 2020) of a query-driven interface for NSTM showing the overview for the company Amazon.com.,[],2020.acl-demos.40deepfigures-results.json,Screenshots,2020.acl-demos.40
2020.acl-demos.40.pdf-Figure5.png,Figure 5: Screenshot (taken on 29 January 2020) of a query-driven interface for NSTM showing the overview for the topic Electric Vehicles.,[],2020.acl-demos.40deepfigures-results.json,Screenshots,2020.acl-demos.40
2020.acl-demos.40.pdf-Figure6.png,Figure 6: Screenshot (taken on 29 January 2020) of a query-driven interface for NSTM showing the overview for the region Canada.,[],2020.acl-demos.40deepfigures-results.json,Screenshots,2020.acl-demos.40
2020.acl-demos.40.pdf-Figure8.png,Figure 8: Screenshot (taken on 29 January 2020) of a context-driven application of NSTM. In the Security column are the companies that have seen the largest increase in news readership over the last day. Each entry in the News Summary column is the summary of the top theme provided by NSTM for the adjacent company.,[u'.    d screenshots of a context-driven user interface figure 8 : screenshot (taken on 29 january 2020) of a context-driven application of nstm.'],2020.acl-demos.40deepfigures-results.json,Screenshots,2020.acl-demos.40
2020.acl-demos.40.pdf-Figure9.png,Figure 9: Screenshot (taken on 29 January 2020) of a context-driven application of NSTM. In the News Topic column are the topics that have seen the largest volume of news readership over the past 8 hours. Each entry in the News Summary column is the summary of the top theme provided by NSTM for the adjacent topic.,[u'. figure 9 : screenshot (taken on 29 january 2020) of a context-driven application of nstm.'],2020.acl-demos.40deepfigures-results.json,Screenshots,2020.acl-demos.40
2020.acl-demos.42.pdf-Figure3.png,Figure 3: The workflow to annotate a NE label and trigger span. (Rumble Fish as Restaurant).,[],2020.acl-demos.42deepfigures-results.json,Screenshots,2020.acl-demos.42
2020.acl-demos.42.pdf-Figure4.png,Figure 4: The workflow to annotate a relation label and NL explanation. (per:nationality as a relation label between Tahawwur Hussain Rana and Canadian).,[],2020.acl-demos.42deepfigures-results.json,Screenshots,2020.acl-demos.42
2020.acl-demos.43.pdf-Figure1.png,"Figure 1: Screenshots of the news chatbot (a) Homepage lists most recently active chatrooms (Zone 1 is an example chatroom) (b) Newly opened chatroom: Zone 2 is an event message, Zone 3 the Question Recommendation module, and Zone 4 a text input for user-initiated questions. Event messages are created via abstractive summarization. (c) Conversation continuation with Q&A examples. Sentences shown are extracted from original articles, whose sources are shown. Answers to questions are bolded.","[u'. some of the top stories at the time of writing are shown in figure 1 (a).', u'information and the system delivers in- formation in the form of news content. The homepage (Figure 1(a) ) lists the most active stories, and a user can select a story to enter', u'most active stories, and a user can select a story to enter its respective chatroom (Figure 1(b) ). The separation into story-specific rooms achieves two objectives:', u'Zone 2 in Figure 1 (b) gives an example of an event message. The event messages form a chronological timeline', u'. a chatroom conversation starts with the system showing the two most recent event messages of the story (figure 1(b) ).', u'. a text box (zone 4 in figure 1 (b)) can be used to ask any free-form question about the story.', u'. figure 1 (c) shows several q&a exchanges.', u'. a list of three questions generated by the algorithm described in section 3 is suggested to the user at the bottom of the conversation (zone 3 in figure 1(b) ).', u'NOQR: No questions are recommended, and the Question Recommendation module (Zone 3 in Figure 1(b) ) is hidden (N=22).']",2020.acl-demos.43deepfigures-results.json,Screenshots,2020.acl-demos.43
2020.acl-demos.5.pdf-Figure2.png,Figure 2: In-paper Search Page of Talk to Paper.,"[u'. we provide a similar interface to find the answer inside a specific paper as shown in figure 2 .', u'. the user can either read the paper or uses in-paper search,  what is the main contribution? to let talk to paper auto scroll and highlight relevant answer spans (figure 2) .']",2020.acl-demos.5deepfigures-results.json,Screenshots,2020.acl-demos.5
2020.acl-demos.5.pdf-Figure3.png,Figure 3: Main search page of Talk to Paper.,"[u'The main search page is similar to the standard Google-like search interface as shown in Figure 3 , including input search box and query auto completion (based on generated questions from GPT-2).The']",2020.acl-demos.5deepfigures-results.json,Screenshots,2020.acl-demos.5
2020.acl-demos.5.pdf-Figure4.png,Figure 4: Annotation page of Talk to Paper.,[u'to annotate the question and answer spans in the in-paper search page as shown in Figure 4 . All annotated data are visible in the preview page. If a user wants to'],2020.acl-demos.5deepfigures-results.json,Screenshots,2020.acl-demos.5
2020.acl-demos.6.pdf-Figure2.png,Figure 2: User interface of SyntagRank when the Italian language is selected and the sentence Edison invento la lampadina (Edison invented the light bulb) is typed as input query. Disambiguation results are displayed in extended view by default. Overlaying letters over the image are detailed in Section 4.,[],2020.acl-demos.6deepfigures-results.json,Screenshots,2020.acl-demos.6
2020.acl-demos.6.pdf-Figure3.png,Figure 3: User interface of the SyntagNet Explorer when the English word mouse is typed as input query.,"[u'. by typing into the query bar a word or mwe which is present in syntagnet 9 (an autocomplete function will provide the user with search suggestions), the interface will switch to the syntagnet explorer (figure 3) .']",2020.acl-demos.6deepfigures-results.json,Screenshots,2020.acl-demos.6
2020.acl-main.219.pdf-Figure6.png,Figure 6: Instructions pane for crowdworkers when collecting the second round of dialogue.,[],2020.acl-main.219deepfigures-results.json,Screenshots,2020.acl-main.219
2020.acl-main.219.pdf-Figure7.png,Figure 7: Instructions pane for crowdworkers when collecting the third round of dialogue.,[],2020.acl-main.219deepfigures-results.json,Screenshots,2020.acl-main.219
2020.acl-main.328.pdf-Figure7.png,Figure 7: HIT screenshots.,"[u'. screenshots of our data collection ui are shown in figure 7 and 8 in the appendix.', u'We show several screenshots of our HIT in Figure  7 and 8.']",2020.acl-main.328deepfigures-results.json,Screenshots,2020.acl-main.328
2020.acl-main.328.pdf-Figure8.png,Figure 8: HIT screenshots.,[],2020.acl-main.328deepfigures-results.json,Screenshots,2020.acl-main.328
W19-7513.pdf-Figure1.png,Figure 1: Input,[],W19-7513deepfigures-results.json,Screenshots,W19-7513
W19-7513.pdf-Figure2.png,Figure 2: Output,[],W19-7513deepfigures-results.json,Screenshots,W19-7513
W19-8010.pdf-Figure1.png,"Figure 1: Main view in tree mode (note the multitoken word at nodes 9 and 10). The search buttons can be hidden to have place for deeper trees. Morpho-syntactic features can be displayed or hidden (to gain vertical space). Search results are highlighted (here dormito). The horizontal and vertical spacing can be modified to accomodate longer sentences. In order to change the font size, a .css style sheet can be modified. A help-Button provides detailed help on how to use the editor interface. (Example taken from the UD Italian-ISDT treebank).",[u'. figure 1 : main view in tree mode (note the multitoken word at nodes 9 and 10).'],W19-8010deepfigures-results.json,Screenshots,W19-8010
W19-8010.pdf-Figure2.png,"Figure 2: Dependency label edit window. All possible labels are proposed via autocompletion. Invalid labels, however, are accepted.",[],W19-8010deepfigures-results.json,Screenshots,W19-8010
W19-8010.pdf-Figure3.png,"Figure 3: Dependency tree in flat mode. Multitoken words are marked as in tree mode. In flat mode, enhanced dependencies are also displayed and can be edited.","[u'. figure 3 shows the flat mode (here, the search functions are hidden).']",W19-8010deepfigures-results.json,Screenshots,W19-8010
W19-8010.pdf-Figure4.png,"Figure 4: Word edit window (proposed a second method of editing enhanced dependencies). For UPOS, XPOS and dependency relation labels, the editor proposes autocompletion of valid values.",[],W19-8010deepfigures-results.json,Screenshots,W19-8010
W19-8603.pdf-Figure1.png,Figure 1: Web application: a production session,[],W19-8603deepfigures-results.json,Screenshots,W19-8603
W19-8908.pdf-Figure2.png,Figure 2: Choosing language.,[u'The first screen of the system (see Figure 2 ) asks the user to choose language and to set the summary length (if a'],W19-8908deepfigures-results.json,Screenshots,W19-8908
W19-8908.pdf-Figure3.png,"Figure 3: Choosing texts, reference and system summary.","[u'. figure 3 shows the input selection interface for the case of a corpus.', u'. figure 3 : choosing texts, reference and system summary.']",W19-8908deepfigures-results.json,Screenshots,W19-8908
W19-8908.pdf-Figure4.png,Figure 4: Rouge metrics computation with comparison to baselines.,[],W19-8908deepfigures-results.json,Screenshots,W19-8908
W19-8908.pdf-Figure5.png,Figure 5: Readability metrics computation.,[],W19-8908deepfigures-results.json,Screenshots,W19-8908
W19-8908.pdf-Figure6.png,Figure 6: Topic metrics computation.,[],W19-8908deepfigures-results.json,Screenshots,W19-8908
W19-8908.pdf-Figure7.png,Figure 7: OCCAMS summary.,[],W19-8908deepfigures-results.json,Screenshots,W19-8908
Y02-1044.pdf-Figure4.png,Figure 4: Language Specification of Sub-Lexica,[],Y02-1044deepfigures-results.json,Screenshots,Y02-1044
Y02-1044.pdf-Figure5.png,Figure 5: XML-Based DTD Manager,"[u'. based on the xml and xsl, diet enables developers to define the dtd, as shown in figure 5 .']",Y02-1044deepfigures-results.json,Screenshots,Y02-1044
Y02-1044.pdf-Figure6.png,Figure 6: XSL-Based Style Manager,[u'. Figure 6 : XSL-Based Style Manager'],Y02-1044deepfigures-results.json,Screenshots,Y02-1044
Y05-1024.pdf-Figure2.png,Figure 2: The screen of the cellular phone displaying the search result.,[u'. figure 2 shows the screen of the cellular phone displaying the search result.'],Y05-1024deepfigures-results.json,Screenshots,Y05-1024
Y05-1024.pdf-Figure3.png,Figure 3: The main page of our system.,"[u""user first accesses the system's main page of our system with the cellular phone ( Figure  3 ). The page contains two hyperlinks along with brief instructions and query examples.""]",Y05-1024deepfigures-results.json,Screenshots,Y05-1024
Y05-1024.pdf-Figure5.png,Figure 5: The body of the passage displayed when the user selects the title in Figure 4.,"[u'passage from the result list, the user retrieves the corresponding body of the passage ( Figure 5 ). If the result list contains no relevant passages, the user can go back to']",Y05-1024deepfigures-results.json,Screenshots,Y05-1024
Y06-1016.pdf-Figure6.png,Fig. 6. Photograph of the prototypes screen.,[],Y06-1016deepfigures-results.json,Screenshots,Y06-1016
Y06-1048.pdf-Figure1.png,Figure 1 Uighur speech recognition figure 2 Uighur speech recognition system identify interface system training interface,"[u'not be very good support. If correct display must be in a special handling procedures. Figure 1, figure 2 is the small vocabulary isolated word Uighur speech recognition system training interface and identify interface,']",Y06-1048deepfigures-results.json,Screenshots,Y06-1048
Y06-1048.pdf-Figure3.png,Figure 3 Chinese speech recognition figure 4 Chinese speech recognition system identify interface system training interface,"[u'is the small vocabulary isolated word Uighur speech recognition system training interface and identify interface, Figure 3 ,']",Y06-1048deepfigures-results.json,Screenshots,Y06-1048
Y07-1008.pdf-Figure2.png,Figure 2: Multilingual online translator-oriented editor.,"[u'editor has an Excel-like interface where all source and target TUs are displayed in parallel (Figure 2 , area I). The editor allows translators to exploit dictionaries and machine translation asynchronously', u'. the editor allows translators to exploit dictionaries and machine translation asynchronously (figure 2 , area ii).', u'detection is proposed at the same time the TU is selected in the main grid (Figure 2 , area III). In fact, during the translation process, the editor proposes suggestions by computing', u'. accordingly, edition functionalities are important: translators are able to add, delete and split tu cells, which is useful for the manual enhancement of the tokenization (figure 2, area iv) .', u'. translators can however choose the rate of similarity (figure 2, area v) and fine-tune the environment parameters.', u'. the selection of the tu in the editor area (figure 2 , area i) activates the sentence tokenization and dictionary suggestions.', u'. beside demgol, there is another ongoing experiment testing of multilingual functionalities in the translation of human rights documents from english into japanese (figure 2 ).']",Y07-1008deepfigures-results.json,Screenshots,Y07-1008
Y07-1036.pdf-Figure7.png,Figure 7: The prototype of a semantic search system.,"[u'., deputy of japanese equities, manager of sales figure 7 : the prototype of a semantic search system.', u'. figure 7 introduces the prototype of a semantic search system that utilizes the relations in quadruples extracted from the wsj corpus.']",Y07-1036deepfigures-results.json,Screenshots,Y07-1036
Y07-1040.pdf-Figure2.png,Figure 2: GUI of the error editor,"[u'. figure 2 : gui of the error editor gui (graphical user interface) of the annotation tool is consisting of a main menu, a sentence window, a morpheme window, a chunk window, a dependency structure window, and a message window.']",Y07-1040deepfigures-results.json,Screenshots,Y07-1040
Y08-1044.pdf-Figure1.png,Figure 1: Main Window of Korean-English MT System,[u'. figure 1 shows the main window of our system.'],Y08-1044deepfigures-results.json,Screenshots,Y08-1044
Y08-1044.pdf-Figure3.png,Figure 3: Error Correction for Morphological Analysis,"[u'. figure 3 is the morphological analysis result of ""         .']",Y08-1044deepfigures-results.json,Screenshots,Y08-1044
Y08-1044.pdf-Figure4.png,Figure 4: Sentence Structure before and after the Correction,[u'. figure 4 is the original and modified tree.'],Y08-1044deepfigures-results.json,Screenshots,Y08-1044
Y08-1044.pdf-Figure5.png,Figure 5: Word Translation Error Correction,[],Y08-1044deepfigures-results.json,Screenshots,Y08-1044
Y08-1044.pdf-Figure6.png,Figure 6: Expression Search,"[u"". sometimes some expressions look unnatural to the user even though the system regards them as natur for the user's confidence, the system provides the function to retrieve the same expression as the one in the translation as in figure 6 ."", u'. Figure 6 : Expression Search']",Y08-1044deepfigures-results.json,Screenshots,Y08-1044
Y10-1001.pdf-Figure2.png,"Figure 2: PodCastle screen snapshot of an interface for correcting speech recognition errors (competitive candidates are presented underneath the normal recognition results). Five errors in this excerpt were corrected by selecting from the candidates. The corrected Japanese sentence means . . . well, actually the ratio of this price range and . . . .","[u'. in this interface, shown in figure 2 , a recognition result excerpt is shown around the cursor and scrolled in synchronization with the audio playback.']",Y10-1001deepfigures-results.json,Screenshots,Y10-1001
Y10-1024.pdf-Figure11.png,Figure 11: Implemented Example 1,"[u"". figure 11 illustrates the analysis result of the first word 'a."", u'. therefore, the syllable structure for this word is as shown in figure 11 .']",Y10-1024deepfigures-results.json,Screenshots,Y10-1024
Y10-1024.pdf-Figure13.png,Figure 13: Implemented Example 3,"[u'. therefore, the syllable structure for this word is as shown in figure 13 .']",Y10-1024deepfigures-results.json,Screenshots,Y10-1024
Y10-1028.pdf-Figure6.png,Figure 6: Screenshot of the prototype implementation.,[u'. a screenshot of the portal can be seen in figure 6 .'],Y10-1028deepfigures-results.json,Screenshots,Y10-1028
Y10-1044.pdf-Figure1.png,"Figure 1 : sample graphical interface of the local RCP prototype application of the TXM platform on Windows. The left panel is a tree view allowing to browse all the different corpora and objects created during the working session (corpus, sub-corpus, partitions, vocabulary lists, concordances, graphics). The upper panel has been split with the window manager to be able to compare side by side two different vocabularies from two sub-corpora and the lower panel is a kwic concordance of all the proper nouns of the corpus7. That concordance displays for each word its form and part of speech (any word property can be displayed in the order selected by the user). On the left side of the concordance, the reference columns displays the date of the production of the text for each concordance line. Any property of structural nodes can be part of the reference display.",[u'. figure 1 shows a sample interface of the windows version.'],Y10-1044deepfigures-results.json,Screenshots,Y10-1044
Y10-1044.pdf-Figure2.png,"Figure 2 : sample interface of the web application prototype of the TXM platform rendered by Firefox. The upper panel displays two different views of the edition of the manuscript  la Queste del Saint Graal  (Marchello-Nizia, to be published) : on the left side the image of one column of the manuscript, on the right side one of the three available critical editions encoded in XML-TEI P5 and rendered in HTML. The lower panel displays a kwic concordance of the Lancelot word. A double-click on a line of the concordance displayed in the upper panel the column numbered 160d (right column of verso of folio 160) of the edition in which the keyword appears highlighted in red. The concordance was built by the same toolbox which generated the one illustrated on figure 1.",[u'. figure 2 shows a sample interface in firefox.'],Y10-1044deepfigures-results.json,Screenshots,Y10-1044
Y10-1048.pdf-Figure1.png,Figure 1: Screen capture from webKOLONs lexical frame:  01 01 VA (talta 01 01 VA).,[],Y10-1048deepfigures-results.json,Screenshots,Y10-1048
Y13-1013.pdf-Figure2.png,Figure 2. Online Platform for Submitting Association Responses,"[u', and asked to input their response on the web interface ( Figure 2) . The English and Chinese instructions are more or less equivalent. It was additionally specified']",Y13-1013deepfigures-results.json,Screenshots,Y13-1013
Y13-1020.pdf-Figure1.png,"Figure 1: A structured view of a clause in the annotated corpus, taken from the web-based interface.",[],Y13-1020deepfigures-results.json,Screenshots,Y13-1020
Y13-1020.pdf-Figure2.png,"Figure 2: A view of the web-based collaborative tagger for annotating the functional and discourse structures of multilingual texts. The web-based interface is divided into three operation panels, namely, the text panel (left), annotation panel (top right) and visual structure panel (bottom right).",[],Y13-1020deepfigures-results.json,Screenshots,Y13-1020
Y13-1049.pdf-Figure1.png,Figure 1. The Interface of CdE.,[],Y13-1049deepfigures-results.json,Screenshots,Y13-1049
Y13-1049.pdf-Figure2.png,Figure 2. The Interface of CEATE.,[],Y13-1049deepfigures-results.json,Screenshots,Y13-1049
2007.sigdial-1.34.pdf-Table1.png,Table 1: Classification of utterances when setting 1 to 75 and 2 to 125,"[u'. the results are listed in table 1 .', u'. we compared the results from table 1 with the results when the utterances with s   2 were simply rejected in which the performance was optimized by considering both the utterance verification and language understanding results.']",2007.sigdial-1.34deepfigures-results.json,tables,2007.sigdial-1.34
2015.jeptalnrecital-court.6.pdf-Table1.png,Table 1  Taille du FTB et nombre de connecteurs dans le FDTB1,[],2015.jeptalnrecital-court.6deepfigures-results.json,tables,2015.jeptalnrecital-court.6
2015.jeptalnrecital-long.14.pdf-Table1.png,TABLE 1  Exemple de lexique pour lencodage de la grammaire TAG de la figure 7,[],2015.jeptalnrecital-long.14deepfigures-results.json,tables,2015.jeptalnrecital-long.14
2015.jeptalnrecital-long.14.pdf-Table2.png,TABLE 2  Interprtation par Gdisc-clause int. pour linterface entre phrase et discours,[],2015.jeptalnrecital-long.14deepfigures-results.json,tables,2015.jeptalnrecital-long.14
2015.jeptalnrecital-long.14.pdf-Table3.png,TABLE 3  Interprtation par GD-STAG sem. pour la smantique du discours,[],2015.jeptalnrecital-long.14deepfigures-results.json,tables,2015.jeptalnrecital-long.14
2020.jeptalnrecital-taln.9.pdf-Table1.png,TABLE 1  Statistique des donnes utilises. Les nombres de phrases sont les nombres de phrases distinctes.,[],2020.jeptalnrecital-taln.9deepfigures-results.json,tables,2020.jeptalnrecital-taln.9
2020.jeptalnrecital-taln.9.pdf-Table2.png,"TABLE 2  Taille du vocabulaire pour les donnes utilises. En comparant les tailles sur lensemble des donnes et celles pour chacune des parties, on observe quil existe un important recouvrement des vocabulaires des trois parties du jeu de donnes. On fait aussi la remarque classique qu contenu quivalent le nombre de mots distincts en franais est plus important que celui de langlais.",[],2020.jeptalnrecital-taln.9deepfigures-results.json,tables,2020.jeptalnrecital-taln.9
2020.jeptalnrecital-taln.9.pdf-Table3.png,"TABLE 3  Caractristiques et nombre de paramtres pour chacun des trois canaux effectuant la prdiction deMDD ,MBD ouMCD . Un canal prend quatre matrices de taille    en entre et retourne une matrice de taille    en sortie. Il aurait t possible de partager les paramtres entre les canaux pourMBD etMCD puisquils effectuent un travail de mme nature.",[],2020.jeptalnrecital-taln.9deepfigures-results.json,tables,2020.jeptalnrecital-taln.9
2020.jeptalnrecital-taln.9.pdf-Table4.png,TABLE 4  Hyper-paramtres utiliss lors de lentrainement.,[],2020.jeptalnrecital-taln.9deepfigures-results.json,tables,2020.jeptalnrecital-taln.9
2020.jeptalnrecital-taln.9.pdf-Table5.png,TABLE 5  Rsultats de traduction sur les phrases du jeu de test. La mthode propose est teste pour deux configurations pour lappariement bilingue : lune utilise les scores donnes par une table de traduction ; lautre utilise un espace de reprsentations vectorielles de mots partag par les langues source et cible. La systme de traduction neuronal OpenNMT est utilis  titre de comparaison.,[],2020.jeptalnrecital-taln.9deepfigures-results.json,tables,2020.jeptalnrecital-taln.9
2020.lrec-1.555.pdf-Table1.png,Table 1: Number of documents and structured documents per publisher in the original corpus,[],2020.lrec-1.555deepfigures-results.json,tables,2020.lrec-1.555
2020.lrec-1.555.pdf-Table2.png,"Table 2: Precision, recall and F-measure of both tools",[],2020.lrec-1.555deepfigures-results.json,tables,2020.lrec-1.555
2020.lrec-1.555.pdf-Table3.png,Table 3: Rate of success for each category of terms with entity-fishing,[],2020.lrec-1.555deepfigures-results.json,tables,2020.lrec-1.555
2020.lrec-1.555.pdf-Table4.png,"Table 4: Precision, recall and F-measure for entity-fishing",[],2020.lrec-1.555deepfigures-results.json,tables,2020.lrec-1.555
2020.lrec-1.87.pdf-Table1.png,Table 1: Types of attentive listening responses and their roles,"[u"". here strings in parentheses show the types of the  listener's attitude that the content of the speaker's utterance is satisfactory for him/her surprise attitude of strong surprise at the content of the speaker's utterance surprise with doubt attitude of surprise or doubt toward the content of the speaker's utterance opinion listener's personal experiences, opinions, or feelings complement listener's status that he/she is eagerly listening to the speaker's utterance greeting acknowledgement of the speaker's presence and willingness to favorably interact with the speaker provoke memory listener's reaction that his/her memory is provoked by the content of the speaker's utterance start thinking listener's reaction that he/she is starting to think about the content of the speaker's utterance thinking process listener's status that he/she is thinking about the content of the speaker's utterance table 1 : types of attentive listening responses and their roles attentive listening responses."", u'based on the degree of empathy shown to narratives in order to explain this relation. Table 1 , which is shown in', u'. the classification was made based on the degree of empathy estimated by considering the response roles in table 1 .']",2020.lrec-1.87deepfigures-results.json,tables,2020.lrec-1.87
2020.sigdial-1.19.pdf-Table1.png,Table 1: Examples of five types of system questions for puttanesca whose correct cuisine is Italian. E and I denote explicit and implicit questions. C and W denote whether the content is correct or wrong. Whq denotes Wh-questions.,"[u'Whq ""What is puttanesca?"" Table 1 : Examples of five types of system questions for puttanesca whose correct cuisine is Italian.']",2020.sigdial-1.19deepfigures-results.json,tables,2020.sigdial-1.19
2020.sigdial-1.19.pdf-Table2.png,Table 2: Multiple correlation coefficients (R) of the models.,[u'. the multiple correlation coefficients for the two impression scores are listed in table 2 .'],2020.sigdial-1.19deepfigures-results.json,tables,2020.sigdial-1.19
2020.sigdial-1.19.pdf-Table3.png,Table 3: Averages over the three positions of the coefficients in the regression models.,"[u'. the averages over the three positions for intelligent are summarized in table 3 , along with those for annoying.']",2020.sigdial-1.19deepfigures-results.json,tables,2020.sigdial-1.19
2020.sigdial-1.19.pdf-Table4.png,Table 4: Predicted and actual impression scores for annoyingwhen the same question types are repeated three times.,"[u'shows the results and Table 4 lists their concrete values. For all question types, the impression scores for the actual cases', u'. furthermore, the differences in the scores for types with wrong content (ew and iw) were larger than those with correct content (ec and ic), as shown in the ""difference"" column in table 4 .']",2020.sigdial-1.19deepfigures-results.json,tables,2020.sigdial-1.19
2020.signlang-1.34.pdf-Table3.png,Table 3: SM - Number of Submodels; N - Number of training samples; SC - Number of Subclasses; Training efficiency on diferent levels of learning tree,"[u'. during tree traversal, we preprocess all the training data table 3 show the number of samples that is possible to retrieve for submodels on different tree levels.', u'. table 3 gives a comparison of the average accuracy of the submodels on the different tree levels, and the amount of training samples with the average number of subclasses.']",2020.signlang-1.34deepfigures-results.json,tables,2020.signlang-1.34
2020.signlang-1.34.pdf-Table4.png,Table 4: Comparison to other arproaches,"[u'. table 4 is a comparison attempt, where we give the number of subclasses in our model against the number of signs in other approaches.']",2020.signlang-1.34deepfigures-results.json,tables,2020.signlang-1.34
C00-1042.pdf-Table1.png,Table 1: Vocabulary sizes for two Turkish corpora.,[],C00-1042deepfigures-results.json,tables,C00-1042
C00-1042.pdf-Table2.png,Table 2: The perplexity of Turkish corpora using word-based trigram language models.,[],C00-1042deepfigures-results.json,tables,C00-1042
C00-1042.pdf-Table4.png,Table 4: Accuracy results for di erent models.,[],C00-1042deepfigures-results.json,tables,C00-1042
C00-1042.pdf-Table5.png,Table 5: The contribution of the individual models for the best case.,[],C00-1042deepfigures-results.json,tables,C00-1042
C00-1044.pdf-Table1.png,"Table 1: Evaluation of the adjective orientation classification and labeling methods (from (Hatzivassiloglou and McKeown, 1997)).","[u'. because the accuracy of the method depends on the density of conjunctions per adjective, hatzivassiloglou and mckeown tested separately their algorithm for adjectives appearing in at least 2, 3, 4, or 5 conjunctions in the corpus; their results are shown in table 1 .']",C00-1044deepfigures-results.json,tables,C00-1044
C00-1044.pdf-Table2.png,"Table 2: Extracted values of gradability indicators, i.e., frequencies of the word with or without the specified inflection or modification, for two adjectives, one gradable (cold) and one primarily non-gradable (civil). The frequencies were computed from the 1987 Wall Street Journal corpus.","[u'by grading words Uninflected 392 20 1,296 1 Inflected for degree 18 0 0 0 Table 2 : Extracted values of gradability indicators, i.e., frequencies of the word with or without the', u'examples for one gradable and one non-gradable adjective are given in Table 2 . To convert these four numbers to a single decision on the gradability of the']",C00-1044deepfigures-results.json,tables,C00-1044
C00-1045.pdf-Table1.png,Table 1: Performance comparison,"[u""algorithm with the annotated muse corpus and Mc-Coy and Strube's newspaper corpus is given in Table 1 . The evaluation has been carried out for the algorithm gnome-np without employing the repetition"", u'The figures in Table 1 show that our algorithm performs very well in both domains, even without using a finer']",C00-1045deepfigures-results.json,tables,C00-1045
C00-1048.pdf-Table1.png,Table 1: Initial overlap between Celex en Fonilex,[],C00-1048deepfigures-results.json,tables,C00-1048
C00-1052.pdf-Table4.png,Table 4: The number of productions found in the transformed trees of sentences in section 23 that do not appear in the corresponding transformed trees from sections 2{ 21. (The subscript epsilon indicates epsilon removal was applied).,"[u'. first, table 3 lists the number of sentences in the test corpus that fail to receive a parse with the various pcfgs mentioned table 4 : the number of productions found in the transformed trees of sentences in section 23 that do not appear in the corresponding transformed trees from sections 2 21.']",C00-1052deepfigures-results.json,tables,C00-1052
C00-1052.pdf-Table5.png,Table 5: Labelled recall and precision scores of PCFGs estimated using various tree-transforms in a transformdetransform framework using test data from section 23.,[u'. table 5 presents the results of this comparison.'],C00-1052deepfigures-results.json,tables,C00-1052
C00-1056.pdf-Table4.png,"Table 4 shows the comparison with MBRtalk [16], Neural Network [10], Weighted finite-state acceptor (WFSA) [19] and direct transliteration [9] even though they are based on different problem domains. MBRtalk and Neural Network models are based on English words pronunciation generation. WFSA is for Englishto-Japanese transliteration. Experiment for training data in MBRtalk makes no sense, since it finds the most similar word in a database that stores all the training data; thus the result would always produce the exact answer. The results show that the model we propose indicates the good performance.",[u'38.1% <Table 4> comparison with other models -word accuracy (precision) of 1-best candidate. Table 4 shows the comparison with MBRtalk'],C00-1056deepfigures-results.json,tables,C00-1056
C00-1060.pdf-Table3.png,"Table 3: Bunsetsu accuracies for four models. Column G indicates whether the grammar is used, R indicates whether the modification candidates are restricted to three, and F denotes the formula; P is the pair formula (6), and T is the Triplet/Quadruplet formula (8), (9).","[u'From the result shown in Table 3 , we can say our method contributes to the improvement of our parser, because of']",C00-1060deepfigures-results.json,tables,C00-1060
C00-1061.pdf-Table6.png,Table 6: Information Usage,[],C00-1061deepfigures-results.json,tables,C00-1061
C00-1063.pdf-Table1.png,Table 1: The accuracy of KNP.,"[u'The accuracy of KNP is shown in Table 1 , which counted whether each phrase modifies a proper head or not. The overall accuracy', u'As shown in Table 1 , KNP detects heads of case components in fairly high accuracy. However, in order to']",C00-1063deepfigures-results.json,tables,C00-1063
C00-1063.pdf-Table2.png,Table 2: Examples of the constructed case frames.,[u'. table 2 shows examples of constructed case frames.'],C00-1063deepfigures-results.json,tables,C00-1063
C00-1063.pdf-Table3.png,Table 3: The accuracy of case detection.,"[u'. as shown in table 3 , the accuracy of the analysis was fairly good: that for topic-markers was 82% and that for clausal modifiers was 73%.']",C00-1063deepfigures-results.json,tables,C00-1063
C00-1064.pdf-Table2.png,Table 2: Summery of Features Selected,[],C00-1064deepfigures-results.json,tables,C00-1064
C00-1066.pdf-Table1.png,Table 1: Examples of keywords for each category,[u'. (total 141 keywords for 47 categories) table 1 lists the examples of keywords for each category.'],C00-1066deepfigures-results.json,tables,C00-1066
C00-1066.pdf-Table2.png,Table 2: Setting experiment data,[u'. table 2 shows the settings of experiment data in detail.'],C00-1066deepfigures-results.json,tables,C00-1066
C00-1068.pdf-Table1.png,Table 1: Comparison of methods,"[u'. the result is shown in table 1 .', u"". the`fa+serr' in table 1 means fa 1 +serr 2 , on the assumption that the con rmed phrases are correctly either accepted or rejected.""]",C00-1068deepfigures-results.json,tables,C00-1068
C00-1072.pdf-Table3.png,Table 3: F-measure performance di erence compared to baseline method in percentage. Columns indicate at di erent summary lengths related to full length documents. Values in the baseline rows are F-measure scores. Values in the t df and topic signature rows are performance increase or decrease divided by their corresponding baseline scores and shown in percentage.,[],C00-1072deepfigures-results.json,tables,C00-1072
C00-1073.pdf-Table1.png,Table 1: Main e ects of dialogue strategy.,"[u'. table 1 summarizes the di erence in performance of njfun for our original reward function and the above alternative evaluation measures, from training eic to test learned strategy for strongcomp.']",C00-1073deepfigures-results.json,tables,C00-1073
C00-1074.pdf-Table1.png,Table 1: Set of templates for transformation rules,"[u'. the set of templates is shown in table 1 2) .', u'Because the template set shown in Table 1 was designed only to make up for the shortcomings of the neuro tagger, the set']",C00-1074deepfigures-results.json,tables,C00-1074
C00-1074.pdf-Table2.png,Table 2: Procedure for learning transformation rules,"[u'According to the learning procedure shown in Table 2 , an ordered list of transformation rules are acquired by applying the template set to', u'the evaluation function (cnt good 0 h 1 cnt bad) used in the learning procedure (Table 2) is a weight to control the strictness of generating a rule. If h is large,']",C00-1074deepfigures-results.json,tables,C00-1074
C00-1074.pdf-Table3.png,Table 3: First 15 transformation rules,[u'. table 3 shows the rst 15 transformation rules.'],C00-1074deepfigures-results.json,tables,C00-1074
C00-1074.pdf-Table4.png,Table 4: Results of POS tagging for testing data,"[u'. in addition to the accuracy of the neuro tagger and hybrid system, the table also shows the accuracy of a baseline model, the hmm, and a rule-based model for comparison.']",C00-1074deepfigures-results.json,tables,C00-1074
C00-1079.pdf-Table3.png,Table 3 shows the evaluation results classified by the types of MWTUs.,"[u'. table 3 shows the evaluation results classified by the types of mwtus.', u'[ In Table 3 , idioms, collocation patterns and compound words have a very low frequency while verbal nouns,']",C00-1079deepfigures-results.json,tables,C00-1079
C00-1080.pdf-Table1.png,Table 1: Parsing algorithms as Grammatical Deduction Systems,[],C00-1080deepfigures-results.json,tables,C00-1080
C00-1080.pdf-Table2.png,Table 2: Parsing systems as CHR programs,[],C00-1080deepfigures-results.json,tables,C00-1080
C00-1080.pdf-Table3.png,Table 3: Part of a CHR-based minimalist parser,[],C00-1080deepfigures-results.json,tables,C00-1080
C00-1081.pdf-Table1.png,Table 1: Corpus.,[],C00-1081deepfigures-results.json,tables,C00-1081
C00-1081.pdf-Table2.png,Table 2: Cross entorpy and accuracy of each model.,[],C00-1081deepfigures-results.json,tables,C00-1081
C00-1082.pdf-Table1.png,Table 1: Results of learning set of Experiment 1,[],C00-1082deepfigures-results.json,tables,C00-1082
C00-1082.pdf-Table2.png,Table 2: Results of test set of Experiment 1,[],C00-1082deepfigures-results.json,tables,C00-1082
C00-1082.pdf-Table3.png,Table 3: Results of learning set of Experiment 2,[],C00-1082deepfigures-results.json,tables,C00-1082
C00-1082.pdf-Table4.png,Table 4: Results of test set of Experiment 2,[],C00-1082deepfigures-results.json,tables,C00-1082
C00-1082.pdf-Table5.png,Table 5: Cases when KNP was incorrect and Method 2 was correct,[u'In Table 5 we show some cases which were partitioned incorrectly with KNP but correctly with 5 In'],C00-1082deepfigures-results.json,tables,C00-1082
C00-2109.pdf-Table1.png,Table 1: Evaluation,[u'The evaluation result of our system is shown in Table 1 . The experiment uses the correctly segmented and part-of-speech tagged sentences of the Kyoto University'],C00-2109deepfigures-results.json,tables,C00-2109
C00-2109.pdf-Table2.png,Table 2: Relationship between beam width and accuracy,[u'. table 2 shows the dependency accuracy and sentence accuracy for beam widths 1 through 20.'],C00-2109deepfigures-results.json,tables,C00-2109
C00-2124.pdf-Table1.png,"Table 1: The e ects of system-internal combination by using di erent output representations. A straight-forward majority vote of the output yields better bracket accuracies and F =1 rates than any included individual classi er. The bracket accuracies in the columns O and C show what percentage of words was correctly classi ed as baseNP start, baseNP end or neither.","[u' Table 1 : The e ects of system-internal combination by using di erent output representations. A straight-forward', u'. the test data results can be found in table 1 .']",C00-2124deepfigures-results.json,tables,C00-2124
C00-2124.pdf-Table2.png,Table 2: Bracket accuracies and F =1 scores for section WSJ 21 of the Penn Treebank with seven individual classi ers and combinations of them. Each combination performs better than its best individual member. The stacked classiers without context information perform best.,[u'. the bracket accuracies and the f =1 scores for test data can be found in table 2 .'],C00-2124deepfigures-results.json,tables,C00-2124
C00-2124.pdf-Table3.png,"Table 3: The overall performance of the majority voting combination of our best ve systems (selected on tuning data performance) applied to the standard data set put forward by Ramshaw and Marcus (1995) together with an overview of earlier work. The accuracy scores indicate how often a word was classi ed correctly with the representation used (O, C or IOB1). The combined system outperforms all earlier reported results for this data set.",[u'. the results can be found in table 3 .'],C00-2124deepfigures-results.json,tables,C00-2124
C00-2128.pdf-Table1.png,Table 1: Experimental results.,"[u""The result of applying the proposed method to our set of metonymies is summarized in Table  1 . A reasonably good result can be seen for 'both relations', i.e. the result obtained"", u"". table 1 also shows that 'both relations' is more accurate than either the result obtained by solely using the a no b relation or the a near b relation."", u'. moreover, if we restrict our attention to the ten metonymies that are acceptable in japanese, all but one were interpreted correctly.']",C00-2128deepfigures-results.json,tables,C00-2128
C00-2128.pdf-Table2.png,"Table 2: Results of applying the proposed method to direct translations of the metonymies in (Kamei and Wakao, 1992).","[u'. these 13 metonymies are shown in table 2 , along with the results of the experiment.', u'. table 2 shows the results of applying the method to the thirteen directly translated metonymies described in section 5.1.', u'. the reason for the higher degree of accuracy is that the metonymies in table  2 are somewhat typical and relatively easy to interpret, while the metonymies collected from japanese sources included a diversity of types and were more difficult to interpret.']",C00-2128deepfigures-results.json,tables,C00-2128
C00-2131.pdf-Table1.png,Table 1: Experimental Result of Word Correspondences,[u'. table 1 shows the result of experiment for nding word correspondences.'],C00-2131deepfigures-results.json,tables,C00-2131
C02-1020.pdf-Table1.png,Table 1: Precision and recall rate,[],C02-1020deepfigures-results.json,tables,C02-1020
C02-1020.pdf-Table2.png,Table 2: Examples of translation pairs extracted by our method,[],C02-1020deepfigures-results.json,tables,C02-1020
C02-1020.pdf-Table3.png,Table 3: Precision rate and recall rate when each kind of features is removed,[u'. table 3 shows the precision rate and the recall rate when each kind of features described in section 3.2 was removed.'],C02-1020deepfigures-results.json,tables,C02-1020
C02-1046.pdf-Table1.png,Table 1: Accuracy of sense preference (spf),[],C02-1046deepfigures-results.json,tables,C02-1046
C02-1046.pdf-Table2.png,Table 2: Accuracy of sense probability (sp),[],C02-1046deepfigures-results.json,tables,C02-1046
C02-1046.pdf-Table3.png,Table 3: Accuracy of translation selection,[],C02-1046deepfigures-results.json,tables,C02-1046
C02-1064.pdf-Table1.png,Table 1: Input keywords and examples of system output.,"[u'we made 30 sets of keywords, with three keywords in each set, as shown in Table 1 . A human subject selected the sets from headwords that were found ten', u'generated when the rightmost two keywords, such as "" and ,"" on each line of Table 1 were input.', u'Next, we conducted experiments using the 30 sets of keywords shown in Table 1 as inputs. We used two keyword-production models: model KM3+MM+DM, which achieved the best results in', u'. the right column of table 1 shows examples of the system output.']",C02-1064deepfigures-results.json,tables,C02-1064
C02-1064.pdf-Table2.png,Table 2: Results of subjective evaluation.,[u'. table 2 shows the results.'],C02-1064deepfigures-results.json,tables,C02-1064
C02-1086.pdf-Table1.png,Table 1. The degree of ambiguities for 24 queries.,[],C02-1086deepfigures-results.json,tables,C02-1086
C02-1086.pdf-Table2.png,Table 2. The retrieval effectiveness for comparison methods.,"[u'11-point average precision value, corresponding result to monolingual (C/M), and performance change are summarized in Table 2 . The retrieval effectiveness of tall_rerank is 0.2780, corresponding to 97.27% of monolingual performance. The']",C02-1086deepfigures-results.json,tables,C02-1086
C02-1088.pdf-Table2.png,Table 2. The number of named entities which belong to each category in the test set,[u'. the number of named entities which belong to each category is shown in table  2 .'],C02-1088deepfigures-results.json,tables,C02-1088
C02-1088.pdf-Table4.png,Table 4. Comparison with two kinds of window size,"[u"". we conduct a comparative experiment using the uchimoto's method, 5 windows (two words before/after the target word) and then we show that our method brings to a better result (table 4 ).""]",C02-1088deepfigures-results.json,tables,C02-1088
C02-1088.pdf-Table6.png,Table 6. The comparison of an ensemble learning and each individual learning,[u'. table 6 shows that the ensemble learning brings a better result than each individual learning method.'],C02-1088deepfigures-results.json,tables,C02-1088
C02-1088.pdf-Table7.png,Table 7. The comparison with the learnings using different features,[u'. table 7 shows that the learning using different kinds of features has the low performance because of the lack of information.'],C02-1088deepfigures-results.json,tables,C02-1088
C02-1141.pdf-Table1.png,Table 1: Proof script for Euclidian division,"[u'difficult to read (even for a mathematician), as the reader can see for himself in Table 1 and  Table 2 . Hence, the need of an NLG system in order to obtain an easy-to-read version', u'. for example, command 2 in table 1 with individual h is translated into the concept c 0 .']",C02-1141deepfigures-results.json,tables,C02-1141
C02-1141.pdf-Table2.png,Table 2: Proof tree for Euclidian division,[],C02-1141deepfigures-results.json,tables,C02-1141
C02-1141.pdf-Table3.png,Table 3: Syntax of standard constructors,"[u'The set of constructors which seem useful for GePhoX and their syntax are shown in Table  3 ; examples of concepts and roles with their semantic are shown underneath', u'; examples of concepts and roles with their semantic are shown underneath Table 3 . The choice of constructors is domain dependent. Constructors other than those used in GePhoX']",C02-1141deepfigures-results.json,tables,C02-1141
C02-1141.pdf-Table4.png,Table 4: GePhoX Domain model,[],C02-1141deepfigures-results.json,tables,C02-1141
C02-1141.pdf-Table5.png,Table 5: DL-Message for Euclidian division,"[u'. table 5 : dl-message for euclidian division nity) which takes into account the user knowledge and whose coherence has been checked.', u'. see below an example of sdrs in gephox, the sdrs built from table 5 .']",C02-1141deepfigures-results.json,tables,C02-1141
C02-1141.pdf-Table6.png,Table 6: sdrs for Euclidian division,[],C02-1141deepfigures-results.json,tables,C02-1141
C02-1141.pdf-Table7.png,Table 7: A Text of proof for Euclidian division,[],C02-1141deepfigures-results.json,tables,C02-1141
C02-1166.pdf-Table2.png,Table 2: Results for separate models,"[u'. table 2 shows the results we obtained on our comparable corpora, for p=10, without combining the different models.', u'Dictionary Corpus Thesaurus F1-score 56.16 62.04 51.34 Table 2 : Results for separate models']",C02-1166deepfigures-results.json,tables,C02-1166
C02-1166.pdf-Table4.png,Table 4: Evaluation of term alignment,[u'. table 4 shows precision and recall for our method.'],C02-1166deepfigures-results.json,tables,C02-1166
C02-2019.pdf-Table1.png,Table 1: Features.,"[u'The features used in our experiments are listed in Table 1 . Each feature consists of a type and a value, which are given in the', u'. the notations ""(0)"" and ""(-1)"" used in the feature type column in table 1 respectively indicate a target string and the morpheme to the left of it.']",C02-2019deepfigures-results.json,tables,C02-2019
C02-2019.pdf-Table2.png,Table 2: Results of Experiments (Segmentation and major POS tagging).,[u'Results of the morphological analysis obtained by our method are shown in Table 2 . Recall is the percentage of morphemes in the test corpus whose segmentation and major'],C02-2019deepfigures-results.json,tables,C02-2019
C02-2019.pdf-Table3.png,Table 3: Results of Experiments (Segmentation and major POS tagging).,[],C02-2019deepfigures-results.json,tables,C02-2019
C02-2019.pdf-Table4.png,Table 4: Results of Experiments (Segmentation and major POS tagging).,[u'. results obtained under this condition are shown in table 4 .'],C02-2019deepfigures-results.json,tables,C02-2019
C04-1057.pdf-Table2.png,Table 2: Modified greedy algorithm versus baseline.,[u'. table 2 compares our modified greedy algorithm to the baseline.'],C04-1057deepfigures-results.json,tables,C04-1057
C04-1071.pdf-Table1.png,Table 1: Precision and recall for the extraction of sentiment units from 200 sentences.,[u'. table 1 shows the results.'],C04-1071deepfigures-results.json,tables,C04-1071
C04-1071.pdf-Table2.png,"Table 2: The breakdown of the results of Experiment 1. The columns and rows show the manual output and the system output, respectively (f: favorable, n: non-sentiment, u: unfavorable). The sum of the bold numbers equals the numerators of the precision and recall.",[u'. their breakdowns in the two parts of table 2 indicate that most of errors where the system wrongly assigned either of sentiments ( human regarded an expression as non-sentiment) have been reduced with the mt framework.'],C04-1071deepfigures-results.json,tables,C04-1071
C04-1071.pdf-Table3.png,Table 3: Comparison of scope of sentiment units. The numbers mean the counts of the better output for each system among 35 sentiment units. The remainder is the outputs that were the same in both systems.,"[u'According to the results shown in Table 3 , the MT method produced less redundant or more informative sentiment units than did relying']",C04-1071deepfigures-results.json,tables,C04-1071
C04-1105.pdf-Table1.png,Table 1: Example translation equivalents selected by the method based on contextual similarity,"[u'. all low values of contextual similarity (see table 1 ) support this fact.', u'. in contrast, the cs method is forced to select a fixed number of translation equivalents for all target words; it is difficult to predetermine a threshold for the contextual similarity, since the range of its values varies with target words (see table 1 ).']",C04-1105deepfigures-results.json,tables,C04-1105
C04-1105.pdf-Table2.png,Table 2: Example translation equivalents selected by the method using the ratio of associated words,[u'. table 2 lists translation equivalents with a ratio of associated words larger than 4% along with their top-four representative associated words.'],C04-1105deepfigures-results.json,tables,C04-1105
C04-1106.pdf-Table1.png,Table 1: Some statistics on the BTEC multilingual corpus.,[u'. the sentences in btec are quite short as the figures in table 1 show.'],C04-1106deepfigures-results.json,tables,C04-1106
C04-1106.pdf-Table2.png,Table 2: Number of analogies in the BTEC multilingual corpus.,"[u'. table 2 shows the counts for each language.', u'. then all counts were redone, and the new figures are listed under the title ""higher estimate"" in table 2 .']",C04-1106deepfigures-results.json,tables,C04-1106
1996.amta-1.17.pdf-Figure4.png,"Figure 4: The ZARDOZ Concept Network and Object Hierarchy. Statements prefixed with ""?"" indicate local slot assignments, while tokens prefixed ""$"" indicate demon attachments for given message types.","[u'. such a sign hierarchy is illustrated in figure 4 .', u'. inherited slot variables, under figure 4 : the zardoz concept network and object hierarchy.']",1996.amta-1.17deepfigures-results.json,trees,1996.amta-1.17
2015.jeptalnrecital-court.31.pdf-Figure1.png,"FIGURE 1  Structure de dpendances du corpus CDGFr-devel pour la phrase  Des directives, ils sen sont donnes ",[],2015.jeptalnrecital-court.31deepfigures-results.json,trees,2015.jeptalnrecital-court.31
2015.jeptalnrecital-long.25.pdf-Figure4.png,FIGURE 4  Architectures de rseaux pour la prdiction locale. Les biais ne sont pas reprsents sur les schmas.,[],2015.jeptalnrecital-long.25deepfigures-results.json,trees,2015.jeptalnrecital-long.25
2016.jeptalnrecital-poster.13.pdf-Figure7.png,FIGURE 7: La rcursivit des rgles,[],2016.jeptalnrecital-poster.13deepfigures-results.json,trees,2016.jeptalnrecital-poster.13
2016.lilt-13.4.pdf-Figure10.png,"FIGURE 10 Decorated parse trees showing nominal and clausal predication in GF. The parse trees shows the abstract function names, the category of each node and the UD labels.","[u'. figure 10 shows the parse trees for the example sentences, john killed him and that she came will make news.']",2016.lilt-13.4deepfigures-results.json,trees,2016.lilt-13.4
2016.lilt-13.4.pdf-Figure11.png,FIGURE 11 Coordination of three adverbials,[u'. the parse tree for this example is shown in figure 11 .'],2016.lilt-13.4deepfigures-results.json,trees,2016.lilt-13.4
2016.lilt-13.4.pdf-Figure12.png,FIGURE 12 Parse trees of the existential clause there is a cat and its translation in Bulgarian,[u'. see figure 12 for the parse trees of the clause in english and bulgarian.'],2016.lilt-13.4deepfigures-results.json,trees,2016.lilt-13.4
2016.lilt-13.4.pdf-Figure13.png,FIGURE 13 Decorated ASTs for the clause John killed him and its passive counterpart He was killed using both local and non-local abstract,"[u'the clause John killed him and its pas-sive counterpart (he was killed ), shown in Figure 13 . The grammatical subject of the passive clause is incorrectly mapped to nsubj label using']",2016.lilt-13.4deepfigures-results.json,trees,2016.lilt-13.4
2016.lilt-13.4.pdf-Figure14.png,FIGURE 14 Decorated parse tree for a copula construction in English,[u'. the choice of copula verb and its figure 14 decorated parse tree for a copula construction in english form is specified in concrete syntax and the abstract syntax tree does not know exactly what the copula verb in the specific language is.'],2016.lilt-13.4deepfigures-results.json,trees,2016.lilt-13.4
2016.lilt-13.4.pdf-Figure15.png,FIGURE 15 Decorated parse tree showing verb phrase complements and prepositional verbs,[u'. figure 15 shows the parse tree for the sentence i like to listen to music in french.'],2016.lilt-13.4deepfigures-results.json,trees,2016.lilt-13.4
2016.lilt-13.4.pdf-Figure16.png,FIGURE 16 Decorated parse tree showing tense and clausal negation with the UD labels,"[u'. in order to understand the necessity of these rules, we will look at a concrete example (shown in figure 16 ).', u'. the decorated parse tree in figure 16 shows the analysis provided by gf for the sentence john had not slept.']",2016.lilt-13.4deepfigures-results.json,trees,2016.lilt-13.4
2016.lilt-13.4.pdf-Figure18.png,FIGURE 18 The categories of core RGL.,"[u'. figure 18 in the appendix shows the hierarchy of categories in the core rgl.', u'an Utt could also be built from a question or an imperative, as shown in Figure 18 . The sentence in turn is built from a clause (Cl) by adding temporal and', u'. figure 18 shows the hierarchy of categories in the core rgl.']",2016.lilt-13.4deepfigures-results.json,trees,2016.lilt-13.4
2016.lilt-13.4.pdf-Figure2.png,"FIGURE 2 Trees for the sentence the black cat sees us and its French translation: (a) abstract syntax tree; (b,c) parse trees; (d,e) dependency trees; (f) abstract dependency tree with unordered word senses.","[u'. figure 2 summarizes the different kinds of trees that we will speak about, by showing different representations of one and the same example.', u'The abstract syntax tree (Figure 2 (a) ) is a non-redundant representation from which all the others can be derived. Parse trees', u'dep1, dep2, and so on. This default can be overridden by an explicit configuration. In Figure 2 , we assume the following configurations to produce the standard UD labels:']",2016.lilt-13.4deepfigures-results.json,trees,2016.lilt-13.4
2016.lilt-13.4.pdf-Figure3.png,FIGURE 3 Dependency tree derivation from a decorated parse tree. The word cat has sees as its head and nsubj as its label. The path from the top to the word sees is a spine.,"[u'. in a decorated parse tree, we mark the dependency labels at each branching point of the tree, as shown in figure 3 , but omit the ""head"" labels.', u'. figure 3 shows the path corresponding to the labelled arc of the word cat.']",2016.lilt-13.4deepfigures-results.json,trees,2016.lilt-13.4
2016.lilt-13.4.pdf-Figure4.png,FIGURE 4 Dependency trees for the cat is black (a) with a,"[u'. a default dummy label ""dep"" can then be used, as in figure 4 (a).', u'. what we get first, by straightforward dependency configurations, is a kind of ""collapsed"" trees ( figure 4 (c) ).']",2016.lilt-13.4deepfigures-results.json,trees,2016.lilt-13.4
2016.lilt-13.4.pdf-Figure5.png,FIGURE 5 Dependency-decorated abstract syntax tree for my two brothers and I would not have bought that red car.,"[u'. figure 5 shows an abstract syntax tree for the sentence my two brothers and i would not have bought that red car and its rgl equivalents in 29 other languages, artificially constructed to show as many structures as possible.', u'The topmost category in Figure 5 is Utt, utterances, which is built from S, sentence; an Utt could also be built', u'. the example shown in figure 5 discussed some of these functions.']",2016.lilt-13.4deepfigures-results.json,trees,2016.lilt-13.4
2016.lilt-14.2.pdf-Figure6.png,Figure 6: Possible Worlds Hierarchy of JwouldKM-TTDLA,[],2016.lilt-14.2deepfigures-results.json,trees,2016.lilt-14.2
2016.lilt-14.4.pdf-Figure2.png,FIGURE 2 Implementation of modal tagsets in UAM Corpus Tool,[],2016.lilt-14.4deepfigures-results.json,trees,2016.lilt-14.4
2016.lilt-14.6.pdf-Figure4.png,FIGURE 4 Decision tree for annotator guidelines.,[],2016.lilt-14.6deepfigures-results.json,trees,2016.lilt-14.6
2017.jeptalnrecital-recital.12.pdf-Figure1.png,FIGURE 1  Possible structural trees of a sequence w1w2w3,[],2017.jeptalnrecital-recital.12deepfigures-results.json,trees,2017.jeptalnrecital-recital.12
2017.jeptalnrecital-recital.12.pdf-Figure2.png,FIGURE 2  Three possible structural trees for example 3,[],2017.jeptalnrecital-recital.12deepfigures-results.json,trees,2017.jeptalnrecital-recital.12
2018.jeptalnrecital-recital.7.pdf-Figure1.png,FIGURE 1: Exemple de gnration de paraphrases par pivot en franais pour le segment anglais rooted in.,[],2018.jeptalnrecital-recital.7deepfigures-results.json,trees,2018.jeptalnrecital-recital.7
2019.jeptalnrecital-recital.11.pdf-Figure3.png,Figure 3 : Arbre reprsentant une des traductions observes de jeudi soir et son lien avec le reste de la phrase (1).,[],2019.jeptalnrecital-recital.11deepfigures-results.json,trees,2019.jeptalnrecital-recital.11
2019.jeptalnrecital-recital.11.pdf-Figure4.png,Figure 4 : Arbre reprsentant une des traductions observes de  une semaine aprs  et son lien avec le reste du contenu.,[],2019.jeptalnrecital-recital.11deepfigures-results.json,trees,2019.jeptalnrecital-recital.11
2019.jeptalnrecital-recital.11.pdf-Figure5.png,Figure 5 : Arbre reprsentant une des traductions observes de  depuis 2002  et son lien avec le reste du contenu.,[],2019.jeptalnrecital-recital.11deepfigures-results.json,trees,2019.jeptalnrecital-recital.11
2019.jeptalnrecital-recital.11.pdf-Figure7.png,Figure 7 : Arbre reprsentant une premire traduction observe de  depuis plusieurs semaines  et son lien avec le reste du contenu (placement de llment temporel en tant quitem 1 dune rgle context).,[],2019.jeptalnrecital-recital.11deepfigures-results.json,trees,2019.jeptalnrecital-recital.11
2019.lilt-17.3.pdf-Figure2.png,FIGURE 2 Analysis of pustak=kii tareef hu-ii The book got praised.,"[u'and Figure 2 respectively. The TAG analysis is only sketched here. The authors use feature-based TAG, which makes', u'and Figure 2 . What the analysis clearly shows is that TAG assumes two lexical items for the']",2019.lilt-17.3deepfigures-results.json,trees,2019.lilt-17.3
2019.lilt-17.3.pdf-Figure4.png,FIGURE 4 Analysis of pustak=kii tareef hu-ii The book got praised.,[],2019.lilt-17.3deepfigures-results.json,trees,2019.lilt-17.3
2019.lilt-18.2.pdf-Figure1.png,FIGURE 1 A CAMR graph and its corresponding graph,[],2019.lilt-18.2deepfigures-results.json,trees,2019.lilt-18.2
2019.lilt-18.2.pdf-Figure3.png,FIGURE 3 A CAMR graph and its corresponding graph,"[u'. this is illustrated in figure 3 , which has the amr annotation for ""the girl wants to study in new york"" and its chinese translation \'\' "".', u'The lexical concepts in the AMR graph of Figure 3 include ""want-01"" and ""study-01"", while the corresponding CAMR concepts are \'\' -02"" and \'\' -01"".', u'. for example, in figure 3 , ""city"" is an abstract concept that represents the type of the named entity ""new york"".']",2019.lilt-18.2deepfigures-results.json,trees,2019.lilt-18.2
2020.acl-demos.38.pdf-Figure4.png,Figure 4: Parallel scan implementation of the linearchain CRF inference algorithm (parallel forward). Here   represents a semiring matrix operation and I is padding to produce a balanced tree.,"[u'. to compute, a( ) in this manner we first pad the sequence length n out to the nearest power of two, and then compute a balanced parallel tree over the parts, shown in figure 4 .', u'. for a( )   i 7,,  6,, 5,,   4,, 3,,  2,, 1,, figure 4 : parallel scan implementation of the linearchain crf inference algorithm (parallel forward).']",2020.acl-demos.38deepfigures-results.json,trees,2020.acl-demos.38
2020.acl-main.133.pdf-Figure1.png,Figure 1: A high-level view of our multi-task learning approach for dialogue coherence modeling.,"[u'.com/ukplab/ acl2020-dialogue-coherence-assessment figure 1 : a high-level view of our multi-task learning approach for dialogue coherence modeling.', u'As illustrated in Figure 1 , our main idea is to benefit from the DAP task for improving the performance']",2020.acl-main.133deepfigures-results.json,trees,2020.acl-main.133
2020.acl-main.133.pdf-Figure2.png,"Figure 2: A low-level illustration of our MTL-based approach to dialogue coherence assessment. The input is dialogue pair p = (diali, dialj). Dashed items represent losses. Models parameters are shared among dialogues.",[u'. figure 2 shows a low-level illustration of our mtl-based approach.'],2020.acl-main.133deepfigures-results.json,trees,2020.acl-main.133
2020.acl-main.134.pdf-Figure1.png,Figure 1: An overview of the linearization system.,"[u'As an overview, Figure 1 illustrates our pipeline for the linearization task, with an unordered dependency tree as input, and', u'. consider the subtree with the words ""this"" and ""with"" in figure 1 , a vanilla bigram model would calculate a much higher score for ""with this"" than ""this with"", while a contextualized bigram model could be aware that it is part of a rather special syntactic construction in english.']",2020.acl-main.134deepfigures-results.json,trees,2020.acl-main.134
2020.acl-main.135.pdf-Figure4.png,"Figure 4: An example of constructed DP- and SRL- based semantic graphs, where 99K indicates CHILD relation, and rectangular, rhombic and circular nodes represent arguments, verbs and modifiers respectively.","[u'. figure 4 shows a real example for the dp-and srl-based semantic graph, respectively.']",2020.acl-main.135deepfigures-results.json,trees,2020.acl-main.135
2020.acl-main.143.pdf-Figure2.png,Figure 2: Visualization of the bilingual word embeddings after Bi-view AT.,"[u'As shown in Figure 2 , we depict the word embeddings of some sampled words in English-Chinese after our Bi-view', u'. it shows that the words which constitute a new ground-truth translation pair do appear as neighboring points in the 2-dimensional visualization of figure 2 .']",2020.acl-main.143deepfigures-results.json,trees,2020.acl-main.143
2020.acl-main.152.pdf-Figure1.png,Figure 1: A monolingual trie storing three sentences.,[u'We build a prefix tree (trie) containing all sentences in the target language corpus (Figure 1 ). Then we translate each sentence in the source language corpus using the trie as'],2020.acl-main.152deepfigures-results.json,trees,2020.acl-main.152
2020.acl-main.208.pdf-Figure1.png,Figure 1: Pseudocode is translated to code for each line and combined to form a valid program. Certain combinations are invalid due to syntactic and semantic constraints.,"[u'. however, if we want to generate programs that can be executed successfully, the inclusion of both syntactic and semantic constraints is cruci as shown in figure 1 , while multiple program fragments may be syntactically correct and represent plausible translations of the corresponding pseudocode, not all of them will lead to executable programs.', u'. for example, any of the code piece candidates in figure 1 could potentially be used in a valid program, but if we naively combine certain subsets of candidates together, the resulting program will be invalid due to the use of undeclared variables or mismatching braces.']",2020.acl-main.208deepfigures-results.json,trees,2020.acl-main.208
2020.acl-main.341.pdf-Figure7.png,"Figure 7: Cases for interpretability of compositional sentiment semantics. The three color blocks between parents and children are the attention weights distributed to left child, the phrase itself and right child.","[u'. in figure 7 , we demonstrate two sentences of which the sentiments of all the phrases are predicted correctly.']",2020.acl-main.341deepfigures-results.json,trees,2020.acl-main.341
2020.acl-main.475.pdf-Figure1.png,"Figure 1. The text-based ideal point model (tbip) separates senators by political party using only speeches. The algorithm does not have access to party information, but senators are coded by their political party for clarity (Democrats in blue circles, Republicans in red xs). The speeches are from the 114th U.S. Senate.","[u'. figure 1 shows a analysis of the speeches of the 114th u.', u'. the fitted ideal points in figure 1 show that the largely separates lawmakers by political party, despite not having access to these labels or votes.', u'Based on the separation of political parties in Figure 1 , we interpret negative ideal points as liberal and positive ideal points as conservative.']",2020.acl-main.475deepfigures-results.json,trees,2020.acl-main.475
2020.acl-main.521.pdf-Figure6.png,Figure 6: BERT attention for the word founding,[],2020.acl-main.521deepfigures-results.json,trees,2020.acl-main.521
2020.acl-main.577.pdf-Figure1.png,Figure 1: The network architectures of our system.,[u'. figure 1 shows an overview of the architecture.'],2020.acl-main.577deepfigures-results.json,trees,2020.acl-main.577
2020.acl-main.606.pdf-Figure1.png,Figure 1: A plausible syntactic analysis of give a topic and then discussion about it. The example is from the TLE corpus. The corrected counterpart of this fragment in TLE is Give a topic and then discuss it.,[u'. figure 1 presents an example from the tle corpus.'],2020.acl-main.606deepfigures-results.json,trees,2020.acl-main.606
2020.acl-main.607.pdf-Figure1.png,"Figure 1: Illustration of the encoder, following the design of Dozat and Manning (2018).",[],2020.acl-main.607deepfigures-results.json,trees,2020.acl-main.607
2020.acl-main.67.pdf-Figure1.png,Figure 1: An AMR graph (left) for sentence He runs as fast as the wind. and its concept graph and relation graph (line graph). Two graphs are aligned with each other based on the node-edge relations in the original graph.,"[u'. figure 1 (left) gives a standard amr graph and its corresponding surface form.', u'. as illustrated in figure 1 , we thus separate the original amr graph into two sub-graphs without labeled edges -concept graph and relation graph.', u'. as illustrated in figure 1 , arg0 is the edge between run-02 and he.']",2020.acl-main.67deepfigures-results.json,trees,2020.acl-main.67
2020.acl-main.67.pdf-Figure2.png,Figure 2: Neighborhood information in different orders.,"[u', and as illustrated in Figure 2 , we can have:']",2020.acl-main.67deepfigures-results.json,trees,2020.acl-main.67
2020.acl-main.775.pdf-Figure2.png,"Figure 2: An illustration of flat-structure annotation variation across treebanks: a set of parallel sentences, all containing the conceptually headless MWE Martin Luther King, Jr. (underlined), from UD 2.2 (treebank code _pud) in English, German, Chinese, Japanese, Turkish, and Portuguese (top to bottom). The intent of this figure is not to critique particular annotation decisions, but to demonstrate the notation, concepts, and data extraction methods used in our paper. To wit: Highlights/black-background indicate well-formed flat-MWE tree fragments according to the principles listed in 4. BIO sequences are induced by the longest-spanning flat arcs. When there is a mismatch between the highlighted tree fragments and the BI spanshere, in the German, Chinese and Turkish examplesit is because the dependency trees do not fully conform to the UD annotation guidelines on headless structures.","[u'. the intent of this figure is not to critique particular annotation decisions, but to demonstrate the notation, concepts, and data extraction methods used in our paper.']",2020.acl-main.775deepfigures-results.json,trees,2020.acl-main.775
2020.cl-2.6.pdf-Figure10.png,"Figure 10 Semantic interlingua and word-alignment (top), followed by syntactic interlingua (bottom), for English and Italian.",[u'. figure 10 shows the shared semantic tree as well as the languagespecific syntactic trees for english and italian.'],2020.cl-2.6deepfigures-results.json,trees,2020.cl-2.6
2020.cl-2.6.pdf-Figure11.png,"Figure 11 An RGL abstract syntax tree annotated with dependency labels, and the corresponding dependency trees for English and Italian. The thick red path from DefArt to cat N shows how the dependency tree algorithm finds the head of a word by walking up the tree until a label is reached, and then walking down along the head spine.","[u'. it is the way the dependency trees have been derived in figure 11 .', u'. the algorithm makes an assumption that holds in figure 11 but not in general: that the leaf nodes in abstract syntax trees are in one-to-one correspondence with words in linearizations.']",2020.cl-2.6deepfigures-results.json,trees,2020.cl-2.6
2020.cl-2.6.pdf-Figure6.png,Figure 6 An abstract syntax tree and word alignment for infix to postfix (Java to Java Virtual Machine) translation.,"[u'. figure 6 shows an abstract syntax tree for an arithmetic expression in java and its translation to java virtual machine assembly language, illustrating variation in the shape, order, and number of tokens.']",2020.cl-2.6deepfigures-results.json,trees,2020.cl-2.6
2020.cl-2.6.pdf-Figure8.png,"Figure 8 The categories of the common abstract syntax of the RGL, showing their main dependencies. The full dependency graph has several cycles. The rectangles show lexical categories subcategorized by their complement lists, such as V2 for two-place verbs taking one NP complement. V* and V2* mean sets of verb categories taking other complements as well, such as VP, AP, or S.","[u'The common abstract syntax of the GF RGL has a set of categories, summarized in Figure 8 .', u'senses; languages of the latter kind are those that have been used in wide-coverage translation. Figure 8 and']",2020.cl-2.6deepfigures-results.json,trees,2020.cl-2.6
2020.cl-2.6.pdf-Figure9.png,"Figure 9 An example of RGL abstract syntax trees (above), with corresponding phrase structure trees for English and Italian (below). The phrase structure trees show two differences in word order: APCN and V2Pron.",[u'. figure 9 shows an example that uses many of the categories.'],2020.cl-2.6deepfigures-results.json,trees,2020.cl-2.6
2020.jeptalnrecital-jep.48.pdf-Figure6.png,FIGURE 6  Structure du signe CEINTURE dans le cadre du Prosodic Model.,"[u""de notre analyse est plus fort que ce que suggre la formalisation illustre dans la Figure 6 . En effet, tandis que l'implmentation de doigts slectionns secondaires""]",2020.jeptalnrecital-jep.48deepfigures-results.json,trees,2020.jeptalnrecital-jep.48
2020.ldl-1.11.pdf-Figure1.png,Figure 1: Example of a very simplified taxonomy in Spanish.,[],2020.ldl-1.11deepfigures-results.json,trees,2020.ldl-1.11
2020.lrec-1.126.pdf-Figure4.png,Figure 4: Syntactic tree visualization.,[u'on the left/right arrows provided on screen. Each POS tag is assigned a different color. Figure 4 shows the syntactic tree for the title of tale 1 also used in example'],2020.lrec-1.126deepfigures-results.json,trees,2020.lrec-1.126
2020.lrec-1.128.pdf-Figure1.png,"Figure 1: An example of Chinese discourse parsing tree from the Chinese Discourse Treebank (Li et al., 2014b)","[u'. figure 1 illustrates an example of chinese discourse parsing tree.', u'., t k } where n i dominates t i for all i (, the node b in figure 1 dominates the text spanning from edu 1 to edu 3, so if we use n j to represent node b, then t j should represent the text span from edu 1 to 3).']",2020.lrec-1.128deepfigures-results.json,trees,2020.lrec-1.128
2020.lrec-1.16.pdf-Figure1.png,Figure 1: Worldbuilders annotation scheme.,[u' Figure 1 . The tags are assigned to sentences and not words and phrases. Worldbuilder also has'],2020.lrec-1.16deepfigures-results.json,trees,2020.lrec-1.16
2020.lrec-1.347.pdf-Figure1.png,Figure 1: A GF abstract syntax tree generated from parsing John drunk hot water,[],2020.lrec-1.347deepfigures-results.json,trees,2020.lrec-1.347
2020.lrec-1.347.pdf-Figure2.png,Figure 2: A GF concrete syntax tree generated from linearising the parse tree of 1 into English,"[u'. the example was generated from parsing the english sentence ""john drunk figure 2 : a gf concrete syntax tree generated from linearising the parse tree of 1 into english hot water"" using gf for the purpose of generating a parse tree.']",2020.lrec-1.347deepfigures-results.json,trees,2020.lrec-1.347
2020.lrec-1.347.pdf-Figure3.png,Figure 3: A GF concrete syntax tree generated from linearising the parse tree of 1 into Runyankore,"[u'. when we linearised this abstract tree to english, runyankore and rukiga, we obtained concrete syntax trees for the languages in figures 2 and 3 for english and runyankore respectively.']",2020.lrec-1.347deepfigures-results.json,trees,2020.lrec-1.347
2020.lrec-1.362.pdf-Figure1.png,Figure 1: An Example of CAMR,[u'. figure 1 shows an example of how camr uses the lexicon to annotate semantic relations between a predicate and its core semantic roles.'],2020.lrec-1.362deepfigures-results.json,trees,2020.lrec-1.362
2020.lrec-1.550.pdf-Figure1.png,Figure 1: Hierarchy of privacy-bearing information (pi) entity types relevant for emails (leaves in green),"[u'. we, finally, came up with the set of privacy-bearing information (henceforth, pi) entity types depicted in figure 1 .', u'parts of the corpus, according to the privacy-bearing (pi) categories described in Section 3 (see Figure 1 ). Annotation was performed on the entity level. Therefore, we did not have to care']",2020.lrec-1.550deepfigures-results.json,trees,2020.lrec-1.550
2020.lrec-1.631.pdf-Figure8.png,Figure 8: Phrase structure of (23),[],2020.lrec-1.631deepfigures-results.json,trees,2020.lrec-1.631
2020.lrec-1.639.pdf-Figure4.png,Figure 4: Trees of various constructions attested in the ABC Treebank,"[u'. (8) in figure 4 at the end of the paper is an example tree obtained by the conversion.', u'. (9) and (10) in figure 4 show trees which are actually obtained by the conversion.', u'paper shows the derivation trees with semantic representations automatically generated for the three sentences in Figure 4 .', u'past(e) K(e)))) S[m] Figure 4 ']",2020.lrec-1.639deepfigures-results.json,trees,2020.lrec-1.639
2020.lrec-1.640.pdf-Figure1.png,Figure 1: A sample phrase structure parse tree.,"[u'. figure 1 shows a sample annotated sentence of interesting complexity.', u'shows a dependency tree representation of the PS tree from Figure 1 . The PS is compatible with dependency structures as head dependencies remain similar in the']",2020.lrec-1.640deepfigures-results.json,trees,2020.lrec-1.640
2020.lrec-1.642.pdf-Figure4.png,Figure 4: An annotated Eukalyptus tree including all layers of annotation.,[u'. a full example tree can be seen in figure 4 .'],2020.lrec-1.642deepfigures-results.json,trees,2020.lrec-1.642
2020.lrec-1.646.pdf-Figure1.png,Figure 1: Use of secondary dependencies and empty nodes in TOROT.,"[u'Example (1) and Figure 1 demonstrate several of these features: This Old East Slavonic sentence has an asyndetic coordination of', u'. figure 1 can again serve as an illustration.', u'. the example in figure 1 thus serves as evidence that this has not happened yet: we observe agreement in the masculine nominative plur we have adopted a maximally conservative annotation policy.', u'. for the primary chronicle (figure 1) , for example, we use the e-pvl.', u"". this is illustrated in figure 1 : in the running text from the e-pvl, ubojaasja '(they) became afraid' is a single token."", u'. in addition, torot uses secondary dependencies to indicate predicate identity (in case of verb ellipsis) and shared dependents (see also figure 1 ).']",2020.lrec-1.646deepfigures-results.json,trees,2020.lrec-1.646
2020.lrec-1.646.pdf-Figure2.png,Figure 2: Coordination in SynTagRus (reproduced with permission from Berdicevskis and Eckhoff 2015),"[u'. in syntagrus, the first conjunct is the head, the conjuction (if present) is its dependent (via the coord relation or sent-coord for sentential coordination), the second conjunct is a dependent on a conjunction (via the coord-conj relation), see figure 2 .']",2020.lrec-1.646deepfigures-results.json,trees,2020.lrec-1.646
2020.lrec-1.646.pdf-Figure3.png,Figure 3: Coordination in TOROT (reproduced with permission from Berdicevskis and Eckhoff 2015),"[u'. in torot, the conjunction is the head (a null conjunction is inserted in case of asyndetic coordination) and all the conjuncts are its dependents, no special relation is used, see figure 3 .']",2020.lrec-1.646deepfigures-results.json,trees,2020.lrec-1.646
2020.lrec-1.647.pdf-Figure1.png,"Figure 1: Typed DAG and structurally annotated proof for Lassy sample WS-U-E-A-0000000236.p.11.s.1.xml, depicting an analysis for the phrase Waarover gaat de machtstrijd (What is the power struggle about?).","[u'. we detail their functionality in the next paragraphs; an illustrative example is also given in figure 1a .', u"". out of the (possibly many) potential proofs, the dataset specifies the one that is linguistically acceptable, determining the correct flow of information within the sentence's constituents (an example of such a derivation is presented in figure 1b ).""]",2020.lrec-1.647deepfigures-results.json,trees,2020.lrec-1.647
2020.lrec-1.652.pdf-Figure1.png,Figure 1: Example of annotation of a false start:,[],2020.lrec-1.652deepfigures-results.json,trees,2020.lrec-1.652
2020.lrec-1.652.pdf-Figure2.png,Figure 2: Example of annotation of a truncated word -,[],2020.lrec-1.652deepfigures-results.json,trees,2020.lrec-1.652
2020.lrec-1.652.pdf-Figure3.png,Figure 3: Example of annotation of a repetition -,[],2020.lrec-1.652deepfigures-results.json,trees,2020.lrec-1.652
2020.lrec-1.652.pdf-Figure4.png,Figure 4: Example of a speech turn divided into several pseudo-sentences (SENT) after manual revision - cest parfait / je viens trois quatre jours,[],2020.lrec-1.652deepfigures-results.json,trees,2020.lrec-1.652
2020.lrec-1.652.pdf-Figure5.png,Figure 5: Example of a multiword adverb (MWADV) - de plus en plus,[u'. its structure (det + nc figure 5 : example of a multiword adverb (mwadv)de plus en plus [transl.'],2020.lrec-1.652deepfigures-results.json,trees,2020.lrec-1.652
2020.lrec-1.652.pdf-Figure6.png,Figure 6: Example of a regular NP construction -,[],2020.lrec-1.652deepfigures-results.json,trees,2020.lrec-1.652
2020.lrec-1.730.pdf-Figure2.png,"Figure 2: Part of sense hierarchy for the word head. Top Part and Leader are guide words, the topics are on the lower-level.","[u'. these more fine-grained topics include edges and extremities of objects, tools, flowers -general words, beer & cider, parts of watercourses, skin complaints & blemishes, ahead, in front and beyond (see figure 2) .']",2020.lrec-1.730deepfigures-results.json,trees,2020.lrec-1.730
2020.lrec-1.776.pdf-Figure2.png,"Figure 2: A sample of the extracted sentence with dependency relations in syntactic annotation, part-of-speech tags, sentiment polarity and translations.","[u'for each word and translation in five languages as it is shown in Figure 2 .', u'. figure 2 shows sample of the extracted dataset with its annotation.']",2020.lrec-1.776deepfigures-results.json,trees,2020.lrec-1.776
2020.lrec-1.777.pdf-Figure5.png,Figure 5: Very simplified depiction of Burmese tones.,[u'. a very simplified diagram of this tonal system is shown in figure 5 .'],2020.lrec-1.777deepfigures-results.json,trees,2020.lrec-1.777
2020.lrec-1.81.pdf-Figure1.png,Figure 1: Our turn-taking model,"[u'. one typical procedure for option (a) is to use the first part of an adjacent pair, , a question or request, affiliated with an explicit technique to address that utterance to a particular figure 1 : our turn-taking model co-participant, by, , calling his/her name or directing the gaze toward that participant.', u', the model can be straightforwardly depicted by the diagram in Figure 1 , which represents a two-step discrimination of the turn-taking type. Distinction between non-completion of the', u'of the existing turn-taking models and proposed a two-stage computational model, similar to that in Figure 1 , using deep learning, which can perform three-way discrimination among turn-switch, turn-holding, and noncompletion in', u', using deep learning, which can perform three-way discrimination among turn-switch, turn-holding, and noncompletion in Figure 1 . However, they did not distinguish options (a) and (b) in the turn-taking rules. The', u'together with the following LUU; obviously, this does not cover all cases of non-TRP in Figure 1 Current speaker has selected next speaker non-selecting Current speaker has not selected next speaker multi-unit', u'. note that the ending-type tag distinguishes only between options (a) and (b/c) in figure 1 but not between (b) and (c).', u'. second, we did not deal with the distinction between completion and non-completion (see figure 1 ).']",2020.lrec-1.81deepfigures-results.json,trees,2020.lrec-1.81
2020.lrec-1.81.pdf-Figure2.png,Figure 2: The taxonomy of the ending types of utterances,"[u'The ending-type tag represents how the utterance (LUU) ends. Figure 2 shows the taxonomy of the ending-type tags, and', u'. the ending types of these utterances include the following three types (see figure 2) : selecting, non-selecting, and multi-unit.']",2020.lrec-1.81deepfigures-results.json,trees,2020.lrec-1.81
2020.lrec-1.81.pdf-Figure3.png,Figure 3: The taxonomy of the beginning types of utterances,"[u'In contrast to the ending-type tag, the beginning-type tag represents how the utterance (LUU) begins. Figure 3 shows the taxonomy of the beginning-type tags, and']",2020.lrec-1.81deepfigures-results.json,trees,2020.lrec-1.81
2020.lrec-1.865.pdf-Figure4.png,Figure 4: A dependency syntax tree example,"[u'KonText is currently able to render PDT-like dependency syntax trees (Figure 4) , support for UD']",2020.lrec-1.865deepfigures-results.json,trees,2020.lrec-1.865
2020.lt4gov-1.1.pdf-Figure2.png,Figure 2: Annotation example of a Mobility report with subentities and attributes.,"[u'. additional subentities and attributes were marked, as summarized in figure 2 .', u'. for instance, given the mobility report in figure 2 , the polarity associated with the mobility action mention ambulates is able.']",2020.lt4gov-1.1deepfigures-results.json,trees,2020.lt4gov-1.1
2020.nuse-1.3.pdf-Figure1.png,"Figure 1: The hierarchical discourse structure of news proposed by van Dijk (van Dijk, 1988). Boxes indicate labels that were directly annotated on the documents; other labels can be inferred. From Yarlott et al. (2018), Figure 1.","[u'(1988) described a hierarchical theory of news discourse, the categories of which are shown in Figure 1 , which we apply to a subset of the news articles of the ACE Phase', u', Figure 1 .']",2020.nuse-1.3deepfigures-results.json,trees,2020.nuse-1.3
C00-1028.pdf-Figure1.png,Figure 1: A portion of Wordnet.,[],C00-1028deepfigures-results.json,trees,C00-1028
C00-1043.pdf-Figure2.png,Figure 2: Question semantic transformation,[],C00-1043deepfigures-results.json,trees,C00-1043
C00-1043.pdf-Figure3.png,Figure 3: Parse tree traversal,[],C00-1043deepfigures-results.json,trees,C00-1043
C00-1054.pdf-Figure1.png,Figure 1: Multimodal architecture,"[u'. in addition to the user interface client, the architecture contains speech and gesture recognition components which process incoming streams of speech and electronic ink, and a multimodal language processing component ( figure 1 ).', u'UI ASR Backend Gesture Recognizer Figure 1 : Multimodal architecture Section 2 provides background on finite-state language processing. In Section 3, we']",C00-1054deepfigures-results.json,trees,C00-1054
C00-1065.pdf-Figure1.png,"FIGURE 1 : some supertags for ""beats""",[u'semantically void Semantic minimality : an elementary tree corresponds at most to one semantic unit Figure 1 shows a non exhaustive set of Supertags (i.e. elementary trees) which can be assigned to'],C00-1065deepfigures-results.json,trees,C00-1065
C00-1065.pdf-Figure2.png,FIGURE 2 :,[],C00-1065deepfigures-results.json,trees,C00-1065
C00-1065.pdf-Figure4.png,FIGURE 4 : Retrieving Inheritance patterns and Supertags from a Hypertag,"[u'. for example, to recover the set of supertags contained in a hypertag, one just needs to perform the cross-product between the 3 dimensions of the hypertag, as shown on figure 4 , in order to obtain all inheritance patterns.', u'. for example ""relativized-object"" can be suppressed in dimension 2 from the hypertag shown on figure 4 , in case no wh element is encountered in a sentence.']",C00-1065deepfigures-results.json,trees,C00-1065
C00-1067.pdf-Figure1.png,Figure 1: DPL structure for the meaning of (1).,[],C00-1067deepfigures-results.json,trees,C00-1067
C00-1067.pdf-Figure3.png,Figure 3: Constraint graph for (1).,[],C00-1067deepfigures-results.json,trees,C00-1067
C00-1078.pdf-Figure1.png,Figure 1: Spanish and English Regularized Parse Trees,[],C00-1078deepfigures-results.json,trees,C00-1078
C00-1085.pdf-Figure1.png,Figure 1: TSG Parse Fragment,[],C00-1085deepfigures-results.json,trees,C00-1085
C00-2088.pdf-Figure6.png,Figure 6: Network underlying (12),[],C00-2088deepfigures-results.json,trees,C00-2088
C00-2088.pdf-Figure7.png,Figure 7: Network underlying (14),"[u""two Selection of 'one' yields the following text, which is generated from the network in Figure 7 :""]",C00-2088deepfigures-results.json,trees,C00-2088
C00-2092.pdf-Figure1.png,Figure 1: A linked tree pair hTs;Tti.,"[u'. figure 1 shows an example of two linked trees, the links are depicted graphically as dashed lines.']",C00-2092deepfigures-results.json,trees,C00-2092
C00-2092.pdf-Figure2.png,Figure 2: The bag of linked subtree pairs of hTs;Tti,"[u'. 4 figure 2 show the bag of linked subtree pairs for the linked tree pair t s t t .', u'. the composition of the linked tree pair t s t t and figure 2 : the bag of linked subtree pairs of t s t t u s u t , written as t s t t ae u s u t , is defined iff the label of the leftmost nonterminal linked frontier node and the label of its linked counterpart are identical to the labels of the root nodes of u s u t .']",C00-2092deepfigures-results.json,trees,C00-2092
C00-2092.pdf-Figure3.png,Figure 3: The composition operation,[u'a translation w s w t is the sum of the probabilities of its derivations: Figure 3 : The composition operation'],C00-2092deepfigures-results.json,trees,C00-2092
C00-2092.pdf-Figure4.png,Figure 4: Example of a linked tree pair in Verbmobil,[u'. figure 4 shows an example of a corrected and linked tree from our correction of the verbmobil corpus.'],C00-2092deepfigures-results.json,trees,C00-2092
C00-2093.pdf-Figure1.png,Figure 1: Rhetorical structure,[],C00-2093deepfigures-results.json,trees,C00-2093
C00-2093.pdf-Figure2.png,Figure 2: Text structure,"[u'. these requirements cannot be met by a text structurer text-clause ban(fda, elixir) contain(elixir, gestodene) ""since"" approve(fda, elixir-plus) ""however"" sentence text-clause text-phrase text-phrase text-phrase text-phrase text-phrase text-phrase figure 2 : text structure that merely returns one or two satisfactory solutions, relying perhaps on a library of schemas.']",C00-2093deepfigures-results.json,trees,C00-2093
C00-2093.pdf-Figure3.png,Figure 3: Adding solution variables,[],C00-2093deepfigures-results.json,trees,C00-2093
C00-2093.pdf-Figure4.png,Figure 4: Completing a solution,[],C00-2093deepfigures-results.json,trees,C00-2093
C00-2093.pdf-Figure5.png,Figure 5: Domain assignments,[],C00-2093deepfigures-results.json,trees,C00-2093
C00-2093.pdf-Figure6.png,Figure 6: Completing the TS,"[u'text-phrase (0) (b) (a) Figure 6 : Completing the TS The algorithm for completing the TS cannot be described fully here,']",C00-2093deepfigures-results.json,trees,C00-2093
C00-2111.pdf-Figure1.1.png,Figure 1.1: A UNL graph deconvertible as Ronaldo has headed the ball into the left corner of the net,[],C00-2111deepfigures-results.json,trees,C00-2111
W12-4625.pdf-Figure10.png,"Figure 10: Disambiguated FRMG output, with depFRMG edges above and converted depFTB edges below","[u'applied to return the best dependency tree as illustrated by the above edges 6 in Figure 10 The depFTB schema used for the dependency version of the FTB is expressed in CONLL', u'. figure 10 shows a depftb version of our illustrative sentence, as produced by the conversion, with the converted edges below the sentence.', u'. however, we also already observe non obvious modifications in figure 10 , for instance, the root of the dependency tree is rendu for depfrmg (because of the argument extraction) while it is voulu for depftb.', u'. more precisely, figure 10 : disambiguated frmg output, with depfrmg edges above and converted depftb edges below frmg and ftb do not use the same set of compound words (for instance for complex prepositions, complex adverbs, or named entities) and it is therefore necessary to retrieve the missing depftb dependencies for the frmg compound words that are not compound in ftb.']",W12-4625deepfigures-results.json,trees,W12-4625
W12-4625.pdf-Figure11.png,"Figure 11. The displacement is controlled by an argument pair (A, q) where A is a deterministic finite-state automaton (DFA) and q a state for A. The DFA represents all possible transitions for the constraint to move up.2 We further impose that all transition labels of A are edge labels in the same Lk for some dimension k. For an automaton example, see the Figure 8.3 Intuitively, the constrained edge e can only move upwards along k-paths, and when several k-edges are possible from a node, the main one is chosen.",[],W12-4625deepfigures-results.json,trees,W12-4625
W12-4625.pdf-Figure6.png,Figure 6: Subject ellipsis + control verb,"[u'For instance, in Figure 6 , we would like to insert the missing link between a deep subject and its', u'solve our problem, we simply need to put two share down constraints as illustrated in Figure 6 . The first constrained edge between pense and partir results from a rule dealing with']",W12-4625deepfigures-results.json,trees,W12-4625
W12-4625.pdf-Figure7.png,Figure 7: Linguistic application of constraint move,"[u'. a component of g is a set of edge labels l, partitioned in l 1     l n , the intuition being that each l i corresponds to a subset of labels for a specific dimension (for instance, a dimension for the set of labels used as thematic roles and another one for quantifiers or for anaphora as illustrated in figure 7 ).', u'. our second example, in figure 7 , illustrates the use of a move up constraint.']",W12-4625deepfigures-results.json,trees,W12-4625
W12-4625.pdf-Figure8.png,Figure 8: Automaton A for the move up constraint in Figure 7,"[u'. for an automaton example, see the figure 8 .', u'. in order to do that, the original de_obj dependency between dont and mre (mother) triggers the addition of an initial ant dependency between the same words (in green) but with a move up constraint built on the automaton a of figure 8 .']",W12-4625deepfigures-results.json,trees,W12-4625
W12-4625.pdf-Figure9.png,Figure 9: Linguistic application of constraint redirect,"[u'. finally, figure 9 illustrates the redirect up constraint.']",W12-4625deepfigures-results.json,trees,W12-4625
W12-4626.pdf-Figure1.png,Figure 1: Sample elementary tree T (left) and sample derivation trees that show possible and impossible orderings of attachment following PRoD on derivation tree rooted with a node representing T .,[u'. figure 1 shows how prod works: trees adjoining into  or must be ordered before those that adjoin into  or  which must be ordered before those that adjoin into .'],W12-4626deepfigures-results.json,trees,W12-4626
W12-4626.pdf-Figure10.png,Figure 10: This figure shows TAG G; it consists of trees that can build up the natural numbers in unary. The left and right trees construct numbers by successively adding 1.,[u'. figure 10 shows a tag g that generates unary strings that can be interpreted as the natural numbers beginning with 2.'],W12-4626deepfigures-results.json,trees,W12-4626
W12-4626.pdf-Figure11.png,Figure 11: Additional tree for TAG G; this allows natural numbers to be represented by the product of other natural numbers.,"[u'. now, we introduce a transderivational constraint that will remove from the grammar any derived tree whose value can be 227 p p sa 1  p oa 1 figure 11 : additional tree for tag g; this allows natural numbers to be represented by the product of other natural numbers.']",W12-4626deepfigures-results.json,trees,W12-4626
W12-4626.pdf-Figure12.png,"Figure 12: This figure shows how atomic propositions are constructed. Atomic propositions would be the following: A1, A11, A111, etc.",[],W12-4626deepfigures-results.json,trees,W12-4626
W12-4626.pdf-Figure13.png,"Figure 13: This figure shows trees that with the atomic proposition trees construct (from left to right) negation, conjunction, and disjunction.",[],W12-4626deepfigures-results.json,trees,W12-4626
W12-4626.pdf-Figure14.png,Figure 14: This figure shows the conversion from a multiply linked semantic node to nodes corresponding to the possible orderings of the links. On the left is a node with links  and . The two trees on the right correspond to the two possible orderings.,[u'. figure 14 : this figure shows the conversion from a multiply linked semantic node to nodes corresponding to the possible orderings of the links.'],W12-4626deepfigures-results.json,trees,W12-4626
W12-4626.pdf-Figure15.png,Figure 15: Example of a tree set to be removed to replicate the effects of the DCCSI. This tree-pair would allow a distributive quantifier in object position to take wide scope over clausal negation.,[],W12-4626deepfigures-results.json,trees,W12-4626
W12-4626.pdf-Figure2.png,Figure 2: Derived and derivation tree for A student read every book with split combination of every book.,[],W12-4626deepfigures-results.json,trees,W12-4626
W12-4626.pdf-Figure3.png,Figure 3: Derived and derivation tree for A student read every book without split combination of every book.,[],W12-4626deepfigures-results.json,trees,W12-4626
W12-4626.pdf-Figure4.png,Figure 4: Elementary trees for the Quantifier Phrase every professor including the syntax tree and the multi-component semantic tree set consisting of the scope-tree and the argument tree.,[],W12-4626deepfigures-results.json,trees,W12-4626
W12-4626.pdf-Figure5.png,Figure 5: Elementary trees for the verb read with negation didnt in its extended projection. The semantic tree does not contain the negation; it is a separate tree.,"[u'in mind, let us proceed to analyse the data in 2: Observe that the unavail- Figure 5 : Elementary trees for the verb ""read"" with negation ""didn\'t"" in its extended projection. The']",W12-4626deepfigures-results.json,trees,W12-4626
W12-4626.pdf-Figure6.png,Figure 6: Derivation trees for John didnt read every book and John didnt read a book on the > reading.,[],W12-4626deepfigures-results.json,trees,W12-4626
W12-4626.pdf-Figure7.png,Figure 7: Derivation trees for John didnt seem to meet everyone and No one seemed to meet everyone on the > reading.,[],W12-4626deepfigures-results.json,trees,W12-4626
W12-4626.pdf-Figure8.png,Figure 8: Derivation for a raising construction: the seem-tree (the raising predicate) adjoins into the meettree .,[],W12-4626deepfigures-results.json,trees,W12-4626
W12-4626.pdf-Figure9.png,Figure 9: The three elementary trees for ditransitive constructions: (a) double object; (b)dative complement (low attachment); dative complement (high attachment),[],W12-4626deepfigures-results.json,trees,W12-4626
W12-4627.pdf-Figure1.png,Figure 1: Tree Set for some fundraiser,"[u'Shieber make use of what has become the standard TAG treatment of quantifiers, given in Figure 1 , augmented with dominance links that are crucial only to the inverse linking case. A']",W12-4627deepfigures-results.json,trees,W12-4627
W12-4627.pdf-Figure10.png,Figure 10: Elementary tree set for pied-piped relative clause in 3,"[u'. first of all, we apply our split semantics to the verbally-headed relative clause tree set, shown in figure 10 .', u'. the derivation of (3) proceeds by substituting such a relativizing dp and its associated semantics into the 1 -annotated nodes in figure 10 , and combining the universal quantifier at the 2 -annotated nodes.']",W12-4627deepfigures-results.json,trees,W12-4627
W12-4627.pdf-Figure11.png,Figure 11: Split scope components for scope rigidity in (5),"[u'. in contrast, we take canonical clauses in languages like japanese to be represented by elementary tree sets like the one in figure 11 .', u'the specified hierarchical constraints. Now, if we continue to assume that all NP NP CP Figure 11 : Split scope components for scope rigidity in', u'. however, a more intriguing possibility retains the idea of multiple scope trees in japanese, as in in figure 11 , but removes the hierarchy constraint that we have imposed on the final positions of the scope trees.']",W12-4627deepfigures-results.json,trees,W12-4627
W12-4627.pdf-Figure2.png,Figure 2: Tree Set for at proposed in Nesson and Shieber (2008),"[u'the complement, given in Figure 2 : on the semantic side, the variable component of the quantifier substitutes into the e,t']",W12-4627deepfigures-results.json,trees,W12-4627
W12-4627.pdf-Figure8.png,Figure 8: Elementary trees for control predicate remembered,[u'. the resulting tree set is given in figure 8 .'],W12-4627deepfigures-results.json,trees,W12-4627
W12-4627.pdf-Figure9.png,Figure 9: Matrix and embedded derived auxiliary trees for interleaving scope in (2),[u'. the result of these derivational steps is the two derived tree sets in figure 9 .'],W12-4627deepfigures-results.json,trees,W12-4627
W12-4701.pdf-Figure4.png,Figure 4: Cet,[],W12-4701deepfigures-results.json,trees,W12-4701
W12-4701.pdf-Figure5.png,Figure 5: Yadi Tarhi,[],W12-4701deepfigures-results.json,trees,W12-4701
W12-4701.pdf-Figure6.png,Figure 6: Purvakalkatvam,[],W12-4701deepfigures-results.json,trees,W12-4701
W12-4701.pdf-Figure7.png,Figure 7: Prayojanam,[],W12-4701deepfigures-results.json,trees,W12-4701
W12-4701.pdf-Figure8.png,Figure 8: Samanakalkatvam,[],W12-4701deepfigures-results.json,trees,W12-4701
W12-4701.pdf-Figure9.png,"Figure 9: Discourse Structure of the commentary on ""Samarthah. Padavidhih. """,[u'. figure 9 shows the relations among the topics.'],W12-4701deepfigures-results.json,trees,W12-4701
W12-4702.pdf-Figure1.png,FIGURE 1  Clustering algorithm based on discourse relations.,"[u'i) Assign the strongest relations determined by SVMs to each connection (refer to Figure  1(a) ).', u'. sentences with these connections are evaluated as having similar content, and aggregated as one cluster (refer figure 1(b) ).', u'clusters from (iii) to minimize the occurrence of the same sentences in multiple clusters (refer Figure 1(c) ).']",W12-4702deepfigures-results.json,trees,W12-4702
W12-4703.pdf-Figure1.png,"Figure 1: Hierarchy of senses in PDTB (Prasad et al., 2008)","[u'. labeling of the relations is done according to a hierarchy of senses (see figure 1 ), including four top-level classes: contingency, comparison, temporal and expansion.']",W12-4703deepfigures-results.json,trees,W12-4703
W12-4704.pdf-Figure1.png,"FIGURE 1  An example of an inter-sentential discourse relation, represented by a thick arrow between roots of the arguments",[u'. example 1 and figure 1 show an intersentential discourse relation of type opposition with explicit connective ale (but).'],W12-4704deepfigures-results.json,trees,W12-4704
W12-4704.pdf-Figure2.png,FIGURE 2  An example of an intra-sentential discourse relation annotated during the first phase,"[u'were annotated manually in the first phase. The situation is illustrated by Example 2 and Figure 2 ; on the tectogrammatical layer, the relation between the two clauses was labelled as ADVS', u'an intra-sentential discourse relation annotated during the first phase (functor of the coordinative node in Figure 2 ); the discourse type is correction (the relation is marked by the arrow with label', u'discourse type is correction (the relation is marked by the arrow with label corr in Figure 2 ).', u'.1 (example 2, figure 2 ), some of the intra-sentential discourse relations were annotated manually during the first phase of the annotations.']",W12-4704deepfigures-results.json,trees,W12-4704
W12-4705.pdf-Figure1.png,Figure 1: Abstract example tree.,"[u'. as an example, figure 1 shows a growing logical form in a scope-less flat first-order logic.', u'. as an example, consider the abstract tree in figure 1 .', u'. consider our abstract example in figure 1 again and suppose that in the current state of the ongoing utterance only the first two words have been uttered.']",W12-4705deepfigures-results.json,trees,W12-4705
W12-4705.pdf-Figure2.png,Figure 2: Incremental syntactic derivation of a simple example sentence.,[u'. its syntactic tree is shown in figure 2 .'],W12-4705deepfigures-results.json,trees,W12-4705
Y05-1015.pdf-Figure1.png,Figure 1. An Example of tree representation in SAPT,"[u'. figure 1 shows the sapt for a simple sentence in the clang domain.', u'complete MR for the node. Algorithm1 with an input which is the tree shown in Figure 1 yields the result in logical form as shown in']",Y05-1015deepfigures-results.json,trees,Y05-1015
Y05-1015.pdf-Figure2.png,Figure 2. Example of a logical form extracted from a SAPT R.Ge:2005,"[u"". for example in figure 2 n3 is determined to be the semantic head of the sentence, since it matches n8's semantic label."", u'yields the result in logical form as shown in Figure 2 .']",Y05-1015deepfigures-results.json,trees,Y05-1015
Y05-1028.pdf-Figure2.png,Figure 2: Atomic dependency structures,[],Y05-1028deepfigures-results.json,trees,Y05-1028
Y05-1028.pdf-Figure3.png,Figure 3: Matrix dependency structure Figure 4: Abbreviated expression of matrix dependency structure,[],Y05-1028deepfigures-results.json,trees,Y05-1028
Y05-1028.pdf-Figure5.png,Figure 5: Generation of concentrated modification structure,[],Y05-1028deepfigures-results.json,trees,Y05-1028
Y05-1028.pdf-Figure6.png,Figure 6: Matrix dependency structure Figure 7: Abbreviated expression of matrix dependency structure,[],Y05-1028deepfigures-results.json,trees,Y05-1028
Y05-1028.pdf-Figure8.png,Figure 8: Generation of concentrated modification structure for a complex case,[],Y05-1028deepfigures-results.json,trees,Y05-1028
Y09-1038.pdf-Figure1.png,Figure 1: Samples of sub-trees used in convolution tree kernel calculation.,"[u'enw(n) functions (these stand for ""internal node weight"" and ""external node weight""). For example, in Figure 1 , while the node with label PP is an external node of sub-trees (1) and']",Y09-1038deepfigures-results.json,trees,Y09-1038
Y09-1038.pdf-Figure2.png,"Figure 2: A syntactic parse tree with AAPDist and ArgDist example. There is a SIMULTAENOUS temporal relation between (move, resign) event pair in this parse tree.","[u'n from ancestor path of event argument arg on the parse tree as depicted in Figure 2 . MAXDIST is used for normalization, and is the maximum value of AAPDist in the', u'. their definitions are similar to the previous kernel functions, though they use a different distance function which measures the distance of a node from an event argument rather than its ancestor path (see figure 2 ).']",Y09-1038deepfigures-results.json,trees,Y09-1038
Y11-1001.pdf-Figure1.png,Figure 1: Flow chart of our proposed approach,[u'. a flow chart of our approach is shown in figure 1 .'],Y11-1001deepfigures-results.json,trees,Y11-1001
Y11-1002.pdf-Figure1.png,Figure 1: Syllabification and Alignment,"[u'. hence, the complex onset in the english syllable ""stein"" (as in figure 1 ) violates the onset constraints in chinese and is therefore resolved into two chinese syllables as """" (si1 tan3) 1 .', u'. the top half of figure 1 illustrates these steps with the english name ""jacobstein"".', u'The middle part of Figure 1 illustrates the sub-syllabification process.', u'The bottom part of Figure 1 shows the alignment step.', u'n Extract segment-i to segment-j Next j Next i Hence for the aligned name in Figure 1 , the following segment pairs will enter into the lexicon:']",Y11-1002deepfigures-results.json,trees,Y11-1002
Y11-1002.pdf-Figure2.png,Figure 2: Maximum Matching and Candidate Generation,"[u'. figure 2 shows an example of maximum matching and candidate generation for the english name ""markstein"".']",Y11-1002deepfigures-results.json,trees,Y11-1002
Y11-1004.pdf-Figure6.png,Figure 6: The link hierarchy,"[u'(see Figure 6 ). 5 So there is one arg1+, one arg1-, one arg2+, one arg2-and so on.', u'. in order to allow break in all these frames, it is specified with the intermediate link-type arg12-124-2-24, which has the four subtypes arg12, arg124, arg2 and arg24 (not displayed in figure 6 ).', u'. this gives the type arg12 (see figure 6 ).']",Y11-1004deepfigures-results.json,trees,Y11-1004
Y11-1050.pdf-Figure1.png,Figure 1: An example pair of English - Malay SSTCs and the corresponding translation elements,"[u'. to illustrate this, we show in figure 1 a pair of source (english) and target (malay) sstcs and the corresponding translation elements.']",Y11-1050deepfigures-results.json,trees,Y11-1050
Y11-1050.pdf-Figure2.png,Figure 2: An example annotation between source sentence 2E and target sentence 2M.,[],Y11-1050deepfigures-results.json,trees,Y11-1050
Y11-1050.pdf-Figure3.png,Figure 3: A shared forest structure constructed based on the representation structures of source sentence 1E and 2E with their word index.,"[u'used source sentences (here 1E and 2E) together with its words index as illustrated in Figure 3 below. The shared forest structure together with its words index is then used to parse', u'. an example of a shared forest for the new source sentence 3e, constructed based on the shared forest structure of sentences 1e and 2e, is given in figure 3 below.']",Y11-1050deepfigures-results.json,trees,Y11-1050
Y11-1050.pdf-Figure4.png,Figure 4: An example shared forest structure for sentence 3E constructed based on structures created in Figure 3.,"[u'. an example of a shared forest for a new source sentence 3e, constructed based on the shared forest structure of sentences 1e and 2e, is given in figure 4 .', u'process based on the bilingual knowledge bank and guided by the shared forest structure of Figure 4 .']",Y11-1050deepfigures-results.json,trees,Y11-1050
Y11-1050.pdf-Figure5.png,Figure 5: Learn-to-translate process based on the bilingual knowledge bank and guided by the shared forest structure of Figure 4.,"[u'as well as the substring to subtree mappings, as shown on the top part of Figure 5 - ', u'NOTE : ** Deletion of subtree * Replacement of subtree Figure 5 : Learn-to-translate process based on the bilingual knowledge bank and guided by the shared forest', u'. the target sub-sstcs are then merged to form a complete sstc for the translated sentence, as shown at the bottom of figure 5 .']",Y11-1050deepfigures-results.json,trees,Y11-1050
Y11-1050.pdf-Figure6.png,Figure 6: An example annotation between source sentence 3E and target sentence 3M after going through the correction done by the linguist on the improper annotations produced by the MT system.,"[u'. in the given example, the resulting target sstc appears to have some errors and it is corrected as highlighted in figure 6 .', u'. translation units : figure 6 : an example annotation between source sentence 3e and target sentence 3m after going through the correction done by the linguist on the improper annotations produced by the mt system.']",Y11-1050deepfigures-results.json,trees,Y11-1050
Y11-1050.pdf-Figure7.png,"Figure 7: A shared forest structure constructed based on the representation structures of source sentence 1E, 2E and 3E with its words Index","[u'. here, we add a shared forest structure constructed from the representation structures of source sentences 1e, 2e and 3e, as shown in figure 7 .']",Y11-1050deepfigures-results.json,trees,Y11-1050
Y12-1033.pdf-Figure1.png,Figure 1. A dependency parse tree of the sentence the man there in coat saw John.,"[u'. an example of dependency parsing is illustrated in figure 1 .', u'the construction of the ""trimmed subtrees"" set of the node ""saw"", for the sentence in Figure 1 . The initial boundary parameters are set large enough so the local window contains the', u'From the parse tree of the sentence in Figure 1 , we extract all chains whose order is larger than 2, since otherwise features defined']",Y12-1033deepfigures-results.json,trees,Y12-1033
Y12-1033.pdf-Figure3.png,Figure 3. Some of the extracted trimmed subtrees by the process described in Figure 2. (c) is identical with a grand-sibling factor in a thirdorder parsing model and (d) is similar to a trisibling factor but siblings are on both sides of the head.,"[u'In Figure 3 we show some of the extracted subtrees in the set (""s w""), among which the', u'. moreover, after #bottom moved to the position figure 3 .', u'is retained. An example is shown below which illustrates a feature for the subtree in Figure 3 (a):']",Y12-1033deepfigures-results.json,trees,Y12-1033
Y12-1033.pdf-Figure4.png,"Figure 4. A complete span for the clause transfer money from the new funds to other investment funds where we omitted some of the details. This structure functions as a relatively independent and complete component in the entire parse tree. Features are encoded over the tuples: <transfer, - ,s2>, <transfer, s2,s1>, <transfer, s1,s0>, <transfer, s0,->.","[u'side, which can also be considered as a head node and sibling subtrees shown in Figure 4 . In our observation, a complete span functions as a relatively independent and complete semantic']",Y12-1033deepfigures-results.json,trees,Y12-1033
Y12-1033.pdf-Figure5.png,Figure 5. All chain type subtrees extracted from the gold-standard parse tree of the sentence the man there in coat saw John.,"[u'. we show these chain type subtrees in figure 5 .', u'. figure 5 .', u'two ends of the subtree in each time, while for all nodes we encode their Figure 5 , a feature can appear as:']",Y12-1033deepfigures-results.json,trees,Y12-1033
Y12-1041.pdf-Figure6.png,Figure 6: The TLTAG Derived Trees for Both Si and Its Paraphrases (Sp1-Sp3) Obtained from the Replacement Technique,"[u""next step, these elementary trees are realized into well-formed surfaces by LTAG's operations shown in Figure 6 . Each sentence paraphrase can be read off of the leaf nodes of its associated""]",Y12-1041deepfigures-results.json,trees,Y12-1041
Y12-1041.pdf-Figure7.png,Figure 7: The TLTAG Derived Trees for Both Sp4 and Sp5 Obtained from the Movement Technique,"[u'In Figure 7 , each sentence paraphrase can be read off of the leaf nodes of its associated']",Y12-1041deepfigures-results.json,trees,Y12-1041
Y12-1041.pdf-Figure8.png,Figure 8: The Elementary Tree for /all and the TLTAG Derived Tree for Sp6 Obtained from the Insertion Technique,"[u'. caused by the insertion technique, an additional tree for the quantifier "" /all"" is selected by the tltag selection process and then realized as part of the sentence paraphrase during the surface realization process as shown in figure 8 .']",Y12-1041deepfigures-results.json,trees,Y12-1041
Y12-1041.pdf-Figure9.png,"Figure 9: The TLTAG Derived Trees for Both Si and Its Paraphrase Obtained from a Combination of the Switching, the Promotion/Demotion and the Nominalization Techniques",[u'. figure 9 illustrates an example of the above process in generating a paraphrase of the initial sentence s i using the combination of the three mentioned techniques.'],Y12-1041deepfigures-results.json,trees,Y12-1041
Y14-1053.pdf-Figure1.png,Figure 1: Decision tree for discourse relations,"[u'Second, we proposed a decision procedure in Figure 1 for classifying a discourse relation. We consider this as a substantial advance since the previous']",Y14-1053deepfigures-results.json,trees,Y14-1053
Y15-1013.pdf-Figure5.png,Figure 5: Improved parsing results with unseen (bold) words.,"[u', and we present a concrete example in Figure 5 . In this example, ""conceivable"" is unseen in the training data, thus cannot be recognized']",Y15-1013deepfigures-results.json,trees,Y15-1013
Y15-1013.pdf-Figure6.png,Figure 6: Improved parsing results on parallel structure of adjectives.,[u'. a concrete example of improvement is presented in figure 6 .'],Y15-1013deepfigures-results.json,trees,Y15-1013
Y15-1014.pdf-Figure1.png,Figure 1: An example dependency tree.,"[u'. figure 1 shows a dependency tree, in which all the links connect headmodifier pairs.']",Y15-1014deepfigures-results.json,trees,Y15-1014
Y15-1014.pdf-Figure2.png,Figure 2: The decompositions of factors.,"[u'define the order of the graph model, some of the decomposition methods are shown in Figure 2 . As the simplest case, the first-order model just considers sub-tree factor of single edge', u'. for the sake of simplicity and the convenient use of neural network, we only consider four models discussed above (the sub-tree patterns of their factors are also shown in figure 2 ).']",Y15-1014deepfigures-results.json,trees,Y15-1014
Y15-1037.pdf-Figure6.png,Figure 6: Plots of Kruskal-Wallis test for criterion C1.,[],Y15-1037deepfigures-results.json,trees,Y15-1037
Y15-2010.pdf-Figure1.png,"Figure 1: For tree-to-string system, it searches the case watashi katta (I buy) akai honwo (red book) because there are dependency pairs in source parse tree and does not search the case watashi akai (I red) because it is not a pair in source parse tree","[u'not only fixes the LQ side parsing error, but also unifies the syntax annotation criterion. Figure 1 : For tree-to-string system, it searches the case watashi katta (I buy) akai honwo (red']",Y15-2010deepfigures-results.json,trees,Y15-2010
Y15-2010.pdf-Figure2.png,"Figure 2: For tree-to-tree system, it searches the case watashi katta (I buy) because it is a dependency pair in source parse tree and target parse tree. It does not search the case watashi akai (I red) because it is not a dependency pair in source parse tree neither in target parse tree. Unlike Fig.1, it does not search akai honwo (red book) because although it is a dependency pair in source side, it is disconnected in target side","[u'case watashi akai (I red) because it is not a pair in source parse tree Figure 2 : For tree-to-tree system, it searches the case watashi katta (I buy) because it is']",Y15-2010deepfigures-results.json,trees,Y15-2010
Y15-2010.pdf-Figure3.png,Figure 3: An example of different syntax annotation,[],Y15-2010deepfigures-results.json,trees,Y15-2010
Y15-2010.pdf-Figure5.png,Figure 5: This is an example of a different expression where again the solid line is the correct parse and the dashed line is the wrong parse caused by DM.,[],Y15-2010deepfigures-results.json,trees,Y15-2010
Y15-2010.pdf-Figure6.png,"Figure 6: An example of different expression cant be detected by projectivity, solid line is the correct parse and the dashed line is the wrong parse caused by DM.",[],Y15-2010deepfigures-results.json,trees,Y15-2010
Y15-2011.pdf-Figure2.png,Figure 2. Realignment from finer-grained to coarser-grained,[],Y15-2011deepfigures-results.json,trees,Y15-2011
Y15-2038.pdf-Figure1.png,Figure 1: RNNM structure with syntax tree,"[u'. rnnm makes use of syntactic trees of sentences, as shown in figure 1 the distributed representations for the child nodes in the syntactic trees.']",Y15-2038deepfigures-results.json,trees,Y15-2038
Y15-2039.pdf-Figure2.png,Figure 2 (a): An example sentence,"[u""sentence structure) words, they tend to receive more votes from other words in its sub-sentence. Figure 2 (a) shows the process, the word 'Chengwei'(Becoming) and 'Touzi'(Investment) which have more incoming dependencies, are selected"", u'which have more incoming dependencies, are selected as main structure words from an example sentence (Figure 2  (a) ). From the dependency perspective, the main structure words are those words with long distance', u'shows the result of the selecting method on the example sentence (Figure 2 (a) ).']",Y15-2039deepfigures-results.json,trees,Y15-2039
Y16-2010.pdf-Figure1.png,Figure 1: Example of preordering.,"[u'S , where the word order is similar to that of the target sentence T (Figure 1) .', u'the process of building the preordering model with the tree structures obtained as explained in Figure 1 from the sentence pairs of the training data of a machine translation system. We now']",Y16-2010deepfigures-results.json,trees,Y16-2010
Y16-2010.pdf-Figure4.png,Figure 4: Hierarchical sub-sentential alignment and generation of tree structures. (a) a best segmentation according to the second diagonal in the soft alignment matrix using the HSSA method coresponds to an Inverted rule in the BTG formalism; (b) a best segmentation according to the main diagonal corresponds to a Straight rule. (b) is a sub-part in (a) to illustrate recursivity.,"[u'. figure 4 shows that segmenting along the second diagonal with the hssa method corresponds to an inverted rule in the btg formalism and that segmenting according to the first diagonal corresponds to straight.', u'.t p of the matrix in figure 4 are related to part of the source sentence and part of the target sentence respectively.', u'. in figure 4 , the saturation of the cells represents the score w(s, t): the darker the color, the higher the score.', u'(S p , T p ), this is similar to the BTG rule Straight (see Figure 4 (b)); or they follow the second diagonal, (S p , T p ) and (S', u'(S p , T p ), this is similar to the BTG rule Inverted (see Figure 4 (a)). In order to decide for the segmentation point and for the direction in']",Y16-2010deepfigures-results.json,trees,Y16-2010
Y16-2012.pdf-Figure1.png,Figure 1: Alignments representations using ITG and bipartite graph. None of the structure contains cycles. The Japanese phrase     means born in bicchu province in English.,"[u'. in fact, an itg-style tree is a bitree consists of one tree in the source side and another tree in the target side (see figure 1 .', u') with the representation of bipartite graph (see Figure 1 .b) .']",Y16-2012deepfigures-results.json,trees,Y16-2012
Y16-2018.pdf-Figure1.png,Figure 1: English and Czech sentence with equivalent meaning and shared dependencies.,"[u'To illustrate this, consider the bilingual example in Figure 1 . The mapping between the words in the two sentences is not 1-1. However, the']",Y16-2018deepfigures-results.json,trees,Y16-2018
Y16-2028.pdf-Figure1.png,Figure 1: Gapped clause Sue Bill,"[u'The structure in Figure 1 provides an analysis of an instance of gapped phrase, Sue Bill, that is introduced to']",Y16-2028deepfigures-results.json,trees,Y16-2028
Y16-3028.pdf-Figure3.png,Figure 3: Call center call analysis framework,"[u'and voice to text analytics) (iii) decision block for deciding similar segments. As shown in Figure 3 , when an audio call is fed to the framework for the analysis, it is']",Y16-3028deepfigures-results.json,trees,Y16-3028
Y17-1001.pdf-Figure1.png,Figure 1: Semantic content of yes and no,"[u'. as illustrated in figure 1 , the particle yes response to the question affirms not the negative proposition but just the truth value of the nucleus meaning [gentle(i)].']",Y17-1001deepfigures-results.json,trees,Y17-1001
Y17-1001.pdf-Figure2.png,Figure 2: Semantic content of ung and ani,"[u"". the answering particle ung 'yes' and ani 'no' will thus have the information as given in figure 2 ."", u""As illustrated in Figure 2 , the answering particle ung 'yes' to the negative question asserts not the value of""]",Y17-1001deepfigures-results.json,trees,Y17-1001
Y17-1011.pdf-Figure2.png,Figure 2: CART tree for the identification of authors,"[u'Classification And Regression Trees (CART) was selected to establish a classification tree, as shown in Figure 2 , using combination of word-final tone motifs and segment-final tone motifs and word length motifs', u'In Figure 2 , the leaf nodes specify a partition of the data, i.e. a division of the']",Y17-1011deepfigures-results.json,trees,Y17-1011
Y17-1012.pdf-Figure1.png,Figure 1. Example of a parsed Indonesian sentence (TL: That allegation does not miss) with dependency structure,[u'. figure 1 shows an example of a parsed indonesian sentence using dependency structure.'],Y17-1012deepfigures-results.json,trees,Y17-1012
Y17-1012.pdf-Figure3.png,Figure 3. Correct dependency tree for sentence Dia tidak malu bertanya di depan umum (He is not ashamed of asking questions in public),[u'. figure 3 and 4 shows the example of this occurrence.'],Y17-1012deepfigures-results.json,trees,Y17-1012
Y17-1012.pdf-Figure4.png,"Figure 4. Parsing result for sentence Dia tidak malu bertanya di depan umum (He is not ashamed of asking questions in public) using 2-Planar, Eisner, and Chu-Liu Edmonds parsing algorithm respectively",[],Y17-1012deepfigures-results.json,trees,Y17-1012
Y17-1015.pdf-Figure1.png,Figure 1: Monolingual and Bilingual,[u'. figure 1 illustrates the similarities and differences between units in the monolingual and bilingu there are varieties of interpretations to mdl-model using different technologies.'],Y17-1015deepfigures-results.json,trees,Y17-1015
Y17-1015.pdf-Figure2.png,Figure 2: A efficient searching path by DL,[u'. 91 figure 2 : a efficient searching path by dl 1.'],Y17-1015deepfigures-results.json,trees,Y17-1015
Y17-1032.pdf-Figure1.png,Figure 1: Annotation of discourse relations in PDiT 2.0. The relations are represented by two orange arrows connecting roots of the arguments. Information about the discourse types and connectives is given at the starting node of the relations.,"[u'. annotation of discourse relations was carried out on top of deep-syntactic trees (on the so called tectogrammatical layer, see example 1 and figure 1 ) and covers relations expressed by a surface-present connective.', u'. it contains two intra-233 sentential discourse relations -a disjunctive alternative expressed by the connective nebo [or], and a gradation expressed by the connective dokonce [even]; the tectogrammatical tree of the relevant part of the sentence, along with the discourse annotation, is depicted in figure 1 .']",Y17-1032deepfigures-results.json,trees,Y17-1032
Y17-1048.pdf-Figure2.png,Figure 2: Two types of attention.,"[u'. in this method, we calculate the attention weight using the final hidden state of birnn ( figure 2 -(a)).', u'. in this method, we calculate the attention weight using the mean vector of the hidden state of birnn for every character in the tweet (figure 2-(b) ).']",Y17-1048deepfigures-results.json,trees,Y17-1048
Y18-1004.pdf-Figure2.png,Figure 2: Visualization of word representations,[u'in the output have the largest number of annotators who listed these words as substitutes. Figure 2 visualizes the actual word representations in the output of'],Y18-1004deepfigures-results.json,trees,Y18-1004
Y18-1005.pdf-Figure1.png,Figure 1: AE+NN+Weighted-Learning,[],Y18-1005deepfigures-results.json,trees,Y18-1005
Y18-1054.pdf-Figure2.png,"Figure 2: Examples of our attentional Tree-LSTM sentiment classification on the test set. The red square indicates a word or phrase to which great attention was paid in the softmax step, and the associated value indicates the attention weight. Root nodes are indicated by the (left) gold and (right) predicted labels (N indicates negative, whereas P indicates positive). We also show the labels for nodes that match an entry in the polar dictionary.","[u'. figure 2a shows that the model classifies ""  (consistency)"" as positive and pays 1/3 attention to it in the final classification step; however, the model correctly classifies the sentence polarity as negative by considering "" (cannot be found)"" through most of the attention.', u'. in figure 2b , the model correctly classifies both ""  (friendship)"" and "" (apprehension),"" and then classifies the sentence polarity by paying great attention to "" (apprehension).', u'. in figure 2c , the model pays attention to both "" (confrontation)"" and "" (mitigated)""; however, it fails to predict the correct polarity of the sentence.', u'. in figure 2d , the model fails to capture negation as it should pay attention to "" (was able to avoid).', u'. figure 2 : examples of our attentional tree-lstm sentiment classification on the test set.']",Y18-1054deepfigures-results.json,trees,Y18-1054
Y18-1063.pdf-Figure10.png,Figure 10: Rule #7 diagram and example,[],Y18-1063deepfigures-results.json,trees,Y18-1063
Y18-1063.pdf-Figure11.png,Figure 11: Rule #8 diagram and example,"[u'In this case, the initial consonant and vowel are Figure 11 describes a transformation applying rule #8 between two syllables and an example.']",Y18-1063deepfigures-results.json,trees,Y18-1063
Y18-1063.pdf-Figure13.png,Figure 13: Spoonerism among three and four syllables,[u'. figure 13 presents spoonerism among three and four syllables.'],Y18-1063deepfigures-results.json,trees,Y18-1063
Y18-1063.pdf-Figure4.png,"Figure 4: Rule #1 diagram and example. C, V, T, and S are Consonant, Vowel, Tone, and Syllable respectively.",[u'. figure 4 describes a transformation applying rule #1 between two syllables and an example.'],Y18-1063deepfigures-results.json,trees,Y18-1063
Y18-1063.pdf-Figure5.png,Figure 5: Rule #2 diagram and example,[u'. figure 5 described a transformation applying rule #2 between two syllables and an example.'],Y18-1063deepfigures-results.json,trees,Y18-1063
Y18-1063.pdf-Figure6.png,Figure 6: Rule #3 diagram and example,[u'. figure 6 describes a transformation applying rule #3 between two syllables and an example.'],Y18-1063deepfigures-results.json,trees,Y18-1063
Y18-1063.pdf-Figure7.png,Figure 7: Rule #4 diagram and example,[],Y18-1063deepfigures-results.json,trees,Y18-1063
Y18-1063.pdf-Figure8.png,Figure 8: Rule #5 diagram and example,[],Y18-1063deepfigures-results.json,trees,Y18-1063
Y18-1063.pdf-Figure9.png,Figure 9: Rule #6 diagram and example,[u'. figure 9 describes a transformation applying rule #6 between two syllables and an example.'],Y18-1063deepfigures-results.json,trees,Y18-1063
Y95-1034.pdf-Figure1.png,Figure 1.,[u'. figure 1 gives an indication of a full stcg rule.'],Y95-1034deepfigures-results.json,trees,Y95-1034
